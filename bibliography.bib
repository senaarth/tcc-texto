@article{robinson2007ethnographically,
  author    = {Robinson, Hugh and Segal, Judith and Sharp, Helen},
  title     = {Ethnographically-informed Empirical Studies of Software Practice},
  journal   = {Information and Software Technology},
  volume    = {49},
  number    = {6},
  pages     = {540--551},
  year      = {2007},
  doi       = {10.1016/j.infsof.2007.02.011}
}

@article{lau1999towards,
  author    = {Lau, Francis},
  title     = {Towards a Framework for Action Research in Information Systems Studies},
  journal   = {Information Technology \& People},
  volume    = {12},
  number    = {2},
  pages     = {148--175},
  year      = {1999},
  doi       = {10.1108/09593849910267206}
}


@book{babbie1990survey,
  author    = {Babbie, Earl R.},
  title     = {Survey Research Methods},
  publisher = {Wadsworth},
  year      = {1990},
}

@article{davison2004principles,
  author    = {Davison, Robert M. and Martinsons, Maris G. and Kock, Ned},
  title     = {Principles of Canonical Action Research},
  journal   = {Information Systems Journal},
  volume    = {14},
  number    = {1},
  pages     = {65--86},
  year      = {2004},
  doi       = {10.1111/j.1365-2575.2004.00162.x}
}

@Inbook{Easterbrook2008,
author="Easterbrook, Steve
and Singer, Janice
and Storey, Margaret-Anne
and Damian, Daniela",
editor="Shull, Forrest
and Singer, Janice
and Sj{\o}berg, Dag I. K.",
title="Selecting Empirical Methods for Software Engineering Research",
bookTitle="Guide to Advanced Empirical Software Engineering",
year="2008",
publisher="Springer London",
address="London",
pages="285--311",
abstract="Selecting a research method for empirical software engineering research is problematic because the benefits and challenges to using each method are not yet well catalogued. Therefore, this chapter describes a number of empirical methods available. It examines the goals of each and analyzes the types of questions each best addresses. Theoretical stances behind the methods, practical considerations in the application of the methods and data collection are also briefly reviewed. Taken together, this information provides a suitable basis for both understanding and selecting from the variety of methods applicable to empirical software engineering.",
isbn="978-1-84800-044-5",
doi="10.1007/978-1-84800-044-5_11",
url="https://doi.org/10.1007/978-1-84800-044-5_11"
}



@article{karlstrom2002aggregating,
  author    = {Karlström, Daniel and Runeson, Per and Wohlin, Claes},
  title     = {Aggregating Viewpoints for Strategic Software Process Improvement},
  journal   = {IEE Proceedings - Software},
  year      = {2002},
  doi       = {10.1049/ip-sen:20020696}
}

@article{pfleeger1994experimental,
  author    = {Pfleeger, Shari Lawrence},
  title     = {Experimental Design and Analysis in Software Engineering Part 1--5},
  journal   = {ACM SIGSOFT Software Engineering Notes},
  year      = {1994},
}


@inproceedings{runeson2009,
  author    = {Runeson and Martin Skoglund},
  title     = {Reference-based search strategies in systematic reviews},
  booktitle = {Proceedings of the 13th International Conference on Empirical Assessment and Evaluation in Software Engineering},
  series    = {Electronic Workshops in Computing (eWIC)},
  publisher = {BCS},
  year      = {2009},
  location  = {Durham University, UK}
}

@misc{jupyter_notebook,
  author    = {Project Jupyter},
  title     = {Jupyter Notebook: Open source platform for data science and interactive computing},
  year      = {2014},
  url       = {https://jupyter.org/},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://jupyter.org/}},
}

@misc{jupyterhub,
  author    = {Project Jupyter},
  title     = {JupyterHub: A multi-user version of the notebook designed for teams and classrooms},
  year      = {2015},
  url       = {https://jupyter.org/hub},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://jupyter.org/hub}},
}

@misc{notion,
  author    = {Notion},
  title     = {Notion: All-in-one workspace for your notes, tasks, wikis, and databases},
  year      = {2018},
  url       = {https://www.notion.so/},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://www.notion.so/}},
}

@misc{git,
  author    = {Git},
  title     = {Git},
  year      = {2005},
  url       = {https://git-scm.com/},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://git-scm.com/}},
}

@misc{github,
  author    = {GitHub},
  title     = {GitHub},
  year      = {2008},
  url       = {https://github.com/about},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://github.com/about}},
}

@misc{forms,
  author    = {Google},
  title     = {Gere insights facilmente com o Google Forms},
  year      = {2023},
  url       = {https://www.google.com/intl/pt-BR/forms/about/},
  note      = {Acessado em: 29/08/2024},
  howpublished = {\url{https://www.google.com/intl/pt-BR/forms/about/}},
}

@inproceedings{olsson2013towards,
  author    = {Helena Holmström Olsson and Jan Bosch and H. Alahyari},
  title     = {Towards {R\&D} as Innovation Experiment Systems: A Framework for Moving Beyond Agile Software Development},
  booktitle = {IASTED Multiconferences - Proceedings of the IASTED International Conference on Software Engineering, SE 2013},
  pages     = {798--805},
  year      = {2013}
}


@article{castellion2008do,
  author    = {George Castellion},
  title     = {Do It Wrong Quickly: How the Web Changes the Old Marketing Rules by Mike Moran},
  journal   = {Journal of Product Innovation Management},
  volume    = {25},
  number    = {6},
  pages     = {633--635},
  year      = {2008},
  publisher = {Wiley-Blackwell}
}

@article{cao2008agile,
  author    = {Liang{-}Jie Cao and Balasubramaniam Ramesh},
  title     = {Agile requirements engineering practices: An empirical study},
  journal   = {IEEE Software},
  volume    = {25},
  number    = {1},
  pages     = {60--67},
  year      = {2008},
  publisher = {IEEE}
}

@inproceedings{olsson2013data,
  author = {Olsson, H. H. and Bosch, J.},
  title = {Towards Data-Driven Product Development: A Multiple Case Study on Post-Deployment Data Usage in Software-Intensive Embedded Systems},
  booktitle = {Proceedings of the Lean Enterprise Software and Systems Conference (LESS)},
  year = {2013},
  month = {December},
  pages = {},
  address = {Galway, Ireland},
  publisher = {},
}

@inproceedings{bakshy2014www,
	Author = {Bakshy, E. and Eckles, D. and Bernstein, M.S.},
	Booktitle = {Proceedings of the 23rd ACM conference on the World Wide Web},
	Organization = {ACM},
	Title = {Designing and Deploying Online Field Experiments},
	Year = {2014}
}

@misc{analytics_experimentation_framework,
    title = {Analytics Experimentation Framework},
    year = {2012},
    author = {Google},
    url = {https://developers.google.com/analytics/legacy/universal-analytics?hl=en},
    note = {Acessado em 27/08/2024}
}

@misc{maxymiser,
    title = {Maxymiser},
    year = {2015},
    author = {Oracle},
    url = {https://www.oracle.com/ae/a/ocom/docs/applications/cx/maxymiser-product-overview.pdf},
    note = {Acessado em 27/08/2024}
}

@misc{planout,
    title = {PlanOut},
    year = {2014},
    author = {Facebook},
    url ={https://github.com/facebookarchive/planout},
    note = {Acessado em 27/08/2024}
}

@misc{xlnt,
    title = {XLNT},
    year = {2013},
    author = {LinkedIn},
    url = {https://engineering.linkedin.com/ab-testing/xlnt-platform-driving-ab-testing-linkedin},
    note = {Acessado em 27/08/2024}
}

@misc{optimizely,
    title = {Optimizely},
    year = {2010},
    author = {Optimizely},
    url = {https://www.optimizely.com/},
    note = {Acessado em 27/08/2024},
}

@misc{wasabi,
    title = {Wasabi},
    year = {2014},
    author = {Intuit},
    url = {https://github.com/intuit/wasabi},
    note = {Acessado em 27/08/2024},
}

@misc{python,
    title = {Python},
    year = {1989},
    author = {Python},
    url = {https://www.python.org/about/},
    note = {Acessado em 15/09/2024},
}


@misc{iso9000,
  title        = {Quality management systems — Fundamentals and vocabulary},
  year         = {2015},
  author       = {{ISO 9000}},
  url          = {https://www.iso.org/standard/45481.html},
  note         = {Acessado em 22/08/2024}
}

@misc{nbr9241,
  title        = {Requisitos Ergonômicos para Trabalho de Escritórios com Computadores},
  year         = {2002},
  author       = {{NBR 9241-11}},
  url          = {https://www.inf.ufsc.br/~edla.ramos/ine5624/_Walter/Normas/Parte%2011/iso9241-11F2.pdf},
  note         = {Acessado em 15/09/2024}
}

@misc{iso9241,
  title        = {Ergonomic requirements for office work with visual display terminals },
  year         = {1998},
  author       = {{ISO 9241-11}},
  url          = {https://scholar.google.com/scholar?&q=ISO%209241-11%3A%20Ergonomic%20requirements%20for%20office%20work%20with%20visual%20display%20terminals%20%28VDTs%29%20-%20Part%2011%20Guidance%20on%20usability%20%281998%29},
  note         = {Acessado em 15/09/2024}
}

@misc{iso9126,
  title        = {Systems and software engineering — Measurement process},
  year         = {2017},
  author       = {{ISO/IEC 9126}},
  url          = {https://www.iso.org/standard/71197.html},
  note         = {Acessado em 14/09/2024}
}
@book{feller2005perspectives,
  title={Perspectives on Free and Open Source Software},
  author={Feller, Joseph and Fitzgerald, Brian and Hissam, Scott and Lakhani, Karim},
  year={2005},
  publisher={MIT Press}
}
@techreport{mccall1977factors,
  author={McCall, J. and Richards, P. A. and Walters, G. F.},
  title={Factors in software quality: concept and definitions of software quality},
  year={1977},
  institution={S.l.: s.n.}
}

@book{boehm1978characteristics,
  author={Boehm, B. W.},
  title={Characteristics of Software Quality},
  year={1978},
  publisher={North-Holland},
  series={TRW Series of Software Technology},
  note={Disponível em: \url{https://books.google.com.br/books?id=jLFXswEACAAJ}}
}

@book{poppendieck2003lean,
  title={Lean software development: an agile toolkit},
  author={Poppendieck, Mary and Poppendieck, Tom},
  year={2003},
  publisher={Addison-Wesley Professional}
}
@article{fitzgerald2015continuous,
author = {Fitzgerald, Brian and Stol, Klaas-Jan},
year = {2015},
month = {07},
pages = {},
title = {Continuous Software Engineering: A Roadmap and Agenda},
volume = {25},
journal = {Journal of Systems and Software},
doi = {10.1016/j.jss.2015.06.063}
}

@article{lindgren2016raising,
  title={Raising the odds of success: the current state of experimentation in product development},
  author={Lindgren, E. and M{\"u}nch, J.},
  journal={Information and Software Technology},
  volume={77},
  pages={80--91},
  year={2016},
  publisher={Elsevier}
}

@misc{iso25000,
  title        = {Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — Guide to SQuaRE},
  author       = {{ISO/IEC 25000}},
  year         = {2014},
  howpublished = {https://www.iso.org/standard/64764.html},
  note         = {Acessado em 22/08/2024}
}


@article{martinez_quality_2019,
author = {Martínez-Fernández, Silverio and Vollmer, Anna Maria and Jedlitschka, Andreas and Franch, Xavier and Lopez, Lidia and Ram, Prabhat and Rodríguez, Pilar and Aaramaa, Sanja and Bagnato, Alessandra and Choraś, Michał and Partanen, Jari},
year = {2019},
month = {05},
pages = {},
title = {Continuously Assessing and Improving Software Quality With Software Analytics Tools: A Case Study},
volume = {PP},
journal = {IEEE Access},
doi = {10.1109/ACCESS.2019.2917403}
}

@misc{iso25010,
  title     = {Systems and software engineering — Systems and software Quality Requirements and Evaluation (SQuaRE) — System and software quality models},
  author = {{ISO/IEC 25010}},
  year      = {2011},
  url       = {https://www.iso.org/standard/78176.html},
  note      = {Acessado em 22/08/2024},
}


@book{ries2011lean,
  title={The Lean Startup: How Today's Entrepreneurs Use Continuous Innovation to Create Radically Successful Businesses},
  author={Ries, Eric},
  year={2011},
  publisher={Crown Business},
  address={New York, US}
}

@book{berczuk_appleton_2002,
  author    = {Steve P. Berczuk and Brad Appleton},
  title     = {Software Configuration Management Patterns: Effective Teamwork, Practical Integration},
  year      = {2002},
  publisher = {Addison-Wesley Longman Publishing Co., Inc.},
}

@book{humble_farley_2010,
  author    = {Jez Humble and David Farley},
  title     = {Continuous Delivery: Reliable Software Releases Through Build, Test, and Deployment Automation},
  edition   = {1st},
  year      = {2010},
  publisher = {Addison-Wesley Professional},
}

@INPROCEEDINGS{rahman_feature_toggle_2016,
  author={Rahman, Md Tajmilur and Querel, Louis-Philippe and Rigby, Peter C. and Adams, Bram},
  booktitle={2016 IEEE/ACM 13th Working Conference on Mining Software Repositories (MSR)}, 
  title={Feature Toggles: Practitioner Practices and a Case Study}, 
  year={2016},
  volume={},
  number={},
  pages={201-211},
  keywords={Browsers;Maintenance engineering;Google;Software;Companies;Facebook;History;feature toggle;feature switch;feature flag;feature management;software engineering},
  doi={}}


@article{hodgson2017featuretoggles,
  title = {Feature Toggles (aka Feature Flags)},
  author = {Hodgson, Pete},
  journal = {martinfowler.com},
  year = {2017},
  month = {October},
  url = {https://martinfowler.com/articles/feature-toggles.html#top},
  note = {Acessado em 22/08/2024}
}

@inproceedings{kohavi_oce_and_ab_tests_2017,
    author    = {Kohavi, Ron and Longbotham, Roger},
    title     = {Online Controlled Experiments and A/B Testing},
    booktitle = {Encyclopedia of Machine Learning and Data Mining},
    publisher = {Springer},
    year      = {2017},
    pages     = {922-929},
    isbn      = {978-1-4899-7685-7},
    doi       = {10.1007/978-1-4899-7687-1_891},
    month     = {April},
}


@book{robson2002real,
  title={Real World Research: A Resource for Social Scientists and Practitioner-Researchers},
  author={Robson, Colin},
  edition={2nd},
  year={2002},
  publisher={Blackwell},
  address={Oxford/Madden}
}


@article{lethbridge2005studying,
  author    = {Timothy C. Lethbridge and Sebastian E. Sim and Janice Singer},
  title     = {Studying software engineers: data collection techniques for software field studies},
  journal   = {Empirical Software Engineering},
  volume    = {10},
  number    = {3},
  pages     = {311--341},
  year      = {2005},
  publisher = {Springer},
  doi       = {10.1007/s10664-005-1290-x}
}


@article{yin_case_study_2009,
  author    = {Yin, Robert K.},
  title     = {Case study research: Design and methods (4th Ed.)},
  journal   = {The Canadian Journal of Action Research (CJAR)},
  volume    = {14},
  number    = {1},
  pages     = {69--71},
  year      = {2009},
  month     = {May},
  doi       = {10.33524/cjar.v14i1.73}
}


@book{runeson_case_study_2012,
    author = {Runeson, Per and Höst, Martin and Rainer, Austen and Regnell, Björn},
    year = {2012},
    month = {02},
    pages = {},
    title = {Case Study Research in Software Engineering -- Guidelines and Examples},
    isbn = {9781118104354},
    journal = {Case Study Research in Software Engineering: Guidelines and Examples},
    doi = {10.1002/9781118181034},
    publisher = {Wiley, Hoboken}
}

@inproceedings{kitchenham_rsl,
    author = {Kitchenham, Barbara and Charters, Stuart},
    year = {2007},
    month = {01},
    pages = {},
    title = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
    booktitle = {Guidelines for performing Systematic Literature Reviews in Software Engineering},
    volume = {2}
}

@inproceedings{basili_experimental_1993,
    author="Basili, Victor R.",
    editor="Rombach, H. Dieter
    and Basili, Victor R.
    and Selby, Richard W.",
    title="The experimental paradigm in software engineering",
    booktitle="Experimental Software Engineering Issues: Critical Assessment and Future Directions",
    year="1993",
    publisher="Springer Berlin Heidelberg",
    address="Berlin, Heidelberg",
    pages="1--12",
    isbn="978-3-540-47903-1"
}


@inproceedings{kevic_characterizing_2017,
	address = {Buenos Aires, Argentina},
	title = {Characterizing {Experimentation} in {Continuous} {Deployment}: {A} {Case} {Study} on {Bing}},
	isbn = {978-1-5386-2717-4},
	shorttitle = {Characterizing {Experimentation} in {Continuous} {Deployment}},
	url = {http://ieeexplore.ieee.org/document/7965436/},
	doi = {10.1109/ICSE-SEIP.2017.19},
	abstract = {The practice of continuous deployment enables product teams to release content to end users within hours or days, rather than months or years. These faster deployment cycles, along with rich product instrumentation, allows product teams to capture and analyze feature usage measurements. Product teams deﬁne a hypothesis and a set of metrics to assess how a code or feature change will impact the user. Supported by a framework, a team can deploy that change to subsets of users, enabling randomized controlled experiments. Based on the impact of the change, the product team may decide to modify the change, to deploy the change to all users, or to abandon the change. This experimentation process enables product teams to only deploy the changes that positively impact the user experience.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} {Track} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Kevic, Katja and Murphy, Brendan and Williams, Laurie and Beckmann, Jennifer},
	month = may,
	year = {2017},
	pages = {123--132},
	file = {Kevic et al. - 2017 - Characterizing Experimentation in Continuous Deplo.pdf:/Users/arthursena/Zotero/storage/9CW5RU2P/Kevic et al. - 2017 - Characterizing Experimentation in Continuous Deplo.pdf:application/pdf},
}

@article{kohavi_controlled_2009,
	title = {Controlled experiments on the web: survey and practical guide},
	volume = {18},
	issn = {1384-5810, 1573-756X},
	shorttitle = {Controlled experiments on the web},
	url = {https://link.springer.com/10.1007/s10618-008-0114-1},
	doi = {10.1007/s10618-008-0114-1},
	language = {en},
	number = {1},
	urldate = {2024-03-30},
	journal = {Data Mining and Knowledge Discovery},
	author = {Kohavi, Ron and Longbotham, Roger and Sommerfield, Dan and Henne, Randal M.},
	month = feb,
	year = {2009},
	pages = {140--181},
}

@inproceedings{fabijan_evolution_2017,
	address = {Buenos Aires},
	title = {The {Evolution} of {Continuous} {Experimentation} in {Software} {Product} {Development}: {From} {Data} to a {Data}-{Driven} {Organization} at {Scale}},
	isbn = {978-1-5386-3868-2},
	shorttitle = {The {Evolution} of {Continuous} {Experimentation} in {Software} {Product} {Development}},
	url = {http://ieeexplore.ieee.org/document/7985712/},
	doi = {10.1109/ICSE.2017.76},
	abstract = {Software development companies are increasingly aiming to become data-driven by trying to continuously experiment with the products used by their customers. Although familiar with the competitive edge that the A/B testing technology delivers, they seldom succeed in evolving and adopting the methodology. In this paper, and based on an exhaustive and collaborative case study research in a large software-intense company with highly developed experimentation culture, we present the evolution process of moving from ad-hoc customer data analysis towards continuous controlled experimentation at scale. Our main contribution is the “Experimentation Evolution Model” in which we detail three phases of evolution: technical, organizational and business evolution. With our contribution, we aim to provide guidance to practitioners on how to develop and scale continuous experimentation in software organizations with the purpose of becoming data-driven at scale.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {2017 {IEEE}/{ACM} 39th {International} {Conference} on {Software} {Engineering} ({ICSE})},
	publisher = {IEEE},
	author = {Fabijan, Aleksander and Dmitriev, Pavel and Olsson, Helena Holmstrom and Bosch, Jan},
	month = may,
	year = {2017},
	pages = {770--780},
	file = {Fabijan et al. - 2017 - The Evolution of Continuous Experimentation in Sof.pdf:/Users/arthursena/Zotero/storage/9MEP6IAZ/Fabijan et al. - 2017 - The Evolution of Continuous Experimentation in Sof.pdf:application/pdf},
}

@misc{bakshy_designing_2014,
	title = {Designing and {Deploying} {Online} {Field} {Experiments}},
	url = {http://arxiv.org/abs/1409.3174},
	abstract = {Online experiments are widely used to compare speciﬁc design alternatives, but they can also be used to produce generalizable knowledge and inform strategic decision making. Doing so often requires sophisticated experimental designs, iterative reﬁnement, and careful logging and analysis. Few tools exist that support these needs. We thus introduce a language for online ﬁeld experiments called PlanOut. PlanOut separates experimental design from application code, allowing the experimenter to concisely describe experimental designs, whether common “A/B tests” and factorial designs, or more complex designs involving conditional logic or multiple experimental units. These latter designs are often useful for understanding causal mechanisms involved in user behaviors. We demonstrate how experiments from the literature can be implemented in PlanOut, and describe two large ﬁeld experiments conducted on Facebook with PlanOut. For common scenarios in which experiments are run iteratively and in parallel, we introduce a namespaced management system that encourages sound experimental practice.},
	language = {en},
	urldate = {2024-03-30},
	publisher = {arXiv},
	author = {Bakshy, Eytan and Eckles, Dean and Bernstein, Michael S.},
	month = sep,
	year = {2014},
	note = {arXiv:1409.3174 [cs, stat]},
	keywords = {Computer Science - Human-Computer Interaction, Computer Science - Programming Languages, Computer Science - Social and Information Networks, H.5.3, Statistics - Applications},
	file = {Bakshy et al. - 2014 - Designing and Deploying Online Field Experiments.pdf:/Users/arthursena/Zotero/storage/5M3HBTQL/Bakshy et al. - 2014 - Designing and Deploying Online Field Experiments.pdf:application/pdf},
}

@incollection{van_der_aalst_building_2012,
	address = {Berlin, Heidelberg},
	title = {Building {Products} as {Innovation} {Experiment} {Systems}},
	volume = {114},
	isbn = {978-3-642-30745-4 978-3-642-30746-1},
	url = {http://link.springer.com/10.1007/978-3-642-30746-1_3},
	abstract = {Traditional software development focuses on specifying and freezing requirements early in the, typically yearly, product development lifecycle. The requirements are defined based on product management’s best understanding. The adoption of SaaS and cloud computing has shown a different approach to managing requirements, adding frequent and rigorous experimentation to the development process with the intent of minimizing R\&D investment between customer proof points. This offers several benefits including increased customer satisfaction, improved and quantified business goals and the transformation to a continuous rather than waterfall development process. In this paper, we present our learnings from studying software companies applying an innovation experiment system approach to product development. The approach is illustrated with three cases from Intuit, the case study company.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {Software {Business}},
	publisher = {Springer Berlin Heidelberg},
	author = {Bosch, Jan},
	editor = {Van Der Aalst, Wil and Mylopoulos, John and Rosemann, Michael and Shaw, Michael J. and Szyperski, Clemens and Cusumano, Michael A. and Iyer, Bala and Venkatraman, N.},
	year = {2012},
	doi = {10.1007/978-3-642-30746-1_3},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {27--39},
	file = {Bosch - 2012 - Building Products as Innovation Experiment Systems.pdf:/Users/arthursena/Zotero/storage/Q3G264E5/Bosch - 2012 - Building Products as Innovation Experiment Systems.pdf:application/pdf},
}

@article{tang_overlapping_2010,
	title = {Overlapping experiment infrastructure: more, better, faster experimentation},
	abstract = {At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modiﬁcations to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google’s overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufﬁcient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper speciﬁcally describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.},
	language = {en},
	author = {Tang, Diane and Agarwal, Ashish and O'Brien, Deirdre and Meyer, Mike},
	file = {Tang et al. - Overlapping experiment infrastructure more, bette.pdf:/Users/arthursena/Zotero/storage/GVVFYJEA/Tang et al. - Overlapping experiment infrastructure more, bette.pdf:application/pdf},
        year = {2010},
}

@misc{elsevier2022,
  author = {Elsevier},
  title = {Banco de dados de resumos e citações organizado por especialistas},
  year = {2024},
  note = {Disponível em: \url{https://www.periodicos.capes.gov.br/}}
}

@misc{mixpanel2024,
  author = {Mixpanel},
  title = {Plataforma de coleta de dados em tempo real da interação de usuários com produtos},
  year = {2024},
  note = {Disponível em: \url{https://mixpanel.com/}}
}


@article{kohavi_unexpected_2011,
	title = {Unexpected results in online controlled experiments},
	volume = {12},
	issn = {1931-0145, 1931-0153},
	url = {https://dl.acm.org/doi/10.1145/1964897.1964905},
	doi = {10.1145/1964897.1964905},
	abstract = {Controlled experiments, also called randomized experiments and A/B tests, have had a profound influence on multiple fields, including medicine, agriculture, manufacturing, and advertising. Offline controlled experiments have been well studied and documented since Sir Ronald A. Fisher led the development of statistical experimental design while working at the Rothamsted Agricultural Experimental Station in England in the 1920s. With the growth of the world-wide-web and web services, online controlled experiments are being used frequently, utilizing software capabilities like ramp-up (exposure control) and running experiments on large server farms with millions of users. We share several real examples of unexpected results and lessons learned.},
	language = {en},
	number = {2},
	urldate = {2024-03-30},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Kohavi, Ron and Longbotham, Roger},
	month = mar,
	year = {2011},
	pages = {31--35},
	file = {Kohavi and Longbotham - 2011 - Unexpected results in online controlled experiment.pdf:/Users/arthursena/Zotero/storage/ZW5RPDNH/Kohavi and Longbotham - 2011 - Unexpected results in online controlled experiment.pdf:application/pdf},
}

@inproceedings{kohavi_online_2013,
	address = {Chicago Illinois USA},
	title = {Online controlled experiments at large scale},
	isbn = {978-1-4503-2174-7},
	url = {https://dl.acm.org/doi/10.1145/2487575.2488217},
	doi = {10.1145/2487575.2488217},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {Proceedings of the 19th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Walker, Toby and Xu, Ya and Pohlmann, Nils},
	month = aug,
	year = {2013},
	pages = {1168--1176},
	file = {Kohavi et al. - 2013 - Online controlled experiments at large scale.pdf:/Users/arthursena/Zotero/storage/P6CNPBIU/Kohavi et al. - 2013 - Online controlled experiments at large scale.pdf:application/pdf},
}

@inproceedings{kohavi_seven_2014,
	address = {New York New York USA},
	title = {Seven rules of thumb for web site experimenters},
	isbn = {978-1-4503-2956-9},
	url = {https://dl.acm.org/doi/10.1145/2623330.2623341},
	doi = {10.1145/2623330.2623341},
	abstract = {Web site owners, from small web sites to the largest properties that include Amazon, Facebook, Google, LinkedIn, Microsoft, and Yahoo, attempt to improve their web sites, optimizing for criteria ranging from repeat usage, time on site, to revenue. Having been involved in running thousands of controlled experiments at Amazon, Booking.com, LinkedIn, and multiple Microsoft properties, we share seven rules of thumb for experimenters, which we have generalized from these experiments and their results. These are principles that we believe have broad applicability in web optimization and analytics outside of controlled experiments, yet they are not provably correct, and in some cases exceptions are known.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Kohavi, Ron and Deng, Alex and Longbotham, Roger and Xu, Ya},
	month = aug,
	year = {2014},
	pages = {1857--1866},
	file = {Kohavi et al. - 2014 - Seven rules of thumb for web site experimenters.pdf:/Users/arthursena/Zotero/storage/FDW7F6HT/Kohavi et al. - 2014 - Seven rules of thumb for web site experimenters.pdf:application/pdf},
}

@inproceedings{erthal_literature_2022,
	address = {Argentina},
	title = {A {Literature} {Study} to {Characterize} {Continuous} {Experimentation} in {Software} {Engineering}},
	url = {https://sol.sbc.org.br/index.php/cibse/article/view/20959},
	doi = {10.5753/cibse.2022.20959},
	abstract = {Continuous Experimentation (CE) has become increasingly popular across industry and academic communities. Given its rapid evolution in software engineering (SE), the lack of a common understanding of CE can jeopardize new implementations and justify research efforts. Therefore, this literature study characterizes CE in SE based on its definitions, processes, and strategies for experimentation available in the technical literature. Seventy-six sources of information provided many different definitions, processes, and experimental procedures used to describe CE in SE. Despite the increasing use of CE in SE, it is impossible to observe a common terminology yet to support its characterization and use.},
	language = {en},
	urldate = {2024-03-30},
	booktitle = {Anais do {XXV} {Congresso} {Ibero}-{Americano} em {Engenharia} de {Software} ({CIbSE} 2022)},
	publisher = {Sociedade Brasileira de Computação},
	author = {Erthal, Vladimir M. and De Souza, Bruno P. De and Santos, Paulo Sérgio M. Dos and Travassos, Guilherme H.},
	month = jun,
	year = {2022},
	pages = {1--15},
	file = {Erthal et al. - 2022 - A Literature Study to Characterize Continuous Expe.pdf:/Users/arthursena/Zotero/storage/INJ6RLYS/Erthal et al. - 2022 - A Literature Study to Characterize Continuous Expe.pdf:application/pdf},
}

@article{neri_adocao_2014,
	title = {Adoção de {Métodos} Ágeis e do {Pensamento} {Lean} na {Gestão} de {Contratos} de {Fornecedores} de {Desenvolvimento} de {Software} em {Organizações} {Públicas} {Brasileiras}: {Um} {Estudo} de {Caso}},
	language = {pt},
	author = {Neri, Hilmer Rodrigues},
	year = {2014},
	file = {Neri - 2014 - Adoção de Métodos Ágeis e do Pensamento Lean na Ge.pdf:/Users/arthursena/Zotero/storage/67YE93S6/Neri - 2014 - Adoção de Métodos Ágeis e do Pensamento Lean na Ge.pdf:application/pdf},
}

@article{neri_cenario_2014,
	title = {Cenário de {Decisões} {Baseado} em {Métricas} de {Software}: {Definição} e {Implementação} de {Cenários} a partir de {Métricas} de {Design} e de {Vulnerabilidade} para {Tomada} de {Decisão}},
	language = {pt},
	author = {Neri, Hilmer Rodrigues},
	year = {2014},
	file = {Neri - 2014 - Cenário de Decisões Baseado em Métricas de Softwar.pdf:/Users/arthursena/Zotero/storage/GP7L68FE/Neri - 2014 - Cenário de Decisões Baseado em Métricas de Softwar.pdf:application/pdf},
}

@article{silva_analisando_2023,
	title = {Analisando variações de desempenho em diferentes versões de um produto de um software como serviço-{SaaS}, em uma organização privada brasileira: um estudo de caso},
	language = {pt},
	author = {Silva, Eduardo Afonso Dutra},
	year = {2023},
	file = {Silva - 2023 - Analisando variações de desempenho em diferentes v.pdf:/Users/arthursena/Zotero/storage/JQNJCURJ/Silva - 2023 - Analisando variações de desempenho em diferentes v.pdf:application/pdf},
}

@article{rego_monitoramento_2014,
	title = {Monitoramento de {Métricas} de {Código}-{Fonte} com suporte de um ambiente de {Data} {Warehousing}: um {Estudo} de {Caso} em uma {Autarquia} da {Administração} {Pública} {Federal}},
	language = {pt},
	author = {Rêgo, Guilherme Baufaker},
	year = {2014},
	file = {Rêgo - 2014 - Monitoramento de Métricas de Código-Fonte com supo.pdf:/Users/arthursena/Zotero/storage/5UHJBTDG/Rêgo - 2014 - Monitoramento de Métricas de Código-Fonte com supo.pdf:application/pdf},
}

@article{neri_estudo_2014,
	title = {Estudo da {Eficácia} e {Eficiência} do {Uso} de um {Ambiente} de {Data} {Warehousing} para {Aferição} da {Qualidade} {Interna} de {Software}: um {Estudo} de {Caso} {Preliminar} no {Tribunal} de {Contas} da {União}},
	language = {pt},
	author = {Neri, Hilmer Rodrigues},
	year = {2014},
	file = {Neri - 2014 - Estudo da Eficácia e Eficiência do Uso de um Ambie.pdf:/Users/arthursena/Zotero/storage/N9QXQM26/Neri - 2014 - Estudo da Eficácia e Eficiência do Uso de um Ambie.pdf:application/pdf},
}

@article{araruna_construcao_2015,
	title = {Construção de um {Ambiente} {Integrado} de {Monitoramento}, {Interpretação} e {Suporte} à {Tomada} de {Decisão} acerca da {Qualidade} {Interna} do {Produto} de {Software}, com uso de um {Ambiente} de {Data} {Warehousing}: um {Estudo} de {Caso} preliminar na {Caixa}},
	language = {pt},
	author = {Araruna, Nilton Cesar Campos},
	year = {2015},
	file = {Araruna - 2015 - Construção de um Ambiente Integrado de Monitoramen.pdf:/Users/arthursena/Zotero/storage/6FZM28L4/Araruna - 2015 - Construção de um Ambiente Integrado de Monitoramen.pdf:application/pdf},
}

@article{kohavi_trustworthy_nodate,
	title = {Trustworthy {Online} {Controlled} {Experiments}: {Five} {Puzzling} {Outcomes} {Explained}},
	abstract = {Online controlled experiments are often utilized to make datadriven decisions at Amazon, Microsoft, eBay, Facebook, Google, Yahoo, Zynga, and at many other companies. While the theory of a controlled experiment is simple, and dates back to Sir Ronald A. Fisher’s experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s, the deployment and mining of online controlled experiments at scale—thousands of experiments now—has taught us many lessons. These exemplify the proverb that the difference between theory and practice is greater in practice than in theory. We present our learnings as they happened: puzzling outcomes of controlled experiments that we analyzed deeply to understand and explain. Each of these took multiple-person weeks to months to properly analyze and get to the often surprising root cause. The root causes behind these puzzling results are not isolated incidents; these issues generalized to multiple experiments. The heightened awareness should help readers increase the trustworthiness of the results coming out of controlled experiments. At Microsoft’s Bing, it is not uncommon to see experiments that impact annual revenue by millions of dollars, thus getting trustworthy results is critical and investing in understanding anomalies has tremendous payoff: reversing a single incorrect decision based on the results of an experiment can fund a whole team of analysts. The topics we cover include: the OEC (Overall Evaluation Criterion), click tracking, effect trends, experiment length and power, and carryover effects.},
	language = {en},
	author = {Kohavi, Ron and Deng, Alex and Frasca, Brian and Longbotham, Roger and Walker, Toby and Xu, Ya},
	file = {Kohavi et al. - Trustworthy Online Controlled Experiments Five Pu.pdf:/Users/arthursena/Zotero/storage/UNNFPBE3/Kohavi et al. - Trustworthy Online Controlled Experiments Five Pu.pdf:application/pdf},
}

@book{wohlin_experimentation_2012,
	address = {Berlin, Heidelberg},
	title = {Experimentation in {Software} {Engineering}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-3-642-29043-5 978-3-642-29044-2},
	url = {http://link.springer.com/10.1007/978-3-642-29044-2},
	language = {en},
	urldate = {2024-04-21},
	publisher = {Springer Berlin Heidelberg},
	author = {Wohlin, Claes and Runeson, Per and Höst, Martin and Ohlsson, Magnus C. and Regnell, Björn and Wesslén, Anders},
	year = {2012},
	doi = {10.1007/978-3-642-29044-2},
	file = {Wohlin et al. - 2012 - Experimentation in Software Engineering.pdf:/Users/arthursena/Zotero/storage/FC85Q629/Wohlin et al. - 2012 - Experimentation in Software Engineering.pdf:application/pdf},
}

@book{juristo_basics_2001,
	address = {Boston, MA},
	title = {Basics of {Software} {Engineering} {Experimentation}},
	copyright = {http://www.springer.com/tdm},
	isbn = {978-1-4419-5011-6 978-1-4757-3304-4},
	url = {http://link.springer.com/10.1007/978-1-4757-3304-4},
	language = {en},
	urldate = {2024-04-28},
	publisher = {Springer US},
	author = {Juristo, Natalia and Moreno, Ana M.},
	year = {2001},
	doi = {10.1007/978-1-4757-3304-4},
	file = {Juristo and Moreno - 2001 - Basics of Software Engineering Experimentation.pdf:/Users/arthursena/Zotero/storage/E6D6D8WU/Juristo and Moreno - 2001 - Basics of Software Engineering Experimentation.pdf:application/pdf},
}

@article{araujo_metodos_2006,
	title = {Métodos {Estatísticos} {Aplicados} à {Engenharia} de {Software} {Experimental}},
	language = {pt},
	author = {Araújo, Marco Antônio P and Murta, Leonardo G P},
	year = {2006},
	file = {Araújo and Murta - 2006 - Métodos Estatísticos Aplicados à Engenharia de Sof.pdf:/Users/arthursena/Zotero/storage/3GZGC6KP/Araújo and Murta - 2006 - Métodos Estatísticos Aplicados à Engenharia de Sof.pdf:application/pdf},
}

@book{endres_handbook_2003,
	address = {Harlow},
	edition = {1. publ},
	series = {The {Fraunhofer} {IESE} series on software engineering},
	title = {A handbook of software and systems engineering: empirical observations, laws and theories},
	isbn = {978-0-321-15420-0},
	shorttitle = {A handbook of software and systems engineering},
	language = {en},
	publisher = {Pearson, Addison Wesley},
	author = {Endres, Albert and Rombach, Dieter},
	year = {2003},
	file = {Endres and Rombach - 2003 - A handbook of software and systems engineering em.pdf:/Users/arthursena/Zotero/storage/8Y2C5X33/Endres and Rombach - 2003 - A handbook of software and systems engineering em.pdf:application/pdf},
}

@article{pai_clinical_2004,
	title = {Clinical Research Methods},
	volume = {17},
	abstract = {Systematic reviews and meta-analyses synthesize data from existing primary research, and well-conducted reviews offer clinicians a practical solution to the problem of staying current in their fields of interest. A whole generation of secondary journals, pre-appraised evidence libraries and periodically updated electronic texts are now available to clinicians. However, not all systematic reviews are of high quality, and it is important to be able to critically assess their validity and applicability. This article is an illustrated guide for conducting systematic reviews. A clear understanding of the process will provide clinicians with the tools to judiciously appraise reviews and interpret them. We hope that it will enable clinicians to conduct systematic reviews, generate high-quality evidence, and contribute to the evidence-based medicine movement.},
	language = {en},
	number = {2},
	journal = {THE NATIONAL MEDICAL JOURNAL OF INDIA},
	author = {Pai, Madhukar and McCULLOCH, MICHAEL and Gorman, Jennifer D and Pai, Nitika and Enanoria, Wayne and Kennedy, Gail and Tharyan, Prathap and Colford, John M},
	year = {2004},
	file = {Pai et al. - 2004 - Clinical Research Methods.pdf:/Users/arthursena/Zotero/storage/I74TL89U/Pai et al. - 2004 - Clinical Research Methods.pdf:application/pdf},
}

@article{basili_goal_1994,
	title = {The Goal Question Metric Approach},
	language = {en},
        year = {1994},
	author = {Basili, Victor R and Caldiera, Gianluigi and Rombach, H Dieter},
	file = {Basili et al. - THE GOAL QUESTION METRIC APPROACH.pdf:/Users/arthursena/Zotero/storage/XYB9Q6XW/Basili et al. - THE GOAL QUESTION METRIC APPROACH.pdf:application/pdf},
        journal ={}
}

@book{endres_secondary_2003,
	address = {Harlow, England ; New York},
	title = {Secondary {Studies}},
	isbn = {978-0-321-15420-0},
	shorttitle = {A handbook of software and systems engineering},
	language = {en},
	publisher = {Pearson Addison Wesley},
	author = {Endres, Albert and Rombach, H. Dieter},
	year = {2003},
	keywords = {Software engineering, Systems engineering},
	file = {Endres and Rombach - 2003 - A handbook of software and systems engineering em.pdf:/Users/arthursena/Zotero/storage/KHZHS7RC/Endres and Rombach - 2003 - A handbook of software and systems engineering em.pdf:application/pdf},
}

@misc{noauthor_systems_nodate,
	title = {Systems and software engineering. {Systems} and software quality requirements and evaluation ({SQuaRE}). {System} and software quality models:},
	shorttitle = {Systems and software engineering. {Systems} and software quality requirements and evaluation ({SQuaRE}). {System} and software quality models},
	url = {https://linkresolver.bsigroup.com/junction/resolve/000000000030215101?restype=standard},
	doi = {10.3403/30215101},
	language = {en},
	urldate = {2024-05-06},
	publisher = {BSI British Standards},
	file = {Systems and software engineering. Systems and soft.pdf:/Users/arthursena/Zotero/storage/UUKWH32S/Systems and software engineering. Systems and soft.pdf:application/pdf},
}

@article{schermann_continuous_2018,
	title = {Continuous {Experimentation}: {Challenges}, {Implementation} {Techniques}, and {Current} {Research}},
	volume = {35},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0740-7459, 1937-4194},
	shorttitle = {Continuous {Experimentation}},
	url = {https://ieeexplore.ieee.org/document/8255793/},
	doi = {10.1109/MS.2018.111094748},
	language = {en},
	number = {2},
	urldate = {2024-05-12},
	journal = {IEEE Software},
	author = {Schermann, Gerald and Cito, Jurgen and Leitner, Philipp},
	month = mar,
	year = {2018},
	pages = {26--31},
	file = {Schermann et al. - 2018 - Continuous Experimentation Challenges, Implementa.pdf:/Users/arthursena/Zotero/storage/B2A22G4F/Schermann et al. - 2018 - Continuous Experimentation Challenges, Implementa.pdf:application/pdf},
}

@inproceedings{auer_current_2018,
	address = {Prague},
	title = {Current {State} of {Research} on {Continuous} {Experimentation}: {A} {Systematic} {Mapping} {Study}},
	isbn = {978-1-5386-7383-6},
	shorttitle = {Current {State} of {Research} on {Continuous} {Experimentation}},
	url = {https://ieeexplore.ieee.org/document/8498229/},
	doi = {10.1109/SEAA.2018.00062},
	abstract = {The systematic evaluation of ideas by experiments are the foundation of continuous experimentation. It allows to assess the value of an idea, remove guessing and subjective opinions from the discussion. The enormous interest of it by practitioners and researchers let the body of knowledge consistently grow. New framework, methods and techniques are developed and its application is constantly expanded to new ﬁelds like cyberphysical systems or social networks.},
	language = {en},
	urldate = {2024-05-12},
	booktitle = {2018 44th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Auer, Florian and Felderer, Michael},
	month = aug,
	year = {2018},
	pages = {335--344},
	file = {Auer and Felderer - 2018 - Current State of Research on Continuous Experiment.pdf:/Users/arthursena/Zotero/storage/DXUTBTQU/Auer and Felderer - 2018 - Current State of Research on Continuous Experiment.pdf:application/pdf},
}

@inproceedings{ros_continuous_2018,
	address = {Gothenburg Sweden},
	title = {Continuous experimentation and {A}/{B} testing: a mapping study},
	isbn = {978-1-4503-5745-6},
	shorttitle = {Continuous experimentation and {A}/{B} testing},
	url = {https://dl.acm.org/doi/10.1145/3194760.3194766},
	doi = {10.1145/3194760.3194766},
	abstract = {Background. Continuous experimentation (CE) has recently emerged as an established industry practice and as a research subject. Our aim is to study the application of CE and A/B testing in various industrial contexts. Objective. We wanted to investigate whether CE is used in different sectors of industry, by how it is reported in academic studies. We also wanted to explore the main topics researched to give an overview of the subject and discuss future research directions. Method. We performed a systematic mapping study of the published literature and included 62 papers, using a combination of database search and snowballing. Results. Most reported software experiments are done online and with software delivered as a service, although varied exemptions exist for e.g., financial software and games. The most frequently researched topics are challenges to conduct experiments and statistical methods for software experiments. Conclusions. The software engineering research on CE is still in its infancy. There are future research opportunities in evaluation research of technical topics and investigations of ethical experimentation. We conclude that the included studies show that A/B testing is applicable to a diversity of software and organisations.},
	language = {en},
	urldate = {2024-05-12},
	booktitle = {Proceedings of the 4th {International} {Workshop} on {Rapid} {Continuous} {Software} {Engineering}},
	publisher = {ACM},
	author = {Ros, Rasmus and Runeson, Per},
	month = may,
	year = {2018},
	pages = {35--41},
	file = {Ros and Runeson - 2018 - Continuous experimentation and AB testing a mapp.pdf:/Users/arthursena/Zotero/storage/JKIVI7AL/Ros and Runeson - 2018 - Continuous experimentation and AB testing a mapp.pdf:application/pdf},
}

@article{anchundia_resources_2020,
	title = {Resources for {Reproducibility} of {Experiments} in {Empirical} {Software} {Engineering}: {Topics} {Derived} {From} a {Secondary} {Study}},
	volume = {8},
	copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
	issn = {2169-3536},
	shorttitle = {Resources for {Reproducibility} of {Experiments} in {Empirical} {Software} {Engineering}},
	url = {https://ieeexplore.ieee.org/document/8951162/},
	doi = {10.1109/ACCESS.2020.2964587},
	abstract = {Background: Replication is a recurrent issue in empirical software engineering (ESE). Although it is a foundation of science, replication is hard to execute despite the many supporting tools meant to facilitate reproducibility. For example, in an experiment, which is the most used method in ESE, the number of replications is not enough compared to other sciences. Objective: In this study, we aim to identify tools that maximize reproducibility in software engineering experiments and how they are applied. Methods: We performed a Systematic Mapping Study and complementary strategies to analyze replication from three concerns (communication, knowledge management, and motivation). We analyzed more than 2,600 studies to get 40 primary studies, using a qualitative analytical tool (Atlas.ti) to create semantic maps for synthesizing our results. Result: We found that tools and practices depend on the experiment domain. Human-oriented experiments tend to use an informal mechanism that is costly and time-consuming. On the other hand, technology-oriented experiments are automated, domain-centric, and specialized so they require a learning process and are not transferable to other domains. Conclusion: Tools and practices still lack acceptation and usability among the ESE research community. Therefore, reproducibility is mostly relegated to internal replication, at which time and costs can be assumed within research groups. A focus on new alternatives should be considered to broaden replication.},
	language = {en},
	urldate = {2024-05-12},
	journal = {IEEE Access},
	author = {Anchundia, Carlos E. and Fonseca C., Efrain R.},
	year = {2020},
	pages = {8992--9004},
	file = {Anchundia and Fonseca C. - 2020 - Resources for Reproducibility of Experiments in Em.pdf:/Users/arthursena/Zotero/storage/I24TWJJJ/Anchundia and Fonseca C. - 2020 - Resources for Reproducibility of Experiments in Em.pdf:application/pdf},
}

@inproceedings{anderson_challenges_2022,
	address = {Pittsburgh, PA, USA},
	title = {Challenges in {Applying} {Continuous} {Experimentation}: {A} {Practitioners}' {Perspective}},
	copyright = {https://doi.org/10.15223/policy-029},
	isbn = {978-1-66549-590-5},
	shorttitle = {Challenges in {Applying} {Continuous} {Experimentation}},
	url = {https://ieeexplore.ieee.org/document/9793934/},
	doi = {10.1109/ICSE-SEIP55303.2022.9793934},
	abstract = {Background: Applying Continuous Experimentation on a large scale is not easily achieved. Although the evolution within large tech organisations is well understood, we still lack a good understanding of how to transition a company towards applying more experiments.
Objective: This study investigates how practitioners define, value and apply experimentation, the blockers they experience and what to do to solve these.
Method: We interviewed and surveyed over one hundred practitioners with regards to experimentation perspectives, from a large financial services and e-commerce organization, based in the Netherlands.
Results: Many practitioners have different perspectives on experimentation. The value is well understood. We have learned that the practitioners are blocked by a lack of priority, experience and well functioning tooling. Challenges also arise around dependencies between teams and evaluating experiments with the correct metrics.
Conclusions: Organisation leaders need to start asking for experiment results and investing in infrastructure and processes to actually enable teams to execute experiments and show the value of their work in terms of value for customers and business.},
	language = {en},
	urldate = {2024-05-12},
	booktitle = {2022 {IEEE}/{ACM} 44th {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Anderson, Kevin and Visser, Denise and Mannen, Jan-Willem and Jiang, Yuxiang and Van Deursen, Arie},
	month = may,
	year = {2022},
	pages = {107--114},
	file = {Anderson et al. - 2022 - Challenges in Applying Continuous Experimentation.pdf:/Users/arthursena/Zotero/storage/DF9FCQRR/Anderson et al. - 2022 - Challenges in Applying Continuous Experimentation.pdf:application/pdf},
}

@article{erthal_characterization_2023,
	title = {Characterization of continuous experimentation in software engineering: {Expressions}, models, and strategies},
	volume = {229},
	issn = {01676423},
	shorttitle = {Characterization of continuous experimentation in software engineering},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167642323000436},
	doi = {10.1016/j.scico.2023.102961},
	abstract = {Objective: To characterize CE from the perspective of its deﬁnitions, processes, and strategies for experimentation available in the technical literature and to evolve the understanding perspectives for “continuous experimentation” and “data-driven development” deﬁnitions.
Method: To select and analyze sources of information in the technical literature dealing with different aspects of continuous experimentation through a Literature Study using an ad hoc search improved with snowballing (backward and forward). Organize the ﬁndings into new perspectives for CE deﬁnitions, processes, and experiment strategies.
Results: It was possible to identify many different deﬁnitions, processes, and experimental strategies used to describe CE in the 72 analyzed empirical papers, making it diﬃcult to decide on their combination to be applied in a real software development project. Therefore, it has been proposed to evolve the CE understanding perspective, to categorize its experiment strategies, and to offer a combined development process for CE combining parts of other processes. Besides, conjectural requirements have been identiﬁed, which can contribute to better differentiating requirements and hypotheses in the CE context.
Conclusion: Likely, a better understanding of CE is still missing. It can contribute towards organizing a common taxonomy to facilitate the possible choices for the experiment strategies. Therefore, there is space for more investigations on its applicability and value in different categories of software systems, despite all the advancements of CE and its promotion in developing modern software systems.},
	language = {en},
	urldate = {2024-05-16},
	journal = {Science of Computer Programming},
	author = {Erthal, Vladimir M. and De Souza, Bruno P. and Dos Santos, Paulo Sérgio M. and Travassos, Guilherme H.},
	month = jul,
	year = {2023},
	pages = {102961},
	file = {Erthal et al. - 2023 - Characterization of continuous experimentation in .pdf:/Users/arthursena/Zotero/storage/GWTHUWR3/Erthal et al. - 2023 - Characterization of continuous experimentation in .pdf:application/pdf},
}

@article{pelaez_practice_2024,
	title = {A practice for specifying user stories in multimedia system design: {An} approach to reduce ambiguity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370340&doi=10.55612%2fs-5002-060-009&partnerID=40&md5=9a590e19db567e98bcfa6de5b1317058},
	abstract = {Various agile approaches have guided the development of solutions based on a minimum viable product for the use of user stories (USs). These approaches provide a generic process for specifying USs for a wide variety of solutions. This study presents an original proposal for a practical approach to the specification of USs for multimedia systems, expressed using the Essence graphical notation language. The practice includes a set of activities, techniques, and tools that can be used by professionals to define the USs of a multimedia system, and which can help to mitigate potential ambiguity problems such as vagueness and insufficiency in formulation. To explore its application, a case study was carried out with the participation of two groups of professionals: an experimental group, and a control group. The results for the analysis factors are promising, and show that by carrying out the activities that make up the practice, a work team can achieve the specification of USs at three levels of concreteness, which contribute to the reduction of problems of vagueness and insufficiency in the USs of a multimedia system. Using this approach, the USs can be guaranteed to meet the value proposition of the multimedia system that will be implemented. © (2024), (ASLERD). All Rights Reserved.},
	author = {Peláez, Carlos Alberto and Solano, Andrés},
	year = {2024},
}

@article{olsson_strategic_2024,
	title = {Strategic {Digital} {Product} {Management} in the {Age} of {AI}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188683557&doi=10.1007%2f978-3-031-53227-6_24&partnerID=40&md5=ff21f9d7826ae4b6d04a0c34aee99441},
	abstract = {The role of software product management is key for building, implementing and managing software products. However, although there is prominent research on software product management (SPM) there are few studies that explore how this role is rapidly changing due to digitalization and digital transformation of the software-intensive industry. In this paper, we study how key trends such as DevOps, data and artificial intelligence (AI), and the emergence of digital ecosystems are rapidly changing current SPM practices. Whereas earlier, product management was concerned with predicting the outcome of development efforts and prioritizing requirements based on these predictions, digital technologies require a shift towards experimental ways-of-working and hypotheses to be tested. To support this change, and to provide guidelines for future SPM practices, we first identify the key challenges that software-intensive embedded systems companies experience with regards to current SPM practices. Second, we present an empirically derived framework for strategic digital product management (SPM4AI) in which we outline what we believe are key practices for SPM in the age of AI. © The Author(s) 2024.},
	author = {Olsson, Helena Holmström and Bosch, Jan},
	year = {2024},
	keywords = {Artificial intelligence, Data, Digital ecosystem, Digital products, Digital transformation, Digitalization, Ecosystems, Embedded systems, Information management, Management IS, Management practises, Metadata, Product management, Software product management, Strategic digital product management},
}

@article{costa_multicriteria_2024,
	title = {Multicriteria {Decision}-{Making} in {Public} {Security}: {A} {Systematic} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195897347&doi=10.3390%2fmath12111754&partnerID=40&md5=4022e1fcd0721b69f61fec3f77e49eda},
	abstract = {The Multiple Criteria Decision-Making/Analysis (MCDM/A) methods have been widely used in several management contexts. In public security, their use enhances managerial decision-making by considering the decision-maker’s preference structure and providing a multidimensional view of problems. However, methodological support for their applications in this field lacks clarity, including selecting appropriate methods, addressing pertinent problematics, and identifying alternatives and criteria. To address this gap, this article conducts a Systematic Literature Review (SLR) to diagnose the state of the art and identify the main directions of the research in multicriteria models applied to public security management. The research methodology involves five main research questions, and the extraction and analysis of data from 51 articles selected through a structured filtering process. The analysis includes identifying the number of publications and citations, as well as listing the MCDM/A approaches and issues employed. Furthermore, the criteria used and the number of criteria considered are discussed, as well as the method employed. Finally, the identification of the main research directions in MCDM/A models applied to public security is presented. The findings suggest that prioritization and classification are common problematics, social criteria are frequently considered, and the AHP method is widely used, often employing fuzzy sets and hybrid models. © 2024 by the authors.},
	author = {Costa, Jefferson and Silva, Maisa},
	year = {2024},
}

@article{kanwal_systematic_2024,
	title = {Systematic review on contract-based safety assurance and guidance for future research},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178994069&doi=10.1016%2fj.sysarc.2023.103036&partnerID=40&md5=42770677fc83293a0709b4a44140a836},
	abstract = {The safety requirements are often described via specifications called contracts. To verify that the system fulfils certain safety requirements, for instance, in the assume-guarantee contract specification, the key safety indicators are organized, so that if certain assumptions hold then the respective behaviour is guaranteed. Safety contracts provide a means of exposing potential incompatibilities early in the development process, selecting components to reuse, certifying systems, and identifying uncertainty sources during the operational phase. There exist several studies on contract-based safety assurance, however, there is not any systematic study in this field. For this, a first Systematic Literature Review (SLR) is carried out to obtain an overview of the various contract-based safety assurance concepts, problems, proposed solutions, and their usefulness. In our study, the identification and selection of the primary studies were based on a well-planned search strategy. The search process identified a total of 2881 studies published between 1969 and 2021, out of which 66 studies were selected through a multi-stage process according to our predefined SLR protocol. This SLR aims to highlight the state-of-the-art of contract-based safety assurance and identify potential gaps for future research. Based on research topics in selected studies, we identified the following main categories: contract type, analysis techniques for system safety, compliance with standards, development stage, domain, level of automation, type of study and evaluation, and tool support. The findings of the systematic review not only highlight that the contracts are even more important for advanced safety-critical systems but also strategies to exploit their full potential should be considered in future studies. The suggestions revealed for future research include the usage of contracts for adapting new behaviour, defining system boundaries, interacting with other systems, managing risk during operation, dynamic/runtime safety assurance, and integration of safety with security. © 2023 The Authors},
	author = {Kanwal, Samina and Muram, Faiz Ul and Javed, Muhammad Atif},
	year = {2024},
	keywords = {Assume-guarantee reasoning, Contract specifications, Contract-based assurance, Development process, Key safety indicator, Model checking, Regulatory compliance, Risk management, Safety assurance, Safety engineering, Safety indicator, Safety requirements, Specifications, Systematic literature review, Systematic Review},
}

@article{quin_b_2024,
	title = {A/{B} testing: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186599305&doi=10.1016%2fj.jss.2024.112011&partnerID=40&md5=eebe13c6f2c48ed90e79c2ef28c186f3},
	abstract = {A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user's point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing. © 2024 The Author(s)},
	author = {Quin, Federico and Weyns, Danny and Galster, Matthias and Silva, Camila Costa},
	year = {2024},
	keywords = {Systematic literature review, A/B test engineering, A/B testing, Controlled experimentation, Data driven decision, Decision making, Decisions makings, End-users, Hypothesis testing, Software design, Software testing, State of the art, Test engineering},
}

@article{stupar_model-based_2023,
	title = {Model-based cloud service deployment optimisation method for minimisation of application service operational cost},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148332611&doi=10.1186%2fs13677-023-00389-8&partnerID=40&md5=f64e50ae46a6dd7b46e8606ff5d35f41},
	abstract = {Many currently existing cloud cost optimisation solutions are aimed at cloud infrastructure providers, and they often deal only with specific types of application services. Unlike infrastructure providers, the providers of cloud applications are often left without a suitable cost optimisation solution, especially concerning the wide range of different application types. This paper presents an approach that aims to provide an optimisation solution for the providers of applications hosted in the cloud environments, applicable at the early phase of a cloud application lifecycle and for a wide range of application services. The focus of this research is the development of the method for identifying optimised service deployment option in available cloud environments based on the model of the service and its context, intending to minimise the operational cost of the cloud service while fulfilling the requirements defined by the service level agreement. A cloud application context metamodel is proposed that includes parameters related to both the application service and the cloud infrastructure relevant for the cost and quality of service. By using the proposed optimisation method, knowledge is gained about the effects of the cloud application context parameters on the service cost and quality of service, which is then used to determine the optimal service deployment option. The service models are validated using cloud applications deployed in laboratory conditions, and the optimisation method is validated using the simulations based on the proposed cloud application context metamodel. The experimental results based on two cloud application services demonstrate the ability of the proposed approach to provide relevant information about the impact of cloud application context parameters on service cost and quality of service and use this information for reducing service operational cost while preserving the acceptable service quality level. The results indicate the applicability and relevance of the proposed approach for cloud applications in the early service lifecycle phase since application providers can gain valuable insights regarding service deployment decision without acquiring extensive datasets for the analysis. © 2023, The Author(s).},
	author = {Stupar, Ivana and Huljenic, Darko},
	year = {2023},
	keywords = {Application contexts, Cloud application context, Cloud applications, Cloud services, Cost management, Cost optimization method, Costs, Costs Optimization, Distributed database systems, Iaa, Life cycle, Operational cost, Optimization method, Quality of service, Quality-of-service, Saa, Service modeling},
}

@article{gomes_systematic_2024,
	title = {Systematic {Literature} {Review} on {Hybrid} {Robotic} {Vehicles}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188890772&doi=10.3390%2frobotics13030034&partnerID=40&md5=c19609c850e3513650980723f5d1a341},
	abstract = {Autonomous vehicles are a continuously rising technology in several industry sectors. Examples of these technologies lie in the advances in self-driving cars and can be linked to extraterrestrial exploration, such as NASA’s Mars Exploration Rovers. These systems present a leading methodology allowing for increased task performance and capabilities, which are no longer limited to active human support. However, these robotic systems may vary in shape, size, locomotion capabilities, and applications. As such, this report presents a systematic literature review (SLR) regarding hybrid autonomous robotic vehicles focusing on leg–wheel locomotion. During this systematic review of the literature, a considerable number of articles were extracted from four different databases. After the selection process, a filtered sample was reviewed. A brief description of each document can be found throughout this report. © 2024 by the authors.},
	author = {Gomes, Diogo F. and Pinto, Vítor H.},
	year = {2024},
}

@article{gutierrez-fernandez_variability_2024,
	title = {Variability management and software product line knowledge in software companies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196035889&doi=10.1016%2fj.jss.2024.112114&partnerID=40&md5=01f89174d622cf80d622addaf2be7c35},
	abstract = {Software product line engineering aims to systematically generate similar products or services within a given domain to reduce cost and time to market while increasing reuse. Various studies recognize the success of product line engineering in different domains. Software variability have increased over the years in many different domains such as mobile applications, cyber–physical systems or car control systems to just mention a few. However, software product line engineering is not as widely adopted as other software development technologies. In this paper, we present an empirical study conducted through a survey distributed to many software development companies. Our goal is to understand their need of software variability management and the level of knowledge the companies have regarding software product line engineering. The survey was answered by 127 participants from more than a hundred of different software development companies. Our study reveals that most of companies manage a catalog of similar products in a way or another (e.g. clone-and-own, common modules that are statically imported,etc.), they mostly document the features of products using text or spreed sheet based documents and more than 66\% of companies identify a base product from which they derive other similar products. We also found a correlation between the lack of Software Product Line (SPL) knowledge and the absence of reuse practices. Notably, this is the first study that explore software variability needs regardless of a company's prior knowledge of SPL. The results encourages further research to understand the reason for the limited knowledge and application of software product line engineering practices, despite the growing demand of variability management. © 2024 The Authors},
	author = {Gutiérrez-Fernández, Antonio M. and Chacón-Luna, Ana Eva and Benavides, David and Fuentes, Lidia and Rabiser, Rick},
	year = {2024},
	keywords = {Software design, Application programs, Computer software reusability, Cost engineering, Different domains, Product line engineering, Reduce costs, Reduce time, Reuse, Software company, Software Product Line, Software variabilities, Time to market, Variability management},
}

@article{espinosa_predictive_2024,
	title = {Predictive models for health outcomes due to {SARS}-{CoV}-2, including the effect of vaccination: a systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182467732&doi=10.1186%2fs13643-023-02411-1&partnerID=40&md5=4558aa7f912a743649e02aff8975d10a},
	abstract = {Background: The interaction between modelers and policymakers is becoming more common due to the increase in computing speed seen in recent decades. The recent pandemic caused by the SARS-CoV-2 virus was no exception. Thus, this study aims to identify and assess epidemiological mathematical models of SARS-CoV-2 applied to real-world data, including immunization for coronavirus 2019 (COVID-19). Methodology: PubMed, JSTOR, medRxiv, LILACS, EconLit, and other databases were searched for studies employing epidemiological mathematical models of SARS-CoV-2 applied to real-world data. We summarized the information qualitatively, and each article included was assessed for bias risk using the Joanna Briggs Institute (JBI) and PROBAST checklist tool. The PROSPERO registration number is CRD42022344542. Findings: In total, 5646 articles were retrieved, of which 411 were included. Most of the information was published in 2021. The countries with the highest number of studies were the United States, Canada, China, and the United Kingdom; no studies were found in low-income countries. The SEIR model (susceptible, exposed, infectious, and recovered) was the most frequently used approach, followed by agent-based modeling. Moreover, the most commonly used software were R, Matlab, and Python, with the most recurring health outcomes being death and recovery. According to the JBI assessment, 61.4\% of articles were considered to have a low risk of bias. Interpretation: The utilization of mathematical models increased following the onset of the SARS-CoV-2 pandemic. Stakeholders have begun to incorporate these analytical tools more extensively into public policy, enabling the construction of various scenarios for public health. This contribution adds value to informed decision-making. Therefore, understanding their advancements, strengths, and limitations is essential. © 2024, The Author(s).},
	author = {Espinosa, Oscar and Mora, Laura and Sanabria, Cristian and Ramos, Antonio and Rincón, Duván and Bejarano, Valeria and Rodríguez, Jhonathan and Barrera, Nicolás and Álvarez-Moreno, Carlos and Cortés, Jorge and Saavedra, Carlos and Robayo, Adriana and Franco, Oscar H.},
	year = {2024},
	keywords = {Article, comorbidity, compartment model, coronavirus disease 2019, COVID-19, data base, Health Care, high income country, hospitalization, human, Humans, Joanna Briggs Institute critical appraisal checklist, low income country, mathematical model, outcome assessment, Outcome Assessment, pandemic, Pandemics, predictive model, public health, SARS-CoV-2, SARS-CoV-2 Alpha, SARS-CoV-2 Delta, SARS-CoV-2 Gamma, SARS-CoV-2 Omicron, Severe acute respiratory syndrome coronavirus 2, susceptible exposed infectious recovered model, susceptible infected recovered model, systematic review, United States, vaccination, Vaccination},
}

@inproceedings{simoes_it_2024,
	title = {{IT} {Workforce} {Outsourcing} {Benefits}, {Challenges} and {Success} {Factors} - {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194866459&doi=10.1145%2f3658271.3658305&partnerID=40&md5=cb10e64c7faf72b7af9a9bc6c3436200},
	abstract = {Context: Organizations are highly dependent on a competent IT workforce. IT workforce outsourcing has become a strategic choice of organizations where IT structures are increasingly needed to make management processes more efficient, cost-effective, innovative, and flexible. Problem: The adoption of IT workforce outsourcing presents benefits, challenges, and success factors that require proper management. Despite their direct or indirect impact on IT workforce management, workforce performance, and the quality of products and services, many organizations may not be fully aware of these factors. Solution: We identified the benefits, challenges, and success factors associated with IT workforce outsourcing in the academic literature. IS Theory: We based the study on the General Systems Theory as our findings highlight intertwined relations between organizations, processes, and people involved in IT workforce outsourcing. Method: We executed a systematic mapping study using Engineering Village and Scopus digital databases and complemented the results with backward and forward snowballing. Summary of Results: Based on 32 studies, we identified 13 benefits, 24 challenges, and 18 success factors. These encompass issues related to aspects such as outsourcing strategy, service capacity, human resources, contractual aspects, and outsourcing planning. Contributions and Impact in the IS Area: We compared the results with a prior field study involving industry practitioners engaged in IT workforce outsourcing. We highlight the importance of hearing from both practitioners and the academic literature. Based on the results, academia can propose related research while practitioners can propose actions to improve the adoption of outsourcing practices, risk reduction, and the relationship between contractors and suppliers, among others. © 2024 ACM.},
	author = {Simões, Carlos Alberto and Santos, Gleison},
	year = {2024},
	keywords = {Academic literature, Audition, Benefit, Challenge, Cost effectiveness, Information systems, Information use, IT structures, IT Workforce, IT workforce outsourcing, Management process, Mapping, Outsourcing, Strategic choice, Success factors, Systematic mapping studies},
}

@article{laureate_systematic_2023,
	title = {A systematic review of the use of topic models for short text social media analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154596265&doi=10.1007%2fs10462-023-10471-x&partnerID=40&md5=9739bfcc768a46989a8b24b876659c8b},
	abstract = {Recently, research on short text topic models has addressed the challenges of social media datasets. These models are typically evaluated using automated measures. However, recent work suggests that these evaluation measures do not inform whether the topics produced can yield meaningful insights for those examining social media data. Efforts to address this issue, including gauging the alignment between automated and human evaluation tasks, are hampered by a lack of knowledge about how researchers use topic models. Further problems could arise if researchers do not construct topic models optimally or use them in a way that exceeds the models’ limitations. These scenarios threaten the validity of topic model development and the insights produced by researchers employing topic modelling as a methodology. However, there is currently a lack of information about how and why topic models are used in applied research. As such, we performed a systematic literature review of 189 articles where topic modelling was used for social media analysis to understand how and why topic models are used for social media analysis. Our results suggest that the development of topic models is not aligned with the needs of those who use them for social media analysis. We have found that researchers use topic models sub-optimally. There is a lack of methodological support for researchers to build and interpret topics. We offer a set of recommendations for topic model researchers to address these problems and bridge the gap between development and applied research on short text topic models. © 2023, The Author(s).},
	author = {Laureate, Caitlin Doogan Poet and Buntine, Wray and Linger, Henry},
	year = {2023},
	keywords = {Systematic Review, Applied research, Evaluation measures, LDA, Short texts, Social media, Social media analysis, Social media datum, Social networking (online), Topic Modeling, Twitter},
}

@article{elder_survey_2024,
	title = {A {Survey} on {Software} {Vulnerability} {Exploitability} {Assessment}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191395395&doi=10.1145%2f3648610&partnerID=40&md5=4d9c74ed045e6b2d7675c61936db9cb8},
	abstract = {Knowing the exploitability and severity of software vulnerabilities helps practitioners prioritize vulnerability mitigation efforts. Researchers have proposed and evaluated many different exploitability assessment methods. The goal of this research is to assist practitioners and researchers in understanding existing methods for assessing vulnerability exploitability through a survey of exploitability assessment literature. We identify three exploitability assessment approaches: assessments based on original, manual Common Vulnerability Scoring System, automated Deterministic assessments, and automated Probabilistic assessments. Other than the original Common Vulnerability Scoring System, the two most common sub-categories are Deterministic, Program State based, and Probabilistic learning model assessments. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author = {Elder, Sarah and Rahman, Md Rayhanur and Fringer, Gage and Kapoor, Kunal and Williams, Laurie},
	year = {2024},
	keywords = {Assessment approaches, Common vulnerability scoring systems, Deterministic programs, Deterministics, Exploitability, Probabilistic assessments, Program state, Software vulnerabilities, State based, Vulnerability mitigation},
}

@article{bekkemoen_explainable_2024,
	title = {Explainable reinforcement learning ({XRL}): a systematic literature review and taxonomy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178051880&doi=10.1007%2fs10994-023-06479-7&partnerID=40&md5=8a5a3e37497d50b95590c9e028c76928},
	abstract = {In recent years, reinforcement learning (RL) systems have shown impressive performance and remarkable achievements. Many achievements can be attributed to combining RL with deep learning. However, those systems lack explainability, which refers to our understanding of the system’s decision-making process. In response to this challenge, the new explainable RL (XRL) field has emerged and grown rapidly to help us understand RL systems. This systematic literature review aims to give a unified view of the field by reviewing ten existing XRL literature reviews and 189 XRL studies from the past five years. Furthermore, we seek to organize these studies into a new taxonomy, discuss each area in detail, and draw connections between methods and stakeholder questions (e.g., “how can I get the agent to do \_?”). Finally, we look at the research trends in XRL, recommend XRL methods, and present some exciting research directions for future research. We hope stakeholders, such as RL researchers and practitioners, will utilize this literature review as a comprehensive resource to overview existing state-of-the-art XRL methods. Additionally, we strive to help find research gaps and quickly identify methods that answer stakeholder questions. © 2023, The Author(s).},
	author = {Bekkemoen, Yanzhe},
	year = {2024},
	keywords = {Systematic literature review, Decision making, Decision-making process, Deep learning, Explainability, Explainable artificial intelligence, Explanation, Interpretability, Literature reviews, Performance, Reinforcement learning, Reinforcement learning systems, Reinforcement learnings, Taxonomies},
}

@article{shuraida_impact_2024,
	title = {The {Impact} of {Feature} {Exploitation} and {Exploration} on {Mobile} {Application} {Evolution} and {Success}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195242164&doi=10.17705%2f1jais.00844&partnerID=40&md5=5e1d823044298ee989de4f14e0ac2639},
	abstract = {Mobile device applications are the largest segment of IS with an estimated 5 billion users. Yet despite their widespread and growing use, there is little research examining how these mobile applications evolve with each new release update. To ensure market success, developers need to satisfy their user base by incorporating users’ reviews and feedback on the one hand and exploring new features and content that allow them to stay competitive on the other. Drawing on the organizational learning and innovation literature, the findings of the present study suggest that a mix of these two activities of exploitation and exploration in consequent app updates is likely to result in the app’s success. We further contribute to this body of work by examining the influence of users’ online review characteristics on exploitation and exploration activities in app development. The findings suggest that users’ convergence on similar issues (review concurrence) is likely to favor an orientation prioritizing exploitation over exploration activities, while the number of user reviews (review volume) has a curvilinear relationship with it. © 2024 by the Association for Information Systems.},
	author = {Shuraida, Shadi and Gao, Qiang and Safadi, Hani and Jain, Radhika},
	year = {2024},
	keywords = {Exploitation, Exploitation and explorations, Feature exploitation, Mobile application development, Mobile application evolution, Mobile applications, Mobile computing, Online user review, Online users, Technological innovation, User reviews},
}

@article{dos_santos_automatic_2024,
	title = {Automatic user story generation: a comprehensive systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195041962&doi=10.1007%2fs41060-024-00567-0&partnerID=40&md5=7b9c1924d2789134c475860878667716},
	abstract = {User stories are the lifeblood of agile software development due to their semi-structured format and ease of implementation. However, the variety of data sources and textual formats used to document software requirements bring a challenge for software development teams. They often need to read and comprehend the client’s needs from different sources and convert them into user stories manually. This process demands time, and it is also prone to errors. As an alternative to remedy this issue, there are studies concerning the automatic generation of user stories. We conducted a systematic literature review (SLR) to identify and analyze existing approaches for automatically generating user stories. We investigated which type of corpora were used for training and testing, which Natural Language Processing (NLP) or Machine Learning (ML) techniques are employed to reach this goal, and how researchers are evaluating the quality of the user stories generated. Our SLR followed established guidelines and investigated state-of-the-art research from prominent academic publishers such as ACM, IEEE Xplore, and ScienceDirect. Studies published until April 2024 were included, with a focus on those addressing the research questions proposed. Our findings indicate a critical shortage of publicly available corpora hindering advancements in this field, especially in the current era of ML. The team also found there is a broad variety of techniques being employed on this topic. Finally, the studies need to pay more attention to guidelines for evaluating user stories quality. The automatic user story generation remains in its early stages. We highlight some opportunities for contribution and discuss the direction of future works. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.},
	author = {dos Santos, Carlos Alberto and Bouchard, Kevin and Minetto Napoleão, Bianca},
	year = {2024},
	keywords = {Systematic literature review, Software design, Agile software development, Data-source, Learning algorithms, Learning systems, Natural language processing systems, Requirement engineering, Requirements engineering, Semi-structured, Software requirements, Story generations, Text generations, Textual format, User stories},
}

@article{liu_design_2024,
	title = {Design for dependability — {State} of the art and trends},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185411375&doi=10.1016%2fj.jss.2024.111989&partnerID=40&md5=7f5b36b728954f77c7c91606f55da869},
	abstract = {This paper presents an overview of design for dependability as a process involving three distinct but interrelated activities: risk analysis, risk mitigation, and risk assessment. Although these activities have been the subject of numerous works, few of them address the issue of their integration into rigorous design flows. Moreover, most existing results focus on dependability for small-size safety-critical systems with specific static architectures. They cannot be applied to large systems, such as autonomous systems with dynamic heterogeneous architectures and AI components. The overwhelming complexity and lack of interpretability of AI present challenges to model-based techniques and require empirical approaches. Furthermore, it is impossible to cope with all potential risks at design time; run-time assurance techniques are necessary to cost-effectively achieve the desired degree of dependability. The paper synthesizes the state of the art showing particularly the impact of new trends stemming from the integration of AI components in design flows. It argues that these trends will have a profound impact on design methods and the level of dependability. It advocates the need for a new theoretical basis for dependability engineering that allows the integration of traditional model-based approaches and data-driven techniques in the search for trade-offs between efficiency and dependability. © 2024},
	author = {Liu, Hezhen and Huang, Chengqiang and Sun, Ke and Yin, Jiacheng and Wu, Xiaoyu and Wang, Jin and Zhang, Qunli and Zheng, Yang and Nigam, Vivek and Liu, Feng and Sifakis, Joseph},
	year = {2024},
	keywords = {Safety engineering, State of the art, AI systems, Dependable AI system, Design, Design flows, Design for dependability, Economic and social effects, Integration, Risk analysis, Risk assessment, Risk mitigation, Risks assessments, Run-time assurance, Runtimes, Safety critical systems, Search engines},
}

@article{travassos_tertiary_2024,
	title = {A {Tertiary} {Study} on the {Convergence} of {Human}–{Computer} {Interaction} and {Artificial} {Intelligence}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191293638&doi=10.1007%2f978-3-031-53957-2_1&partnerID=40&md5=0c7138f0d7563d7b36599adb4b058895},
	abstract = {The emergence of technologies like artificial intelligence (AI) has strengthened computational solutions by using powerful algorithms that can support learning behaviors from available data and replicating such behaviors in software. A possible behavior regards the interaction of humans with computers. In this context, Human–Computer Interaction (HCI) promotes the design and evaluation of intuitive and high-usability interactive systems for human users. However, it is not clear yet the convergence between HCI and AI and how such a convergence can improve the computational solutions to benefit the end users despite the many secondary studies regarding HCI and AI available in the technical literature. Therefore, it is necessary to characterize the convergence between HCI and AI revealed in secondary studies, aiming at a better understanding of the challenges and implications of conveying the convergence of these two areas of interest from the perspective of research by undertaking a tertiary study to acquire knowledge from secondary studies published in the Scopus database until 2022. Thirty-eight secondary studies from 17 countries provided evidence of the possible convergence of HCI and AI. The main purposes of converging these areas are to foster user trust and satisfaction, to improve communication and user experience, to increase learnability and performance, and to enhance environmental observation and services. These findings have been observed in 26 problem domains and 23 system domains, performed with many distinct instruments. The convergence of HCI and AI is of worldwide interest, with a growing use of HCI methods and criteria to support developing and improving AI software systems. However, such convergence does not concern specific areas but concerns AI software systems characteristics, human use, and their interaction. Promoting the convergence of HCI and AI is challenging without a clear vision of the target software system. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	author = {Travassos, Guilherme Horta and Felizardo, Katia Romero and Morandini, Marcelo and Kolski, Christophe},
	year = {2024},
	keywords = {Artificial intelligence, Systematic literature review, Computational solutions, Computer software, Contemporary software system engineering, Design and evaluations, Evidence Based Software Engineering, Human computer interaction, Intelligence software, Learning behavior, Software-systems, Support learning, Tertiary study, User interfaces},
}

@article{pedrosa_immersive_2024,
	title = {Immersive {Learning} {Environments} for {Self}-regulation of {Learning}: {A} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176965303&doi=10.1007%2f978-3-031-47328-9_36&partnerID=40&md5=afd01b0fe3773e626ee444d9ca7af52f},
	abstract = {Self-regulation of learning (SRL) plays a decisive role in learning success but characterizing learning environments that facilitate development of SRL skills constitutes a great challenge. Given the growing interest in Immersive Learning Environments (ILE), we sought to understand how ILE are built with attention to SRL, via a literature review of pedagogical uses, practices and strategies with ILE that have an explicit focus on SRL. From a final corpus of 25 papers, we collected 134 extracts attesting use of ILE for SRL. We classified and mapped them using the Beck, Morgado \& O’Shea framework and its three dimensions of the immersion phenomenon: system, narrative and challenge. There is a predominance of uses of ILE for SRL aligned with Challenge-based immersion: Skill Training, Collaboration, Engagement, and Interactive Manipulation and Exploration. In contrast, uses aligned with System-based immersion (Emphasis, Accessibility, Seeing the Invisible) were not identified. There were few cases of use of Narrative-based immersion. Uses combining the three dimensions of immersive had residual prevalence. We concluded that there is greater tendency in studies of SRL in ILE to enact active roles (aligned with the Challenge dimension of immersion). The low prevalence of Narrative immersion and System immersion evidence gaps in the diversity of pedagogical uses of ILE to develop SRL, which indicate opportunities for research and creation of innovative educational practices. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Pedrosa, Daniela and Morgado, Leonel and Beck, Dennis},
	year = {2024},
	keywords = {Literature reviews, Computer aided instruction, Deregulation, Educational practice, Educational strategy, Educational use, Immersion, Immersive learning, Learning environments, Pedagogical use, Self regulation, Self-regulated learning},
}

@article{jordanov_containerized_2024,
	title = {Containerized {Microservices} for {Mobile} {Applications} {Deployed} on {Cloud} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195214138&doi=10.3991%2fijim.v18i10.45929&partnerID=40&md5=e3396147196473e57712536d54efef0c},
	abstract = {This study explores the transformative role of containerized microservices in the sphere of mobile application development, especially within public cloud ecosystems. It focuses on how technologies such as Docker and Kubernetes contribute to improving deployment, scalability, and overall management of mobile applications, with an emphasis on containerizing backend services. We analyze their efficiency in streamlining deployment processes, focusing on how they improve the application’s performance and reliability. Additionally, we examine various alternative deployment strategies, such as blue-green, rolling, and canary releases, to emphasize their effectiveness in minimizing risks and facilitating smooth transitions in dynamic cloud environments. The study takes a comprehensive approach to achieve this goal, which includes a systematic review of existing literature, a thorough examination of relevant use cases, and an assessment of open-source technologies. Our findings reveal not only the practical benefits of these strategies but also their strategic application, offering important insights for software engineers and decision-makers. This study emphasizes the significance of integrating and optimizing containerized microservices in mobile app development to achieve more efficient, scalable, and manageable application lifecycles on cloud-based platforms. © 2024 by the authors of this article.},
	author = {Jordanov, Jordan and Simeonidis, Dimitrios and Petrov, Pavel},
	year = {2024},
	keywords = {Decision making, Life cycle, Application programs, Mobile application development, Mobile applications, Mobile computing, Cloud systems, Containerization, Containers, Deployment process, It focus, Microservice, Open source software, Performance and reliabilities, Public clouds, Virtualization, Virtualizations},
}

@article{kompella_innovations_2024,
	title = {Innovations, strategic organizational actions, and sailing-ship effect: illustrated with an {IT} product},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182492367&doi=10.1108%2fJSTPM-08-2022-0125&partnerID=40&md5=19fae5223ac33e5337300c31268e3cfb},
	abstract = {Purpose: In socio-technical transition theory, resistance by existing technology and regime resistance plays a key role. The resistance is in the form of intentional improvements; eventually, the regime destabilizes and adopts the new technology, referred to as the sailing-ship effect. Researchers used a structural view and examined it as a strategic action and its relationship with new technology (competitive/symbiotic) in non-fast-changing sailing systems. This study uses a microlevel view and examines it in a fast-changing where products/services are developed by integrating existing technology with new product innovations; their success depends on addressing technical/market uncertainty. This study examines the sailing-ship effect in a fast-changing system and contributes to the socio-technical transition theory. Design/methodology/approach: The authors need to examine the phenomena of the sailing-ship effect in its setting, and a case-study method is appropriate. The selected case provided diverse analytic and heuristic perspectives to examine the phenomena; therefore, it was a single case study. Findings: In an IT scenario, the strategic actions decide and realize agility and competitive advantage by formulating appropriate goals with required budgets and coevolutionary changes to resources at product, process and organizational levels, addressing technical/market uncertainty. Moreover, the agility displayed by strategic actions determines the relationship with new technology, which is interspersed. Finally, it provided insights into struggle, navigation and negotiations, forming strategic actions to display the sailing-ship effect. Research limitations/implications: The study selected a Banking Financial Services and Insurance product of an IT Services company. As start-ups exhibit inherent (emergent) agility, the authors can examine agility as a combination of emergent and strategic actions by selecting a start-up. Practical implications: The study highlights the strategic actions specific to an IT services company. It developed its product and services by steering clear from IT innovations such as native cloud and continuous deployment. It improved its products/services with necessary organizational changes and achieved the desired agility and competitive advantage. Therefore, organizations devise appropriate strategic actions to combat the sailing-ship effect apart from setting goals and selecting IT innovations. Originality/value: The study expands the socio-technical transition theory by selecting a fast-changing system. It provided insights into the relationship between existing and new technology and the strategic actions necessary to manage technical and market uncertainty and achieve the desired competitive advantage, or the sailing-ship effect. © 2024, Emerald Publishing Limited.},
	author = {Kompella, Lakshminarayana},
	year = {2024},
}

@article{chen_understanding_2024,
	title = {Understanding and evaluating software reuse costs and benefits from industrial cases—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189036914&doi=10.1016%2fj.infsof.2024.107451&partnerID=40&md5=c4671380bf15db91cc961767f9aa77df},
	abstract = {Context: Software reuse costs and benefits have been investigated in several primary studies, which have been aggregated in multiple secondary studies as well. However, existing secondary studies on software reuse have not critically appraised the evidence in primary studies. Moreover, there has been relatively less focus on how software reuse costs and benefits were measured in the primary studies, and the aggregated evidence focuses more on software reuse benefits than reuse costs. Objective: This study aims to cover the gaps mentioned in the context above by synthesizing and critically appraising the evidence reported on software reuse costs and benefits from industrial cases. Method: We used a systematic literature review (SLR) to conduct this study. The results of this SLR are based on a final set of 30 primary studies. Results: We identified nine software reuse benefits and six software reuse costs, in which better quality and improved productivity were investigated the most. The primary studies mostly used defect-based and development time-based metrics to measure reuse benefits and costs. Regarding the reuse practices, the results show that software product lines, verbatim reuse, and systematic reuse were the top investigated ones, contributing to more reuse benefits. The quality assessment of the primary studies showed that most of them are either of low (20\%) or moderate (67\%) quality. Conclusion: Based on the number and quality of the studies, we conclude that the strength of evidence for better quality and improved productivity as reuse benefits is high. There is a need to conduct more high quality studies to investigate, not only other reuse costs and benefits, but also how relatively new reuse-related practices, such as InnerSource and microservices architecture, impact software reuse. © 2024 The Author(s)},
	author = {Chen, Xingru and Usman, Muhammad and Badampudi, Deepika},
	year = {2024},
	keywords = {Systematic literature review, Costs, Computer software reusability, Reuse, Cost and benefits, Development time, Evaluating software, Software reuse benefit, Software reuse cost, Software-reuse},
}

@article{stojanov_tertiary_2023,
	title = {A {Tertiary} {Study} on {Microservices}: {Research} {Trends} and {Recommendations}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182954403&doi=10.1134%2fS0361768823080200&partnerID=40&md5=aea0b73069a88175c9a499a8ffd106c5},
	abstract = {Abstract: The development and adoption of microservices, as one of the most promising directions for developing heterogeneous distributed software systems, have been driven by dynamic changes in business and technology. In addition to the development of new applications, a significant aspect of microservices is the migration from legacy monolithic systems to microservice architectures. Such development trends are accompanied by an increase in the number of primary and secondary publications addressing microservices, highlighting the need to systematize research at a higher level. The objective of this study is to comprehensively analyze secondary studies in the field of microservices from the following five aspects: (1) publishing trends, (2) quality trends of secondary studies, (3) research trends, (4) domains of implementation, and (5) future research directions. The study follows the guidelines for conducting a systematic literature review. The findings were derived from 44 secondary studies published in the period from January 2016 to January 2023. These studies were organized and analyzed to address the five proposed research questions pertaining to the study objectives. The findings suggest that the most promising research directions are related to the development, implementation, and validation of new approaches, methods, and tools that encompass all the phases of the life cycle. Additionally, these research directions have applications in a variety of business and human life domains. Recommendations for further literature reviews relate to improvement of quality assessment of selected studies, more detailed review of architecture quality attributes, inquiry of human factor issues, and certain maintenance and operation issues. From the methodological aspect, recommendations relate to using social science qualitative methods for more detailed analysis of selected studies, and inclusion of gray literature that will bring the real experience of experts from industry. © 2023, Pleiades Publishing, Ltd.},
	author = {Stojanov, Z. and Hristoski, I. and Stojanov, J. and Stojkov, A.},
	year = {2023},
	keywords = {Systematic literature review, Life cycle, Tertiary study, Microservice, Development trends, Distributed software system, Dynamic changes, Future research directions, Legacy systems, Monolithic systems, New applications, Research trends},
}

@book{sharp_humans_2023,
	title = {Humans in the loop: {People} at the heart of systems development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195009972&doi=10.1007%2f978-3-031-45304-5_23&partnerID=40&md5=745be4cd63797d17ef996d3e9070fc1d},
	abstract = {Despite increased automation in the process, people are (still) at the heart of software systems development. This chapter adopts a sociotechnical perspective and explores three areas that characterize the role of humans in software systems development: people as creators, people as users, and people in partnership with systems. Software is created by specialist developers such as software engineers and non-specialists such as "makers." Software developers build communities and operate within several cultures (e.g., professional, company, and national), all of which affect both the development process and the resulting product. Software is used by people. Users also operate within communities and cultures which influence product use, and how systems are used feeds back into future systems development. People and systems are interdependent: they work in partnership to achieve a wide range of goals. However, software both supports what people want to do and shapes what can be done. © The Author(s) 2024. All rights reserved.},
	author = {Sharp, Helen},
	year = {2023},
}

@article{umar_advances_2024,
	title = {Advances in automated support for requirements engineering: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183775500&doi=10.1007%2fs00766-023-00411-0&partnerID=40&md5=13e42fd60463aa53bb491243c6804cfd},
	abstract = {Requirements Engineering (RE) has undergone several transitions over the years, from traditional methods to agile approaches emphasising increased automation. In many software development projects, requirements are expressed in natural language and embedded within large volumes of text documents. At the same time, RE activities aim to define software systems' functionalities and constraints. However, manually executing these tasks is time-consuming and prone to errors. Numerous research efforts have proposed tools and technologies for automating RE activities to address this challenge, which are documented in published works. This review aims to examine empirical evidence on automated RE and analyse its impact on the RE sub-domain and software development. To achieve our goal, we conducted a Systematic Literature Review (SLR) following established guidelines for conducting SLRs. We aimed to identify, aggregate, and analyse papers on automated RE published between 1996 and 2022. We outlined the output of the support tool, the RE phase covered, levels of automation, development approach, and evaluation approaches. We identified 85 papers that discussed automated RE from various perspectives and methodologies. The results of this review demonstrate the significance of automated RE for the software development community, which has the potential to shorten development cycles and reduce associated costs. The support tools primarily assist in generating UML models (44.7\%) and other activities such as omission of steps, consistency checking, and requirement validation. The analysis phase of RE is the most widely automated phase, with 49.53\% of automated tools developed for this purpose. Natural language processing technologies, particularly POS tagging and Parser, are widely employed in developing these support tools. Controlled experimental methods are the most frequently used (48.2\%) for evaluating automated RE tools, while user studies are the least employed evaluation method (8.2\%). This paper contributes to the existing body of knowledge by providing an updated overview of the research literature, enabling a better understanding of trends and state-of-the-art practices in automated RE for researchers and practitioners. It also paves the way for future research directions in automated requirements engineering. © Crown 2024.},
	author = {Umar, Muhammad Aminu and Lano, Kevin},
	year = {2024},
	keywords = {Systematic literature review, Software design, Natural language processing systems, Requirement engineering, Requirements engineering, Agile approaches, Automated requirement engineering, Automated support, Automation, Computational linguistics, Engineering activities, Natural languages, Software development projects, Support tool, Unified Modeling Language},
}

@article{song_when_2024,
	title = {When debugging encounters artificial intelligence: state of the art and open challenges},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186138673&doi=10.1007%2fs11432-022-3803-9&partnerID=40&md5=9eede2fdcbf17988a79b522100b5aa0c},
	abstract = {Both software debugging and artificial intelligence techniques are hot topics in the current field of software engineering. Debugging techniques, which comprise fault localization and program repair, are an important part of the software development lifecycle for ensuring the quality of software systems. As the scale and complexity of software systems grow, developers intend to improve the effectiveness and efficiency of software debugging via artificial intelligence (artificial intelligence for software debugging, AI4SD). On the other hand, many artificial intelligence models are being integrated into safety-critical areas such as autonomous driving, image recognition, and audio processing, where software debugging is highly necessary and urgent (software debugging for artificial intelligence, SD4AI). An AI-enhanced debugging technique could assist in debugging AI systems more effectively, and a more robust and reliable AI approach could further guarantee and support debugging techniques. Therefore, it is important to take AI4SD and SD4AI into consideration comprehensively. In this paper, we want to show readers the path, the trend, and the potential that these two directions interact with each other. We select and review a total of 165 papers in AI4SD and SD4AI for answering three research questions, and further analyze opportunities and challenges as well as suggest future directions of this cross-cutting area. © Science China Press 2024.},
	author = {Song, Yi and Xie, Xiaoyuan and Xu, Baowen},
	year = {2024},
	keywords = {Safety engineering, Software design, State of the art, Life cycle, Computer software, Software-systems, Artificial intelligence techniques, Current fields, Fault localization, Hot topics, Image recognition, Machine learning, Machine-learning, Program debugging, Program repair, Repair, Software debugging, Software development life-cycle},
}

@article{liu_conversation-based_2024,
	title = {Conversation-based hybrid {UI} for the repertory grid technique: {A} lab experiment into automation of qualitative surveys},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183466132&doi=10.1016%2fj.ijhcs.2024.103227&partnerID=40&md5=9c4644f20ae1abe675a64043de47f49c},
	abstract = {A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language. © 2024 The Authors},
	author = {Liu, Yunxing and Martens, Jean-Bernard},
	year = {2024},
	keywords = {Automation, Chatbots, Graphical user interfaces, Grid techniques, Hybrid UI, Hybrid User Interfaces, Lab. experiment, Qualitative survey automation, Qualitative surveys, Repertory grid technique, Repertory grids, Users' experiences},
}

@article{diaz_how_2024,
	title = {How can feature usage be tracked across product variants? {Implicit} {Feedback} in {Software} {Product} {Lines}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186517167&doi=10.1016%2fj.jss.2024.112013&partnerID=40&md5=d2166267177422aa1cf29c0b37bd4de1},
	abstract = {Implicit feedback involves gathering information about software usage to grasp how and when the software is utilized. This study investigates the integration of implicit feedback mechanisms into Software Product Line Engineering (SPLE). While common in product-based development for identifying bugs, usability issues, and informing requirement prioritization, its adoption in SPLs has been limited due to the unique challenges posed by SPLE, such as the emphasis on Domain Engineering over Application Engineering, and the need for systematic reuse of shared assets. We propose a novel approach to incorporate feedback practices into Domain Engineering, thereby shifting the focus from individual product variants to the SPL platform, and specifically moving from product-based feedback to feature-based feedback. Based on a case study, we suggest that product derivation includes a second step that injects the trackers at the time of derivation, using a Feedback Model that complements the Configuration Model for feedback analysis.To test this approach, we introduce FEACKER, an extension to pure::variants as the variability manager. FEACKER injects trackers when the product variant is derived. The findings are validated through a Technology Acceptance Model (TAM) evaluation and a focus group discussion, providing insights into the feasibility, acceptance, and potential impact of platform-based feedback in SPLE. The results indicate agreement on the benefits of conducting feedback analysis at the platform level and the perception that FEACKER seamlessly extends the capabilities of pure::variants. © 2024 The Authors},
	author = {Díaz, Oscar and Medeiros, Raul and Al-Hajjaji, Mustafa},
	year = {2024},
	keywords = {Software design, Product line engineering, Software Product Line, Computer software, Program debugging, Application engineering, Continuous development, Domain engineering, Feedback analysis, Feedback mechanisms, Implicit feedback, Product variants, Requirements prioritization},
}

@article{cauz_text_2024,
	title = {Text readability in augmented reality: a multivocal literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186594558&doi=10.1007%2fs10055-024-00949-6&partnerID=40&md5=2be8cca4f03f6481cf9e0d914a2d6e02},
	abstract = {Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations. © The Author(s) 2024.},
	author = {Cauz, Maxime and Clarinval, Antoine and Dumas, Bruno},
	year = {2024},
	keywords = {Literature reviews, 'current, Augmented reality, Contextual constraints, Decision trees, Game Engine, Industrial research, Legibility, Mixed reality, Optical see-through, Readability, State of research, Text},
}

@article{rico_experiences_2024,
	title = {Experiences from conducting rapid reviews in collaboration with practitioners — {Two} industrial cases},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178453626&doi=10.1016%2fj.infsof.2023.107364&partnerID=40&md5=1973bcd1a7948080b1e54df8a4a4d573},
	abstract = {Context: Evidence-based software engineering (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice. Objective: This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and industry. Through the lens of two case studies, we delve into the practical application and experience of conducting RRs. Methods: We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We collected data through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis. Results: We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners. Conclusion: The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange. © 2023 The Author(s)},
	author = {Rico, Sergio and Ali, Nauman Bin and Engström, Emelie and Höst, Martin},
	year = {2024},
	keywords = {Software engineering, Systematic Review, Literature reviews, Evidence Based Software Engineering, Industrial research, Industry-academia collaboration, Knowledge exchange, Knowledge management, Literature studies, Rapid review, Research relevance, Study design, Systematic method},
}

@inproceedings{klymenko_integration_2024,
	title = {On the {Integration} of {Privacy}-{Enhancing} {Technologies} in the {Process} of {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980857&doi=10.5220%2f0012632500003690&partnerID=40&md5=11923a80a832d27738266388d3b58f9d},
	abstract = {The class of technologies known as Privacy-Enhancing Technologies (PETs) has been receiving rising attention in the academic sphere. In practice, however, the adoption of such technologies remains low. Beyond the actual implementation of a PET, it is not clear where along the process of software engineering PETs should be considered, and which activities must take place to facilitate their implementation. In this light, we aim to investigate the placement of PETs in the software engineering process, specifically from the perspective of privacy requirements engineering. To do this, we conduct a systematic literature review and interview 10 privacy experts, exploring the integration of PETs into the software engineering process, as well as identifying associated challenges along with their potential solutions. We systematize our findings in a unified process diagram that illustrates the roles and activities involved in the implementation of PETs in software systems. In addition, we map the identified solution concepts to the diagram, highlighting which stages of the software engineering process are vital in tackling the corresponding challenges and supporting the adoption of PETs. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	author = {Klymenko, Alexandra and Meisenbacher, Stephen and Favaro, Luca and Matthes, Florian},
	year = {2024},
	keywords = {Software engineering, Systematic literature review, Requirement engineering, Requirements engineering, Software-systems, Data privacy, Engineering education, Privacy engineerings, Privacy enhancing technologies, Privacy requirements, Process diagram, Software engineering process, Solution concepts, Unified process},
}

@article{cacciuttolo_sensor_2024,
	title = {Sensor {Technologies} for {Safety} {Monitoring} in {Mine} {Tailings} {Storage} {Facilities}: {Solutions} in the {Industry} 4.0 {Era}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194092250&doi=10.3390%2fmin14050446&partnerID=40&md5=b1e13d13f95a8659defb3f6d305755f9},
	abstract = {The recent tailings storage facility (TSF) dam failures recorded around the world have concerned society in general, forcing the mining industry to improve its operating standards, invest greater economic resources, and implement the best available technologies (BATs) to control TSFs for safety purposes and avoid spills, accidents, and collapses. In this context, and as the era of digitalization and Industry 4.0 continues, monitoring technologies based on sensors have become increasingly common in the mining industry. This article studies the state of the art of implementing sensor technologies to monitor structural health and safety management issues in TSFs, highlighting advances and experiences through a review of the scientific literature on the topic. The methodology applied in this article adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and utilizes scientific maps for data visualization. To do so, three steps were implemented: (i) a quantitative bibliometric analysis, (ii) a qualitative systematic review of the literature, and (iii) a mixed review to integrate the findings from (i) and (ii). As a result, this article presents the main advances, gaps, and future trends regarding the main characteristics of the sensor technologies applied to monitor TSF structural health and safety management in the era of digitalization. According to the results, the existing research predominantly investigates certain TSF sensor technologies, such as wireless real-time monitoring, remote sensors (RS), unmanned aerial vehicles (UAVs), unmanned survey vessels (USVs), artificial intelligence (AI), cloud computing (CC), and Internet of Things (IoT) approaches, among others. These technologies stand out for their potential to improve the safety management monitoring of mine tailings, which is particularly significant in the context of climate change-related hazards, and to reduce the risk of TSF failures. They are recognized as emerging smart mining solutions with reliable, simple, scalable, secure, and competitive characteristics. © 2024 by the authors.},
	author = {Cacciuttolo, Carlos and Guzmán, Valentina and Catriñir, Patricio and Atencio, Edison},
	year = {2024},
}

@article{sumardi_effect_2024,
	title = {{THE} {EFFECT} {OF} {ISLAMIC} {ATTRIBUTES} {TO} {CONSUMER} {SATISFACTION}: {A} {META}-{ANALYSIS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191822486&doi=10.30892%2fgtg.53201-1215&partnerID=40&md5=f4f1dd059b68da375bbab257d3c39e60},
	abstract = {In the growing halal industry, there are differences of opinion among researchers about the effect of Islamic attributes on consumer satisfaction. Therefore, this paper aims to evaluate the effect of Islamic attributes on consumer satisfaction. The Prisma flow diagram indicated 23 papers and consists of 59 studies to analyze with JASP Software. The study identifies significant authors, dominant publishers, methodology, and theories commonly employed in this topic. The result proves that catering to Muslim needs through Islamic attributes can significantly enhance consumer satisfaction and th e presence of other variables as moderators will strengthen tourist satisfaction. © 2024 Editura Universitatii din Oradea. All rights reserved.},
	author = {Sumardi, Retno Santi and Mahomed, Anuar Shah Bali and Aziz, Yuhanis Ab},
	year = {2024},
	keywords = {diagram, hotel industry, Islamism, life satisfaction, meta-analysis, software},
}

@article{pretorius_when_2024,
	title = {When rationality meets intuition: {A} research agenda for software design decision-making},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189632288&doi=10.1002%2fsmr.2664&partnerID=40&md5=7a7dd1858ceb779f6079fb0c96876131},
	abstract = {As society's reliance on software systems escalates over time, so too does the cost of failure of these systems. Meanwhile, the complexity of software systems, as well as of their designs, is also ever-increasing, influenced by the proliferation of new tools and technologies to address intended societal needs. The traditional response to this complexity in software engineering and software architecture has been to apply rationalistic approaches to software design through methods and tools for capturing design rationale and evaluating various design options against a set of criteria. However, research from other fields demonstrates that intuition may also hold benefits for making complex design decisions. All humans, including software designers, use intuition and rationality in varying combinations. The aim of this article is to provide a comprehensive overview of what is known and unknown from existing research regarding the use and performance consequences of using intuition and rationality in software design decision-making. To this end, a systematic literature review has been conducted, with an initial sample of 3909 unique publications and a final sample of 26 primary studies. We present an overview of existing research, based on the literature concerning intuition and rationality use in software design decision-making and propose a research agenda with 14 questions that should encourage researchers to fill identified research gaps. This research agenda emphasizes what should be investigated to be able to develop support for the application of the two cognitive processes in software design decision-making. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Pretorius, Carianne and Razavian, Maryam and Eling, Katrin and Langerak, Fred},
	year = {2024},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Application programs, Software-systems, Behavioral research, Design decision-making, Intuition, Rationality, Research agenda, Software design decisions, Software Evolution, Software process},
}

@article{hannousse_twenty-two_2024,
	title = {Twenty-two years since revealing cross-site scripting attacks: {A} systematic mapping and a comprehensive survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191018882&doi=10.1016%2fj.cosrev.2024.100634&partnerID=40&md5=f261951f45b0df8972c9f9c044dc6651},
	abstract = {Cross-site scripting (XSS) is one of the major threats menacing the privacy of data and the navigation of trusted web applications. Since its disclosure in late 1999 by Microsoft security engineers, several techniques have been developed with the aim of securing web navigation and protecting web applications against XSS attacks. XSS has been and is still in the top 10 list of web vulnerabilities reported by the Open Web Applications Security Project (OWASP). Consequently, handling XSS attacks has become one of the major concerns of several web security communities. Despite the numerous studies that have been conducted to combat XSS attacks, the attacks continue to rise. This motivates the study of how the interest in XSS attacks has evolved over the years, what has already been achieved to prevent these attacks, and what is missing to restrain their prevalence. In this paper, we conduct a systematic mapping and a comprehensive survey with the aim of answering all these questions. We summarize and categorize existing endeavors that aim to handle XSS attacks and develop XSS-free web applications. The systematic mapping yielded 157 high-quality published studies. By thoroughly analyzing those studies, a comprehensive taxonomy is drawn out outlining various techniques used to prevent, detect, protect, and defend against XSS attacks and vulnerabilities. The study of the literature revealed a remarkable interest bias toward basic (84.71\%) and JavaScript (81.63\%) XSS attacks as well as a dearth of vulnerability repair mechanisms and tools (only 1.48\%). Notably, existing vulnerability detection techniques focus solely on single-page detection, overlooking flaws that may span across multiple pages. Furthermore, the study brought to the forefront the limitations and challenges of existing attack detection and defense techniques concerning machine learning and content-security policies. Consequently, we strongly advocate the development of more suitable detection and defense techniques, along with an increased focus on addressing XSS vulnerabilities through effective detection (hybrid solutions) and repair strategies. Additionally, there is a pressing need for more high-quality studies to overcome the limitations of promising approaches such as machine learning and content-security policies while also addressing diverse XSS attacks in different languages. Hopefully, this study can serve as guidance for both the academic and practitioner communities in the development of XSS-free web applications. © 2024 Elsevier Inc.},
	author = {Hannousse, Abdelhakim and Yahiouche, Salima and Nait-Hamoud, Mohamed Cherif},
	year = {2024},
	keywords = {Mapping, Machine learning, Machine-learning, Repair, Data privacy, Cross site scripting, Defense techniques, High quality, Network security, Security systems, Systematic mapping, WEB application, Web applications, WEB security, XSS attack, XSS vulnerability},
}

@article{silva_using_2024,
	title = {Using {Hypotheses} to {Manage} {Technical} {Uncertainty} and {Architecture} {Evolution} in a {Software} {Start}-up},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195806801&doi=10.1109%2fMS.2024.3383628&partnerID=40&md5=e5c7d0c12e832411fc2d35cc91e1d614},
	abstract = {This article presents the case of a start-up applying a technique named ArchHypo that uses hypotheses to express uncertainties related to the software architecture. Ten months after identifying the hypotheses, it was assessed how the usage of this technique impacted their decisions. © 1984-2012 IEEE.},
	author = {Silva, Kelson and Melegati, Jorge and Wang, Xiaofeng and Ferreira, Mauricio and Guerra, Eduardo},
	year = {2024},
	keywords = {Computer software, Uncertainty},
}

@article{khan_revolutionizing_2024,
	title = {Revolutionizing software developmental processes by utilizing continuous software approaches},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179315668&doi=10.1007%2fs11227-023-05818-8&partnerID=40&md5=b3d09e613cf9ddb415938502bdd55553},
	abstract = {The development of smart and innovative software applications in various disciplines has inspired our lives by providing various cutting-edge technologies spanning from online to smart and efficient systems. The proliferation of innovative internet-enabled tools has transformed the nation into a globalized world where individuals can participate on various platforms, collaborate in activities, communicate on issues, and exchange information safely and consistently. Coordination and cooperation are essential in software development. It gathers all software developers in one space, encouraging them to discuss goals and work rationally to accomplish the project goal. In recent years, continuous software development and deployment have become increasingly common in software engineering. Continuous software engineering (CSE) is a method that involves a variety of strategies to increase the regularity of novel and modified software versions. CSE enables a continuous learning and improvement process through rapid software update iteration by combining continuous integration and delivery. Continuous integration is a method that has arisen in order to remove gaps between development and deployment. Software engineers must handle uncertainty and alter stakeholders' requirements, which is possible through continuous software developmental strategies that manage the overall software cycle and produce high-quality software applications. The proposed study is a systematic review related to continuous software development and deployment and focuses on achieving four aims: (1) To explore the impacts of continuous development on software, (2) to pinpoint various tools used to carry out this process, (3) to highlight the challenges faced in adopting continuous approaches for development and (4) to analyze the phases of continuous software engineering. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	author = {Khan, Habib Ullah and Afsar, Waseem and Nazir, Shah and Noor, Asra and Kundi, Mahwish and Maashi, Mashael and Alshahrani, Haya Mesfer},
	year = {2024},
	keywords = {Software design, Application programs, Computer software reusability, Automation, Automated systems, Continuous integrations, Continuous software, Continuous software engineerings, Cutting edge technology, Iterative methods, Online systems, Reusable softwares, Software applications, Software approach, Software deployment, Software upgradation},
}

@article{taskeen_research_2023,
	title = {A research landscape on software defect prediction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150786328&doi=10.1002%2fsmr.2549&partnerID=40&md5=d3fda2148e5228abe618078eb6140777},
	abstract = {Software defect prediction is the process of identifying defective files and modules that need rigorous testing. In the literature, several secondary studies including systematic reviews, mapping studies, and review studies have been reported. However, no research work such as a tertiary study that combines secondary studies has focused on providing a landscape of software defect prediction useful to understand the body of knowledge. Motivated by this, we intend to perform a tertiary study by following a systematic literature review protocol to provide a research landscape of the targeted domain. We synthesize the quality of the secondary studies and investigate the employed techniques and the performance evaluation measures for evaluating the software defect prediction model. Furthermore, this study aims at exploring different datasets employed in the reported experimentation. Moreover, the current study intends at highlighting the research trends, gaps, and opportunities in the targeted research domain. The results indicate that none of the reported defect prediction techniques can be regarded as the best; however, the reported techniques performed better in different testing situations. In addition, machine learning (ML)-based techniques perform better than traditional statistical techniques mainly due to the potential of discovering the defects and generating generalized results. Moreover, the obtained results highlight the need for further work in the domain of ML-based techniques. Furthermore, publicly available datasets should be considered for experimentation or replication purposes. The potential future work can focus on data quality, ethical ML, cross-project defect prediction, early defect prediction process, class imbalance problem, and model overfitting. © 2023 John Wiley \& Sons Ltd.},
	author = {Taskeen, Anam and Khan, Saif Ur Rehman and Felix, Ebubeogu Amarachukwu},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, Software testing, Mapping, Systematic mapping studies, Tertiary study, Machine-learning, Body of knowledge, Defect prediction, Defects, Forecasting, Mapping studies, Performances evaluation, Software defect prediction},
}

@article{rodriguez_ux_2023,
	title = {{UX} debt in an agile development process: evidence and characterization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173738867&doi=10.1007%2fs11219-023-09652-2&partnerID=40&md5=d1b89ddbc5b6fa37cb681b4f6d464e97},
	abstract = {The metaphor of technical debt (TD) has generated a conceptual framework on factors that weaken the quality of software and accumulate a repair cost. However, user-related aspects like user experience (UX) receive little consideration among TD types, for reasons like the substantial focus on code TD, some dynamics inherent to agile processes, and an apparent lack of cumulative cost over time. This article has two main goals: first, to present evidence of the existence of UXDebt as a type of TD, with a cumulative cost for the development team as well as stakeholders; second, to propose a definition and characterization of UXDebt that may serve as a frame for further research on methods and tools for continuous management within agile processes. For the first goal, we have compiled evidence on the current state of UXDebt from three sources: a literature review, a survey among software engineering professionals in agile teams, and the analysis of UX issues in GitHub. All sources have evidenced some form of UXDebt; surveyed practitioners have recognized its poor management with a cost for the entire team that accumulates over time. Moreover, issue tracking systems allow to visualize and measure a technical form of UXDebt. For the second goal, we have defined a conceptual model that characterizes UXDebt in terms of both technical and non-technical aspects. On the technical side, we propose the notion of UX smells which allows us to discuss concrete management activities. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Rodriguez, Andres and Gardey, Juan Cruz and Grigera, Julian and Rossi, Gustavo and Garrido, Alejandra},
	year = {2023},
	keywords = {Software engineering, Development process, A/B testing, Users' experiences, Agile development, Agile process, Cumulative cost, Human resource management, Odors, Refactorings, Technical debts, User testing, UX smell},
}

@inproceedings{unkelos-shpigel_revise_2023,
	title = {Revise {That} {Again}: {Are} {You} {Motivated}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180004904&doi=10.1145%2f3617553.3617885&partnerID=40&md5=ff5086d61dda21c9c557267e97ae7636},
	abstract = {Requirements engineering (RE) presents several challenges stemming from the required collaboration and knowledge transfer between analysists, developers, and customers. Motivation theories have been used occasionally to analyze and encourage motivation and engagement of stakeholders in RE tasks. In recent years, gamification techniques have been used in software engineering tasks, and specifically, in RE tasks in order to promote stakeholder engagement. However, existing research works seldom offer a rigorous method for designing gamification environments for RE tasks. This paper describes a socio-Technical environment, which was built for requirements elicitation and specification. This environment allows researchers and team managers to decide on different mechanisms to gamify the current RE task in practice. The environment was evaluated by experts and was further tested with the participation of students in two proof of concept studies for demonstrating its functionality, yielding some anecdotic results. © 2023 ACM.},
	author = {Unkelos-Shpigel, Naomi and Berencwaig, Barak and Kas, Sharon},
	year = {2023},
	keywords = {Software engineering, Requirement engineering, Requirements engineering, Knowledge management, Human resource management, Engagement, Engineering tasks, Gamification, Knowledge transfer, Motivation, Motivation and engagements, Motivation theories, Sociotechnical, Stakeholder engagement, Technical environments},
}

@article{ros_theory_2024,
	title = {A theory of factors affecting continuous experimentation ({FACE})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179585225&doi=10.1007%2fs10664-023-10358-z&partnerID=40&md5=c9efd3f16565fe8e41091c8792e51a17},
	abstract = {Context: Continuous experimentation (CE) is used by many companies with internet-facing products to improve their business models and software solutions based on user data. Some companies deliberately adopt a systematic experiment-driven approach to software development while some companies use CE in a more ad-hoc fashion. Objective: The goal of this study is to identify factors for success in CE that explain the variations in the utility and efficacy of CE between different companies. Method: We conducted a multi-case study of 12 companies involved with CE and performed 27 interviews with practitioners at these companies. Based on that empirical data, we then built a theory of factors at play in CE. Results: We introduce a theory of Factors Affecting Continuous Experimentation (FACE). The theory includes three factors, namely 1) processes and infrastructure for CE, 2) the user problem complexity of the product offering, and 3) incentive structures for CE. The theory explains how these factors affect the effectiveness of CE and its ability to achieve problem-solution and product-market fit. Conclusions: Our theory may inspire practitioners to assess an organisation’s potential for adopting CE and to identify factors that pose challenges in gaining value from CE practices. Our results also provide a basis for defining practitioner guidelines and a starting point for further research on how contextual factors affect CE and how these may be mitigated. © 2023, The Author(s).},
	author = {Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
	year = {2024},
	keywords = {A/B testing, Business software, Case-studies, Continuous experimentation, Data driven, Data-driven development, Empirical research, Facing products, Multi-case study, Theory building},
}

@article{erat_emotion_2024,
	title = {Emotion recognition with {EEG}-based brain-computer interfaces: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186412474&doi=10.1007%2fs11042-024-18259-z&partnerID=40&md5=da6a586436c707ff0124ec5644173527},
	abstract = {Electroencephalography (EEG)-based Brain-Computer Interface (BCI) systems for emotion recognition have the potential to assist the enrichment of human–computer interaction with implicit information since they can enable understanding of the cognitive and emotional activities of humans. Therefore, these systems have become an important research topic today. This study aims to present trends and gaps on this topic by performing a systematic literature review based on the 216 published scientific literature gathered from various databases including ACM, IEEE Xplore, PubMed, Science Direct, and Web of Science from 2016 to 2020. This review gives an overview of all the components of EEG based BCI system from the signal stimulus module which includes the employed device, signal stimuli, and data processing modality, to the signal processing module which includes signal acquisition, pre-processing, feature extraction, feature selection, classification algorithms, and performance evaluation. Thus, this study provides an overview of all components of an EEG-based BCI system for emotion recognition and examines the available evidence in a clear, concise, and systematic way. In addition, the findings are aimed to inform researchers about the issues on what are research trends and the gaps in this field and guide them in their research directions. © The Author(s) 2024.},
	author = {Erat, Kübra and Şahin, Elif Bilge and Doğan, Furkan and Merdanoğlu, Nur and Akcakaya, Ahmet and Durdu, Pınar Onay},
	year = {2024},
	keywords = {Systematic literature review, Affective brain-computer interface, Affective Computing, Biomedical signal processing, Brain computer interface, Classification (of information), Cognitive systems, Data handling, Electroencephalography, Electrophysiology, Emotion, Emotion recognition, Emotion Recognition, Feature extraction, Implicit informations, Interface system, Research topics, Scientific literature, Speech recognition, Web of Science},
}

@book{bratianu_knowledge_2024,
	title = {Knowledge translation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190356698&doi=10.1108%2f9781803828893&partnerID=40&md5=7ecf8b9fc80a0ff97fa83a5e6a870d8d},
	abstract = {Knowledge translation is a topic that originated in the field of health sciences where the need to move research to practice is of critical importance. In parallel, the field of knowledge sciences has developed a research base around knowledge transfer, knowledge sharing, knowledge exchange, knowledge articulation and knowledge absorption. This book brings these two important tracks together and synthesizes the fragmented literatures. It also draws from essential work on human communication and considers how these concepts are affected by the knowledge economy. The book raises awareness of the critical role that knowledge translation plays in every academic field of study, and in everyday life. To demonstrate this role, the book presents a grounding model that readers can use to better see their knowledge translation challenges and opportunities. Drawing on the author teams experience in a range of domains and sectors, the book explores knowledge translation in the fields of manufacturing, infectious diseases, automated call centers, regulatory development and compliance, financial lending, transportation safety, and doctor-patient discourse. All rights reserved.},
	author = {Bratianu, Constantin and Garcia-Perez, Alexeis and Dal Mas, Francesca and Bedford, Denise},
	year = {2024},
}

@inproceedings{costa_testing_2024,
	title = {Testing on {Dynamically} {Adaptive} {Systems}: {Challenges} and {Trends}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193917885&doi=10.5220%2f0012555900003690&partnerID=40&md5=15527034acfbfd71c1daf5ea77036493},
	abstract = {Dynamically Adaptive Systems (DAS) are systems capable of modifying themselves automatically according to the surrounding environment. Traditional testing approaches are ineffective for these systems due to their dynamic aspects, making fault detection complex. Although various testing approaches have been proposed for DASs, there is no up-to-date overview of the approaches, challenges, and trends. This research therefore presents the results of a systematic literature review to identify the challenges, approaches and trends in testing dynamically adaptable systems. For this objective, 25 articles between 2020 and 2023 were analyzed to answer our research questions. As a result, approaches and their characteristics were identified, such as what type of system they can be applied to, what activity is included in the testing process, and at what level of testing. We also highlighted challenges that are still being faced and trends in testing dynamically adaptive systems. For a more in-depth analysis of the results related to the challenges, grounded theory procedures were applied to organize them and encourage future research that seeks to overcome and mitigate them. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	author = {Costa, Isabely do Nascimento and Santos, Ismayle S. and Andrade, Rossana M.C.},
	year = {2024},
	keywords = {Systematic literature review, Systematic Review, Software testing, Adaptable system, Adaptive systems, Dynamic aspects, Dynamically adaptive systems, Fault detection, Faults detection, Research questions, Software testings, Surrounding environment, Testing process},
}

@article{barcelos_requirements_2024,
	title = {Requirements engineering in industry 4.0: {State} of the art and directions to continuous requirements engineering},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186634667&doi=10.1002%2fsys.21753&partnerID=40&md5=f62bda5bb7a47f95a40a286270f546c2},
	abstract = {The 4th Industrial Revolution, also known as Industry 4.0, intends to transform manufacturing processes into smart factories with full digitalization and intelligent, decentralized, and flexible production. In this scenario, Industry 4.0 systems (i.e., software-intensive systems that automate smart factories) have required rigorous and continuous development, but smart factory companies often have difficulty dealing with Requirements Engineering (RE) where requirements continuously change and emerge at runtime to support the changeability of complex production processes. Such requirements encompass engineering (e.g., mechanical, electrical, electronic, production/manufacturing) and business areas and involve the vertical and horizontal integration of heterogeneous manufacturing systems. There is also a lack of a panorama of how Industry 4.0 projects have performed with RE activities. The main goal of this paper is to present the state-of-the-art research concerning RE in Industry 4.0 and draw attention to the next most urgent steps. For this, we selected and examined studies that address RE for Industry 4.0, noting that much of this literature is recent but does not fully address the complexity and dynamism of the requirements for Industry 4.0. Grounded on these studies and our academic and industry experience, we highlight the need for Continuous Requirements Engineering (CRE) for Industry 4.0. Significance and Practitioner Points: The main implications of this paper are: (i) For researchers: It offers the state of the art of RE in the context of Industry 4.0 and points out several important open issues that require an urgent investigation through new research topics; and (ii) For practitioners: It provides directions for new or even existing Industry 4.0 projects on how to deal with RE activities aiming to overcome the several challenges to perform them. © 2024 Wiley Periodicals, Inc.},
	author = {Barcelos, Leonardo Vieira and Antonino, Pablo Oliveira and Nakagawa, Elisa Yumi},
	year = {2024},
	keywords = {State of the art, Requirement engineering, Requirements engineering, Engineering activities, Continuous requirement engineering, Decentralized production, Flexible production, Industrial revolutions, Industry 4.0, Manufacturing process, Smart factory},
}

@inproceedings{nayak_gdpr_2024,
	title = {{GDPR} {Compliant} {ChatGPT} {Playground}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196073971&doi=10.1109%2fICETCS61022.2024.10543557&partnerID=40&md5=abdead7dc470268c7eadcc3ac9c14abb},
	abstract = {ChatGPT is an AI based conversational tool developed by OpenAPI on the design principles of Large Language Model (LLM) and a publicly accessible tool. ChatGPT has increasingly becoming popular tool for enabling applications involving interactive and contextual search across corporate companies, profit/non-profit organizations, educational/medical institutions, and researcher's community to name a few.The corporate companies are abided by GDPR (General Data Protection Regulation) compliance checks and restricted to share confidential, personal or sensitive information's (hereinafter referred as critical data) into public domains. As ChatGPT server is hosted outside corporate boundaries, to fulfil GDPR compliance check, the corporate companies must have necessary systems in place of any data leaving outside of corporate boundaries.We are proposing a novel solution in identifying GDPR noncompliant DPP (Data Privacy and Protection) entities from the prompt query given to ChatGPT. To achieve this, from the corporate documents we first manually tag critical entities from 'named entity tagging tool' in building the corporate specific DPP entities knowledgebase, build custom NER (Named Entity Recognition) model on top of prebuilt corporate specific DPP entities knowledgebase, an ChatGPT playground interface to accept any user's prompt query, before firing the query to ChatGPT we validate the user's prompt query against custom NER model to detect if any corporate specific DPP entities are present, warn the user by highlighting the corporate specific DPP entities if present to facilitate user in negating the same, we enabled feedback loop from the user for the highlighted corporate specific DPP entities to improvise the custom NER model and logging all the input prompt queries fired to ChatGPT to enable corporate auditing process by using techniques from Natural Language Processing (NLP), Information Extraction, Information Retrieval (IR) and Custom NER model. © 2024 IEEE.},
	author = {Nayak, Shiva Prasad and Pasumarthi, Suresh and Rajagopal, Bharathi and Verma, Ashwani Kumar},
	year = {2024},
	keywords = {Natural language processing systems, Natural languages, ChatGPT, Compliance control, Custom named entity recognition model, General data protection regulations, Information extraction, Information retrieval, Language model, Language processing, Large language model, Named entity recognition, Natural language processing, Recognition models, Sensitive data},
}

@article{rainer_reporting_2024,
	title = {Reporting case studies in systematic literature studies—{An} evidential problem},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195473840&doi=10.1016%2fj.infsof.2024.107501&partnerID=40&md5=b030b447b2cf73523aec750252a31d2a},
	abstract = {Context: The term and label, “case study”, is not used consistently by authors of primary studies in software engineering research. It is not clear whether this problem also occurs for systematic literature studies (SLSs). Objective: To investigate the extent to which SLSs in/correctly use the term and label, “case study”, when classifying primary studies. Methods: We systematically collect two sub-samples (2010–2021 \& 2022) comprising a total of eleven SLSs and 79 primary studies. We examine the designs of these SLSs, and then analyse whether the SLS authors and the primary-study authors correctly label the respective primary study as a “case study”. Results: 76\% of the 79 primary studies are misclassified by SLSs (with the two sub-samples having 60\% and 81\% misclassification, respectively). For 39\% of the 79 studies, the SLSs propagate a mislabelling by the original authors, whilst for 37\%, the SLSs introduce a new mislabel, thus making the problem worse. SLSs rarely present explicit definitions for “case study” and when they do, the definition is not consistent with established definitions. Conclusions: SLSs are both propagating and exacerbating the problem of the mislabelling of primary studies as “case studies”, rather than – as we should expect of SLSs – correcting the labelling of primary studies, and thus improving the body of credible evidence. Propagating and exacerbating mislabelling undermines the credibility of evidence in terms of its quantity, quality and relevance to both practice and research. © 2024 The Author(s)},
	author = {Rainer, Austen and Wohlin, Claes},
	year = {2024},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Systematic mapping studies, Literature studies, Case-studies, Credible evidence, Labelings, Misclassifications, Software engineering research, Sub-samples},
}

@article{waqas_using_2024,
	title = {Using {LowCode} and {NoCode} {Tools} in {DevOps}: {A} {Multivocal} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186469250&doi=10.1007%2f978-3-031-50590-4_5&partnerID=40&md5=c5ae8e1a062bce7898965cd9cbe9abcd},
	abstract = {DevOps and Low-code/No-code tools are two trends that are gaining interest in both academic and industry environments. This study analyzed the usage of low-code and no-code tools in DevOps. It examined the available tools, their applications, and the associated concerns or limitations. To do so, a Multivocal Literature Review (MVLR) was conducted to include academic and grey literature. The results reveal the utilization of a range of tools across multiple aspects of DevOps, including application delivery, workflow automation, process management, continuous integration, deployment, monitoring and logging. Despite their potential benefits, concerns and limitations such as script standardization, security risks, limited customization, vendor lock-in, and scalability issues persist. This review, alongside identified concerns, emphasizes the increasing adoption and benefits of Low-code/No-code tools in DevOps. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	author = {Waqas, Muhammad and Ali, Zohaib and Sánchez-Gordón, Mary and Kristiansen, Monica},
	year = {2024},
}

@article{lin_shape_2024,
	title = {Shape or size matters? {Towards} standard reporting of tensile testing parameters for human soft tissues: systematic review and finite element analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190110729&doi=10.3389%2ffbioe.2024.1368383&partnerID=40&md5=481782653b6ae903cfe0c77a6615b1b3},
	abstract = {Material properties of soft-tissue samples are often derived through uniaxial tensile testing. For engineering materials, testing parameters (e.g., sample geometries and clamping conditions) are described by international standards; for biological tissues, such standards do not exist. To investigate what testing parameters have been reported for tensile testing of human soft-tissue samples, a systematic review of the literature was performed using PRISMA guidelines. Soft tissues are described as anisotropic and/or hyperelastic. Thus, we explored how the retrieved parameters compared against standards for engineering materials of similar characteristics. All research articles published in English, with an Abstract, and before 1 January 2023 were retrieved from databases of PubMed, Web of Science, and BASE. After screening of articles based on search terms and exclusion criteria, a total 1,096 articles were assessed for eligibility, from which 361 studies were retrieved and included in this review. We found that a non-tapered shape is most common (209 of 361), followed by a tapered sample shape (92 of 361). However, clamping conditions varied and were underreported (156 of 361). As a preliminary attempt to explore how the retrieved parameters might influence the stress distribution under tensile loading, a pilot study was performed using finite element analysis (FEA) and constitutive modeling for a clamped sample of little or no fiber dispersion. The preliminary FE simulation results might suggest the hypothesis that different sample geometries could have a profound influence on the stress-distribution under tensile loading. However, no conclusions can be drawn from these simulations, and future studies should involve exploring different sample geometries under different computational models and sample parameters (such as fiber dispersion and clamping effects). Taken together, reporting and choice of testing parameters remain as challenges, and as such, recommendations towards standard reporting of uniaxial tensile testing parameters for human soft tissues are proposed. Copyright © 2024 Lin, Pirrung, Niestrawska, Ondruschka, Pinter, Henyš and Hammer.},
	author = {Lin, Alvin C. and Pirrung, Felix and Niestrawska, Justyna A. and Ondruschka, Benjamin and Pinter, Gerald and Henyš, Petr and Hammer, Niels},
	year = {2024},
	keywords = {Aspect ratio, Aspect-ratio, Bone, Dogbone, Dumbbell, Finite element method, Geometry, Histology, Human soft tissue, ISO material testing standard, ISO Standards, Load testing, Material testing, Non-tapered, Rectangular, Soft tissue, Stress analysis, Stress concentration, Tapered, Tensile stress, Tensile testing, Testing standards, Tissue engineering},
}

@article{cunha_insight_2024,
	title = {An insight into the capabilities of professionals and teams in agile software development: {An} update of the systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186425034&doi=10.24138%2fjcomss-2023-0172&partnerID=40&md5=0fb0b3eb775f0b92eb3fb70e8c650c20},
	abstract = {Agile Software Development (ASD) confronts the challenge of effectively measurement and predicting the capabilities of software engineers and teams to improve individual performance, team efficiency, and project success. This study delves into exploring and identifying gaps and research prospects in assessing and predicting human capabilities within ASD. Thus, we conducted a Systematic Literature Review, building upon a prior review from 2001 to 2016 by different authors. To encompass primary studies published after 2016, we extended to 2022. Further, our study extends the scope of the previous SLR with a new research question to identify key attributes in publications focused on agile team formation. Our findings disclosed new attributes for evaluating and predicting the capabilities of professionals engaged in ASD, such as Openness to Creativity and Agile Adaptation. These attributes boost individual performance, contribute to ameliorated team productivity, and facilitate the precise composition of teams. Moreover, this study expands our prior study, providing more details on capability identification and research design, extends the analysis of attributes and prediction models, provides a more granular discussion of discoveries and comparisons with prior review, and more indepth discussion about practical implications and thoroughly examines study validity. We observed that technical metrics were more prevalent than social and innovative aspects. Also, the study identified the prediction of agile capabilities as an emerging research domain necessitating further scrutiny due to the scarcity of existing models. The majority of studies (78\%) supplied detailed metric descriptions, facilitating the evolution of the capabilities repository and supporting future investigations in this domain. Ultimately, these findings can aid agile practitioners in formulating team composition decisions based on individuals’ and teams’ appraised and foreseen abilities. © 2024, Croatian Communications and Information Society. All rights reserved.},
	author = {Cunha, Felipe and Perkusich, Mirko and Guimaraes, Everton and Santos, Ramon and Rique, Thiago and Albuquerque, Danyllo and Perkusich, Angelo and Almeida, Hyggo and Gorgônio, Kyller Costa},
	year = {2024},
}

@inproceedings{tseng_unlocking_2024,
	title = {Unlocking the {Potential} of {Open} {Government} {Data}: {Exploring} the {Strategic}, {Technical}, and {Application} {Perspectives} of {High}-{Value} {Datasets} {Opening} in {Taiwan}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195261385&doi=10.1145%2f3657054.3657089&partnerID=40&md5=39e4f20be1e5ec31effab47f6affbaed},
	abstract = {Today, data has an unprecedented value as it forms the basis for data-driven decision-making, including serving as an input for AI models, where the latter is highly dependent on the availability of the data. However, availability of data in an open data format creates a little added value, where the value of these data, i.e., their relevance to the real needs of the end user, is key. This is where the concept of “high-value dataset” (HVD) comes into play, which has become popular in recent years. Defining and opening HVD is a process consisting of a set of interrelated steps, the implementation of which may vary from one country or region to another. Therefore, there has recently been a call to conduct research in a country or region setting considered to be of greatest national value. So far, only a few studies have been conducted at the regional or national level, most of which consider only one step of the process, such as identifying HVD or measuring their impact. With this study, we answer this call and examine the national case of Taiwan by exploring the entire lifecycle of HVD opening. As such, the aim of the paper is to understand and evaluate the life cycle of high-value dataset publishing in one of the world’s leading producers of information and communication technology (ICT) products - Taiwan. To do this, we conduct a qualitative study with exploratory interviews with representatives from government agencies in Taiwan responsible for HVD opening, exploring the entire HVD lifecycle. As such, we examine (1) strategic aspects related to the HVD determination process, (2) technical aspects, and (3) application aspects. © 2024 Copyright held by the owner/author(s).},
	author = {Tseng, Hsien-Lee and Nikiforova, Anastasija},
	year = {2024},
	keywords = {Ecosystems, Decision making, Life cycle, e-government, High-value dataset, High-value dataset”, Impact, OGD, Open Data, Open data ecosystem, Open datum, Open government data, Public data, Public data ecosystem, Public values, SDG, Sustainable development, Sustainable development goal},
}

@inproceedings{chen_murs_2023,
	title = {{MuRS}: {Mutant} {Ranking} and {Suppression} using {Identifier} {Templates}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180550494&doi=10.1145%2f3611643.3613901&partnerID=40&md5=9ee43fb4c7f4d5a78da0b97896dcccb4},
	abstract = {Diff-based mutation testing is a mutation testing approach that only mutates lines affected by a code change under review. This approach scales independently of the code-base size and introduces test goals (mutants) that are directly relevant to an engineer's goal such as fixing a bug, adding a new feature, or refactoring existing functionality. Google's mutation testing service integrates diff-based mutation testing into the code review process and continuously gathers developer feedback on mutants surfaced during code review. To enhance the developer experience, the mutation testing service uses a number of manually-written rules that suppress not-useful mutants - mutants that have consistently received negative developer feedback. However, while effective, manually implementing suppression rules requires significant engineering time. This paper proposes and evaluates MuRS, an automated approach that groups mutants by patterns in the source code under test and uses these patterns to rank and suppress future mutants based on historical developer feedback on mutants in the same group. To evaluate MuRS, we conducted an A/B testing study, comparing MuRS to the existing mutation testing service. Despite the strong baseline, which uses manually-written suppression rules, the results show a statistically significantly lower negative feedback ratio of 11.45\% for MuRS versus 12.41\% for the baseline. The results also show that MuRS is able to recover existing suppression rules implemented in the baseline. Finally, the results show that statement-deletion mutant groups received both the most positive and negative developer feedback, suggesting a need for additional context that can distinguish between useful and not-useful mutants in these groups. Overall, MuRS is able to recover existing suppression rules and automatically learn additional, finer-grained suppression rules from developer feedback. © 2023 Owner/Author.},
	author = {Chen, Zimin and Salawa, Małgorzata and Vijayvergiya, Manushree and Petrović, Goran and Ivanković, Marko and Just, René},
	year = {2023},
	keywords = {Software testing, Refactorings, Automated approach, Code changes, Code review, Codes (symbols), Developer feedback, Engineering time, Google+, Mutation testing, Review process, Source codes, Verification},
}

@article{bourai_deep_2024,
	title = {Deep learning-assisted medical image compression challenges and opportunities: systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191883296&doi=10.1007%2fs00521-024-09660-8&partnerID=40&md5=13b002c543c49b3905dbde4e4d088646},
	abstract = {Over the preceding decade, there has been a discernible surge in the prominence of artificial intelligence, marked by the development of various methodologies, among which deep learning emerges as a particularly auspicious technique. The captivating attribute of deep learning, characterised by its capacity to glean intricate feature representations from data, has served as a catalyst for pioneering approaches and methodologies spanning a multitude of domains. In the face of the burgeoning exponential growth in digital medical image data, the exigency for adept image compression methodologies has become increasingly pronounced. These methodologies are designed to preserve bandwidth and storage resources, thereby ensuring the seamless and efficient transmission of data within medical applications. The critical nature of medical image compression accentuates the imperative to confront the challenges precipitated by the escalating deluge of medical image data. This review paper undertakes a comprehensive examination of medical image compression, with a predominant focus on sophisticated, research-driven deep learning techniques. It delves into a spectrum of approaches, encompassing the amalgamation of deep learning with conventional compression algorithms and the application of deep learning to enhance compression quality. Additionally, the review endeavours to explicate these fundamental concepts, elucidating their inherent characteristics, merits, and limitations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	author = {Bourai, Nour El Houda and Merouani, Hayet Farida and Djebbar, Akila},
	year = {2024},
	keywords = {Systematic Review, Deep learning, Learning systems, ]+ catalyst, 3d predictor, Digital storage, Exponential growth, Feature representation, Image compression, Lossless compression, Lossy compressions, Medical applications, Medical image compression, Medical images datum, Medical imaging},
}

@article{krafft_black-box_2024,
	title = {Black-{Box} {Testing} and {Auditing} of {Bias} in {ADM} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194259767&doi=10.1007%2fs11023-024-09666-0&partnerID=40&md5=c0ad5488881bfd7adbca316b7f704c7c},
	abstract = {For years, the number of opaque algorithmic decision-making systems (ADM systems) with a large impact on society has been increasing: e.g., systems that compute decisions about future recidivism of criminals, credit worthiness, or the many small decision computing systems within social networks that create rankings, provide recommendations, or filter content. Concerns that such a system makes biased decisions can be difficult to investigate: be it by people affected, NGOs, stakeholders, governmental testing and auditing authorities, or other external parties. Scientific testing and auditing literature rarely focuses on the specific needs for such investigations and suffers from ambiguous terminologies. With this paper, we aim to support this investigation process by collecting, explaining, and categorizing methods of testing for bias, which are applicable to black-box systems, given that inputs and respective outputs can be observed. For this purpose, we provide a taxonomy that can be used to select suitable test methods adapted to the respective situation. This taxonomy takes multiple aspects into account, for example the effort to implement a given test method, its technical requirement (such as the need of ground truth) and social constraints of the investigation, e.g., the protection of business secrets. Furthermore, we analyze which test method can be used in the context of which black box audit concept. It turns out that various factors, such as the type of black box audit or the lack of an oracle, may limit the selection of applicable tests. With the help of this paper, people or organizations who want to test an ADM system for bias can identify which test methods and auditing concepts are applicable and what implications they entail. © The Author(s) 2024.},
	author = {Krafft, Tobias D. and Hauer, Marc P. and Zweig, Katharina},
	year = {2024},
	keywords = {Decision making, Social networking (online), Taxonomies, Machine learning, Machine-learning, Behavioral research, Algorithmics, Auditing, Automated decision making, Bias, Black boxes, Black-box testing, Computing system, Decision-making systems, Small decisions, Social sciences computing, Test method},
}

@inproceedings{alshehhi_6dvf_2024,
	title = {{6DVF}: {A} {Framework} for the {Development} and {Evaluation} of {Mobile} {Data} {Visualisations}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193993437&doi=10.5220%2f0012692900003687&partnerID=40&md5=1273a2dbb4880051c8da6756c713353f},
	abstract = {Mobile apps, in particular tracking apps, rely heavily on data visualisations to empower end-users to make decisions about their health, sport, finance, household, and more. This has prompted app designers and developers to invest more effort in delivering quality visualisations. Many frameworks, including the Visualisation and Design Framework and Google Material Design, have been developed to guide the creation of informative and well-designed charts. However, our study reveals the need to incorporate additional aspects in the design process of such data visualisations to address user characterisation and needs, the nature of data to visualise, and the experience on small smart screens. In this paper, we introduce the Six-Dimensions Data Visualization Framework (6DVF), specifically designed for data visualisation on mobile devices. The 6DVF encompasses user characteristics and needs, data attributes, chart styling, interaction, and the mobile environment. We conducted two evaluation studies to measure the effectiveness and practicality of our 6DVF in guiding designers, pinpointing areas for improvement—especially in data completeness and usability for end-users. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	author = {Alshehhi, Yasmeen Anjeer and Ahmad, Khlood and Abdelrazek, Mohamed and Bonti, Alessio},
	year = {2024},
	keywords = {End-users, Design, Data visualization, Evaluation methods, Human-centered computing, Mobile app, Mobile data, Visualization, Visualization design guideline, Visualization designs, Visualization evaluation method, Visualization framework, Visualization technique},
}

@article{ramadhan_combining_2024,
	title = {Combining intelligent tutoring systems and gamification: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167368098&doi=10.1007%2fs10639-023-12092-x&partnerID=40&md5=43507a0e5e0cd0e0cd9b55014c46e64e},
	abstract = {One of the Information and Communication Technology (ICT) developments used in the learning process is the Intelligent Tutoring System (ITS), and gamification can overcome boredom, lack of interest or motivation, and monotony when using the ITS. In this study, the application of ITS equipped with Gamification is called ITS + G. Currently, several studies have built the ITS + G. However, there has not been a Systematic Literature Review (SLR) that synthesizes the characteristics of the ITS and Gamification combination. Several previous SLRs have been carried out and discussed the ITS only and several other SLRs discussed gamification only. Therefore, this SLR focused on the characteristics of ITS and gamification as a unit. This study succeeded in synthesizing that ITS + G has the potential to be applied to both STEM and non-STEM subjects. Three main game elements are mostly used in ITS + G: levels, points, and progress bars, which are supported for several reasons. Several techniques that have been used to measure the success of ITS + G are synthesized. Several positive impacts of ITS + G are revealed. Some negative impacts that need to be considered and studied in future research are also noticed. The results of this study could be the basis for ITS + G research in the future and increase the repertoire of knowledge related to ITS and Gamification. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	author = {Ramadhan, Arief and Warnars, Harco Leslie Hendric Spits and Razak, Fariza Hanis Abdul},
	year = {2024},
}

@article{russo_towards_2024,
	title = {Towards a {Comprehensive} {Framework} for the {Multidisciplinary} {Evaluation} of {Organizational} {Maturity} on {Business} {Continuity} {Program} {Management}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152037415&doi=10.1080%2f19393555.2023.2195577&partnerID=40&md5=d074612c8a1ed3cb1576ba883e6469fd},
	abstract = {Organizational dependency on Information and Communication Technology (ICT) drives the preparedness challenge to cope with business process disruptions. Business Continuity Management (BCM) encompasses effective planning to enable business functions to resume to an acceptable state of operation within a defined timeframe. This paper presents a systematic literature review that communicates the strategic guidelines to streamline the organizational processes in the BCM program, culminating in the Business Continuity Plan design, according to the organization’s maturity. The systematic literature review methodology follows the Evidence-Based Software Engineering protocol assisted by the Parsifal tool, using the EbscoHost, ScienceDirect, and Scopus databases, ranging from 2000 to February 2021. International Standards and Frameworks guide the BCM program implementation, however, there is a gap in communicating metrics and what needs to be measured in the BCM program. The major paper result is the confirmation of the identified gap, through the analysis of the studies that, according to the BCM components, report strategic guidelines to streamline the BCM program. The analysis quantifies and discusses the contribution of the studies on each BCM component to design a framework supported by metrics, that allows assessing the organization’s preparedness in each BCM component, focusing on Information Systems and ICT strategies. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	author = {Russo, Nelson and Reis, Leonilde and Silveira, Clara and Mamede, Henrique S.},
	year = {2024},
	keywords = {Software engineering, Information management, Systematic literature review, Information systems, Information use, Business continuity, Business continuity management, Business continuity plans, Information and Communication Technologies, Management components, Management projects, Organisational, Organizational maturity, Program management},
}

@article{cannon_exploring_2024,
	title = {Exploring {Knowledge}-{Based} {Systems} for {Commercial} {Mortgage} {Underwriting}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181982238&doi=10.1007%2f978-3-031-50385-6_9&partnerID=40&md5=d52fdc852e47217b293e25ba18fad7c1},
	abstract = {While the residential mortgage industry has benefited from automated mortgage applications and underwriting, the commercial mortgage industry still relies heavily on manual underwriting. The paper aims to present the state of research in the domain of knowledge-based systems (KBS) and commercial mortgage underwriting and to identify the challenges in commercial mortgage underwriting by conducting a systematic literature review (SLR). The SLR uses the review process outlined by Kitchenham and includes a hybrid coding approach to analyze the data. The paper finds that KBS and ontologies to automate commercial mortgage underwriting were not studied yet. It also identifies several challenges in mortgage underwriting in data, process, and underwriting categories. The findings of this paper can be used for further research in the field of commercial mortgage underwriting and KBS. KBS appear suitable to address the identified challenges and can be integrated into information system automation or artificial intelligence (AI) implementations for commercial mortgage underwriting. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Cannon, K. Patricia and Preis, Simon J.},
	year = {2024},
	keywords = {Systematic literature review, State of research, Review process, Commercial mortgage underwriting, Domain of knowledge, Hybrid coding, Knowledge based systems, Knowledge-based systems, Mortgage industry, Ontology, Ontology's, Underwriting challenge},
}

@article{iqbal_exploring_2024,
	title = {Exploring issues of story-based effort estimation in {Agile} {Software} {Development} ({ASD})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189756864&doi=10.1016%2fj.scico.2024.103114&partnerID=40&md5=5c6fe43964d8140912f49f9bc15524e9},
	abstract = {Context: Effort estimation based on user stories plays a pivotal role in agile software development, where accurate predictions of project efforts are vital for success. While various supervised ML tools attempt to estimate effort, the prevalence of estimation errors presents significant challenges, as evidenced by the CHAOS report by the Standish Group, which highlights incorrect estimations contributing to a substantial percentage of failed agile projects. Objectives: This research delves into the domain of user story-based effort estimation in agile software development, aiming to explore the issues arising from inaccurate estimations. The primary goal is to uncover these issues comprehensively and propose potential solutions, thus enhancing the efficacy of the user story-based estimation method. Methods: To achieve the research objectives, a systematic literature review (SLR) is conducted, surveying a wide range of sources to gather insights into issues surrounding user story-based effort estimation. The review encompasses diverse estimation methods, user story attributes, and the array of challenges that can result from inaccurate estimations. Results: The SLR reveals a spectrum of issues undermining the accuracy of user story-based effort estimation. It identifies internal factors like communication, team expertise, and composition as crucial determinants of estimation reliability. Consistency in user stories, technical complexities, and task engineering practices also emerge as significant contributors to estimation inaccuracies. The study underscores the interconnectedness of these issues, emphasizing the need for a standardized protocol to minimize inaccuracies and enhance estimation precision. Conclusion: In light of the findings, it becomes evident that addressing the multi-dimensional factors influencing user story-based effort estimation is imperative for successful agile software development. The study underscores the interplay of various aspects, such as team dynamics, task complexity, and requirement engineering, in achieving accurate estimations. By recognizing these challenges and implementing recommended solutions, software development processes can avoid failures and enhance their prospects of success in the agile paradigm. © 2024 Elsevier B.V.},
	author = {Iqbal, Muhammad and Ijaz, Muhammad and Mazhar, Tehseen and Shahzad, Tariq and Abbas, Qamar and Ghadi, YazeedYasin and Ahmad, Wasim and Hamam, Habib},
	year = {2024},
	keywords = {Systematic literature review, Software design, User stories, Agile, Effort Estimation, Issues and challenges, Ml, User story effort estimation},
}

@article{verbeke_safeguarding_2024,
	title = {Safeguarding {Users} of {Consumer} {Mental} {Health} {Apps} in {Research} and {Product} {Improvement} {Studies}: an {Interview} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183317493&doi=10.1007%2fs12152-024-09543-8&partnerID=40&md5=3cfcc42befa8c56721a1a2b252cfaa03},
	abstract = {Mental health-related data generated by app users during the routine use of Consumer Mental Health Apps (CMHAs) are being increasingly leveraged for research and product improvement studies. However, it remains unclear which ethical safeguards and practices should be implemented by researchers and app developers to protect users during these studies, and concerns have been raised over their current implementation in CMHAs. To better understand which ethical safeguards and practices are implemented, why and how, 17 app developers and researchers were interviewed who had been involved in using CMHA data for studies. Interviewees discussed the impact on stakeholder interests, sufficiency thresholds and procedural alterations of informed consent, data protection, gathering app user perspectives and representing users in app design and study conduct, and ensuring adequate support. Although the reasoning behind how and why these ethical safeguards and practices should be implemented showed considerable variability and several gaps, interviewees converged on various general lines of reasoning. This allowed for the development of a coherent and nuanced account that could prove useful for future CMHA studies and which could stimulate further normative investigation of the role of ethical safeguards and practices in these studies. © 2024, The Author(s), under exclusive licence to Springer Nature B.V.},
	author = {Verbeke, Kamiel and Jain, Charu and Shpendi, Ambra and Borry, Pascal},
	year = {2024},
	keywords = {human, article, consumer, informed consent, interview, mental health, reasoning, research ethics, role playing},
}

@inproceedings{vyakaranam_preliminary_2024,
	title = {Preliminary {Study}: {Speech} {Emotion} {Recognition} in {Online} {Teaching} {From} the {Perspective} of {Educators} {Especially} {Late} {Deafened}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191480308&doi=10.1109%2fICoSEIT60086.2024.10497503&partnerID=40&md5=a411b6e9dde2de86d370a3b44da0cd85},
	abstract = {An effective Human Computer Interaction (HCI) for recognizing human emotions automatically is known to as affective computing. Affective computing is achieved through programming, which is meant to detect emotions from facial expressions or verbal utterances. These detected emotions can be further used in many applications. Speech emotion recognition (SER) is one such affective computing application where the detected emotions can be used effectively, particularly in online education. Such an application would allow for the teaching strategies to be tailored for the students, on the basis of their understanding of the ongoing class, which is reflected in their verbal feedback. This valuable application of technology will enhance and support education for all, especially the educators who have issues with hearing verbal feedback from students. The main objective of this study is to find out from educators (with and without hearing impairment) on the necessity of knowing student emotions to enhance teaching outcomes for effective teaching and also the benefits of having an automatic detection system which would display student emotions in an online classroom setting. The outcome of this research clearly indicated how a preliminary user requirement study highlights the need and benefits of implementing a SER system. © 2024 IEEE.},
	author = {Vyakaranam, Aparna and Ramayah, Bavani and Maul, Tomas},
	year = {2024},
	keywords = {Audition, Human computer interaction, Affective Computing, Emotion Recognition, Speech recognition, Computing applications, E-learning, Facial Expressions, Hearing impaired, On-line education, Online teaching, Recognizing Human Emotion, Speech emotion recognition, Student emotions, Students, Teaching strategy},
}

@article{alfayez_technical_2024,
	title = {Technical debt ({TD}) through the lens of {Twitter}: {A} survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150285686&doi=10.1002%2fsmr.2547&partnerID=40&md5=61dbe64fbea9285e8238a45ee0af5365},
	abstract = {Technical debt (TD) is a metaphor used to refer to the added software system costs acquired from taking shortcuts. Unfortunately, large amounts of TD can lead to serious consequences, and, thus, the management of TD is essential. Due to TD being a relatively new subject of study, many aspects of TD remain ambiguous. Fortunately, Twitter has been proven to hold a wealth of information on many subjects. As such, this survey study aims to gain a better understanding on how interest in TD has evolved over time and how TD is addressed on Twitter. A total of 128,897 TD-related tweets were scrapped from Twitter and analyzed using a number of proxy measures and Latent Dirichlet Allocation (LDA). The results revealed that interest in TD on Twitter has been generally increasing since the platform's early stages. Furthermore, TD-related tweets were found to revolve around 11 distinct categories. The TD in games category was discovered to be the most popular category, followed by TD communication and TD repayment. The results highlight that TD is a diverse and overarching topic that contains many potential avenues for further exploration. Software engineering researchers, practitioners, and educators can utilize this study to help steer their TD-related future efforts. © 2023 John Wiley \& Sons Ltd.},
	author = {Alfayez, Reem and Winn, Robert and Ding, Yunyan and Alfayez, Ghaida and Boehm, Barry},
	year = {2024},
	keywords = {Software engineering, Social networking (online), Topic Modeling, Twitter, Software-systems, Technical debts, Large amounts, Proxy measure, Statistics, Survey study, System costs, Through the lens, Wealth of information},
}

@article{gamon-sanz_industries_2024,
	title = {Industries, frameworks, and key drivers of lean startup: a systematic literature review; [{Sectores}, marcos de trabajo y factores clave del lean startup: una revisión sistemática de la literatura]},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194199657&doi=10.18845%2fte.v18i2.7137&partnerID=40&md5=7a8ba0c27daa55420998a5bf874847ce},
	abstract = {Lean Startup provides an iterative, hypothesis-driven approach to business creation and product development, promoting data-driven decision-making by involving potential users and customers during the development cycle. Despite increased academic attention, the debate on the benefits of Lean Startup is open. To contribute to the understanding of how organisations incorporate Lean Startup principles, this study conducts a systematic literature review and identifies the industries in which its application has been analysed, the adaptation models followed by organisations, and the key drivers for its adoption in new and established organisations. The study's findings contribute to advancing knowledge on the adoption of Lean Startup principles, tools, and techniques in organisations. The results also help to the academic debate surrounding the usefulness and application of Lean Startup principles across different organisational contexts. © 2024 Business School, Instituto Tecnologico de Costa Rica. All rights reserved.},
	author = {Gamón-Sanz, Alejandro and Alegre, Joaquín and Chiva, Ricardo},
	year = {2024},
}

@article{borstler_acceptance_2024,
	title = {Acceptance behavior theories and models in software engineering — {A} mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190986067&doi=10.1016%2fj.infsof.2024.107469&partnerID=40&md5=aeb245400acdd1eab3fe1df63e4db457},
	abstract = {Context: The adoption or acceptance of new technologies or ways of working in software development activities is a recurrent topic in the software engineering literature. The topic has, therefore, been empirically investigated extensively. It is, however, unclear which theoretical frames of reference are used in this research to explain acceptance behaviors. Objective: In this study, we explore how major theories and models of acceptance behavior have been used in the software engineering literature to empirically investigate acceptance behavior. Method: We conduct a systematic mapping study of empirical studies using acceptance behavior theories in software engineering. Results: We identified 47 primary studies covering 56 theory uses. The theories were categorized into six groups. Technology acceptance models (TAM and its extensions) were used in 29 of the 47 primary studies, innovation theories in 10, and the theories of planned behavior/ reasoned action (TPB/TRA) in six. All other theories were used in at most two of the primary studies. The usage and operationalization of the theories were, in many cases, inconsistent with the underlying theories. Furthermore, we identified 77 constructs used by these studies of which many lack clear definitions. Conclusions: Our results show that software engineering researchers are aware of some of the leading theories and models of acceptance behavior, which indicates an attempt to have more theoretical foundations. However, we identified issues related to theory usage that make it difficult to aggregate and synthesize results across studies. We propose mitigation actions that encourage the consistent use of theories and emphasize the measurement of key constructs. © 2024 The Author(s)},
	author = {Börstler, Jürgen and Ali, Nauman bin and Petersen, Kai and Engström, Emelie},
	year = {2024},
	keywords = {Software design, Mapping, Behavioral research, Mapping studies, Acceptance behavior, Fitness, Innovations diffusion, TAM, Technology adoption, Theory and models, Theory use in software engineering, TPB, TRA},
}

@article{parashar_machine_2024,
	title = {Machine {Learning} for {Prediction} of {Cardiovascular} {Disease} and {Respiratory} {Disease}: {A} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182185184&doi=10.1007%2fs42979-023-02529-y&partnerID=40&md5=690bb2c326e3d41a6ad69c7ffb6e6e89},
	abstract = {Cardiovascular (CVD) and respiratory diseases (RD) have been in the active domain for machine learning (ML) researchers as these diseases significantly contribute to mortality in humans. Some studies suggest that CVD problems such as cerebrovascular problems, dysrhythmia, inflammatory heart disease, ischemic heart disease (IHD), and RD related problems remain high even after COVID-19 infection clears up. To the best of our knowledge, this is the only study that surveyed these two diseases. This paper’s goal is to explore the existing state of the art in the application of ML in the detection, categorization, and prediction of disorders related to CVD and RD. The review highlights ML algorithms used in prediction of CVD and RD related diseases, datasets used by the articles, technique used for feature selection, features selected for the study, dataset used in the article was unimodal or multimodal, and performance of the algorithm. In CVD category, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used Feature Selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. In case of RDs, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used feature selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. The intent of this review is to stimulate the interest of scientists in this challenging field and to acquaint them with current advances in the field. To design a system that predict CVD or RD in a patient using uni or multi modal datasets, approaches such as data cleaning, feature selection, region of interest (ROI) identification, and classification are applied. This article provided details related to publicly available datasets, most used classification algorithm with performance metric. © 2024, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	author = {Parashar, Gaurav and Chaudhary, Alka and Pandey, Dilkeshwar},
	year = {2024},
}

@article{pessoa_journey_2024,
	title = {A {Journey} to {Identify} {Users}' {Classification} {Strategies} to {Customize} {Game}-{Based} and {Gamified} {Learning} {Environments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173024793&doi=10.1109%2fTLT.2023.3317396&partnerID=40&md5=1f2cf8f37cdfb1825e95025ee65cc6ef},
	abstract = {Game designers and researchers have sought to create gameful environments that consider user preferences to increase engagement and motivation. In this sense, it is essential to identify the most suitable game elements for users' profiles. Designers and researchers must choose strategies to classify users into predefined profiles and select the most appropriate game elements for each user. This activity may challenge designers, learning designers, and researchers since they must base their choice on personal aspects that require a deep understanding. Therefore, this article aims to assist game designers, learning designers, and researchers in selecting user classification strategies to customize and personalize game-based and gamified learning environments. By conducting systematic literature mapping, we consolidate the most common strategies and explore their applications in games and gamification. Our analysis, based on 25 publications, reveals that we can classify the strategies according to user interaction, user personality, learning style, and motivation for learning. Strategies based on user interactions emerge as the most popular, while questionnaires and log data systems are commonly used instruments for identifying user profiles. The findings of this SLM offer valuable knowledge for game designers and researchers to define the criteria that will be used to evaluate the effect of games and gamified environments in educational contexts. © 2008-2011 IEEE.},
	author = {Pessoa, Marcela and Lima, Marcia and Pires, Fernanda and Haydar, Gabriel and Melo, Rafaela and Rodrigues, Luiz and Oliveira, David and Oliveira, Elaine and Galvao, Leandro and Gadelha, Bruno and Isotani, Seiji and Gasparini, Isabela and Conte, Tayana},
	year = {2024},
	keywords = {Software design, Computer aided instruction, Users' experiences, Gamification, Motivation, Computer games, Distal outcome, Game, Game design, Game elements, Game-based Learning, Job analysis, Learning performance, Psychology, Systematic, Task analysis, User classification, User profile, User type},
}

@article{rahman_systematic_2024,
	title = {A {Systematic} {Literature} {Review} on {Software} {Maintenance} {Offshoring} {Decisions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191657835&doi=10.1016%2fj.infsof.2024.107475&partnerID=40&md5=0ecbbd1c575abec8a390ebb7b093a2cc},
	abstract = {Context: Over the last decades, the rapid expansion of the internet has prompted an increasing number of organizations that have taken their work global and have outsourced their information technology (IT) activities to specialized suppliers. The longest part of the software life cycle includes software maintenance, which consumes 60-70\% of the total IT budget. Therefore, organizations have adopted offshoring strategies to reduce maintenance costs and free up resources to focus on their core competencies. Offshore outsourcing decision-making involves technical, social, and other influencing factors; however, there is a limited understanding of the key factors associated with offshoring software maintenance within the global software development context. Objective: This work presents the factors that have influenced the decision-making process of offshoring software maintenance. Further, this research sheds light on decision-making by identifying the models, frameworks, and software tools used within this context. Method: A systematic literature review is conducted, delving into the factors related to the decision-making and analyzing the models, frameworks and tools supporting offshoring software maintenance. Results: This study identifies the top 10 key factors concerning the decision-making process, namely human communication, cost reduction, organizational and employee maturity, project management practices, IT infrastructure support, language constraints, knowledge-based support, changes in requirements, legal issues and cultural diversity. In addition, the models, frameworks, and tools used in the decision-making process of software maintenance are analyzed, and research gaps are identified. Conclusion: The findings reveal that the software industry lacks effective and efficient models tailored explicitly for software offshoring within the global software development landscape. Overall, this study provides valuable insights into the decision-making dynamics of software maintenance offshoring by identifying key factors and research gaps that can pave the way for developing more effective decision support systems. © 2024 Elsevier B.V.},
	author = {Rahman, Hanif Ur and da Silva, Alberto Rodrigues and Alzayed, Asaad and Raza, Mushtaq},
	year = {2024},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Life cycle, Outsourcing, Decision-making process, Computer software, Knowledge based systems, Budget control, Cost reduction, Decision support systems, Factor, Global software development, Key factors, Modelling framework, Modelling tools, Off-shoring, Offshore oil well production, Project management, Research gaps},
}

@article{ataei_application_2023,
	title = {Application of microservices patterns to big data systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158161899&doi=10.1186%2fs40537-023-00733-4&partnerID=40&md5=b9d883d26f0ffb65d220eb10e5ae0f8d},
	abstract = {The panorama of data is ever evolving, and big data has emerged to become one of the most hyped terms in the industry. Today, users are the perpetual producers of data that if gleaned and crunched, have the potential to reveal game-changing patterns. This has introduced an important shift regarding the role of data in organizations and many strive to harness to power of this new material. Howbeit, institutionalizing data is not an easy task and requires the absorption of a great deal of complexity. According to the literature, it is estimated that only 13\% of organizations succeeded in delivering on their data strategy. Among the root challenges, big data system development and data architecture are prominent. To this end, this study aims to facilitate data architecture and big data system development by applying well-established patterns of microservices architecture to big data systems. This objective is achieved by two systematic literature reviews, and infusion of results through thematic synthesis. The result of this work is a series of theories that explicates how microservices patterns could be useful for big data systems. These theories are then validated through expert opinion gathering with 7 experts from the industry. The findings emerged from this study indicates that big data architectures can benefit from many principles and patterns of microservices architecture. © 2023, The Author(s).},
	author = {Ataei, Pouya and Staegemann, Daniel},
	year = {2023},
	keywords = {Systematic literature review, Microservice, Architecture, Big data, Big data architecture, Computer architecture, Data architectures, Data engineering, Data systems, Microservice pattern, Power, System development},
}

@article{larsen_statistical_2024,
	title = {Statistical {Challenges} in {Online} {Controlled} {Experiments}: {A} {Review} of {A}/{B} {Testing} {Methodology}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174052217&doi=10.1080%2f00031305.2023.2257237&partnerID=40&md5=6c9c5ad886579d9d80d246f2a4580971},
	abstract = {The rise of internet-based services and products in the late 1990s brought about an unprecedented opportunity for online businesses to engage in large scale data-driven decision making. Over the past two decades, organizations such as Airbnb, Alibaba, Amazon, Baidu, Booking.com, Alphabet’s Google, LinkedIn, Lyft, Meta’s Facebook, Microsoft, Netflix, Twitter, Uber, and Yandex have invested tremendous resources in online controlled experiments (OCEs) to assess the impact of innovation on their customers and businesses. Running OCEs at scale has presented a host of challenges requiring solutions from many domains. In this article we review challenges that require new statistical methodologies to address them. In particular, we discuss the practice and culture of online experimentation, as well as its statistics literature, placing the current methodologies within their relevant statistical lineages and providing illustrative examples of OCE applications. Our goal is to raise academic statisticians’ awareness of these new research opportunities to increase collaboration between academia and the online industry. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	author = {Larsen, Nicholas and Stallrich, Jonathan and Sengupta, Srijan and Deng, Alex and Kohavi, Ron and Stevens, Nathaniel T.},
	year = {2024},
}

@article{zhao_identifying_2024,
	title = {Identifying the primary dimensions of {DevSecOps}: {A} multi-vocal literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191655409&doi=10.1016%2fj.jss.2024.112063&partnerID=40&md5=950cf6baeee3cf5ed2efac499a8425da},
	abstract = {Context: Security as a key non-functional requirement of software development is often ignored and devalued in DevOps programs, with security seen as an inhibitor to high velocity required in DevOps implementation. Hence, the DevSecOps approach as a security-orientated expansion to DevOps, has aimed to integrate security into DevOps implementation by promoting collaboration among development, operation and security teams. DevSecOps is a topical concept and rapidly emerging area of practice in both academic and industrial settings. Objective: We reviewed both the white and grey literature to identify recent researches and practical trends of DevSecOps, aiming to: (a) review, document and analyze the current state of DevSecOps in the existing literature; (b) investigate the application of DevSecOps in Global Software Engineering (GSE) contexts. Method: A Multi-vocal Literature Review on DevSecOps and its global application was conducted, by executing a dual-track strategy including white (104 studies) and grey (43 studies) literature from 2012 to 2021. A Thematic Analysis was performed to identify, synthesize and analyze the themes within data for reporting the MLR results. Results: Through the Multi-vocal Literature Review and Thematic Analysis, this paper identifies five major aspects of DevSecOps (Definitions, Challenges, Practices, Tools/Technologies, and Metrics/Measurement); collects related themes of each aspect; and generates a Challenge-Practice-Tool-Metric (CPTM) model by integrating the themes of the latter four aspects within a lifecycle model. Moreover, an unexplored area relating to the global application of DevSecOps has been identified. Conclusion: Based on MLR results, a CPTM (Challenge-Practice-Tool-Metric) model is built to reveal the current status of DevSecOps. The model provides a breakdown and a broad landscape of DevSecOps, from which researchers and practitioners may select an area of focus to improve their knowledge or practice. With DevSecOps spanning the many stages of the lifecycle, we believe the model will enable emphases and absences such as global aspects to be investigated. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2024 The Author(s)},
	author = {Zhao, Xiaofan and Clear, Tony and Lal, Ramesh},
	year = {2024},
	keywords = {Software design, Life cycle, Application programs, Literature reviews, Devsecop, Global applications, Global software engineering, METRIC model, Multivocal literature review, Non-functional requirements, Open science, Security, Thematic analysis},
}

@article{gorkovenko_data-enhanced_2023,
	title = {Data-{Enhanced} {Design}: {Engaging} {Designers} in {Exploratory} {Sensemaking} with {Multimodal} {Data}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181706477&doi=10.57698%2fv17i3.01&partnerID=40&md5=d19ee20c1a2b039444e080ad65da687a},
	abstract = {Research in the wild can reveal human behaviors, contexts, and needs around products that are difficult to observe in the lab. Telemetry data from the use of physical products can help facilitate in the wild research, in particular by suggesting hypotheses that can be explored through machine learning models. This paper explores ways for designers without strong data skills to engage with multimodal data to develop a contextual understanding of product use. This study is framed around a lightweight version of a data enhanced design research process where multimodal telemetry data was captured by a GoPro camera attached to a bicycle. This was combined with the video data and conversation with the rider to carry out an exploratory sensemaking process and generate design research questions that could potentially be addressed through data capture, annotation, and machine learning. We identify a range of ways that designers could make use of the data for ideation and developing context through annotating and exploring the data. Participants used data and annotation practices to connect the micro and macro, spot interesting moments, and frame questions around an unfamiliar problem. The work follows the designers’ questions, methods, and explorations, both immediate concerns and speculations about working at larger scales with machine learning models. This points to the possibility of tools that help designers to engage with machine learning, not just for optimization and refinement, but for creative ideation in the early stages of design processes. © 2023 Gorkovenko, Jenkins, Vaniea, \& Murray-Rust.},
	author = {Gorkovenko, Katerina and Jenkins, Adam and Vaniea, Kami and Murray-Rust, Dave},
	year = {2023},
}

@article{birkeland_research_2024,
	title = {Research areas and methods of interest in {European} intraday electricity market research—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189751189&doi=10.1016%2fj.segan.2024.101368&partnerID=40&md5=feb6e626b69abee383d73de8104038fa},
	abstract = {This paper establishes a robust foundation for the expansion of European intraday electricity market research through a systematic literature review. The review encompasses 132 primary studies from various libraries, categorizing them based on research area, methodologies, dataset context, and dataset date. The resulting taxonomy identifies six major research groups: Bidding, Market Modeling, Price Forecasting, Market Design, Forecast Errors, and Market Abuse. The analysis of the review results leads to actionable recommendations for future European intraday electricity market research. These recommendations include the utilization of close-to-live datasets to accurately reflect the impacts of the energy transition, the exploration of market abuse in the energy market industry, and the broadening of national electricity market studies beyond Germany. This systematic review aims to benefit various stakeholders, including academic researchers, industry participants, and European regulators, by providing a structured and objective compilation of existing research and offering insights into the identified gaps within the intraday electricity market research landscape. © 2024 The Author(s)},
	author = {Birkeland, Dane and AlSkaif, Tarek},
	year = {2024},
	keywords = {Systematic literature review, Forecasting, Electric industry, Intraday electricity market, Market abuse, Market bidding, Market model, Market researches, Power markets, Price forecasting, Research areas, Research groups, Research method},
}

@inproceedings{ramzan_test-driven_2024,
	title = {Test-{Driven} {Development} ({TDD}) in {Small} {Software} {Development} {Teams}: {Advantages} and {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190268315&doi=10.1109%2fICACS60934.2024.10473291&partnerID=40&md5=20eefd4834bd0370b52239da636512c5},
	abstract = {In the context of small software development teams, this research article gives a thorough investigation of the adoption of test-driven development (TDD) approaches. It aims to highlight the benefits that TDD offers, such as improved code quality through modularization and proactive defect spotting which results in effective debugging and development processes. It also discusses the complex issues that arise when TDD is implemented in smaller teams, such as the learning curve and resource constraints. This study significantly advances the understanding of how TDD can be used to optimize software development techniques in organizations or software houses having small development teams. It also explores the possible advantages and challenges. © 2024 IEEE.},
	author = {Ramzan, Hafiz Arslan and Ramzan, Sadia and Kalsum, Tehmina},
	year = {2024},
	keywords = {Development process, Software design, Software testing, Code quality, Debugging process, Development approach, Improved efficiency, Learning curves, Modular construction, Modularizations, Small team, Software development teams, Test driven development},
}

@article{ganeshkumar_discovery_2024,
	title = {Discovery, development, and deployment of a user-centered point-of-care digital information system to treat and track hypertension and diabetes patients under {India} {Hypertension} {Control} {Initiative} 2019–2022, {India}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362882&doi=10.1177%2f20552076241250153&partnerID=40&md5=0f65a3332b2783d660a88b13a123271d},
	abstract = {Background: Hypertension affects 28.5\% of Indians aged 18–69. Real-time registration and follow-up of persons with hypertension are possible with point-of-care digital information systems. We intend to describe herein the experiences of discovering, developing, and deploying a point-of-care digital information system for public health facilities under the India Hypertension Control Initiative. Methods: We have adopted an agile and user-centered approach in each phase in selected states of India since 2017. A multidisciplinary team adopted a hybrid approach with quantitative and qualitative methods, such as contextual inquiries, usability testing, and semi-structured interviews with healthcare workers, to document and monitor utility and usability. Results: During the discovery phase, we adopted a storyboard technique to understand the requirement of a digital information system. The participatory approach in discovery phase co-designed the information system with the nurses and doctors at Punjab state of India. Simple, which is the developed information system, has a front-end Android mobile application for healthcare workers and a backend dashboard for program managers. As of October 2022, over 24,31,962 patients of hypertension and 8,99,829 diabetes were registered in the information system of 10,017 health facilities. The median duration of registering a new patient was 50 seconds, and for recording a follow-up visit was 14 seconds in the app. High satisfaction was reported in 100 app users’ quarterly interviews. Conclusion: Simple was implemented by administering a user-centered approach and agile techniques. It demonstrated high utility and usability among users, highlighting the benefits of a user-centered approach for effective digital health solutions. © The Author(s) 2024.},
	author = {Ganeshkumar, Parasuraman and Bhatnagar, Aarti and Burka, Daniel and Durgad, Kiran and Krishna, Ashish and Das, Bidisha and Chandak, Mahima and Sharma, Meenakshi and Shivasankar, Roopa and Pathni, Anupam Khungar and Kunwar, Abhishek and Kaur, Prabhdeep},
	year = {2024},
}

@article{pranajaya_examining_2024,
	title = {Examining the influence of financial inclusion on investment decision: {A} bibliometric review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183975319&doi=10.1016%2fj.heliyon.2024.e25779&partnerID=40&md5=84031b0918cf4b2a700171cfb937cf8c},
	abstract = {This study delves into the contemporary landscape of potential financial inclusion in investment decision-making, leveraging bibliometric research methods. Analyzing 161 publications from the Scopus database (2006–2023), the authors employ performance analysis and scientific mapping tools, including VOSviewer and Biblioshiny R studio. Through co-citation analysis, bibliographic coupling, co-occurrence of keywords analysis, thematic mapping, and thematic evolution analysis, the study uncovers the essential characteristics of the research field. The Result underscores that Innovative financial technologies are positioned as enablers of financial inclusion, with fintech's potential to drive positive social impact. The findings underscore that fostering financial literacy, addressing challenges in fintech adoption, and supporting entrepreneurship are crucial for maximizing the benefits of financial technologies. Overall, the study advocates for a comprehensive approach that combines financial inclusion, individual attitudes, and expertise, and fintech innovation to enhance access to financial services and expand investment opportunities for a more inclusive and prosperous economic landscape. However, the study acknowledges limitations, such as reliance on a single database and exclusion of specific keywords, urging a more inclusive approach to ensure a comprehensive understanding of relevant literature in this dynamic field. © 2024 The Authors},
	author = {Pranajaya, Eko and Alexandri, Mohammad Benny and Chan, Arianis and Hermanto, Bambang},
	year = {2024},
}

@article{moguel-sanchez_bots_2023,
	title = {Bots in {Software} {Development}: {A} {Systematic} {Literature} {Review} and {Thematic} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182955496&doi=10.1134%2fS0361768823080145&partnerID=40&md5=ce1e41e228545c5e52caa01baa3d1388},
	abstract = {Abstract: Modern Software Engineering thrives with innovative tools that aid developers in creating better software grounded on quality standards. Software bots are an emerging and exciting trend in this regard, supporting numerous software development activities. As an emerging trend, few studies describe and analyze different bots in software development. This research presents a systematic literature review covering the state of the art of applied and proposed bots for software development. Our study spans literature from 2003 to 2022, with 82 different bots applied in software development activities, covering 83 primary studies. We found four bot archetypes: chatbots which focus on direct communication with developers to aid them, analysis bots that display helpful information in different tasks, repair bots for resolving software defects, and development bots that combine aspects of other bot technologies to provide a service to the developer. The primary benefits of using bots are increasing software quality, providing useful information to developers, and saving time through the partial or total automation of development activities. However, drawbacks are reported, including limited effectiveness in task completion, high coupling to third-party technologies, and some prejudice from developers toward bots and their contributions. We discovered that including Bots in software development is a promising field of research in software engineering that has yet to be fully explored. © 2023, Pleiades Publishing, Ltd.},
	author = {Moguel-Sánchez, R. and Martínez-Palacios, C. S. Sergio and Ocharán-Hernández, J.O. and Limón, X. and Sánchez-García, A.J.},
	year = {2023},
	keywords = {Systematic literature review, Software design, State of the art, Chatbots, Thematic analysis, Bot, Botnet, Computer software selection and evaluation, Development activity, Development bot, Emerging trends, Quality standard, Thematic synthesis},
}

@article{dominguez_role_2024,
	title = {The role of ontologies in smart contracts: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192928073&doi=10.1016%2fj.jii.2024.100630&partnerID=40&md5=25e13e4895435b8b01c9f92b71783473},
	abstract = {The aim of this systematic literature review is to provide a comprehensive understanding of how ontologies address current Smart Contract challenges, identify application scenarios, and present tools and technologies associated with their use. This systematic literature review (SLR), following Kitchenham's methodology, analyses peer-reviewed articles from 2015 to August 2022 from databases such as Scopus, IEEE, Science Direct, Springer Link and ACM. Of the 501 publications identified, 21 are selected for in-depth review based on inclusion, exclusion and quality assessment criteria. The results of this SLR show that ontologies provide solutions to the challenges faced by Smart Contracts mainly at the creation stage. They allow the terms of the contract and the roles of the parties to be defined. Ontologies also enable the development of Smart Contract templates. This facilitates their use by people without technical programming expertise. Despite these potential solutions to the challenges that Smart Contracts face throughout their lifecycle, they lack verification. This increases the vulnerabilities to which Smart Contracts are exposed. Developing validation and verification tools could facilitate using ontologies to create Smart Contracts for different real-world cases. © 2024 Elsevier Inc.},
	author = {Dominguez, Johnny Alvarado and Gonnet, Silvio and Vegetti, Marcela},
	year = {2024},
	keywords = {Systematic literature review, Life cycle, 'current, Ontology, Ontology's, Application scenario, Assessment criteria, Block-chain, Blockchain, Inclusion-exclusion, Quality assessment, Smart contract, Tools and technologies, Validation and verification},
}

@article{khoshnevis_search-based_2024,
	title = {Search-based approaches to optimizing software product line architectures: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188512328&doi=10.1016%2fj.infsof.2024.107446&partnerID=40&md5=4dd032426e87fee8e0653cb59b8215a1},
	abstract = {Context: Software product line architecture (PLA) plays an important role in developing software product lines (SPLs) and other configurable systems. Search-based (SB) approaches can optimize the design of PLAs according to a given set of metrics as fitness functions. Although this area has been explored by researchers, there is a lack of synthesis of search-based PLA (SBPLA) research. A comprehensive review would offer valuable insights into previous contributions and identify areas for further research. Objective: The objective of this work is to identify and summarize quality-assessed peer-reviewed studies on search-based PLA design from the aspects of the research scope, problems, contributions, evaluation, and open issues. Methods: We conducted a systematic literature review based on Kitchenham's methodology. Based on a predefined search protocol we identified related studies limited to the ones published between 2000 and 2022 in journals and conference proceedings. Results: Out of 686 initial search results, 34 papers were finally selected after a set of deep search, and criteria application activities. We provided a taxonomy of optimization problems in SBPLA and found that PLA remodularization and refactoring were the two categories most emphasized by the researchers. We also provided several other categorizations regarding contributions, research design, open issues, and other subjects of interest. Conclusions: The interest in SBPLA design has been growing since 2014. PLA cloning and re-engineering problems have never been addressed in the literature. Performing subjective evaluation with the participation of experts from the industry will be profitable, as a complementary method to objective experimental evaluation, and therefore carrying out quanti-qualitative research. © 2024 Elsevier B.V.},
	author = {Khoshnevis, Sedigheh and Ardestani, Omid},
	year = {2024},
	keywords = {Systematic literature review, Software design, Software Product Line, Computer software, Configurable systems, Metrics as fitness functions, Product line architecture, Quality control, Search-based, Search-based software architecture, Search-based software engineering, Software architecture, Software product line architecture},
}

@article{galbin-nasui_bug_2024,
	title = {Bug reports priority classification models. {Replication} study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189942059&doi=10.1007%2fs10515-024-00432-1&partnerID=40&md5=084ec887cec6bbb2f20eb711594e7b6a},
	abstract = {Bug tracking systems receive a large number of bugs on a daily basis. The process of maintaining the integrity of the software and producing high-quality software is challenging. The bug-sorting process is usually a manual task that can lead to human errors and be time-consuming. The purpose of this research is twofold: first, to conduct a literature review on the bug report priority classification approaches, and second, to replicate existing approaches with various classifiers to extract new insights about the priority classification approaches. We used a Systematic Literature Review methodology to identify the most relevant existing approaches related to the bug report priority classification problem. Furthermore, we conducted a replication study on three classifiers: Naive Bayes (NB), Support Vector Machines (SVM), and Convolutional Neural Network (CNN). Two sets of experiments are performed: first, our own NLTK implementation based on NB and CNN, and second, based on Weka implementation for NB, SVM, and CNN. The dataset used consists of several Eclipse projects and one project related to database systems. The obtained results are better for the bug priority P3 for the CNN classifier, and overall the quality relation between the three classifiers is preserved as in the original studies. The replication study confirmed the findings of the original studies, emphasizing the need to further investigate the relationship between the characteristics of the projects used as training and those used as testing. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
	author = {Galbin-Nasui, Andreea and Vescan, Andreea},
	year = {2024},
	keywords = {Program debugging, BN, Bug priority prediction, Bug reports, Bug tracking system, Classification approach, Classification models, Convolutional neural network, Convolutional neural networks, Naive bayes, Replication study, Support vector machines, Support vectors machine},
}

@article{biazotto_technical_2024,
	title = {Technical debt management automation: {State} of the art and future perspectives},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180417326&doi=10.1016%2fj.infsof.2023.107375&partnerID=40&md5=db74021970a8b325b1de4b98f924dcab},
	abstract = {Context: Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system's maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most. Objectives: The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM. Methods: We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation. Results: We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges. Conclusion: The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts. © 2023 The Author(s)},
	author = {Biazotto, João Paulo and Feitosa, Daniel and Avgeriou, Paris and Nakagawa, Elisa Yumi},
	year = {2024},
	keywords = {State of the art, Mapping, Systematic mapping studies, Automation, 'current, Technical debts, Codes (symbols), Future perspectives, Management activities, Management automations, Optimal decisions, Technical debt management},
}

@article{chueca_consolidation_2024,
	title = {The consolidation of game software engineering: {A} systematic literature review of software engineering for industry-scale computer games},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172685092&doi=10.1016%2fj.infsof.2023.107330&partnerID=40&md5=ef2b467462eb9ad92be33ef6e585775f},
	abstract = {Context: Game Software Engineering (GSE) is a branch of Software Engineering (SE) that focuses on the development of video game applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE. Objective: This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field. Method: A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE. Results: The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work. Conclusion: The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge. © 2023 Elsevier B.V.},
	author = {Chueca, Jorge and Verón, Javier and Font, Jaime and Pérez, Francisca and Cetina, Carlos},
	year = {2024},
	keywords = {Systematic literature review, State of the art, Application programs, Human computer interaction, 'current, Computer games, Best practices, Engineering research, Game software, Game software engineering, Industry-scale, SLR, Video-games, Volume difference},
}

@article{soto_automated_2024,
	title = {Automated {Diagnosis} of {Prostate} {Cancer} {Using} {Artificial} {Intelligence}. {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175991217&doi=10.1007%2f978-3-031-46813-1_6&partnerID=40&md5=3de2e59f800c9fe45cbfe0f5dbe022d5},
	abstract = {Prostate cancer is one of the most preventable causes of death. Periodic testing, seconded by precursors such as living habits, heritage, and exposure to specific materials, help healthcare providers achieve early detection, a desirable scenario that positively correlates with survival. However, the currently available diagnosing mechanisms have a great opportunity of improvement in terms of invasiveness, sensitivity and timing before patients reach advanced stages with a significant probability of metastasis. Supervised artificial intelligence enables early diagnosis and excludes patients from unpleasant biopsies. In this work, we gathered information about methodologies, techniques, metrics, and benchmarks to accomplish early prostate cancer detection, including pipelines with associated patents and knowledge transfer mechanisms, intending to find the reasons precluding the solutions from being massively adopted in the standards of care. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Soto, Salvador and Pollo-Cattaneo, María F. and Yepes-Calderon, Fernando},
	year = {2024},
	keywords = {Artificial intelligence, Systematic literature review, Knowledge management, Automated diagnosis, Automatic pathology diagnose, Causes of death, Diagnosis, Diseases, Early diagnosis, Health care providers, Invasiveness, Pathology, Periodic testing, Prostate cancers, Specific materials, Urology},
}

@article{jui_fairness_2024,
	title = {Fairness issues, current approaches, and challenges in machine learning models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183668121&doi=10.1007%2fs13042-023-02083-2&partnerID=40&md5=c5b3de8f74c7dc7e2d5d8886887b03d0},
	abstract = {With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness. © 2024, The Author(s).},
	author = {Jui, Tonni Das and Rivas, Pablo},
	year = {2024},
	keywords = {Decision making, Decision-making process, Learning algorithms, Machine learning, Machine-learning, 'current, Mapping studies, Bias reduction, Fair prediction, Machine learning algorithms, Machine learning models, Model fairness, Training data},
}

@book{kumar_ethical_2024,
	title = {The ethical frontier of {AI} and data analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189601406&doi=10.4018%2f979-8-3693-2964-1&partnerID=40&md5=c591996a91637e79d3e1dd27cc10b011},
	abstract = {In the advancing fields of artificial intelligence (AI) and data science, a pressing ethical dilemma arises. As technology continues its relentless march forward, ethical considerations within these domains become increasingly complex and critical. Bias in algorithms, lack of transparency, data privacy breaches, and the broader societal repercussions of AI applications are demanding urgent attention. This ethical quandary poses a formidable challenge for researchers, academics, and industry professionals alike, threatening the very foundation of responsible technological innovation. Navigating this ethical minefield requires a comprehensive understanding of the multifaceted issues at hand. The Ethical Frontier of AI and Data Analysis is an indispensable resource crafted to address the ethical challenges that define the future of AI and data science. Researchers and academics who find themselves at the forefront of this challenge are grappling with the evolving landscape of AI and data science ethics. Underscoring the need for this book is the current lack of clarity on ethical frameworks, bias mitigation strategies, and the broader societal implications, which hinder progress and leave a void in the discourse. As the demand for responsible AI solutions intensifies, the imperative for this reliable guide that consolidates, explores, and advances the dialogue on ethical considerations grows exponentially. Tailored for researchers, academics, and professionals, this publication serves as a beacon of ethical excellence. With a comprehensive exploration of bias, fairness, transparency, and accountability, it guides readers through the intricate web of ethical considerations. From foundational philosophical frameworks to real-world case studies, the book offers a roadmap to not only understand but actively shape the ethical trajectory of AI and data science. It is more than a book; it serves as a transformative tool for those seeking to align technological innovation with ethical standards and societal values. © 2024 by IGI Global. All rights reserved.},
	author = {Kumar, Rajeev and Joshi, Ankush and Sharan, Hari Om and Peng, Sheng-Lung and Dudhagara, Chetan R.},
	year = {2024},
}

@article{iftikhar_tertiary_2024,
	title = {A tertiary study on links between source code metrics and external quality attributes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174715019&doi=10.1016%2fj.infsof.2023.107348&partnerID=40&md5=b6a93845d92de37d2044158738593673},
	abstract = {Context: Several secondary studies have investigated the relationship between internal quality attributes, source code metrics and external quality attributes. Sometimes they have contradictory results. Objective: We synthesize evidence of the link between internal quality attributes, source code metrics and external quality attributes along with the efficacy of the prediction models used. Method: We conducted a tertiary review to identify, evaluate and synthesize secondary studies. We used several characteristics of secondary studies as indicators for the strength of evidence and considered them when synthesizing the results. Results: From 711 secondary studies, we identified 15 secondary studies that have investigated the link between source code and external quality. Our results show : (1) primarily, the focus has been on object-oriented systems, (2) maintainability and reliability are most often linked to internal quality attributes and source code metrics, with only one secondary study reporting evidence for security, (3) only a small set of complexity, coupling, and size-related source code metrics report a consistent positive link with maintainability and reliability, and (4) group method of data handling (GMDH) based prediction models have performed better than other prediction models for maintainability prediction. Conclusions: Based on our results, lines of code, coupling, complexity and the cohesion metrics from Chidamber \& Kemerer (CK) metrics are good indicators of maintainability with consistent evidence from high and moderate-quality secondary studies. Similarly, four CK metrics related to coupling, complexity and cohesion are good indicators of reliability, while inheritance and certain cohesion metrics show no consistent evidence of links to maintainability and reliability. Further empirical studies are needed to explore the link between internal quality attributes, source code metrics and other external quality attributes, including functionality, portability, and usability. The results will help researchers and practitioners understand the body of knowledge on the subject and identify future research directions. © 2023 The Author(s)},
	author = {Iftikhar, Umar and Ali, Nauman Bin and Börstler, Jürgen and Usman, Muhammad},
	year = {2024},
	keywords = {Tertiary study, Forecasting, Data handling, Codes (symbols), Code quality, Computer programming languages, Evidence, External quality, Internal quality, Maintainability, Object oriented programming, Products quality, Quality attributes, Quality modeling, Reliability, Source code metrics, Tertiary review},
}

@article{ferrari_transforming_2023,
	title = {On transforming model-based tests into code: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169886627&doi=10.1002%2fstvr.1860&partnerID=40&md5=5329e23c5dcfa2e46c74b6c32b471ebf},
	abstract = {Model-based test design is increasingly being applied in practice and studied in research. Model-based testing (MBT) exploits abstract models of the software behaviour to generate abstract tests, which are then transformed into concrete tests ready to run on the code. Given that abstract tests are designed to cover models but are run on code (after transformation), the effectiveness of MBT is dependent on whether model coverage also ensures coverage of key functional code. In this article, we investigate how MBT approaches generate tests from model specifications and how the coverage of tests designed strictly based on the model translates to code coverage. We used snowballing to conduct a systematic literature review. We started with three primary studies, which we refer to as the initial seeds. At the end of our search iterations, we analysed 30 studies that helped answer our research questions. More specifically, this article characterizes how test sets generated at the model level are mapped and applied to the source code level, discusses how tests are generated from the model specifications, analyses how the test coverage of models relates to the test coverage of the code when the same test set is executed and identifies the technologies and software development tasks that are on focus in the selected studies. Finally, we identify common characteristics and limitations that impact the research and practice of MBT: (i) some studies did not fully describe how tools transform abstract tests into concrete tests, (ii) some studies overlooked the computational cost of model-based approaches and (iii) some studies found evidence that bears out a robust correlation between decision coverage at the model level and branch coverage at the code level. We also noted that most primary studies omitted essential details about the experiments. © 2023 John Wiley \& Sons Ltd.},
	author = {Ferrari, Fabiano C. and Durelli, Vinicius H. S. and Andler, Sten F. and Offutt, Jeff and Saadatmand, Mehrdad and Müllner, Nils},
	year = {2023},
	keywords = {Model checking, Specifications, Systematic literature review, Software design, Software testing, Codes (symbols), Abstracting, Concretes, Model based testing, Model specifications, Model-based test, Test case, Test case generation, Test case transformation, Test coverage criteria, Test sets, Test-coverage},
}

@article{karcher_quality_2024,
	title = {Quality methods in virtual and augmented reality with a focus on education: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187115518&doi=10.1007%2fs11301-023-00403-y&partnerID=40&md5=1d4ec757b2a3f73d0c92a57fa035ed24},
	abstract = {With the goal of developing a unified approach for implementation of training for quality methods—with the help of innovative assistance systems—the current state of research is determined within the scope of this work. These quality methods include Quality Management Systems such as Lean Management and Six Sigma. A systematic literature search is conducted to determine the current state of research on Augmented and Virtual Reality data glasses, which are considered here as innovative assistance systems. This search extends without restriction to the date of data collection at the beginning of the year 2022, as Augmented and Virtual Reality data glasses are considered to be particularly immersive technologies. Based on the databases Scopus and Web of Science, an extended systematic literature review was used for the research. By answering the research question and classifying the implemented research work, an overview of the current state of virtual and augmented reality research will be given. This makes it clear that further research is needed, especially with regard to the training of quality methods, to develop specific models and action guidelines. © The Author(s) 2024.},
	author = {Karcher, Amelie and Arnold, Dominik and Kuhlenkötter, Bernd},
	year = {2024},
}

@article{wairimu_evaluation_2024,
	title = {On the {Evaluation} of {Privacy} {Impact} {Assessment} and {Privacy} {Risk} {Assessment} {Methodologies}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184332904&doi=10.1109%2fACCESS.2024.3360864&partnerID=40&md5=847d81293ca4d943618bb7001110f885},
	abstract = {Assessing privacy risks and incorporating privacy measures from the onset requires a comprehensive understanding of potential impacts on data subjects. Privacy Impact Assessments (PIAs) offer a systematic methodology for such purposes, which are closely related to Data Protection Impact Assessments (DPIAs), particularly outlined in Article 35 of the General Data Protection Regulation (GDPR). The core of a PIA is a Privacy Risk Assessment (PRA). PRAs can be integrated as part of full-fledged PIAs or independently developed to support PIA processes. Although these methodologies have been identified as essential enablers of privacy by design, their effectiveness has been criticized because of the lack of evidence of their rigorous and systematic evaluation. Hence, we conducted a Systematic Literature Review (SLR) to identify published PIA and PRA methodologies and assess how and to what extent they have been scientifically validated or evaluated. We found that these methodologies are rarely evaluated for their performance in practice, and most of them have only been validated in limited studies. Most validation evidence is found with PRA methodologies. Of the evaluated methodologies, PIAs were the most evaluated, where case studies were the predominant evaluation method. These evaluated methodologies can be easily transferred to an industrial setting or used by practitioners, as they provide evidence of their use in practice. In addition, the findings in this study can be used to inform researchers of the current state-of-the-art, and practitioners can understand the benefits and current limitations of the methodologies and adopt evidence-based practices. © 2013 IEEE.},
	author = {Wairimu, Samuel and Iwaya, Leonardo Horn and Fritsch, Lothar and Lindskog, Stefan},
	year = {2024},
	keywords = {Risk management, Risk assessment, Data privacy, General data protection regulations, Systematic, Data protection impact assessments, Guideline, Impact assessments, Maturity, Privacy, Privacy impact assessment, Privacy risks, Risks management, Threat modeling, Validity},
}

@article{oliveira_mobile_2024,
	title = {Mobile {Health} from {Developers}’ {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180425615&doi=10.1007%2fs42979-023-02455-z&partnerID=40&md5=8f2aa4333b07b435a23d001076ad88c5},
	abstract = {This paper extends an initial investigation of eHealth from the developers’ perspective. In this extension, our focus is on mobile health data. Despite the significant potential of this development area, few studies try to understand the challenges faced by these professionals. This perspective is relevant to identify the most used technologies and future perspectives for research investigation. Using a KDD-based process, this work analyzed eHealth and mHealth discussions from Stack Overflow (SO) to comprehend this developers’ community. We got and processed 6082 eHealth and 1832 mHealth questions. The most discussed topics include manipulating medical images, electronic health records with the HL7 standard, and frameworks to support mobile health (mHealth) development. Concerning the challenges faced by these developers, there is a lack of understanding of the DICOM and HL7 standards, the absence of data repositories for testing, and the monitoring of health data in the background using mobile and wearable devices. Our results also indicate that discussions have grown mainly on mHealth, primarily due to monitoring health data through wearables and about how to optimize resource consumption during health-monitoring. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	author = {Oliveira, Pedro Almir M. and Junior, Evilasio Costa and Andrade, Rossana M. C. and Santos, Ismayle S. and Neto, Pedro A. Santos},
	year = {2024},
}

@article{papatheocharous_context_2024,
	title = {Context factors perceived important when looking for similar experiences in decision-making for software components: {An} interview study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190424140&doi=10.1002%2fsmr.2668&partnerID=40&md5=6ed3d43b32c8b6b926652c2475b95b30},
	abstract = {During software evolution, decisions related to components' origin or source significantly impact the quality properties of the product and development metrics such as cost, time to market, ease of maintenance, and further evolution. Thus, such decisions should ideally be supported by evidence, i.e., using previous experiences and information from different sources, even own previous experiences. A hindering factor to such reuse of previous experiences is that these decisions are highly context-dependent and it is difficult to identify when previous experiences come from sufficiently similar contexts to be useful in a current setting. Conversely, when documenting a decision (as a decision experience), it is difficult to know which context factors will be most beneficial when reusing the experience in the future. An interview study is performed to identify a list of context factors that are perceived to be most important by practitioners when using experiences to support decision-making for component sourcing, using a specific scenario with alternative sources of experiences. We observed that the further away (from a company or an interviewee) the experience evidence is, as is the case for online experiences, the more context factors are perceived as important by practitioners to make use of the experience. Furthermore, we discuss and identify further research to make this type of decision-making more evidence-based. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Papatheocharous, Efi and Wohlin, Claes and Badampudi, Deepika and Carlson, Jan and Wnuk, Krzysztof},
	year = {2024},
	keywords = {Decision making, Decisions makings, Open source software, Software Evolution, Components off the shelves, Context factors, Decision experience, Experience source, In-house, Interview study, Open systems, Open-source softwares, Software-component},
}

@article{liang_co-design_2024,
	title = {Co-design personal sleep health technology for and with university students},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191036091&doi=10.3389%2ffdgth.2024.1371808&partnerID=40&md5=8f5ef4e6d87e2f109fef9327baa66073},
	abstract = {University students often experience sleep disturbances and disorders. Personal digital technologies present a great opportunity for sleep health promotion targeting this population. However, studies that engage university students in designing and implementing digital sleep health technologies are scarce. This study sought to understand how we could build digital sleep health technologies that meet the needs of university students through a co-design process. We conducted three co-design workshops with 51 university students to identify design opportunities and to generate features for sleep health apps through workshop activities. The generated ideas were organized using the stage-based model of self-tracking so that our findings could be well-situated within the context of personal health informatics. Our findings contribute new design opportunities for sleep health technologies targeting university students along the dimensions of sleep environment optimization, online community, gamification, generative AI, materializing sleep with learning, and personalization. 2024 Liang, Melcer, Khotchasing and Hoang.},
	author = {Liang, Zilu and Melcer, Edward and Khotchasing, Kingkarn and Hoang, Nhung Huyen},
	year = {2024},
	keywords = {Article, human, adult, cohort analysis, data analysis, depersonalization, female, gamification, heart rate, human experiment, learning, male, nervous system function, personal sleep health technology, sleep disorder, sleep education, sleep environment, sleep hygiene, sleep literacy education, sleep quality, university student},
}

@article{von_scherenberg_data_2024,
	title = {Data {Sovereignty} in {Information} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186488145&doi=10.1007%2fs12525-024-00693-4&partnerID=40&md5=81079ebc4fe9e7a98618fa6d1beea7fc},
	abstract = {Data has become a strategic asset for societal prosperity and economic competitiveness. There has long been an academic consensus that the value of data unfolds during its use. Consequently, many stakeholders have called for expanding the use and reuse of data, including the public and open variety, as well as that from private data providers. However, citizens and organizations want self-determination over their data use, that is, data sovereignty. This fundamentals paper applies a literature review to conceptualize the term in Information Systems (IS) research by summarizing current findings and definitions to add further structure to the field. It contributes to the current research streams by introducing a core conceptual model consisting of seven interacting core aspects, involving trust between data providers and consumers for data assets, supported by data infrastructure and contractual agreements on all data lifecycle stages. We evaluate and discuss this conceptual model through recent field examples and provide an overview of future research opportunities. © The Author(s) 2024.},
	author = {von Scherenberg, Franziska and Hellmeier, Malte and Otto, Boris},
	year = {2024},
}

@article{kosasih_review_2024,
	title = {A review of explainable artificial intelligence in supply chain management using neurosymbolic approaches},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177643935&doi=10.1080%2f00207543.2023.2281663&partnerID=40&md5=73d8d50d41b48d9e9b68eb4ff7b68865},
	abstract = {Artificial Intelligence (AI) has emerged as a complementary technology in supply chain research. However, the majority of AI approaches explored in this context afford little to no explainability, which is a significant barrier to a broader adoption of AI in supply chains. In recent years, the need for explainability has been a strong impetus for research in hybrid AI methodologies that combine neural architectures with logic-based reasoning, which are collectively referred to as Neurosymbolic AI. The aim of this paper is to provide a comprehensive overview of supply chain management literature that employs approaches within the neurosymbolic AI spectrum. To that end, a systematic review is conducted, followed by bibliometric, descriptive and thematic analyses on the identified studies. Our findings indicate that researchers have primarily focused on the limited subset of neurofuzzy approaches, while some supply chain applications, such as performance evaluation and sustainability, and sectors such as pharmaceutical and construction have received less attention. To help address these gaps, we propose five pillars of neurosymbolic AI research for supply chains and provide four use cases of applying unexplored neurosymbolic AI approaches to address typical problems in supply chain management, including a discussion of prerequisites for adopting such technologies. We envision that the findings and contributions of this survey will help encourage further research in neurosymbolic AI for supply chains and increase adoption of such technologies within supply chain practice. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
	author = {Kosasih, Edward Elson and Papadakis, Emmanuel and Baryannis, George and Brintrup, Alexandra},
	year = {2024},
	keywords = {Artificial intelligence, Systematic Review, Explainability, Based reasonings, Bibliometrics analysis, Descriptive analysis, Hybrid artificial intelligences, Neural architectures, Neural-networks, Neurosymbolic artificial intelligence, Spectra's, Supply chain management},
}

@article{malcher_what_2023,
	title = {What do we know about requirements management in software ecosystems?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173886355&doi=10.1007%2fs00766-023-00407-w&partnerID=40&md5=30247f2698bd677da0e7068f7c8ff3fd},
	abstract = {Among the activities in requirements engineering (RE), requirements management ensures that requirements are tracked throughout their life cycle, changes are controlled, and inconsistencies are corrected. Requirements management has become increasingly critical in new ways of developing software and emerging contexts such as software ecosystems (SECO). The changing nature of the SECO introduces complexity in requirements management and results in varied flows of emergent requirements, making managing requirements in SECO challenging. Hence, understanding how requirements management is performed in SECO can help requirements managers improve their practices. This work aims to characterize requirements management in SECO. We have conducted a systematic mapping study (SMS) to achieve this goal. We selected 29 studies using a hybrid search strategy (database search and snowballing). We defined nine characteristics of requirements management in SECO that differentiate it from requirements management in traditional software development. We identified four types of approaches to support requirements management in SECO: tool, method, model, and practice. We found that only three selected studies present an assessment of their approaches. Finally, we characterize requirements management in SECO as an open, informal, collaborative, and decentralized process involving multi-party actors susceptible to power relations. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Malcher, Paulo and Silva, Eduardo and Viana, Davi and Santos, Rodrigo},
	year = {2023},
	keywords = {Ecosystems, Management IS, Software design, Life cycle, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Search engines, Database searches, Hybrid search strategies, Managing requirements, Method model, Requirement management, Software ecosystems, Support requirements},
}

@article{zhao_unraveling_2024,
	title = {Unraveling quantum computing system architectures: {An} extensive survey of cutting-edge paradigms},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179094225&doi=10.1016%2fj.infsof.2023.107380&partnerID=40&md5=fd9ef3dd6d9f7e6bae0e8fa8439e9741},
	abstract = {Context: The convergence of physics and computer science in the realm of quantum computing systems has sparked a profound revolution within the computer industry. However, despite such promise, the existing focus on quantum software systems primarily centers on the generation of quantum source code, inadvertently overlooking the pivotal role of the overall software architecture. Objectives: In order to provide comprehensive guidance to researchers and practitioners engaged in quantum software development, employing an architecture-centered development model, an extensive literature review was conducted pertaining to existing research on quantum software architecture. The analysis encompasses a detailed examination of the characteristics exhibited by these studies and the identification of prospective challenges that lie ahead in the field of quantum software architecture. Methods: We have closely examined instances of quantum software engineering, quantum modeling languages, quantum design patterns, and quantum communication security to gain insights into the distinctive attributes associated with various software architecture approaches. Results: Our findings underscore the critical significance of prioritizing software architecture in the development of robust and efficient quantum software systems. Through the synthesis of these multifaceted aspects, both researchers and practitioners can devise quantum software solutions that are inherently architecture-centric. Conclusion: The software architecture of quantum computing systems plays a pivotal role in determining their ultimate success and usability. Given the ongoing advancements in quantum computing technology, the migration of traditional software architecture development methods to the domain of quantum software development holds significant importance. © 2023 Elsevier B.V.},
	author = {Zhao, Xudong and Xu, Xiaolong and Qi, Lianyong and Xia, Xiaoyu and Bilal, Muhammad and Gong, Wenwen and Kou, Huaizhen},
	year = {2024},
	keywords = {Software design, Computer software, Software-systems, Source codes, Software architecture, Computer industry, Cutting edges, Development model, Modeling languages, Quantum communication, Quantum computers, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Systems architecture},
}

@article{gwenhure_gamification_2024,
	title = {Gamification of {Cybersecurity} {Awareness} for {Non}-{IT} {Professionals}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188663020&doi=10.17083%2fijsg.v11i1.719&partnerID=40&md5=c675878ca8dc3cf7e3db076c1d3b4664},
	abstract = {This literature review delves into research on the gamification of cybersecurity awareness for non-IT professionals, aiming to provide an accurate report on known and unknown information regarding three key questions: the impact of gamification on cybersecurity awareness interest and engagement, measurable results related to game elements and their connection to specific learning goals, and the long-term effectiveness of gamified cybersecurity. Examining five relevant papers, the findings confirm short-term effectiveness and indicate that the incorporation of various game elements, such as storytelling, team leaderboards, and interactive scenarios, results in increased knowledge, improved engagement, and positive behavior changes aligned with specific cybersecurity awareness learning goals. However, the review also identifies recurring gaps in evaluating individual game elements and customizing gamification strategies for non-IT professionals. Highlighting a critical gap in understanding long-term effectiveness, we argue for further empirical studies to consider habituation effects, emphasizing the need for a nuanced understanding of gamification's impact on cybersecurity awareness over an extended period. Thus, the review contributes to the existing body of knowledge by emphasizing the necessity for empirical studies focusing on sustained, long-term effectiveness and habituation effects in gamified cybersecurity initiatives. © 2024, Serious Games Society. All rights reserved.},
	author = {Gwenhure, Anderson Kevin and Rahayu, Flourensia Sapty},
	year = {2024},
}

@article{villarrubia_designscrumagility_2024,
	title = {{DesignScrum}–{An} agility educational resource powered by creativity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181492975&doi=10.1002%2fspe.3308&partnerID=40&md5=4783fe66f11d0583de6f94a416b680d6},
	abstract = {Agile methods have been widely adopted by the industry and its teaching has seen a surge, particularly in the software development field. However, these methods have a number of limitations which affect product outcomes, such as the fact that many software development companies now use Scrum to get developers to work without interruption between iterations, rather than to maintain a sustainable rhythm. Agile experts have stated the importance of incorporating creativity into Scrum, and although there are several agile resources that help with the learning process, it seems essential to approach such learning from a practical point of view. Furthermore, none of these resources introduce creativity. In this paper, we present an educational resource in the form of a serious game that allows you to acquire all the key concepts of agile and creative methods. The game is based on the use of LEGO pieces to simulate a real project, applying the key concepts of the Scrum and Design Thinking frameworks in a gamified way. It was assessed in a professional training centre of computer science by using surveys through which participants evaluated their previous knowledge of agile and creativity methods. We analysed the improvement of these competences, as well as the general level of satisfaction with the game. After the game, the results showed that the participants' knowledge of the Scrum and Design Thinking frameworks had improved and that they were very satisfied with the whole experience. © 2024 The Authors. Software: Practice and Experience published by John Wiley \& Sons Ltd.},
	author = {Villarrubia, Carlos and Vara, Juan Manuel and Granada, David and Gómez-Macías, Cristian and Pérez-Blanco, Francisco Javier},
	year = {2024},
	keywords = {Software design, Learning systems, Gamification, Agile, Agile methods, Agile resource, Creativity, Design thinking, Educational resource, Personnel training, Practice and experience, Scra, Serious games, Software practices},
}

@inproceedings{langner_challenges_2024,
	title = {Challenges for capturing data within data-driven design processes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194079420&doi=10.1017%2fpds.2024.212&partnerID=40&md5=b2cc9c1ee795f82f17c520d338fdb953},
	abstract = {Cyber-Physical-Systems provide extensive data gathering opportunities along the lifecycle, enabling data-driven design to improve the design process. However, its implementation faces challenges, particularly in the initial data capturing stage. To identify those, a comprehensive approach combining a systematic literature review and an industry survey was applied. Four groups of interrelated challenges were identified as most relevant to practitioners: data selection, data availability in systems, knowledge about data science processes and tools, and guiding users in targeted data capturing. © 2024 Proceedings of the Design Society. All rights reserved.},
	author = {Langner, Christopher and Paliyenko, Yevgeni and Müller, Benedikt and Roth, Daniel and Guertler, Matthias R. and Kreimeyer, Matthias},
	year = {2024},
	keywords = {Embedded systems, Systematic literature review, Life cycle, Design, Capturing data, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Data capturing, Data gathering, Data-driven design, Design-process, Industry surveys, Internet of thing, Internet of things},
}

@article{minhas_lessons_2023,
	title = {Lessons learned from replicating a study on information-retrieval-based test case prioritization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174265778&doi=10.1007%2fs11219-023-09650-4&partnerID=40&md5=746a6f83a8dbadb4bea7db51dccd8466},
	abstract = {Replication studies help solidify and extend knowledge by evaluating previous studies’ findings. Software engineering literature showed that too few replications are conducted focusing on software artifacts without the involvement of humans. This study aims to replicate an artifact-based study on software testing to address the gap related to replications. In this investigation, we focus on (i) providing a step-by-step guide of the replication, reflecting on challenges when replicating artifact-based testing research and (ii) evaluating the replicated study concerning the validity and robustness of the findings. We replicate a test case prioritization technique proposed by Kwon et al. We replicated the original study using six software programs, four from the original study and two additional software programs. We automated the steps of the original study using a Jupyter notebook to support future replications. Various general factors facilitating replications are identified, such as (1) the importance of documentation; (2) the need for assistance from the original authors; (3) issues in the maintenance of open-source repositories (e.g., concerning needed software dependencies, versioning); and (4) availability of scripts. We also noted observations specific to the study and its context, such as insights from using different mutation tools and strategies for mutant generation. We conclude that the study by Kwon et al. is partially replicable for small software programs and could be automated to facilitate software practitioners, given the availability of required information. However, it is hard to implement the technique for large software programs with the current guidelines. Based on lessons learned, we suggest that the authors of original studies need to publish their data and experimental setup to support the external replications. © 2023, The Author(s).},
	author = {Minhas, Nasir Mehmood and Irshad, Mohsin and Petersen, Kai and Börstler, Jürgen},
	year = {2023},
	keywords = {Software testing, Open source software, Software testings, Information retrieval, Replication study, Open systems, Prioritization techniques, Regression testing, Replication, SIR, Software artefacts, Software project, Technique, Test case prioritization},
}

@article{idlahcen_exploring_2024,
	title = {Exploring data mining and machine learning in gynecologic oncology},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183390317&doi=10.1007%2fs10462-023-10666-2&partnerID=40&md5=e20c182acb99fcbd45f1023721557013},
	abstract = {Gynecologic (GYN) malignancies are gaining new and much-needed attention, perpetually fueling literature. Intra-/inter-tumor heterogeneity and “frightened” global distribution by race, ethnicity, and human development index, are pivotal clues to such ubiquitous interest. To advance “precision medicine” and downplay the heavy burden, data mining (DM) is timely in clinical GYN oncology. No consolidated work has been conducted to examine the depth and breadth of DM applicability as an adjunct to GYN oncology, emphasizing machine learning (ML)-based schemes. This systematic literature review (SLR) synthesizes evidence to fill knowledge gaps, flaws, and limitations. We report this SLR in compliance with Kitchenham and Charters’ guidelines. Defined research questions and PICO crafted a search string across five libraries: PubMed, IEEE Xplore, ScienceDirect, SpringerLink, and Google Scholar—over the past decade. Of the 3499 potential records, 181 primary studies were eligible for in-depth analysis. A spike (60.53\%) corollary to cervical neoplasms is denoted onward 2019, predominantly featuring empirical solution proposals drawn from cohorts. Medical records led (23.77\%, 53 art.). DM-ML in use is primarily built on neural networks (127 art.), appoint classification (73.19\%, 172 art.) and diagnoses (42\%, 111 art.), all devoted to assessment. Summarized evidence is sufficient to guide and support the clinical utility of DM schemes in GYN oncology. Gaps persist, inculpating the interoperability of single-institute scrutiny. Cross-cohort generalizability is needed to establish evidence while avoiding outcome reporting bias to locally, site-specific trained models. This SLR is exempt from ethics approval as it entails published articles. © 2024, The Author(s).},
	author = {Idlahcen, Ferdaous and Idri, Ali and Goceri, Evgin},
	year = {2024},
	keywords = {Systematic literature review, Machine learning, Machine-learning, Diagnosis, Data mining, Female reproductive system, Global distribution, Gynecologic AI, Gynecologic malignancies, Human development index, Neoplasm, Oncology, Reproductive systems, Tumor heterogeneity, Tumors},
}

@article{koch_cloud-based_2023,
	title = {Cloud-{Based} {Reinforcement} {Learning} in {Automotive} {Control} {Function} {Development}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172153714&doi=10.3390%2fvehicles5030050&partnerID=40&md5=916bf16f7660e18b017ab960469db241},
	abstract = {Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process. Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner. Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry. To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library. It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training. We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario. The evolved control strategy produces a smooth trajectory with energy savings of up to 14\%. The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework. © 2023 by the authors.},
	author = {Koch, Lucas and Roeser, Dennis and Badalian, Kevin and Lieb, Alexander and Andert, Jakob},
	year = {2023},
}

@article{hernandez_requirements_2023,
	title = {Requirements management in {DevOps} environments: a multivocal mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146221014&doi=10.1007%2fs00766-023-00396-w&partnerID=40&md5=4b84297f3c4a81569ac37e71e82d3cce},
	abstract = {Attention is currently being focused on DevOps, which aims to reduce software development time by means of close collaboration between the development and operations areas. However, little effort has been made to determine the role of requirements management in DevOps. The objective of this study is to help both researchers and practitioners by providing an overview of the best practices regarding requirements engineering in DevOps and identifying which areas still need to be investigated. A multivocal mapping study has, therefore, been carried out in order to study which methodologies, techniques and tools are used to support requirements management in DevOps environments. After applying the review protocol, 37 papers from formal literature and 14 references from grey literature were selected for analysis. The general conclusions obtained after analysing these papers were that, within DevOps, more attention should be paid to: (1) the reuse of requirements in order to identify systems and software artefacts that can serve as a basis for the specification of new projects; (2) the communication of requirements between the different areas of an organisation and the stakeholders of a project; (3) the traceability of requirements in order to identify the relationship with other requirements, artefacts, tasks and processes; (4) non-functional requirements in order to identify the requirements of the operations area in the early phases of a project; and finally (5) specific requirements tools that should be seamlessly integrated into the DevOps toolchain. All these issues must be considered without ignoring the agile and continuous practices of development, operations and business teams. More effort must also be made to validate new methodologies in industry so as to assess and determine their strengths and weaknesses. © 2023, The Author(s).},
	author = {Hernández, Rogelio and Moros, Begoña and Nicolás, Joaquín},
	year = {2023},
	keywords = {Software design, Computer software reusability, Mapping, Requirement engineering, Requirements engineering, Development time, Mapping studies, Best practices, Requirement management, Agile requirement engineering, Agile requirements, Development and operations, Environmental management, Multivocal mapping study},
}

@article{spinellis_open_2023,
	title = {Open reproducible scientometric research with {Alexandria3k}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178516434&doi=10.1371%2fjournal.pone.0294946&partnerID=40&md5=81c76085ccde3fc925309b30cdec682b},
	abstract = {Considerable scientific work involves locating, analyzing, systematizing, and synthesizing other publications, often with the help of online scientific publication databases and search engines. However, use of online sources suffers from a lack of repeatability and transparency, as well as from technical restrictions. Alexandria3k is a Python software package and an associated command-line tool that can populate embedded relational databases with slices from the complete set of several open publication metadata sets. These can then be employed for reproducible processing and analysis through versatile and performant queries. We demonstrate the software’s utility by visualizing the evolution of publications in diverse scientific fields and relationships among them, by outlining scientometric facts associated with COVID-19 research, and by replicating commonly-used bibliometric measures and findings regarding scientific productivity, impact, and disruption. Copyright: © 2023 Diomidis Spinellis.},
	author = {Spinellis, Diomidis},
	year = {2023},
	keywords = {Metadata, Article, coronavirus disease 2019, human, software, Alexandria3k software, bibliometrics, Bibliometrics, Databases, dynamics, Factual, factual database, journal impact factor, measurement repeatability, metadata, methodology, online analytical processing, productivity, proof of concept, publication, Research Design, scientometrics, search engine, Search Engine},
}

@article{soares_trends_2023,
	title = {Trends in continuous evaluation of software architectures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147748180&doi=10.1007%2fs00607-023-01161-1&partnerID=40&md5=ff7020687f6de6e3c5a34f6dda65d4d7},
	abstract = {The software industry is increasingly facing the need for continuous deployment of systems. This leads to the adoption of continuous activities, including planning, integration, and deployment (a.k.a. Continuous Software Engineering (CSE)). At the same time, systems should exhibit high-quality architectures, which are often achieved through architecture evaluation methods. However, there is little insight of how such evaluation happens in the context of CSE. To cover this gap, we investigate in this work the state of the art of continuous evaluation of software architectures in CSE, including agile processes like SCRUM. For this, we systematically examine the literature to collect and summarize evidence. Our results show a diversity of means for evaluating architectures in continuous mode to support the continuous evolution of systems. We also found how such evaluation has been incorporated within continuous development processes and agile processes like SCRUM and Crystal. We finally derive the main trends and open issues in the area, aiming to support the community to better understand and further consolidate the field of continuous evaluation of software architectures. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.},
	author = {Soares, Rodrigo C. and Capilla, Rafael and dos Santos, Vinicius and Nakagawa, Elisa Yumi},
	year = {2023},
	keywords = {State of the art, High quality, Continuous software engineerings, Agile process, Evaluation methods, Quality control, Software architecture, Agile manufacturing systems, Architecture evaluation, Continuous architecture evaluation, Software industry, Time systems},
}

@inproceedings{zellmer_product-structuring_2023,
	title = {Product-{Structuring} {Concepts} for {Automotive} {Platforms}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175948175&doi=10.1145%2f3579027.3608988&partnerID=40&md5=f19a532ed313f3874c3d38986b4968ef},
	abstract = {The products of the automotive industry are facing one of the biggest changes: becoming digital smart devices on wheels. Driven by the rising amount of vehicle functions, electronic control units, and software, today's vehicles are becoming cyber-physical systems that are increasingly complex and hard to manage over their life cycle. To handle these challenges, the automotive industry is adopting and integrating methods like software product-line engineering, electrics/electronics platforms, and product generation. While these concepts are widely recognized in their respective research areas and various domains, there is limited research regarding the practical effectiveness of implementing these concepts in a software-driven automotive context. In this paper, we investigate existing product-structuring concepts and methods that consider both hardware and software artifacts, and their applicability to the automotive as well as other cyber-physical industries. For this purpose, we conducted a systematic mapping study to capture a comprehensive overview of existing product-structuring concepts and methods, based on which we discuss how the state-of-the-art can or cannot help solve the challenges of the automotive industry. Specifically, we analyze the practical applicability of the existing solutions to help practitioners apply them and to guide future research. © 2023 ACM.},
	author = {Zellmer, Philipp and Holsten, Lennart and Leich, Thomas and Krüger, Jacob},
	year = {2023},
	keywords = {Embedded systems, Life cycle, Mapping, Systematic mapping studies, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Big changes, Digital devices, Electric electronics, Electric lines, Lifecycle management, Product structuring, Product-structuring concept, Productline},
}

@article{ali_intelligent_2023,
	title = {Intelligent {Decision} {Support} {Systems}—{An} {Analysis} of {Machine} {Learning} and {Multicriteria} {Decision}-{Making} {Methods}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187262298&doi=10.3390%2fapp132212426&partnerID=40&md5=dbf125e709aa73f98b50e57c1475067f},
	abstract = {Context: The selection and use of appropriate multi-criteria decision making (MCDM) methods for solving complex problems is one of the challenging issues faced by decision makers in the search for appropriate decisions. To address these challenges, MCDM methods have effectively been used in the areas of ICT, farming, business, and trade, for example. This study explores the integration of machine learning and MCDM methods, which has been used effectively in diverse application areas. Objective: The objective of the research is to critically analyze state-of-the-art research methods used in intelligent decision support systems and to further identify their application areas, the significance of decision support systems, and the methods, approaches, frameworks, or algorithms exploited to solve complex problems. The study provides insights for early-stage researchers to design more intelligent and cost-effective solutions for solving problems in various application domains. Method: To achieve the objective, literature from the years 2015 to early 2020 was searched and considered in the study based on quality assessment criteria. The selected relevant literature was studied to respond to the research questions proposed in this study. To find answers to the research questions, pertinent literature was analyzed to identify the application domains where decision support systems are exploited, the impact and significance of the contributions, and the algorithms, methods, and techniques which are exploited in various domains to solve decision-making problems. Results: Results of the study show that decision support systems are widely used as useful decision-making tools in various application domains. The research has collectively studied machine learning, artificial intelligence, and multi-criteria decision-making models used to provide efficient solutions to complex decision-making problems. In addition, the study delivers detailed insights into the use of AI, ML and MCDM methods to the early-stage researchers to start their research in the right direction and provide them with a clear roadmap of research. Hence, the development of Intelligent Decision Support Systems (IDSS) using machine learning (ML) and multicriteria decision-making (MCDM) can assist researchers to design and develop better decision support systems. These findings can help researchers in designing more robust, efficient, and effective multicriteria-based decision models, frameworks, techniques, and integrated solutions. © 2023 by the authors.},
	author = {Ali, Rahman and Hussain, Anwar and Nazir, Shah and Khan, Sulaiman and Khan, Habib Ullah},
	year = {2023},
}

@article{yang_automatic_2023,
	title = {Automatic {Essay} {Evaluation} {Technologies} in {Chinese} {Writing}—{A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174165294&doi=10.3390%2fapp131910737&partnerID=40&md5=a2e9a5b11c5c1af33cd825ae61aeaf0e},
	abstract = {Automatic essay evaluation, an essential application of natural language processing (NLP) technology in education, has been increasingly employed in writing instruction and language proficiency assessment. Because automatic Chinese Essay Evaluation (ACEE) has made some breakthroughs due to the rapid development of upstream Chinese NLP technology, many evaluation tools have been applied in teaching practice and high-risk evaluation processes. However, the development of ACEE is still in its early stages, with many technical bottlenecks and challenges. This paper systematically explores the current research status of corpus construction, feature engineering, and scoring models in ACEE through literature to provide a technical perspective for stakeholders in the ACEE research field. Literature research has shown that constructing the ACEE public corpus is insufficient and lacks an effective platform to promote the development of ACEE research. Various shallow and deep features can be extracted using statistical and NLP techniques in ACEE. However, there are still substantial limitations in extracting grammatical errors and features related to syntax and traditional Chinese Literary style. For the construction of scoring models, existing studies have shown that traditional machine learning and deep learning methods each have advantages in different corpora and feature selections. The deep learning model, which exhibits strong adaptability and multi-task joint learning potential, has broader development space regarding model scalability. © 2023 by the authors.},
	author = {Yang, Hongwu and He, Yanshan and Bu, Xiaolong and Xu, Hongwen and Guo, Weitong},
	year = {2023},
}

@article{sadeghiani_what_2023,
	title = {What pivot is: {Touching} an elephant in the dark},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85155452238&doi=10.1016%2fj.digbus.2023.100056&partnerID=40&md5=4b53b6bc91a08e0cc741882f699640c0},
	abstract = {The term ‘pivot’ first appeared in the practical literature, demonstrating its role in the Lean Startup, and immediately emerged in the academic literature. However, it suffered from conflicting perceptions and practice-academy divide. Moreover, recently, a growing number of scholars introduced it as a response to the pandemic crisis, at times as an umbrella term. In this study, we first test the concept ‘pivot’ against clarity criteria and find its problematic issues based on a qualitative content analysis of the literature; then, using a multi-case study, we get a critical distance from the pivot's origin; parsimoniously reposition it as ‘substitution’, differentiate it from ‘pivoting’ as its process theory; and discuss it in relation to vision, strategy, and business model. Finally, we propose a conceptual basis and some rules for operationalization. © 2023 The Authors},
	author = {Sadeghiani, Ayoob and Anderson, Alistair},
	year = {2023},
}

@book{furtado_controlled_2023,
	title = {Controlled experimentation of software product lines},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160474401&doi=10.1007%2f978-3-031-18556-4_19&partnerID=40&md5=dd6f8e961dba036421d866a0cbda7a31},
	abstract = {The process of experimentation is one of several scientific methods that can provide evidence for a proof of a theory. This process is counterpoint to the real world observation method, thus providing a reliable body of knowledge. However, in the experimentation for emerging areas and in the consolidation process in scientific and industrial communities, such as the software product line (SPL), there has been a constant lack of adequate documentation of experiments that makes it difficult to repeat, replicate, and reproduce studies in SPL. Therefore, this chapter presents a set of guidelines for the quality assessment of SPL experiments with its conceptual model to support the understanding of the proposed guidelines, as well as an ontology for SPL experiments, called OntoExper-SPL, in addition to support the teaching experimentation in SPL. Thus, these points aim to improve the planning, conduction, analysis, sharing, and documentation of SPL experiments, supporting the construction of a reliable and reference body of knowledge in such a context in addition to enabling improvement in the teaching of SPL experiments. © Springer Nature Switzerland AG 2023. All rights reserved.},
	author = {Furtado, Viviane R. and Vignando, Henrique and Luz, Carlos D. and Steinmacher, Igor F. and Kalinowski, Marcos and OliveiraJr, Edson},
	year = {2023},
}

@article{yin_predicting_2023,
	title = {Predicting {Changes} in {User}-{Driven} {Requirements} {Using} {Conditional} {Random} {Fields} in {Agile} {Software} {Development}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112600382&doi=10.1109%2fTEM.2021.3083513&partnerID=40&md5=ce8def99d83c5673583af52f67d3b7a6},
	abstract = {Agile development encourages requirements change. The accurate predictions of changes in user requirements could help software evolve in the right direction to increase user satisfaction. Previous research on requirement predictions mostly relies on historical defects or user feedback. In this article, we aim to predict requirement changes based on user-system interactions in agile software development. We focus on the user-system interaction behaviors used to infer user intentions and predict requirement changes to drive the incremental iterations of agile development. Through a prototype system with two incremental iterations, an embedded program in the system captures the user runtime interaction behavior data. We utilize the conditional random fields to explore the user potential intentions and infer the emerging requirements accordingly. The increased accuracy of results in the iterations proves the effectiveness of our approach in predicting user requirement changes. © 1988-2012 IEEE.},
	author = {Yin, Ming and Meng, Danli and Zhu, Dan and Wang, Yibo and Jiang, Jijiao},
	year = {2023},
	keywords = {Software design, Software testing, Agile software development, Behavioral research, Forecasting, Digital storage, Agile manufacturing systems, Behaviour patterns, Conditional random field, Hidden Markov models, Hidden-Markov models, Interactive computer systems, Micromechanical device, Prototype, Random fields, Real - Time system, Real time systems, Requirements change, Software, User driven, User intention inference, User-driven development, User's intentions},
}

@article{innocente_framework_2023,
	title = {A framework study on the use of immersive {XR} technologies in the cultural heritage domain},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121296&doi=10.1016%2fj.culher.2023.06.001&partnerID=40&md5=eb7e017ab19913950cb9d4779e742030},
	abstract = {Most cultural promotion and dissemination are nowadays performed through the digitization of heritage sites and museums, a necessary requirement to meet the new needs of the public. Augmented Reality (AR), Mixed Reality (MR), and Virtual Reality (VR) have the potential to improve the experience quality and educational effect of these sites by stimulating users’ senses in a more natural and vivid way. In this respect, head-mounted display (HMD) devices allow visitors to enhance the experience of cultural sites by digitizing information and integrating additional virtual cues about cultural artifacts, resulting in a more immersive experience that engages the visitor both physically and emotionally. This study contributes to the development and incorporation of AR, MR, and VR applications in the cultural heritage domain by providing an overview of relevant studies utilizing fully immersive systems, such as headsets and CAVE systems, emphasizing the advantages that they bring when compared to handheld devices. We propose a framework study to identify the key features of headset-based Extended Reality (XR) technologies used in the cultural heritage domain that boost immersion, sense of presence, and agency. Furthermore, we highlight core characteristics that favor the adoption of these systems over more traditional solutions (e.g., handheld devices), as well as unsolved issues that must be addressed to improve the guests’ experience and the appreciation of the cultural heritage. An extensive search of Google Scholar, Scopus, IEEE Xplore, ACM Digital Library, and Wiley Online Library databases was conducted, including papers published from January 2018 to September 2022. To improve review reporting, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were used. Sixty-five papers met the inclusion criteria and were classified depending on the study's purpose: education, entertainment, edutainment, touristic guidance systems, accessibility, visitor profiling, and management. Immersive cultural heritage systems allow visitors to feel completely immersed and present in the virtual environment, providing a stimulating and educational cultural experience that can improve the quality and learning purposes of cultural visits. Nonetheless, the analyzed studies revealed some limitations that must be faced to give a further impulse to the adoption of these technologies in the cultural heritage domain. © 2023 Consiglio Nazionale delle Ricerche (CNR)},
	author = {Innocente, Chiara and Ulrich, Luca and Moos, Sandro and Vezzetti, Enrico},
	year = {2023},
}

@book{liu_space-air-ground_2023,
	title = {Space-air-ground integrated network security},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169763096&doi=10.1007%2f978-981-99-1125-7&partnerID=40&md5=0650c0f66edd4c9e68a8c9fea0b9de9a},
	abstract = {This book focuses on security science and technology, data and information security, and mobile and network security for space-air-ground integrated networks (SAGINs). SAGIN are expected to play an increasingly important role in providing real-time, flexible, and integrated communication and data transmission services in an efficient manner. Today, SAGINs have been widely developed for a range of applications in navigation, environmental monitoring, traffic management, counter-terrorism, etc. However, security becomes a major concern, since the satellites, spacecrafts, and aircrafts are susceptible to a variety of traditional/specific network-based attacks, including eavesdropping, session hijacking, and illegal access. In this book, we review the theoretical foundations of SAGIN security. We also address a range of related security threats and provide cutting-edge solutions in the aspect of ground network security, airborne network security, space network security, and provide future trends in SAGIN security. The book goes from an introduction to the topic's background, to a description of the basic theory, and then to cutting-edge technologies, making it suitable for readers at all levels including professional researchers and beginners. To gain the most from the book, readers should have taken prior courses in information theory, cryptography, network security, etc. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023. All rights reserved.},
	author = {Liu, Jianwei and Bai, Lin and Jiang, Chunxiao and Zhang, Wei},
	year = {2023},
}

@article{stradowski_industrial_2023,
	title = {Industrial applications of software defect prediction using machine learning: {A} business-driven systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150041533&doi=10.1016%2fj.infsof.2023.107192&partnerID=40&md5=adef39458fbdf72dbe9232624eef7f22},
	abstract = {Context: Machine learning software defect prediction is a promising field of software engineering, attracting a great deal of attention from the research community; however, its industry application tents to lag behind academic achievements. Objective: This study is part of a larger project focused on improving the quality and minimising the cost of software testing of the 5G system at Nokia, and aims to evaluate the business applicability of machine learning software defect prediction and gather lessons learnt. Methods: The systematic literature review was conducted on journal and conference papers published between 2015 and 2022 in popular online databases (ACM, IEEE, Springer, Scopus, Science Direct, and Google Scholar). A quasi-gold standard procedure was used to validate the search, and SEGRESS guidelines were used for transparency, reporting, and replicability. Results: We have selected and analysed 32 publications out of 397 found by our automatic search (and seven by snowballing). We have identified highly relevant evidence of methods, features, frameworks, and datasets used. However, we found a minimal emphasis on practical lessons learnt and cost consciousness — both vital from a business perspective. Conclusion: Even though the number of machine learning software defect prediction studies validated in the industry is increasing (and we were able to identify several excellent papers on studies performed in vivo), there is still not enough practical focus on the business aspects of the effort that would help bridge the gap between the needs of the industry and academic research. © 2023 The Authors},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	keywords = {Systematic literature review, Software testing, Costs, Application programs, Machine learning, Machine-learning, Defects, Forecasting, Software defect prediction, 5G mobile communication systems, Cost minimization, Effort and cost minimization, Industry applications, Lesson learnt, Machine learning software, Real-world, Research communities},
}

@article{watson_augmented_2023,
	title = {Augmented {Behavioral} {Annotation} {Tools}, with {Application} to {Multimodal} {Datasets} and {Models}: {A} {Systematic} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171380178&doi=10.3390%2fai4010007&partnerID=40&md5=8cad8eef1ab5b05d19d8bea8f2461e3d},
	abstract = {Annotation tools are an essential component in the creation of datasets for machine learning purposes. Annotation tools have evolved greatly since the turn of the century, and now commonly include collaborative features to divide labor efficiently, as well as automation employed to amplify human efforts. Recent developments in machine learning models, such as Transformers, allow for training upon very large and sophisticated multimodal datasets and enable generalization across domains of knowledge. These models also herald an increasing emphasis on prompt engineering to provide qualitative fine-tuning upon the model itself, adding a novel emerging layer of direct machine learning annotation. These capabilities enable machine intelligence to recognize, predict, and emulate human behavior with much greater accuracy and nuance, a noted shortfall of which have contributed to algorithmic injustice in previous techniques. However, the scale and complexity of training data required for multimodal models presents engineering challenges. Best practices for conducting annotation for large multimodal models in the most safe and ethical, yet efficient, manner have not been established. This paper presents a systematic literature review of crowd and machine learning augmented behavioral annotation methods to distill practices that may have value in multimodal implementations, cross-correlated across disciplines. Research questions were defined to provide an overview of the evolution of augmented behavioral annotation tools in the past, in relation to the present state of the art. (Contains five figures and four tables). © 2023, Multidisciplinary Digital Publishing Institute (MDPI). All rights reserved.},
	author = {Watson, Eleanor and Viana, Thiago and Zhang, Shujun},
	year = {2023},
}

@article{xu_systematic_2023,
	title = {A systematic mapping study on machine learning methodologies for requirements management},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145104779&doi=10.1049%2fsfw2.12082&partnerID=40&md5=d8c1d596f27bfb9f05d19106fa83be54},
	abstract = {Requirements management (RM) plays an important role in requirements engineering. The development of machine learning (ML) is in full swing, and many ML software management techniques had been used to improve the performance of RM methods. However, as no research study is known that exists systematically to summarise the ML methods used in RM. To fill this gap, this paper adopts the systematic mapping study to survey the state-of-the-art ML methods for RM primary studies and were finally selected in this mapping, which was published on 36 conferences and journals. The 24 factors affecting the ML method of RM are determined, of which 9, 11 and 4 are the three parts of RM, namely requirements baseline maintenance, requirements traceability and requirements change management separately. The 18 objectives of the ML method for RM are summarised, of which 6, 7 and 5 are the three parts of RM. The eight ML methods used in RM and their time sequence are summarised. The 18 evaluation indexes for RM in the ML method are determined, and the performance of these methods on these parameters is analysed. The research direction of this paper is of great significance to the research of researchers in demand management. © 2022 The Authors. IET Software published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author = {Xu, Chi and Li, Yuanbang and Wang, Bangchao and Dong, Shi},
	year = {2023},
	keywords = {Management IS, Mapping, Systematic mapping studies, Performance, Requirement engineering, Requirements engineering, Machine learning, Machine-learning, Requirement management, Machine learning software, Full-swing, Machine learning methods, On-machines},
}

@article{stradowski_machine_2023,
	title = {Machine learning in software defect prediction: {A} business-driven systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144824314&doi=10.1016%2fj.infsof.2022.107128&partnerID=40&md5=c3aa54434eb54f1abaa22279bb12f8ef},
	abstract = {Context: Machine learning is a valuable tool in software engineering allowing fair defect prediction capabilities at a relatively small expense. However, although the practical usage of machine learning in defect prediction has been studied over many years, there is not sufficient systematic effort to analyse its potential for business application. Objective: The following systematic mapping study aims to analyse the current state-of-the-art in terms of machine learning software defect prediction modelling and to identify and classify the emerging new trends. Notably, the analysis is done from a business perspective, evaluating the opportunities to adopt the latest techniques and methods in commercial settings to improve software quality and lower the cost of development life cycle. Method: We created a broad search universe to answer our research questions, performing an automated query through the Scopus database to identify relevant primary studies. Next, we evaluated all found studies using a classification scheme to map the extent of business adoption of machine learning software defect prediction based on the keywords used in the publications. Additionally, we use PRISMA 2020 guideline to validate reporting. Results: After the application of the selection criteria, the remaining 742 primary studies included in Scopus until February 23, 2022 were mapped to classify and structure the research area. The results confirm that the usage of commercial datasets is significantly smaller than the established datasets from NASA and open-source projects. However, we have also found meaningful emerging trends considering business needs in analysed studies. Conclusions: There is still a considerable amount of work to fully internalise business applicability in the field. Performed analysis has shown that purely academic considerations dominate in published research; however, there are also traces of in vivo results becoming more available. Notably, the created maps offer insight into future machine learning software defect prediction research opportunities. © 2022 Elsevier B.V.},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	keywords = {Life cycle, Mapping, Systematic mapping studies, Open source software, Machine learning, Machine-learning, Defect prediction, Defects, Forecasting, Software defect prediction, Computer software selection and evaluation, Quality control, Cost minimization, Effort and cost minimization, Machine learning software, Business applicability, Business applications, Cost benefit analysis, NASA, Prediction capability, Query processing},
}

@article{medeiros_visualizations_2023,
	title = {Visualizations for the evolution of {Variant}-{Rich} {Systems}: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140215376&doi=10.1016%2fj.infsof.2022.107084&partnerID=40&md5=448386c9479f057b5f1f362d53b945fa},
	abstract = {Context: Variant-Rich Systems (VRSs), such as Software Product Lines or variants created through clone \& own, aim at reusing existing assets. The long lifespan of families of variants, and the scale of both the code base and the workforce make VRS maintenance and evolution a challenge. Visualization tools are a needed companion. Objective: We aim at mapping the current state of visualization interventions in the area of VRS evolution. We tackle evolution in both functionality and architecture. Three research questions are posed: What sort of analysis is being conducted to assess VRS evolution? (Analysis perspective); What sort of visualizations are displayed? (Visualization perspective); What is the research maturity of the reported interventions? (Maturity perspective). Methods: We performed a systematic mapping study including automated search in digital libraries, expert knowledge, and snowballing. Results: The study reports on 41 visualization approaches to cope with VRS evolution. Analysis wise, feature identification and location is the most popular scenario, followed by variant integration towards a Software Product Line. As for visualization, nodelink diagram visualization is predominant while researchers have come up with a wealth of ingenious visualization approaches. Finally, maturity wise, almost half of the studies are solution proposals. Most of the studies provide proof-of-concept, some of them also include publicly available tools, yet very few face proof-of-value. Conclusions: This study introduces a comparison framework where to frame future studies. It also points out distinct research gaps worth investigating as well as shortcomings in the evidence about relevance and contextual considerations (e.g., scalability). © 2022 The Author(s)},
	author = {Medeiros, Raul and Martinez, Jabier and Díaz, Oscar and Falleri, Jean-Rémy},
	year = {2023},
	keywords = {Software design, Software Product Line, Mapping, Systematic mapping studies, Computer software, Product variants, Mapping studies, Visualization, Digital libraries, Evolution, Lifespans, System evolution, System maintenance, Variant-rich system, Visualization tools},
}

@inproceedings{claderon-blas_medical_2023,
	title = {Medical {Recommender} {Systems}: a {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192802746&doi=10.1109%2fENC60556.2023.10508695&partnerID=40&md5=eb26de296c721791c678cf1e70075fea},
	abstract = {Medical recommender systems are applications in the field of health. These systems use Artificial Intelligence techniques to provide personalized recommendations to healthcare professionals and patients, based on available and relevant patient information. Software engineering is essential in developing medical recommender systems, as these systems must be accurate, reliable, and secure for use in clinical settings. This work presents a Systematic Literature Review based on the Kitchenham and Charters guide, in order to explore the Artificial Intelligence techniques used in this type of system, which can be incorporated or improved by software developers who participate in this type of project. Twelve primary studies were selected, where mainly machine learning approaches were identified (algorithms based on decision trees, neural networks, Bayesian classifiers and clustering such as k-means), matrix approaches, based on rules, among others. Precision, Recall, and Root Mean Square Error (RMSE) were the main measures used to evaluate the performance of these systems. Finally, the studies propose always increasing the sample size of the tests carried out, including relevant patient information such as social networks and clinical information, as well as exploring other algorithms and approaches that allow improving the results of the recommendation. © 2023 IEEE.},
	author = {Claderón-Blas, Javier A. and Cerdan, María Angélica and Sánchez-García, Ángel J. and Domingue-Isidro, Saúl},
	year = {2023},
	keywords = {Systematic literature review, Learning algorithms, Artificial intelligence techniques, Machine learning, Decision trees, Software, Bayesian networks, Clinical settings, Health care professionals, K-means clustering, Mean square error, Medical recommende system, Metric, Patient information, Personalized recommendation, Recommender systems, System use},
}

@article{muhammad_human_2023,
	title = {Human factors in developing automated vehicles: {A} requirements engineering perspective},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166738978&doi=10.1016%2fj.jss.2023.111810&partnerID=40&md5=a7e6d5d79bc57f87ebe0adaee8fd4182},
	abstract = {Automated Vehicle (AV) technology has evolved significantly both in complexity and impact and is expected to ultimately change urban transportation. Due to this evolution, the development of AVs challenges the current state of automotive engineering practice, as automotive companies increasingly include agile ways of working in their plan-driven systems engineering—or even transition completely to scaled-agile approaches. However, it is unclear how knowledge about human factors (HF) and technological knowledge related to the development of AVs can be brought together in a way that effectively supports today's rapid release cycles and agile development approaches. Based on semi-structured interviews with ten experts from industry and two experts from academia, this qualitative, exploratory case study investigates the relationship between HF and AV development. The study reveals relevant properties of agile system development and HF, as well as the implications of these properties for integrating agile work, HF, and requirements engineering. According to the findings, which were evaluated in a workshop with experts from academia and industry, a culture that values HF knowledge in engineering is key. These results promise to improve the integration of HF knowledge into agile development as well as to facilitate HF research impact and time to market. © 2023 The Author(s)},
	author = {Muhammad, Amna Pir and Knauss, Eric and Bärgman, Jonas},
	year = {2023},
	keywords = {Requirement engineering, Requirements engineering, Automation, 'current, Agile development, Agile, Agile manufacturing systems, Automated vehicles, Automotive companies, Engineering perspective, Engineering practices, Human engineering, Property, Urban transportation, Vehicle technology, Vehicles},
}

@inproceedings{de_souza_challenges_2023,
	title = {On {Challenges} and {Opportunities} of {Using} {Continuous} {Experimentation} in the {Engineering} of {Contemporary} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166029896&doi=10.1145%2f3592813.3592927&partnerID=40&md5=b03d1df07b3686a2afc6fb07af95d244},
	abstract = {Context: Modern Information Systems require the use of contemporary software systems such as Cyber-Physical Systems, Embedded Systems, and Smart Cities-based Systems, eventually built under the paradigm of the Internet of Things. These Contemporary Software Systems (CSS) add new challenges for their construction, maintainability, and evolution, including the involvement of many actors with the software project and the necessary management of dependencies among hardware/things, software systems, and people. Problem: These technological challenges jeopardize the final quality of modern information systems due to the lack of adequate mechanisms supporting the engineering of CSS. Solution: Continuous Experimentation (CE) deserves some investigation regarding its suitability to mitigate and reduce engineering CSS risks. IS Theory: This research is under the General Systems Theory and is consistent with the Systems Information challenges regarding building smart cities-based systems. Method: To undertake a Structured Literature Review (StLR) supported with snowballing to reveal CE’s empirical studies. Results: The StLR identified seven primary studies on CE adoption to support CSS building. Many studies are in the domain of embedded systems and CPS. Besides, the findings allowed us to conjecture a set of challenges and opportunities regarding using CE in CSS engineering. Conclusion: There are emergent technologies to support CE’s execution in the context of web-based systems. However, several challenges and gaps surround CE’s use for engineering CSS. Furthermore, the lack of software technologies, blueprints, or concrete guidance to promote CE in these software systems can motivate further investigations into its use in engineering the important parts of modern information systems. © 2023 Copyright held by the owner/author(s).},
	author = {de Souza, Bruno P. and dos Santos, Paulo Sérgio M. and Travassos, Guilherme H.},
	year = {2023},
	keywords = {Embedded systems, Information systems, Information use, Literature reviews, Computer software, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Industry 4.0, Project management, Cybe-physical systems, Cyber-physical systems, Software project, Blueprints, Embedded-system, Smart city, Software systems risks, Technological challenges},
}

@article{mars_survey_2023,
	title = {A survey on automation approaches of smart contract generation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153378608&doi=10.1007%2fs11227-023-05262-8&partnerID=40&md5=77add21c036dff71f65ebea7c8b82f5f},
	abstract = {In the blockchain environment, smart contracts are computer programs that run on the blockchain platform. However, the development of smart contracts is a major challenge for developers, since blockchain platforms are still evolving. Owing to the inherited nature of blockchain, developing smart contracts without introducing vulnerabilities is not an easy task, as the deployed code is immutable and can be invoked by anyone with access to the network. Smart contracts have proved to be error-prone in practice due to the complexity of programming. Additionally, non-functional requirements, such as service cost, security, performance, authorization, and authentication, should be well implemented and defined in computer systems. In this paper, we aim to present a systematic literature review to outline in detail different approaches of smart contracts generation. Furthermore, we present a comparison of the existing approaches based on a classification according to automation paradigm and a set of defined criteria. Finally, we discuss the gaps in the literature, as well as identify a set of potential challenges which can significantly strengthen the existing work. The study shows that the examined works focused only on a limited number of specific features, such as authorization, asset control, and security. Additionally, formal verification of smart contracts and data privacy are poorly addressed. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Mars, Rawya and Cheikhrouhou, Saoussen and Kallel, Slim and Hadj Kacem, Ahmed},
	year = {2023},
	keywords = {Systematic literature review, Automation, Data privacy, Network security, Non-functional requirements, Block-chain, Blockchain, Smart contract, Authentication, Authorization, Error prones, Security authentication, Security performance, Service authentication, Service costs, Service security, Smart contract generation},
}

@article{sun_design_2023,
	title = {A {Design} of {Automatic} {Magnetic} {Properties} {Measurement} {System} for {Under}/{Postgraduate} {Open} {Experimental} {Project}: {Effective}, {Low} {Cost}, and {Scalable}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135754845&doi=10.1109%2fTE.2022.3194356&partnerID=40&md5=a88d8dccf12434e4ff710534ab824b0e},
	abstract = {Contribution: This research designs an experimental education project of an automatic material magnetism properties measurement system. It explores how the do-it-yourself (DIY), hands-on establishment, and hardware-software integration experiment system could be leveraged to enhance the understanding of the electromagnetism theory and nonelectrical quantity measurement application. Background: Designing and establishing a prototype of an automatic measurement system are essential, inspiring and beneficial to comprehending and consolidating the theoretical principles and simulations in electromagnetism and nonelectrical quantity measurement technology courses. Furthermore, in the vision of batch deployment and flexible group teaching, a cost-efficiency, hard/soft integration system for the electromagnetism property measurement definitely fits the aim of innovation laboratory education. Intended Outcomes: This project is intended to guide the students to finish a comprehensive research-oriented project loop for multiple skills training. It is also intended to encourage student motivation and confidence through theory investigation, hands-on group collaboration, and tunning for performance calibration. Application Design: The design of the proposed project includes the literature review, hardware selection and assembly, system integration, software programming, interface visualization, and system performance calibration. The participants are assessed with both objective exam questions and subjective open survey for training objectives to reveal their increased knowledge, interests, skill confidence, and creative thinking. Findings: The proposed project has been used in 4-year laboratory open experiments with students at different levels from freshmen undergraduates to 1st-year postgraduates. A use case and survey assessment demonstrate tremendous knowledge, skills, and confidence improvement among students in electromagnetism and measurement technology courses. © 1963-2012 IEEE.},
	author = {Sun, Xiaohua and Liu, Haochen and Liang, Peng},
	year = {2023},
	keywords = {Application programs, E-learning, Students, Personnel training, Calibration, Curricula, Epstein frame, Epstein frames, Experiential learning, Hard/soft integration system, Integration systems, Magnetic properties, Magnetic property measurements, Magnetism, Material magnetic property measurement, Measurement technologies, Property measurement, Surveys, Teaching, Virtual instrument},
}

@article{trieflinger_discovery_2023,
	title = {The discovery effort worthiness index: {How} much product discovery should you do and how can this be integrated into delivery?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150361731&doi=10.1016%2fj.infsof.2023.107167&partnerID=40&md5=d7394c3ca1383f10bc646afa2ddc0576},
	abstract = {Context: In a world of high dynamics and uncertainties, it is almost impossible to have a long-term prediction of which products, services, or features will satisfy the needs of the customer. To counter this situation, the conduction of Continuous Improvement or Design Thinking for product discovery are common approaches. A major constraint in conducting product discovery activities is the high effort to discover and validate features and requirements. In addition, companies struggle to integrate product discovery activities into their agile processes and iterations. Objective: This paper aims at suggests a supportive tool, the “Discovery Effort Worthiness (DEW) Index”, for product owners and agile teams to determine a suitable amount of effort that should be spent on Design Thinking activities. To operationalize DEW, proposals for practitioners are presented that can be used to integrate product discovery into product development and delivery. Method: A case study was conducted for the development of the DEW index. In addition, we conducted an expert workshop to develop proposals for the integration of product discovery activities into the product development and delivery process. Results: First, we present the "Discovery Effort Worthiness Index" in form of a formula. Second, we identified requirements that must be fulfilled for systematic integration of product discovery activities into product development and delivery. Third, we derived from the requirements proposals for the integration of product discovery activities with a company's product development and delivery. Conclusion: The developed "Discovery Effort Worthiness Index" provides a tool for companies and their product owners to determine how much effort they should spend on Design Thinking methods to discover and validate requirements. Integrating product discovery with product development and delivery should ensure that the results of product discovery are incorporated into product development. This aims to systematically analyze product risks to increase the chance of product success. © 2023 Elsevier B.V.},
	author = {Trieflinger, Stefan and Lang, Dominic and Spies, Selina and Münch, Jürgen},
	year = {2023},
	keywords = {Product management, Integration, Uncertainty, Agile development, Design thinking, Scra, Customer satisfaction, High dynamic, Long-term prediction, Product design, Product development, Product discovery, Product roadmaps, Product service},
}

@article{kotti_machine_2023,
	title = {Machine {Learning} for {Software} {Engineering}: {A} {Tertiary} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152600375&doi=10.1145%2f3572905&partnerID=40&md5=1e148a3bcb1e10b59bf60db05eeba5f0},
	abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches. © 2023 Association for Computing Machinery.},
	author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
	year = {2023},
	keywords = {Systematic literature review, Software testing, Life cycle, Tertiary study, Machine learning, Machine-learning, Industrial research, Software testings, Computer software selection and evaluation, Additional key word and phrasestertiary study, Key words, Life cycle activities, Machine learning techniques, Software engineering life-cycle, Software Quality},
}

@article{sadi_webapik_2023,
	title = {{WEBAPIK}: a body of structured knowledge on designing web {APIs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149938330&doi=10.1007%2fs00766-023-00401-2&partnerID=40&md5=31e57dd7c1ca274b4ed4f36deb2f8c7f},
	abstract = {With the rise in initiatives such as software ecosystems and Internet of Things (IoT), developing robust web Application Programming Interfaces (web APIs) has become an increasingly important practice. One main concern in developing web APIs is that they expose back-end systems and data toward clients. This exposure threatens critical non-functional requirements, such as the security of back-end systems, the performance of provided services, and the privacy of communications with clients. Although dealing with non-functional requirements during software design has been long studied, there is still little guide on addressing these requirements in web APIs. In this paper, we present WEBAPIK, a body of structured knowledge on addressing non-functional requirements in the design of web APIs. WEBAPIK is comprised of 27 distinct non-functional requirements, 37 distinct design techniques to address some of the identified requirements, and the trade-offs of 22 design techniques, presented in two forms of natural language and knowledge graphs. The design knowledge compiled in WEBAPIK is systematically extracted and aggregated from 80 heterogeneous online literature resources, including 7 books, 15 weblogs and tutorial, 5 vendor white papers, 6 design standards, and 47 research papers. These resources are systematically retrieved from two search engines of Google and Google Scholar and five research databases of Web of Science, IEEE Xplore, ACM Digital Library, SpringerLink, and ScienceDirect in two periods of March to August 2018 and August 2022. WEBAPIK gathers and structures expert and scholarly discussions to provide insight about addressing non-functional requirements in the design of web APIs. The structure brought to the design knowledge makes it amenable towards extension and creates the potential for employing it in the database of knowledge-based systems that aid software developers in design decision-making. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Sadi, Mahsa H. and Yu, Eric},
	year = {2023},
	keywords = {Systematic Review, Decision making, Software design, Application programs, Computer software reusability, Economic and social effects, Search engines, WEB application, Web applications, Knowledge based systems, Non-functional requirements, Software architecture, Quality attributes, Internet of things, Digital libraries, Application programming interfaces (API), Applications programming interfaces, Commerce, Design Patterns, Knowledge reuse, Reviews, Trade off, Web API},
}

@inproceedings{de_campos_usability_2023,
	title = {Usability and {User} {Experience} {Evaluation} of {Touchable} {Holographic} {Solutions}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184291519&doi=10.1145%2f3638067.3638071&partnerID=40&md5=6667873e7cd3469430b5d782f20258a7},
	abstract = {Interacting with holograms using the hands is on track to reach a broad audience in the coming years. Therefore, a current challenge is understanding how to evaluate this new interaction scenario concerning Usability and User eXperience (UX). In this use context, the user interacts with virtual objects in your real environment. Some already known evaluation technologies have been applied to this type of solution, although they were not created considering aspects such as immersion and presence, typical in this interactive environment. Thus, this paper presents a Systematic Mapping Study (SMS) that sought to identify usability and UX evaluation technologies applied to touchable holographic solutions in augmented reality or mixed reality environments. Furthermore, the SMS sought to answer questions about evaluation technologies and holographic solutions. The SMS examined 3551 publications and selected 40 that presented 106 usability or UX evaluation technologies in a touchable holographic solution. The results shed light on methods and aspects little addressed and showed patterns and preferences for combinations of devices, gestures, and feedback types. This work contributes to HCI researchers by better understanding the state of the art of usability and UX evaluation technologies applied to touchable holographic solutions, classifying them, and discussing the main gaps and opportunities. © 2023 ACM.},
	author = {De Campos, Thiago Prado and Damasceno, Eduardo Filgueiras and Valentim, Natasha Malveira Costa},
	year = {2023},
	keywords = {Mapping, Systematic mapping studies, Users' experiences, 'current, Augmented reality, Mixed reality, Holograms, Interactive Environments, Real environments, Usability, Usability and user experience evaluation, Usability engineering, Use context, Virtual objects},
}

@article{molleri_backsourcing_2023,
	title = {Backsourcing of {IT} with focus on software development—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164225673&doi=10.1016%2fj.jss.2023.111771&partnerID=40&md5=b4c1364eb0462d21b5de6514e26d9187},
	abstract = {Context: Backsourcing is the process of insourcing previously outsourced activities. Backsourcing can be a viable alternative when companies experience environmental or strategic changes, or challenges with outsourcing. While outsourcing and related processes have been extensively studied, few studies report experiences with backsourcing. Objectives: We summarize the results of the research literature on backsourcing of IT, with a focus on software development. By identifying practically relevant experience, we present findings that may help companies considering backsourcing. In addition, we identify gaps in the current research literature and point out areas for future work. Method: Our systematic literature review (SLR) started with a search for empirical studies on the backsourcing of IT. From each study, we identified the context in which backsourcing occurred, the factors leading to the decision, the backsourcing process, and the outcomes of backsourcing. We employed inductive coding to extract textual data from the papers and qualitative cross-case analysis to synthesize the evidence. Results: We identified 17 papers that reported 26 cases of backsourcing, six of which were related to software development. The cases came from a variety of contexts. The most common reasons for backsourcing were improving quality, reducing costs, and regaining control of outsourced activities. We model the backsourcing process as containing five sub-processes: change management, vendor relationship management, competence building, organizational build-up, and transfer of ownership. We identified 14 positive outcomes and nine negative outcomes of backsourcing. We also aggregated the evidence and detailed three relationships of potential use to companies considering backsourcing. Finally, we have highlighted the knowledge areas of software engineering associated with the backsourcing of software development. Conclusion: The backsourcing of IT is a complex process; its implementation depends on the prior outsourcing relationship and other contextual factors. Our systematic literature review contributes to a better understanding of this process by identifying its components and their relationships based on the peer-reviewed literature. Our results can serve as a motivation and baseline for further research on backsourcing and provide guidelines and process fragments from which practitioners can benefit when they engage in backsourcing. © 2023 Elsevier Inc.},
	author = {Molléri, Jefferson Seide and Lassenius, Casper and Jørgensen, Magne},
	year = {2023},
	keywords = {Systematic literature review, Software design, Outsourcing, 'current, Quality control, Engineering research, Backsourcing, Empirical studies, Environmental change, Insourcing, Software engineering management, Strategic challenges, Strategic changes, Textual data},
}

@article{omerkhel_exploring_2023,
	title = {{EXPLORING} {STRATEGIES} {FOR} {OVERCOMING} {ISSUES} {OF} {USER} {INVOLVEMENT} {IN} {AGILE} {SOFTWARE} {DEVELOPMENT}: {A} {SYSTEMATIC} {LITERATURE} {REVIEW}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175462629&partnerID=40&md5=c6798a876dfb842582a5174b0b26b09f},
	abstract = {The present systematic literature review (SLR) explores the challenges and strategies associated with managing users during requirement elicitation within agile software development. Drawing insights from an analysis of 24 relevant studies, this study comprehensively examines the issues that arise and the effective approaches to overcome them. The findings reveal five prominent challenges of user involvement during requirement elicitation. The most dominant issues identified are the lack of user involvement, insufficient user knowledge, and a deficit in the expertise of the Product Owner. These challenges can hinder the effective integration of user perspectives and needs into the development process. To address these challenges, the study identifies seven strategies that Product Owners can adopt to facilitate effective user involvement. These strategies include Mind Maps, User Interface Mockups, Workshops, Hybridism (combining agile and non-agile techniques), Face-to-Face Meetings, Continuous Delivery, and Training and Learning initiatives. The application of these strategies empowers Product Owners and software practitioners to enhance user involvement, improve communication, and streamline the requirement elicitation process in agile software development. The outcomes of this SLR provide valuable insights for both researchers and software practitioners, exploring the complex dynamics of user involvement in agile contexts. By recognizing these challenges and deploying effective strategies, software development teams can ensure more successful requirement elicitation processes, leading to the creation of software products that better align with user needs and expectations. This review contributes to a deeper understanding of user involvement challenges and offers actionable guidance for optimizing the requirement elicitation within agile software development paradigm. © 2023 Little Lion Scientific.},
	author = {Omerkhel, Qudrattullah and Yusop, Othman Mohd and Ismail, Saiful Adli and Azmi, Azri},
	year = {2023},
}

@article{wang_systematic_2023,
	title = {A {Systematic} {Literature} {Review} of {Software} {Traceability} {Links} {Automation} {Techniques}; [软件跟踪链自动化技术研究综述]},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173597434&doi=10.11897%2fSP.J.1016.2023.01919&partnerID=40&md5=f05d3c01adf3d021d5a13587264108ce},
	abstract = {As an important software capability, software traceability aims to capture, link and trace each crucial software artifact via constructing the traceability links between them. The stud-y of software traceability covers many aspects, including traceability modelling, traceability assessment and traceability implementation. Traceability links interconnect software artifacts with each other and use the resulting associative networks to resolve issues with software products and their development processes. Traceability links provide critical support for many software engineering activities, including impact analysis, software verification, test case selection, compliance verification, system security assurance and defect detection. Traceability links refer to a specific relationship between a pair of software artifacts, one of which is the source artifact and the other is the target artifact. They records various relationships between software artifacts such as dependencies, influences, and causal relationships. The direction of traceability links can be oneway or two-way. Various traceability links can help software developers to understand, develop and manage systems both efficiently and effectively. At the same time, traceability links can help people involved in all phases of software development activities to accomplish their development tasks. Requirements traceability links, as the most widely used traceability links, enable the construction and maintenance of traceability links between requirements and other software artifacts. Moreover, traceability links also include the establishment of links between code and tests, design and code, models and code, defects and code, and so on. In recent years, the creation, maintenance and validation of traceability links with information retrieval, natural language processing, machine learning, and deep learning can reduce the manual handling cost of traceability links by developers, and therefore have received extensive attentions from academia and industry. There arc also some works reviewing software traceability links approaches and techniques. In this paper, we focus on the automation techniques of the creation, maintenance and validation of traceability links so as to sort out and summarize the research progress in the past ten years. The main content includes the statistics and analysis of approaches and techniques for automated creation, maintenance and validation of traceability links, the application research of traceability links, the state-of-the-art traceability links related evaluation research and tools support, and the key problems of the current traceability links techniques. The problems arc summarized from the technical difficulties around seventh parts: the complexity of the tracing software, the granularity problem, the unsatisfying accuracy, the type limitation, the validation efficiency, the application scale and time, and the incomprehensive evaluation of traceability links. . Besides, several possible solution ideas and future development trends of the problems arc elaborated, including the construction of horizontal traceability links between software artifacts, the scalable and configurable automation techniques of traceability links, the integration of traditional approaches with artificial intelligence techniques, the creation of multiple types of traceability links using intermediary artifacts, the interactive verification of traceability links, the real-time retrieval of traceability links and open sourcing of related source codes. This review also reveals that: (1) The creation of traceability links has received a lot of academic attentions, but the research on the maintenance, verification, and application of traceability links needs more attention; (2) Requirements-to-code links are the most concerned type of researchers, followed by rcquirements-to-design and design-to-code, while other traceability links such as code/data-to-model and screenshot-to-defect are also starting to enter the vision of researchers; (3) With the continuous development of artificial intelligence (AI), Al-bascd techniques such as Naive Baycs, SVM, Bert, Doc2Vcc, RNN have been widely used in the creation, maintenance and verification of traceability links; (4) In the creation of traceability links, it is difficult to achieve good results by relying only on information retrieval and artificial intelligence techniques. Information retrieval, machine learning or deep learning techniques should be combined with traditional heuristics, model-based methods and so on, to make up for the deficiencies in AI technologies and traditional methods to further improve the quality of traceability links; (5) Research on traceability links automation techniques in complex environments, cross-platform and cross-language should be on the agenda in the future. © 2023 Science Press. All rights reserved.},
	author = {Wang, Ye and Hu, Kun and Jiang, Bo and Xia, Xin and Tang, Xian-Shu},
	year = {2023},
	keywords = {Software design, Software testing, Application programs, Deep learning, Learning algorithms, Learning systems, Natural language processing systems, Requirements engineering, Automation, Natural languages, Machine-learning, Defects, Compliance control, Language processing, Natural language processing, Codes (symbols), Verification, Computer software selection and evaluation, Software artefacts, Automation techniques, Computer software maintenance, Software traceability, Software traceability link, Traceability links},
}

@article{torre_how_2023,
	title = {How consistency is handled in model-driven software engineering and {UML}: an expert opinion survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128744172&doi=10.1007%2fs11219-022-09585-2&partnerID=40&md5=ec55130b1fa92036e10fb41920ba4ebe},
	abstract = {Model-driven software engineering (MDSE) is an established approach for developing complex software systems. The unified modelling language (UML) is one of the most used model languages for applying the MDSE approach. UML has 14 diagram types that describe different perspectives of a software system under development. These diagrams are strongly dependent on each other and must be consistent with one another. The main objectives of this paper are as follows: (1) to understand (i) how aware experts are of model consistency issues and (ii) how relevant these issues are to experts, in order to understand model consistency in the MDSE/UML contexts, and more importantly, (2) to validate a set of 116 UML consistency rules that was systematically collected from the literature, so as to identify the rules that should always be enforced. We conducted a personal opinion survey with 106 experts in SE and MDSE, by means of an online questionnaire. The survey results describe an overview of how the topic of MDSE/UML consistency is handled by experts in the field. In addition, this survey identified a set of 52 UML consistency rules which should always be checked in every UML diagram. The majority of these 52 rules were understood by the majority of respondents and are general-purpose rules that are involved in the Design software development phase. This subset of 52 rules could be considered to be (1) added to the UML standard, (2) used as a reference to researchers who study UML/MDSE, and (3) used as a practical example for teaching purposes. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Torre, Damiano and Genero, Marcela and Labiche, Yvan and Elaasar, Maged},
	year = {2023},
	keywords = {Software design, Computer software, Unified Modeling Language, Surveys, Empirical studies, Expert opinion, Model consistency, Model-driven software engineering consistency, Model-driven software engineerings, Opinion surveys, Personal opinion survey, Software engineering model, Unified modeling language consistency rule},
}

@article{bianco_reducing_2023,
	title = {Reducing the user labeling effort in effective high recall tasks by fine-tuning active learning},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146621225&doi=10.1007%2fs10844-022-00772-y&partnerID=40&md5=27482213cefa9ee93b873591699a61b7},
	abstract = {High recall Information REtrieval (HIRE) aims at identifying only and (almost) all relevant documents for a given query. HIRE is paramount in applications such as systematic literature review, medicine, legal jurisprudence, among others. To address the HIRE goals, active learning methods have proven valuable in determining informative and non-redundant documents to reduce user effort for manual labeling. We propose a new active learning framework for the HIRE task. REVEAL-HIRE selects a very reduced set of documents to be labeled, significantly mitigating the user’s effort. The proposed approach selects the most representative documents by exploiting a novel, specifically designed active learning strategy for HIRE, called REVEAL (RelEVant rulE-based Active Learning). REVEAL aims at selecting the maximum number of relevant documents for a given query based on discriminative rule-based patterns and a penalization factor. The method is applied to the top-ranked documents to choose the most informative ones to be labeled, a hard task due to data skewness – most documents are irrelevant for a given query. The enhanced active learning process is repeated incrementally until a stopping point is achieved, using REVEAL to identify the point in the process when relevant documents should stop to be sampled. Experimental results in several standard benchmark datasets (e.g. 20-Newsgroups, Trec Total Recall, and CLEF eHealth) demonstrate that REVEAL-HIRE can reduce the user labeling effort up to 3 times (320\% of reduction) in comparison with state-of-the-art baselines while keeping the effectiveness at the highest levels. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Bianco, Guilherme Dal and Duarte, Denio and Gonçalves, Marcos André},
	year = {2023},
	keywords = {Artificial intelligence, Learning systems, Classification (of information), Information retrieval, Labelings, Active Learning, Fine tuning, Hire, Labeling process, Relevant documents, Rule based, SSAR, Supervised classifiers, User labeling},
}

@article{yang_seeing_2023,
	title = {Seeing the {Whole} {Elephant}: {Systematically} {Understanding} and {Uncovering} {Evaluation} {Biases} in {Automated} {Program} {Repair}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161982187&doi=10.1145%2f3561382&partnerID=40&md5=c5af7c7df149502ee029204053597e14},
	abstract = {Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation. © 2023 Association for Computing Machinery.},
	author = {Yang, Deheng and Lei, Yan and Mao, Xiaoguang and Qi, Yuhua and Yi, Xin},
	year = {2023},
	keywords = {Systematic literature review, Open source software, Repair, Key words, Additional key word and phrasesautomated program repair, Bias studies, Constraint-based, Empirical evaluations, Executables, Petroleum reservoir evaluation, Repair techniques, Systematic exploration, Validation strategies},
}

@article{alonso_systematic_2023,
	title = {A systematic mapping study and practitioner insights on the use of software engineering practices to develop {MVPs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145972508&doi=10.1016%2fj.infsof.2022.107144&partnerID=40&md5=27902f1fdcb0c646416221c72975d06a},
	abstract = {Background: Many startup environments and even traditional software companies have embraced the use of MVPs (Minimum Viable Products) to allow quickly experimenting solution options. The MVP concept has influenced the way in which development teams apply Software Engineering (SE) practices. However, the overall understanding of this influence of MVPs on SE practices is still poor. Objective: Our goal is to characterize the publication landscape on practices that have been used in the context of software MVPs and to gather practitioner insights on the identified practices. Method: We conducted a systematic mapping study using a hybrid search strategy that consists of a database search and parallel forward and backward snowballing. Thereafter, we discussed the mapping study results in two focus groups sessions involving twelve industry practitioners that extensively use MVPs in their projects to capture their perceptions on the findings of the mapping study. Results: We identified 33 papers published between 2013 and 2020. We observed some trends related to MVP ideation (or MVP conception) and evaluation practices. For instance, regarding ideation, we found six different approaches (e.g., Design Thinking, Lean Inception) and mainly informal end-user involvement practices (e.g., workshops, interviews). Regarding evaluation, there is an emphasis on end-user validations based on practices such as usability tests, A/B testing, and usage data analysis. However, there is still limited research related to MVP technical feasibility assessment and effort estimation. Practitioners of the focus group sessions reinforced the confidence in our results regarding ideation and evaluation practices, being aware of most of the identified practices. They also reported how they deal with the technical feasibility assessments (involving developers during the ideation and conducting informal experiments) and effort estimation in practice (based on expert opinion and using practices common to agile methodologies, such as Planning Poker). Conclusion: Our analysis suggests that there are opportunities for solution proposals and evaluation studies to address literature gaps concerning technical feasibility assessment and effort estimation. Overall, more effort needs to be invested into empirically evaluating the existing MVP-related practices. © 2022 Elsevier B.V.},
	author = {Alonso, Silvio and Kalinowski, Marcos and Ferreira, Bruna and Barbosa, Simone D.J. and Lopes, Hélio},
	year = {2023},
	keywords = {Software company, Mapping, Systematic mapping studies, Search engines, Systematic mapping, Mapping studies, Effort Estimation, Feasibility assessment, Focus groups, Minimum viable product, Software engineering practices},
}

@article{reis_automated_2023,
	title = {Automated guided vehicles position control: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122319071&doi=10.1007%2fs10845-021-01893-x&partnerID=40&md5=d765d8c0523ba75a0782a045796b8994},
	abstract = {Automated Guided Vehicles (AGVs) are essential elements of manufacturing intralogistics and material handling. Improving the position accuracy along the AGV trajectory allows the vehicle to work on narrower aisles with lower error tolerance. Despite the increasing number of papers in AGVs and mobile robots’ position control research area, there is a lack of curatorial work presenting and analyzing the control strategies applied in the problem domain. Therefore, the main objective is to analyze the published researches of the past seven years on the position control of AGVs to recognize research patterns, gaps, and tendencies, outlining the research field. The paper proposes a systematic literature review to investigate the research field from the controller design perspective. Its protocol and procedures are presented in detail. Four main research topics were addressed: the control strategies used in the AGV position control problem, how the literature presents the AGV operating requirement of position accuracy, how the literature validate the proposed controller and present their results regarding the system’s position accuracy, and the technological tendencies the proposed solutions reveals. Besides, within the main topics, other points were investigated, such as the AGV application area, the considered mathematical model, the sensors and guidance system used, and the maximum payload of the vehicle and operation under different load conditions. The data synthesis shows the predominant control strategies applied to the problem and the interaction among distinct control theory areas, indicating a notable interaction of Intelligent Control techniques to the other strategies. The paper’s contributions are using a systematic literature review method over the AGV position control publications, presenting an overview of the research area, analyzing the research question topics from selected articles, and proposing a research agenda. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Reis, Wallace Pereira Neves dos and Couto, Giselle Elias and Junior, Orides Morandin},
	year = {2023},
	keywords = {Systematic literature review, Research areas, Automated guided vehicles, Automatic guided vehicles, Control strategies, Control system synthesis, Controllers, Essential elements, Intelligent control, Material handling, Materials handling, Mobile robots, Path tracking, Position accuracy, Position control, Research fields, Vehicle position},
}

@article{borstler_investigating_2023,
	title = {Investigating acceptance behavior in software engineering—{Theoretical} perspectives},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146227386&doi=10.1016%2fj.jss.2022.111592&partnerID=40&md5=4df944097102b7c6c0fb2ecbb7397c86},
	abstract = {Background: Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology. Objective: To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development. Method: We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking). Results: We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior. Conclusions: Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories. © 2022 The Author(s)},
	author = {Börstler, Jürgen and Ali, Nauman bin and Svensson, Martin and Petersen, Kai},
	year = {2023},
	keywords = {Software design, Behavioral research, Software engineering research, Acceptance behavior, TAM, TPB, Dual-process theories, Oriented process, Software development practices, Technology acceptance, Theory, UTAUT},
}

@article{kotti_impact_2023,
	title = {Impact of {Software} {Engineering} {Research} in {Practice}: {A} {Patent} and {Author} {Survey} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139448523&doi=10.1109%2fTSE.2022.3208210&partnerID=40&md5=d72b5940144a9dec58e6b088e8d70a4e},
	abstract = {Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers published in four leading SE venues between 1975-2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26\% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic. © 1976-2012 IEEE.},
	author = {Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis},
	year = {2023},
	keywords = {Software engineering, Economic and social effects, Industrial research, Software engineering research, Engineering research, Software, Cost benefit analysis, Surveys, Empirical studies, Collaboration, Interview, Patent, Patent citation, Patents and inventions, Practical impact, Quantitative and qualitative analysis, Survey analysis},
}

@article{al-ahmad_overview_2023,
	title = {Overview on {Case} {Study} {Penetration} {Testing} {Models} {Evaluation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160556929&doi=10.28991%2fESJ-2023-07-03-025&partnerID=40&md5=9da6f31eb866e14860834a06ff6466c6},
	abstract = {Model evaluation is a cornerstone of scientific research as it represents the findings' accuracy and model performance. A case study is commonly used in evaluating software engineering models. Due to criticism in terms of generalization from a single case study and testers, deciding on the number of case studies used for evaluation and the number of testers has been one of the researchers’ challenges. Multiple case studies with multiple testers can be difficult in some domains, such as penetration testing, due to the complexity and time needed to prepare test cases. This study aims to review the literature and examine the evaluation methods used pertaining to the number of case studies and testers involved. This study is beneficial for researchers, students, and penetration testers as it provides case study design steps that are useful to determine the appropriate number of test cases and testers required. The paper's findings and novelty highlight that a single case study with a single tester is enough to evaluate a model. It also strikes a balance between what is enough for the evaluation and the need to reduce criticisms of a single case study by using two case studies with a single tester. © 2023 by the authors. Licensee ESJ, Italy.},
	author = {Al-Ahmad, Ahmad S. and Kahtan, Hasan and Alzoubi, Yehia I.},
	year = {2023},
}

@article{miloudi_systematic_2023,
	title = {Systematic {Review} of {Machine} {Learning}-{Based} {Open}-{Source} {Software} {Maintenance} {Effort} {Estimation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143977206&doi=10.2174%2f2666255816666220609110712&partnerID=40&md5=4d4bca6f178d40ff4531940a93a67f66},
	abstract = {Background: Software maintenance is known as a laborious activity in the software lifecycle and is often considered more expensive than other activities. Open-Source Software (OSS) has gained considerable acceptance in the industry recently, and the Maintenance Effort Estimation (MEE) of such software has emerged as an important research topic. In this context, researchers have conducted a number of open-source software maintenance effort estimation (O-MEE) studies based on statistical as well as machine learning techniques for better estimation. Objective: The objective of this study is to perform a systematic literature review (SLR) to analyze and summarize the empirical evidence of O-MEE ML techniques in current research through a set of five Research Questions (RQs) related to several criteria (e.g. data pre-processing tasks, data mining tasks, tuning parameter methods, accuracy criteria and statistical tests, as well as ML techniques reported in the literature that outperformed). Methods: We performed a systematic literature review of 36 primary empirical studies published from 2000 to June 2020, selected based on an automated search of six digital databases. Results: The findings show that Bayesian networks, decision tree, support vector machines and instance-based reasoning were the ML techniques most used; few studies opted for ensemble or hybrid techniques. Researchers have paid less attention to O-MEE data pre-processing in terms of feature selection, methods that handle missing values and imbalanced datasets, and tuning parameters of ML techniques. Classification data mining is the task most addressed using different accuracy criteria such as Precision, Recall, and Accuracy, as well as Wilcoxon and Mann-Whitney statistical tests. Conclusion: This SLR identifies a number of gaps in the current research and suggests areas for further investigation. For instance, since OSS includes different data source formats, researchers should pay more attention to data pre-processing and develop new models using ensemble techniques since they have proved to perform better. © 2023 Bentham Science Publishers.},
	author = {Miloudi, Chaymae and Cheikhi, Laila and Abran, Alain},
	year = {2023},
	keywords = {Systematic literature review, Life cycle, Learning algorithms, Learning systems, Open source software, 'current, Decision trees, Effort Estimation, Support vector machines, Open systems, Open-source softwares, Data mining, Bayesian networks, Machine learning techniques, Empirical studies, Computer software maintenance, Data preprocessing, Ensemble techniques, Maintenance effort estimation, Maintenance efforts, Statistical tests},
}

@article{anuar_multivocal_2023,
	title = {A multivocal literature review on record management potential components in {CRUD} operation for web application development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131085987&doi=10.1142%2fS1793962323410192&partnerID=40&md5=c69d2412dddcbff1a771c169b69a7201},
	abstract = {In recent years, web application frameworks have been widely practised by many developers to increase programming productivity as the frameworks are more flexible, rapidly built using CRUD operation, MVC-based, secure and most of them are published under an open-source license which will reduce the final cost of development. Although the CRUD automation in the web application framework boosts the development process, there are many important aspects of a web application absent from the CRUD output. Therefore, this multivocal literature review investigates the record management aspects that are required in modern WA and the perceived benefit of integrating the record management aspect into CRUD operation. The study extracted 284 publications from respectable scientific resources and the grey resources literature created by WA development practitioners outside academic mediums. After a detailed review process, only 14 scientific primary studies and 13 gray studies were considered for this review based on defined inclusion and exclusion criteria. The review shows that the most important aspects required in WA are search, role-based access control, retention, appraisal, search, audit trail, digital archiving, sharing, reporting, inactive files management and several other features. These important aspects have been analyzed and characterized according to its function and features. The method and procedure for integrating the specified aspect into CRUD operation are identified and discussed. Integrating and implementing the specified record management features into CRUD operation will boost the WA development productivity by producing more features as a standard output with integrated record management functions. © 2023 World Scientific Publishing Company.},
	author = {Anuar, Asyraf Wahi and Kama, Nazri and Azmi, Azri and Rusli, Hazlifah Mohd},
	year = {2023},
	keywords = {Literature reviews, Open source software, Multivocal literature review, Access control, CRUD, Electronic records managements, Final costs, Open source license, Productivity, Record management, Web application development, Web application frameworks, Web frameworks},
}

@article{findrik_drivers_2023,
	title = {Drivers and barriers for consumers purchasing bioplastics – {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153893687&doi=10.1016%2fj.jclepro.2023.137311&partnerID=40&md5=63fcad7bc912638f6c3e6e550db8c7b5},
	abstract = {Plastic pollution has adverse effects on the ecosystem and on the human body. Bioplastics might provide an alternative to plastic for environmentally friendly consumers. A systematic literature review was conducted to analyze and summarize the state of the art regarding consumers' response to bioplastics using a hierarchy of effects model. The model holistically represents the relevant steps in understanding consumers' journey from stimuli to the behavioral stage. The review was based on 67 scientific journal articles on consumer studies related to bioplastics published in English language (all peer-reviewed). Most studies researched bioplastic packaging applications e.g., food and beverage packaging using quantitative consumer surveys. Many studies focused on consumer preferences and willingness to pay for bioplastics, while awareness, knowledge, and post-purchase behavior—usage and disposal—was the least researched. Many of the studies applied text or oral and rarely real product stimuli. The results of the synthesis pointed out some purchasing barriers e.g. consumers' low knowledge about the environmental impact, characteristics such as material source or end-of-life character of bioplastics; or consumers' uncertainty about bioplastic recognition versus conventional plastics. Drivers of consumers' purchasing bioplastics are also identified, for instance consumers' positive attitude, available product information or consumers' green value. Bioplastic products meeting consumers' preferences such as low price, biogenic resource base, and local origin also act as purchasing drivers. Studies also found that bioplastic related information of a product influences consumers’ willingness to pay. The review revealed research gaps, highlighting in particular the need for cross-cultural studies, non-hypothetical research designs and the analysis of labelling systems related to bioplastic products. © 2023 The Authors},
	author = {Findrik, Edina and Meixner, Oliver},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, State of the art, Adverse effect, Bio-plastics, Consumer, Consumer behavior, Consumers' preferences, Environmental impact, Human bodies, Labeling, Plastic pollutions, Purchasing, Reinforced plastics, Sales, Willingness to pay},
}

@article{pizard_assessing_2023,
	title = {Assessing attitudes towards evidence-based software engineering in a government agency},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141479463&doi=10.1016%2fj.infsof.2022.107101&partnerID=40&md5=958be07a4396062f45df6bce9c1d9292},
	abstract = {Context: Evidence-based practice (EBP) has allowed several disciplines to become more mature by emphasizing the use of evidence from well-designed and well-conducted research in decision-making. Its application in SE, Evidence-based software engineering (EBSE) can help to bridge the gap between academia and industry by bringing together academic rigor and research of practical relevance. To achieve this, it seems necessary to improve its adoption. Objective: We sought both to study the attitudes towards EBSE of stakeholders working in a government agency (GA) and to assess whether knowledge of EBSE would impact their working practices. Method: We conducted a multi-stage field investigation in an Uruguayan national GA that is responsible for digital policies. First, we organized an EBSE awareness lecture and we collected and analyzed participants’ perceptions of the value and limitations of EBSE. Sixteen months later, in a second stage, we contacted the agency and asked participants whether they had made use of the information about EBSE we presented to them. Results: Initially, participants reported that EBSE seemed useful for tackling challenging problems and, in particular, considered its use appropriate given the agency's responsibilities. Perceived barriers to EBSE adoption were the need for institutional support, the lack of government practice reports, inadequate skills or motivation, the cost of conducting systematic reviews, and the lack of evidence about emerging issues. In the follow-up survey, although the participants were not undertaking systematic reviews themselves, many reported improvements in how they searched for and evaluated information to support their work. Conclusion: Our study presents some insights to better understand EBSE adoption. With the exception of GA-specific issues, perceived value and barriers to adoption were consistent with those reported in software engineering and other disciplines. Our follow-up study confirms the potential value of evidence in the context of IT regulatory and government bodies. © 2022 Elsevier B.V.},
	author = {Pizard, Sebastián and Acerenza, Fernando and Vallespir, Diego and Kitchenham, Barbara},
	year = {2023},
	keywords = {Systematic Review, Decision making, Decisions makings, Application programs, Search engines, Evidence Based Software Engineering, Surveys, Focus groups, Bridges, Evidence-based practices, Field investigation, Follow-up Studies, Government agencies, ITS applications, Working practices},
}

@inproceedings{junior_towards_2023,
	title = {Towards {Federated} {Ontology}-{Driven} {Data} {Integration} in {Continuous} {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174524159&doi=10.1145%2f3613372.3613380&partnerID=40&md5=3e4b6d35c3cbb7db05fb9d3cadd95ac4},
	abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest. © 2023 ACM.},
	author = {Júnior, Paulo Sérgio Santos and Almeida, João Paulo A. and Barcellos, Monalessa},
	year = {2023},
	keywords = {Decision making, Decisions makings, Software design, Computer software reusability, Continuous software engineerings, Ontology, Ontology's, Project management, Quality assessment, Software engineering practices, Assessment tool, Daily activity, Data integration, Data items, Integrated data, Project management tools},
}

@article{russo_exploring_2023,
	title = {Exploring a {Multidisciplinary} {Assessment} of {Organisational} {Maturity} in {Business} {Continuity}: {A} {Perspective} and {Future} {Research} {Outlook}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192352194&doi=10.3390%2fapp132111846&partnerID=40&md5=0304e1dbc51d2f583b2eab9b338ef4fc},
	abstract = {In a competitive business landscape heavily reliant on information and communication technology, organisations must be prepared to address disruptions in their business operations. Business continuity management involves effective planning for the swift reestablishment of business processes in the short term. However, there are still obstacles to implementing business continuity plans, which can be justified by various factors. The purpose of this study is to present the perspectives and future research paths based on a systematic literature review from the peer-reviewed literature published from 1 January 2000 to 31 December 2021. This systematic literature review adheres to the guidelines established by evidence-based software engineering and leverages the Parsifal online tool. The primary research results identify and establish connections between the common components and activities of business continuity management as defined in international standards and frameworks to identify gaps in the existing knowledge. These findings will contribute to the development of a framework that provides a practical approach applicable to organisations of all sizes, taking into account each aspect of business continuity management, with a particular emphasis on information and communication technology systems. This paper’s contribution lies in offering insights from a systematic literature review regarding the strategic principles for designing and implementing a business continuity plan, along with a comprehensive overview of related research. Furthermore, it presents a path forward to guide future research efforts aimed at addressing the gaps in the literature concerning continuity planning. © 2023 by the authors.},
	author = {Russo, Nelson and Mamede, Henrique São and Reis, Leonilde and Martins, José and Branco, Frederico},
	year = {2023},
}

@article{zhao_systematic_2023,
	title = {A {Systematic} {Survey} of {Just}-in-{Time} {Software} {Defect} {Prediction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147853196&doi=10.1145%2f3567550&partnerID=40&md5=f90ec760e71ec41329ce66005b60aa30},
	abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP. © 2023 Association for Computing Machinery.},
	author = {Zhao, Yunhua and Damevski, Kostadin and Chen, Hui},
	year = {2023},
	keywords = {Machine learning, Machine-learning, Forecasting, Software defect prediction, Computer software selection and evaluation, Change defect density, Change-level software defect prediction, Defect density, Defects density, Just in time production, Just-in-time, Just-in-time software defect prediction, Quality assurance, Release software defect prediction, Searching-based algorithm, Software change, Software change metric},
}

@article{khan_analysis_2023,
	title = {Analysis of {Cursive} {Text} {Recognition} {Systems}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167725839&doi=10.1145%2f3592600&partnerID=40&md5=bd2a595a4a3f8ecc36b5dc5846efdd8a},
	abstract = {Regional and cultural diversities around the world have given birth to a large number of writing systems and scripts, which consist of varying character sets. Developing an optimal character recognition for such a varying and large character set is a challenging task. Unlimited variations in handwritten text due to mood swings, varying writing styles, changes in medium of writing, and many more puzzle the research community. To overcome this problem, researchers have proposed various techniques for the automatic recognition of cursive languages like Urdu, Pashto, and Arabic. With the passage of time, the field of text recognition matured, and the number of publications exponentially increased in the targeted field. It is very difficult to find all the techniques developed, calculate the time and resource consumptions, and understand the cost-benefit tradeoffs among these techniques. These tradeoffs resist making this technology able for practical use. To address these tradeoffs, this article systematic analysis to identify gaps in the literature and suggest new enhanced solution accordingly. A total of 153 of the most relevant articles from 2008 to 2022 are analyzed in this systematic literature review (SLR) work. This systematic review process shows (1) the list of techniques suggested for cursive text recognition purposes and its capabilities, (2) set of feature extraction techniques proposed, and (3) implementation tools used to design and simulate the empirical studies in this specialized field. We have also discussed the emerging trends and described their implications for the research community in this specialized domain. This systematic assessment will ultimately help researchers to perform an overview of the existing character/text recognition approaches, recognition capabilities, and time consumption and subsequently identify the areas that requires a significant attention in the near future. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author = {Khan, Sulaiman and Nazir, Shah and Khan, Habib Ullah},
	year = {2023},
	keywords = {Systematic literature review, Research communities, Cost benefit analysis, Key words, Commerce, Additional key word and phrasescursive language, Character recognition, Character sets, Cultural diversity, Feature technique, Recognition algorithm, Recognition systems, Text recognition, Time consumption},
}

@inproceedings{bernardes_understanding_2023,
	title = {On the {Understanding} of the {Role} of {Continuous} {Experimentation} in {Technology}-{Based} {Startup}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174536882&doi=10.1145%2f3613372.3613414&partnerID=40&md5=1bce7cfc7bf056d361b28ebbb75f6671},
	abstract = {Technology startups are constantly emerging, trying to create innovative solutions in environments of uncertainty, and because they face numerous challenges, they have high failure rates. The scarcity of resources and the product's lack of adherence to market needs are among the challenges. In an attempt to alleviate these challenges, initiatives such as Continuous Experimentation arise. This approach supports systematical tests of hypotheses, helping teams prioritize deliveries that increase perceived value by the users. Our interview-based study aimed to identify how Continuous Experimentation is being adopted and how it underlies software engineering activities throughout the product development cycle of technology-based startups. We found that data-driven decisions and reduced development effort are benefits of adopting such an approach while the low competence and education in experimentation is among the main challenges, suggesting that there is room for qualifying professionals in the matter. © 2023 ACM.},
	author = {Bernardes, Matheus and Marczak, Sabrina},
	year = {2023},
	keywords = {Software engineering, Systematic literature review, Engineering education, Uncertainty, Continuous experimentation, Entrepreneurship, Failure analysis, Failure rate, Innovative solutions, Lean startup, Market needs, Technology start-up, Technology-based},
}

@article{kabir_meta-synthesis_2023,
	title = {A {Meta}-{Synthesis} of the {Barriers} and {Facilitators} for {Personal} {Informatics} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173236525&doi=10.1145%2f3610893&partnerID=40&md5=55ea134f22e9de4c81f1c9058f760ae0},
	abstract = {Personal informatics (PI) systems are designed for diverse users in the real world. Even when these systems are usable, people encounter barriers while engaging with them in ways designers cannot anticipate, which impacts the system's effectiveness. Although PI literature extensively reports such barriers, the volume of this information can be overwhelming. Researchers and practitioners often find themselves repeatedly addressing the same challenges since sifting through this enormous volume of knowledge looking for relevant insights is often infeasible. We contribute to alleviating this issue by conducting a meta-synthesis of the PI literature and categorizing people's barriers and facilitators to engagement with PI systems into eight themes. Based on the synthesized knowledge, we discuss specific generalizable barriers and paths for further investigations. This synthesis can serve as an index to identify barriers pertinent to each application domain and possibly to identify barriers from one domain that might apply to a different domain. Finally, to ensure the sustainability of the syntheses, we propose a Design Statements (DS) block for research articles. © 2023 Owner/Author.},
	author = {Kabir, Kazi Sinthia and Wiese, Jason},
	year = {2023},
	keywords = {Different domains, Real-world, Applications domains, Barrier and facilitator, Informatics systems, Meta-synthesis, Personal informatics, Self-tracking, Synthesised, System effectiveness},
}

@article{bertolino_devopret_2023,
	title = {{DevOpRET}: {Continuous} reliability testing in {DevOps}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088248050&doi=10.1002%2fsmr.2298&partnerID=40&md5=7e6aa3c834bb767e1422efdaf28f9791},
	abstract = {To enter the production stage, in DevOps practices candidate software releases have to pass quality gates, where they are assessed to meet established target values for key indicators of interest. We believe software reliability should be an important such indicator, as it greatly contributes to the end-user satisfaction. We propose DevOpRET, an approach for reliability testing as part of the acceptance testing stage in DevOps. DevOpRET relies on operational-profile–based testing, a common reliability assessment technique. DevOpRET leverages usage and failure data monitored in operations to continuously refine its estimate. We evaluate accuracy and efficiency of DevOpRET through controlled experiments with a real-world open source platform and with a microservice architectures benchmark. The results show that DevOpRET provides accurate and efficient estimates of the true reliability over subsequent DevOps cycles. © 2020 John Wiley \& Sons, Ltd.},
	author = {Bertolino, Antonia and Angelis, Guglielmo De and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
	year = {2023},
	keywords = {Software testing, Open source software, Acceptance tests, End-user satisfactions, Key indicator, Operational profile, Production stage, Quality gates, Reliability testing, Software release, Software reliability, Software reliability testing, Software-Reliability, Target values},
}

@article{ahmad_requirements_2023,
	title = {Requirements engineering framework for human-centered artificial intelligence software systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163544926&doi=10.1016%2fj.asoc.2023.110455&partnerID=40&md5=de91d4822852a3733b100d5ff3c485e6},
	abstract = {Context: Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Objective: Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. Method: In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. Results: The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. Conclusion: We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software. © 2023 The Author(s)},
	author = {Ahmad, Khlood and Abdelrazek, Mohamed and Arora, Chetan and Agrahari Baniya, Arbind and Bano, Muneera and Grundy, John},
	year = {2023},
	keywords = {Software engineering, Requirement engineering, Requirements engineering, Intelligence software, Software-systems, Machine learning, Machine-learning, Engineering education, E-learning, Conceptual model, Empirical Software Engineering, Engineering frameworks, Human-centered, In-buildings, Software solution, Virtual reality},
}

@article{belle_bolstering_2023,
	title = {Bolstering the {Persistence} of {Black} {Students} in {Undergraduate} {Computer} {Science} {Programs}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181846534&doi=10.1145%2f3617896&partnerID=40&md5=1a67a519327ea45a06612612bb99fadb},
	abstract = {Background: People who are racialized, gendered, or otherwise minoritized are underrepresented in computing professions in North America. This is reflected in undergraduate computer science (CS) programs, in which students from marginalized backgrounds continue to experience inequities that do not typically affect White cis-men. This is especially true for Black students in general, and Black women in particular, whose experience of systemic, anti-Black racism compromises their ability to persist and thrive in CS education contexts. Objectives: This systematic mapping study endeavours to (1) determine the quantity of existing non-deficit-based studies concerned with the persistence of Black students in undergraduate CS; (2) summarize the findings and recommendations in those studies; and (3) identify areas in which additional studies may be required. We aim to accomplish these objectives by way of two research questions: (RQ1) What factors are associated with Black students’ persistence in undergraduate CS programs?; and (RQ2) What recommendations have been made to further bolster Black students’ persistence in undergraduate CS education programs? Methods: This systematic mapping study was conducted in accordance with PRISMA 2020 and SEGRESS guidelines. Studies were identified by conducting keyword searches in seven databases. Inclusion and exclusion criteria were designed to capture studies illuminating persistence factors for Black students in undergraduate CS programs. To ensure the completeness of our search results, we engaged in snowballing and an expert-based search to identify additional studies of interest. Finally, data were collected from each study to address the research questions outlined above. Results: Using the methods outlined above, we identified 16 empirical studies, including qualitative, quantitative, and mixed-methods studies informed by a range of theoretical frameworks. Based on data collected from the primary studies in our sample, we identified 13 persistence factors across four categories: (I) social capital, networking, \& support; (II) career \& professional development; (III) pedagogical \& programmatic interventions; and (IV) exposure \& access. This data-collection process also yielded 26 recommendations across six stakeholder groups: (i) researchers; (ii) colleges and universities; (iii) the computing industry; (iv) K-12 systems and schools; (v) governments; and (vi) parents. Conclusion: This systematic mapping study resulted in the identification of numerous persistence factors for Black students in CS. Crucially, however, these persistence factors allow Black students to persist, but not thrive, in CS. Accordingly, we contend that more needs to be done to address the systemic inequities faced by Black people in general, and Black women in particular, in computing programs and professions. As evidenced by the relatively small number of primary studies captured by this systematic mapping study, there exists an urgent need for additional, asset-based empirical studies involving Black students in CS. In addition to foregrounding the intersectional experiences of Black women in CS, future studies should attend to the currently understudied experiences of Black men. © 2023 Copyright held by the owner/author(s).},
	author = {Belle, Alvine B. and Sutherland, Callum and Adesina, Opeyemi O. and Kpodjedo, Sègla and Ojong, Nathanael and Cole, Lisa},
	year = {2023},
	keywords = {Mapping, Systematic mapping studies, Economic and social effects, Engineering education, Research questions, Students, Empirical studies, Anti-black racism in computer science, Black student in computer science, Computer Science Education, Computer science programs, Education computing, Employment, Equity diversity inclusion, Equity in computer science education, Student persistences},
}

@article{beke_what_2023,
	title = {What managers can learn from knowledge intensive technology startups? {Exploring} the skillset for developing adaptive organizational learning capabilities of a successful start-up enterprise in management education},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163816643&doi=10.1556%2f204.2022.00027&partnerID=40&md5=ce39554a7e9a704186ca5f51032ae19b},
	abstract = {The study shows what management students could learn from technology startups from an organizational learning (learning organization) perspective; and whether or on what level this entrepreneurial mindset is built into management education. First, the organizational learning patterns and adaptive entrepreneurial skillset of startups are identified, based on a review of the recent literature focusing on knowledge-intensive technology startups’ organizational learning patterns. Then, qualitative interviews and document analysis are applied to find out whether or on what level the improvement of these skills for developing an adaptive and successful startup are present as ‘learning organizations’ are integrated in top Central-European higher management education curricula. Based on the literature review, the theoretical framework is introduced, consisting of five pillars of ‘start-up learning’: ambidextrous entrepreneurial learning, business model development, failure and experiential learning, benchmarking and learning from others, and agile product development. The empirical research looks for these pillars in management MSc programs of a top Central-European business school. The most important findings reveal that the analyzed management education programs strongly prepare students with benchmarking skills. However, the study also showed that the culture and experience of failure and the capability of learning from failure are missing from these education programs. © 2022 The Author(s).},
	author = {Beke, Diána Dóra and Sólyom, Andrea and Klér, Andrea Juhászné},
	year = {2023},
}

@article{dowlut_forecasting_2023,
	title = {Forecasting resort hotel tourism demand using deep learning techniques – {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165006073&doi=10.1016%2fj.heliyon.2023.e18385&partnerID=40&md5=cbaba4fc27e8cfc275704c2f67103ae5},
	abstract = {In the hospitality industry, revenue management is vital for the sustainability of the business. Powering this strategic concept is the occupancy rate (OR) forecast. Predicting occupancy of the hotel is essential for managers’ decision-making process as it gives an estimate of the future business performance. However, the fast-changing marketing demands in the tourism sector, boosted by the advent of online booking, generating accurate forecast figures is nowadays a tough task - needing personnel with advance technical skills and expensive software. The aim of the Systematic Literature review is to provide an insight of the use of Deep Learning techniques for OR prediction. The latest trends in this field over five years (from 2017 to 2022) are highlighted. Through this SRL, three research questions are answered. The questions are related to the variables, deep learning algorithms for prediction and the evaluation metrics used for evaluating the models developed. The Snowballing methodology was used to carry out the SLR. 50 papers were selected for the final analysis. Five categories of variables were identified. LSTM was found to be the most popular deep learning algorithm used to build prediction models. Seven performance metrics were found and among them MAPE was the most popular. To conclude it was found that the hybrid model, CNN-LSTM, to increase accuracy and required more investigation. © 2023 The Authors},
	author = {Dowlut, Noomesh and Gobin-Rahimbux, Baby},
	year = {2023},
}

@inproceedings{gerosa_systematic_2023,
	title = {A {Systematic} {Literature} {Review} on {Physical} and {Action} {Based} {Activities} in {Computing} {Education} for {Early} {Years} and {Primary}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175996894&doi=10.1145%2f3605468.3605500&partnerID=40&md5=8f2258fc74e302b7466acdc655af879e},
	abstract = {Educational systems worldwide are including computer science (CS) education in compulsory curriculums from a very young age. Many activities have been proposed to teach young children CS which include different approaches such as unplugged, physical computing, or completely virtual programming interfaces. Despite this, more research is needed to understand which pedagogical approaches capitalise on young children's cognitive and affective capacities throughout their development to promote learning outcomes. Grounded cognition (GC) proposes that our perception and thought are highly influenced by our bodily experiences and that dynamic actions such as movement affect our understanding of the world around us. For young children, experiences integrating cognitive and sensory-motor aspects are often used. These activities could be conceptualised as grounded activities, as they incorporate concrete representation and action. However, the extent to which these activities impact children's learning outcomes has, to our knowledge, not been explored thus far. Moreover, the theoretical background informing these activities and how these map onto the grounded cognition background is often an under-reported aspect in the literature. This study aims to bridge this gap by conducting a systematic literature review. We identified empirical research reporting CS learning activities with a grounded cognition approach and analysed its activities, CS concepts targeted, how their theoretical background informed their pedagogical design and their outcomes. This paper has important implications for computer science education. Firstly, it presents the empirical evidence using this theoretical background with an emphasis on activity design, which will be useful for academics or practitioners looking to incorporate grounded cognition theory into their instruction. Secondly, it identifies significant gaps in the current practices, specifically in the links between theory and practice and thus is a stepping stone for further research in this interdisciplinary area. © 2023 ACM.},
	author = {Gerosa, Anaclara and Kallia, Maria and Cutts, Quintin},
	year = {2023},
	keywords = {Systematic literature review, Engineering education, Teaching strategy, Computer Science Education, Education computing, Computation theory, Computing, Computing education, Early childhood educations, Educational systems, Learning outcome, Primary, Young children},
}

@article{mei_deriving_2023,
	title = {Deriving {Thresholds} of {Object}-{Oriented} {Metrics} to {Predict} {Defect}-{Proneness} of {Classes}: {A} {Large}-{Scale} {Meta}-{Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153954593&doi=10.1142%2fS0218194023500110&partnerID=40&md5=61e07db7b45db195ab4a1982d2cc5279},
	abstract = {Many studies have explored the methods of deriving thresholds of object-oriented (i.e. OO) metrics. Unsupervised methods are mainly based on the distributions of metric values, while supervised methods principally rest on the relationships between metric values and defect-proneness of classes. The objective of this study is to empirically examine whether there are effective threshold values of OO metrics by analyzing existing threshold derivation methods with a large-scale meta-analysis. Based on five representative threshold derivation methods (i.e. VARL, ROC, BPP, MFM, and MGM) and 3268 releases from 65 Java projects, we first employ statistical meta-analysis and sensitivity analysis techniques to derive thresholds for 62 OO metrics on the training data. Then, we investigate the predictive performance of five candidate thresholds for each metric on the validation data to explore which of these candidate thresholds can be served as the threshold. Finally, we evaluate their predictive performance on the test data. The experimental results show that 26 of 62 metrics have the threshold effect and the derived thresholds by meta-analysis achieve promising results of GM values and significantly outperform almost all five representative (baseline) thresholds. \#c World Scientific Publishing Company.},
	author = {Mei, Yuanqing and Rong, Yi and Liu, Shiran and Guo, Zhaoqiang and Yang, Yibiao and Lu, Hongmin and Tang, Yutian and Zhou, Yuming},
	year = {2023},
	keywords = {Defects, Class A, Defect proneness, Large-scales, Meta-analysis, Metric values, Object oriented, Object oriented metrics, OO metrics, Predictive performance, Sensitivity analysis, Threshold},
}

@article{barcellos_flatsat_2023,
	title = {{FlatSat} {Platforms} for {Small} {Satellites}: {A} {Systematic} {Mapping} and {Classification}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177718452&doi=10.1109%2fJMASS.2023.3249044&partnerID=40&md5=21cad600af368cb7aa793fc91cd172cd},
	abstract = {Recent trends indicate an increase in the number of small satellite missions, which can be developed more quickly and at a lower cost than traditional satellites. This has led to a growing interest in university-based satellite development, despite a lack of expertise in the space field, which has resulted in a high failure rate for such missions. To address this issue, the implementation of robust and reliable verification and validation (V\&V) methods has become essential, and it has been demonstrated that the use of a FlatSat during the V\&V campaign increases reliability. Despite the significance of FlatSat, there is a dearth of information on the platforms used to implement it, as well as a classification scheme for locating them. This article contributes to bridging this gap by conducting a systematic mapping of 65 works that were selected based on specific criteria and subsequently analyzed. The primary characteristics of the platforms are enumerated, and a new classification for FlatSat platforms into Raw, Bridge, Dock, and Modular is proposed. In order to provide a comprehensive understanding of the topic, the principal tests conducted on these platforms were also covered. © 2019 IEEE.},
	author = {Barcellos, Joao Claudio Elsen and Spengler, Anderson Wedderhoff and Seman, Laio Oriel and Silva, Raphael Diego Comesanha E and Roldan, Hector Pettenghi and Bezerra, Eduardo Augusto},
	year = {2023},
	keywords = {Embedded systems, Mapping, Systematic mapping, Classification (of information), Embedded-system, Failure analysis, Cubesats, Flatsats, Low-costs, Recent trends, Small satellite mission, Small satellites, Small-satellite, Verification and validation, Verification-and-validation},
}

@article{boaye_belle_evidence-based_2023,
	title = {Evidence-based decision-making: {On} the use of systematicity cases to check the compliance of reviews with reporting guidelines such as {PRISMA} 2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257221&doi=10.1016%2fj.eswa.2023.119569&partnerID=40&md5=826373e77ccc2d151c6b2bc7bfbe1bb6},
	abstract = {Background and context: Systematic reviews aim to provide high-quality evidence-based syntheses for efficacy under real-world conditions and allow understanding the correlations between exposures and outcomes. They are increasingly popular and have several stakeholders (e.g., healthcare providers, researchers, educators, students, journal editors, policy makers, managers) to whom they help make informed recommendations for practice or policy. Problem: Systematic reviews usually exhibit low methodological and reporting quality. To tackle this, reporting guidelines have been developed to support systematic reviews reporting and assessment. Following such guidelines is crucial to ensure that a review is transparent, complete, trustworthy, reproducible, and unbiased. However, systematic reviewers usually fail to adhere to existing reporting guidelines, which may significantly decrease the quality of the reviews they report and may result in systematic reviews that lack methodological rigor, yield low-credible findings and may mislead decision-makers. Methods: To assure that a review complies with reporting guidelines, we rely on assurance cases that are an emerging way of arguing and relaying various safety–critical systems’ requirements in an extensive manner, as well as checking the compliance of such systems with standards to support their certification. Since the nature of assurance cases makes them applicable to various domains and requirements/properties, we therefore propose a new type of assurance cases called systematicity cases. Systematicity cases focus on the systematicity property and allow arguing that a review is systematic i.e., that it sufficiently complies with the targeted reporting guideline. The most widespread reporting guidelines include PRISMA (Preferred Reporting Items for Systematic reviews and meta-Analyses). We measure the confidence in a systematicity case representing a review as a means to quantify the systematicity of that review i.e., the extent to which that review is systematic. We rely on rule-based Artificial Intelligence to create a knowledge-based system that automatically supports the inference mechanism that a given systematicity case embodies and that allows making a decision regarding the systematicity of a given review. Results: An empirical evaluation performed on 25 reviews (self-identifying as systematic) showed that these reviews exhibit a suboptimal systematicity. More specifically, the systematicity of the analyzed reviews varies between 32.96\% and 66.49\% and its average is 54.42\%. More efforts are therefore needed to report systematic reviews of higher quality. More experiments are also needed to further explore the factors hindering and/or assuring the systematicity of reviews. Audience: The main beneficiaries of our work are journal reviewers, journal editors, managers, policymakers, researchers, organizations developing reporting guidelines, peer reviewers, students, insurers, evidence users, as well as reporting guidelines developers. © 2023 The Author(s)},
	author = {Boaye Belle, Alvine and Zhao, Yixi},
	year = {2023},
	keywords = {Regulatory compliance, Systematic Review, Decision making, Compliance control, Knowledge-based systems, Meta-analysis, Assurance case, Assurance case (systematicity case), Guideline adherence, Knowledge representation, Knowledge representation and reasoning, Preferred reporting item for systematic review and meta-analyze statement, Reporting guideline adherence, Systematicity},
}

@article{bermejo_arvr_2023,
	title = {{AR}/{VR} {Teaching}-{Learning} {Experiences} in {Higher} {Education} {Institutions} ({HEI}): {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163713996&doi=10.3390%2finformatics10020045&partnerID=40&md5=32e5bb794c27f1d7f8eb7f209e9158b4},
	abstract = {During the last few years, learning techniques have changed, both in basic education and in higher education. This change has been accompanied by new technologies such as Augmented Reality (AR) and Virtual Reality (AR). The combination of these technologies in education has allowed a greater immersion, positively affecting the learning and teaching processes. In addition, since the COVID-19 pandemic, this trend has been growing due to the diversity of the different fields of application of these technologies, such as heterogeneity in their combination and their different experiences. It is necessary to review the state of the art to determine the effectiveness of the application of these technologies in the field of university higher education. In the present paper, this aim is achieved by performing a systematic literature review from 2012 to 2022. A total of 129 papers were analyzed. Studies in our review concluded that the application of AR/VR improves learning immersion, especially in hospitality, medicine, and science studies. However, there are also negative effects of using these technologies, such as visual exhaustion and mental fatigue. © 2023 by the authors.},
	author = {Bermejo, Belen and Juiz, Carlos and Cortes, David and Oskam, Jeroen and Moilanen, Teemu and Loijas, Jouko and Govender, Praneschen and Hussey, Jennifer and Schmidt, Alexander Lennart and Burbach, Ralf and King, Daniel and O’Connor, Colin and Dunlea, Davin},
	year = {2023},
}

@article{fernandes_digital_2023,
	title = {Digital {Alternative} {Communication} for {Individuals} with {Amyotrophic} {Lateral} {Sclerosis}: {What} {We} {Have}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169140808&doi=10.3390%2fjcm12165235&partnerID=40&md5=e23f9bba2b47e8dc0e6ea6345f35fddd},
	abstract = {Amyotrophic Lateral Sclerosis is a disease that compromises the motor system and the functional abilities of the person in an irreversible way, causing the progressive loss of the ability to communicate. Tools based on Augmentative and Alternative Communication are essential for promoting autonomy and improving communication, life quality, and survival. This Systematic Literature Review aimed to provide evidence on eye-image-based Human–Computer Interaction approaches for the Augmentative and Alternative Communication of people with Amyotrophic Lateral Sclerosis. The Systematic Literature Review was conducted and guided following a protocol consisting of search questions, inclusion and exclusion criteria, and quality assessment, to select primary studies published between 2010 and 2021 in six repositories: Science Direct, Web of Science, Springer, IEEE Xplore, ACM Digital Library, and PubMed. After the screening, 25 primary studies were evaluated. These studies showcased four low-cost, non-invasive Human–Computer Interaction strategies employed for Augmentative and Alternative Communication in people with Amyotrophic Lateral Sclerosis. The strategies included Eye-Gaze, which featured in 36\% of the studies; Eye-Blink and Eye-Tracking, each accounting for 28\% of the approaches; and the Hybrid strategy, employed in 8\% of the studies. For these approaches, several computational techniques were identified. For a better understanding, a workflow containing the development phases and the respective methods used by each strategy was generated. The results indicate the possibility and feasibility of developing Human–Computer Interaction resources based on eye images for Augmentative and Alternative Communication in a control group. The absence of experimental testing in people with Amyotrophic Lateral Sclerosis reiterates the challenges related to the scalability, efficiency, and usability of these technologies for people with the disease. Although challenges still exist, the findings represent important advances in the fields of health sciences and technology, promoting a promising future with possibilities for better life quality. © 2023 by the authors.},
	author = {Fernandes, Felipe and Barbalho, Ingridy and Bispo Júnior, Arnaldo and Alves, Luca and Nagem, Danilo and Lins, Hertz and Arrais Júnior, Ernano and Coutinho, Karilany D. and Morais, Antônio H. F. and Santos, João Paulo Q. and Machado, Guilherme Medeiros and Henriques, Jorge and Teixeira, César and Dourado Júnior, Mário E. T. and Lindquist, Ana R. R. and Valentim, Ricardo A. M.},
	year = {2023},
	keywords = {human, systematic review, amyotrophic lateral sclerosis, augmentative and alternative communication, clinical article, eye tracking, eyelid reflex, human computer interaction, Review},
}

@article{guimaraes_responsible_2023,
	title = {Responsible innovation assessment tools: a systematic review and research agenda},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171182021&doi=10.1108%2fTECHS-11-2022-0043&partnerID=40&md5=85afe10c7e7cdc6cf361c0cf9ba06032},
	abstract = {Purpose: Responsible innovation assessment tools (RIATs) are key instruments that can help organizations, associations and individuals measure responsible innovation. Accordingly, this study aims to review the current status of research on responsible innovation and, in particular, of studies that either present the relevance of RIATs or provide empirical evidence of their adoption. Design/methodology/approach: A systematic literature review is conducted to identify and review how RIATs are being addressed in academic research and the applications that are proposed. A systematic process is implemented using the Web of Science and Scopus bibliographic databases, aiming not only to summarize existing studies, but also to include a perspective on gaps and future research. Findings: A total of 119 publications were identified and included in the review process. The study identifies that RIATs have attracted growing interest from the scientific community, with a greater predominance of studies involving qualitative and mixed methods. A well-balanced mix of conceptual and exploratory studies is also registered, with a greater predominance of analysis of RIATs application domains in the past years, with greater incidence in the finance, water, energy, construction, manufacturing and health sectors. Originality/value: This study is pioneering in identifying 16 dimensions and 60 sub-dimensions for measuring responsible innovation. It also suggests the need to include multidimensional perspectives and individuals with interdisciplinary competencies in this process. © 2022, Emerald Publishing Limited.},
	author = {Guimarães, Cristina and Amorim, Vasco and Almeida, Fernando},
	year = {2023},
}

@article{daun_context_2023,
	title = {Context modeling for cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128518651&doi=10.1002%2fsmr.2451&partnerID=40&md5=ae0f37165027c923e06274ec9f617fd9},
	abstract = {When developing cyber-physical systems (CPS), the context is of vital importance. CPS interact with the world not only through sensing the environment and acting upon it (like embedded systems) but also by communicating with other CPS (like systems in the Internet of Things [IoT]). This means that the context interactions CPS must deal with are much greater than regular embedded or IoT systems: On the one hand, external systems and human users constrain the specific interaction among them. On the other hand, properties of these external systems, human users, and laws, regulations, or standards constrain the way the CPS must be developed. In this paper, we propose a comprehensive, ontologically grounded context modeling framework to systematically explore the problem space in which a CPS under development will operate. This allows for the systematic elicitation of requirements for the CPS, early validation and verification of its properties, and safety assessment of its context interactions at runtime. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Daun, Marian and Tenbergen, Bastian},
	year = {2023},
	keywords = {Embedded systems, Requirement engineering, Requirements engineering, Verification, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Internet of things, Collaborative system network, Collaborative systems, Context, Context analyse, Context analysis, context modeling, Context models, dynamic context, Dynamic contexts, Laws and legislation, Model dynamics, Model-based engineering, Systems networks, Validation},
}

@article{saleemi_ubiquitous_2023,
	title = {Ubiquitous healthcare: a systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091477767&doi=10.1007%2fs12652-020-02513-x&partnerID=40&md5=3506b055c1c76d4dcdb81cf6a121df29},
	abstract = {Ubiquitous healthcare is an emerging area that employs ubiquitous technologies to enable technology oriented environment for healthcare professionals for provision of efficient and effective healthcare services. In past years, research community has proposed various technological solutions in different healthcare areas such as chronic disease monitoring, gait analysis, mood and fall detection, neuropathic monitoring, physiological and vital signs monitoring, pulmonogical monitoring, etc. However, in-depth analysis of these proposed solutions is required to analyze the form of proposed ubiquitous healthcare solutions; the extent ubiquitous technologies are integrated in these solutions; the type of real problem addressed; and how far these solutions are evaluated in real world settings? The addressal of these questions is critical to understand and evaluate the progress made in the area of ubiquitous healthcare and identify the challenges that are hindering the progress in this area. Therefore, in this research, a systematic research technique in the form of mapping study (also known as scoping study) is employed for in-depth analysis of evidences available on ubiquitous healthcare. The mapping study adopts a systematic approach to construct chain of evidences related to a particular topic and is a well-defined research technique in evidence based software engineering. This study identified a total of 103 primary studies, published between 2007 and 2018, for analysis of area under investigation. The study findings reveal that research trend in ubiquitous healthcare is horizontally spread by involving broad range of healthcare areas. The proposed solutions largely fall under the category of validation studies where experiments are conducted in laboratory settings rather real world environment. Another interesting finding is the lack of involvement of relevant healthcare community in proposed solution design. The challenges such as context awareness, data ownership, privacy and security, usability and trust are limiting the adoption of proposed solutions. Therefore, more extensive studies are required to first evaluate the applicability of proposed solutions in their respective environment, second, engagement and ownership of relevant community in solution design need to be considered. Third, the broad coverage of healthcare areas does not provide significant clusters of similar research in any particular area therefore future research should focus on strengthening these areas by conducting evaluation based longitudinal studies. In this way, the effects of proposed solutions can only be measured objectively and can be added to the body of knowledge. Finally, this research provides a thorough insight into the research on ubiquitous healthcare and offers an opportunity to conduct further research in this area. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
	author = {Saleemi, Maria and Anjum, Maria and Rehman, Mariam},
	year = {2023},
	keywords = {Software engineering, Systematic literature review, Mapping, Mapping studies, Ambient assisted living, Assisted living, Bodyarea networks (BAN), Digital health, E-services, Ehealth, Emerging healthcare technology, Fall detection, Healthcare technology, Medical computing, mHealth, Patient monitoring, Pattern recognition, Telecare, Telehealth, Telemedicine, Telemedicine and wellness, Ubiquitous health care},
}

@article{alfayez_what_2023,
	title = {What is asked about technical debt ({TD}) on {Stack} {Exchange} question-and-answer ({Q}\&{A}) websites? {An} observational study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146921032&doi=10.1007%2fs10664-022-10269-5&partnerID=40&md5=148ce9655aa6c1affadbd9c4f0406f3b},
	abstract = {Technical debt (TD) is a term coined by agile software pioneer Ward Cunningham to account for the added software system effort or cost resulting from taking early software project shortcuts. Previous research on TD has extensively outlined and discussed the various consequences derived from accumulating TD and the difficulty in managing it. A review of the software engineering literature revealed that Stack Exchange question-and-answer (Q\&A) websites can provide valuable, real world perspectives on a number of software engineering topics. Therefore, this study aims to observe how the TD term is utilized on Stack Exchange Q\&A websites. Specifically, this study utilizes a dataset derived from three Stack Exchange Q\&A websites, which are Stack Overflow (SO), Software Engineering (SE), and Project Management (PM), to retrieve and analyze 578 TD-related questions. The results unveiled that TD-related questions can be categorized into 14 different categories, a total of 636 unique tags are utilized in the acquired set of TD-related questions, and a few TD-related categories both lack accepted answers and have a longer median time to receive an accepted answer than other categories. This study’s findings highlight the TD-related challenges that are addressed by Stack Exchange Q\&A website users, which may prove beneficial in steering future TD-related efforts. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Alfayez, Reem and Ding, Yunyan and Winn, Robert and Alfayez, Ghaida and Harman, Christopher and Boehm, Barry},
	year = {2023},
	keywords = {Computer software, Software-systems, Technical debts, Project management, Technical debt management, Software project, Real-world, Agile softwares, Observational study, Question-and-answer website, Stackoverflow, Websites},
}

@inproceedings{koana_ownership_2023,
	title = {Ownership in the {Hands} of {Accountability} at {Brightsquid}: {A} {Case} {Study} and a {Developer} {Survey}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180546160&doi=10.1145%2f3611643.3613890&partnerID=40&md5=ee4f1bde53f308b0d3a0a4497d826c01},
	abstract = {The COVID-19 pandemic has accelerated the adoption of digital health solutions. This has presented significant challenges for software development teams to swiftly adjust to the market needs and demand. To address these challenges, product management teams have had to adapt their approach to software development, reshaping their processes to meet the demands of the pandemic. Brighsquid implemented a new task assignment process aimed at enhancing developer accountability toward the customer. To assess the impact of this change on code ownership, we conducted a code change analysis. Additionally, we surveyed 67 developers to investigate the relationship between accountability and ownership more broadly. The findings of our case study indicate that the revised assignment model not only increased the perceived sense of accountability within the production team but also improved code resilience against ownership changes. Moreover, the survey results revealed that a majority of the participating developers (67.5\%) associated perceived accountability with artifact ownership. © 2023 ACM.},
	author = {Koana, Umme Ayman and Chew, Francis and Carlson, Chris and Nayebi, Maleknaz},
	year = {2023},
	keywords = {Product management, Software design, COVID-19, Human resource management, Case-studies, Software development teams, Computer software selection and evaluation, Software Quality, Market needs, Accountability, Management team, Market demand, Ownership, Tasks assignments},
}

@article{thajeel_machine_2023,
	title = {Machine and {Deep} {Learning}-based {XSS} {Detection} {Approaches}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163933483&doi=10.1016%2fj.jksuci.2023.101628&partnerID=40&md5=7e7780a4abc9d6b6e38e915244463792},
	abstract = {Web applications are paramount tools for facilitating services providing in the modern world. Unfortunately, the tremendous growth in the web application usage has resulted in a rise in cyberattacks. Cross-site scripting (XSS) is one of the most frequent cyber security attack vectors that threaten the end user as well as the service provider with the same degree of severity. Recently, an obvious increase of the Machine learning and deep learning ML/DL techniques adoption in XSS attack detection. The goal of this review is to come with a special attention and highlight of Machine learning and deep learning approaches. Thus, in this paper, we present a review of recent advances applied in ML/DL for XSS attack detection and classification. The existing proposed ML/DL approaches for XSS attack detection are analyzed and taxonomized comprehensively in terms of domain areas, data preprocessing, feature extraction, feature selection, dimensionality reduction, Data imbalance, performance metrics, datasets, and data types. Our analysis reveals that the way of how the XSS data is preprocessed considerably impacts the performance and the attack detection models. Proposing a full preprocessing cycle reveals how various ML/DL approaches for XSS attacks detection take advantage of different input data preprocessing techniques. The most used ML/DL and preprocessing stages have also been identified. The limitations of existing ML/DL-based XSS attack detection mechanisms are highlighted to identify the potential gaps and future trends. © 2023 The Author(s)},
	author = {Thajeel, Isam Kareem and Samsudin, Khairulmizam and Hashim, Shaiful Jahari and Hashim, Fazirulhisyam},
	year = {2023},
}

@article{ferreira_towards_2023,
	title = {Towards an understanding of reliability of software-intensive systems-of-systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172749&doi=10.1016%2fj.infsof.2023.107186&partnerID=40&md5=0d674782eca671dd406f7e52554660b5},
	abstract = {Context: Large-scale software-intensive Systems-of-Systems (SoS) have become present in several critical domains and have sometimes depended on diverse trending technologies, such as cloud computing and machine learning. At the same time, the SoS dynamic architecture makes it difficult to assure SoS reliability leading to diverse studies with specific solutions, while the need for a shared view of what precisely SoS reliability refers to still exists. Objective: The main contribution of this article is to go towards an understanding of SoS reliability. We present a conceptual model whose concepts as well as their definitions and relationships were defined by systematically examining the literature of the field. Methods: We surveyed 36 practitioners and researchers regarding ambiguity, explanatory power, parsimony, generality, and utility of our model. Next, we adjusted our model according to their contribution. Results: We reach a conceptual model containing 29 concepts and their relationships that help to comprehend SoS reliability. In addition, we provided a glossary with a definition of each concept of our conceptual model. We also proposed a SoS reliability definition grounded on the literature. Conclusions: By organizing the knowledge of SoS reliability, this conceptual model makes it possible to expand the body of knowledge in the area and opens several opportunities for further investigations; in particular, this model serves as a basis for novel solutions aiming to assure SoS reliability. © 2023 Elsevier B.V.},
	author = {Ferreira, Francisco Henrique Cerdeira and Nakagawa, Elisa Yumi and Santos, Rodrigo Pereira dos},
	year = {2023},
	keywords = {Machine-learning, Empirical studies, Software reliability, Conceptual model, Large-scales, Cloud-computing, Critical domain, Distributed computer systems, Software intensive systems, System of systems, System reliability, System-of-systems},
}

@article{khan_sql_2023,
	title = {{SQL} and {NoSQL} {Database} {Software} {Architecture} {Performance} {Analysis} and {Assessments}—{A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163681824&doi=10.3390%2fbdcc7020097&partnerID=40&md5=ca9df47cac308d21d9cec88cac5c7444},
	abstract = {The competent software architecture plays a crucial role in the difficult task of big data processing for SQL and NoSQL databases. SQL databases were created to organize data and allow for horizontal expansion. NoSQL databases, on the other hand, support horizontal scalability and can efficiently process large amounts of unstructured data. Organizational needs determine which paradigm is appropriate, yet selecting the best option is not always easy. Differences in database design are what set SQL and NoSQL databases apart. Each NoSQL database type also consistently employs a mixed-model approach. Therefore, it is challenging for cloud users to transfer their data among different cloud storage services (CSPs). There are several different paradigms being monitored by the various cloud platforms (IaaS, PaaS, SaaS, and DBaaS). The purpose of this SLR is to examine the articles that address cloud data portability and interoperability, as well as the software architectures of SQL and NoSQL databases. Numerous studies comparing the capabilities of SQL and NoSQL of databases, particularly Oracle RDBMS and NoSQL Document Database (MongoDB), in terms of scale, performance, availability, consistency, and sharding, were presented as part of the state of the art. Research indicates that NoSQL databases, with their specifically tailored structures, may be the best option for big data analytics, while SQL databases are best suited for online transaction processing (OLTP) purposes. © 2023 by the authors.},
	author = {Khan, Wisal and Kumar, Teerath and Zhang, Cheng and Raj, Kislay and Roy, Arunabha M. and Luo, Bin},
	year = {2023},
	keywords = {Systematic literature review, Data handling, Digital storage, Software architecture, BASE, Cloud analytics, Data Analytics, Database software, Database systems, DBaaS, Horizontal expansion, Map-reduce, MapReduce, Performance assessment, Performances analysis, SQL and NoSQL database, SQL database},
}

@article{bovo_digital_2023,
	title = {Digital twins for the rapid startup of manufacturing processes: a case study in {PVC} tube extrusion},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164837924&doi=10.1007%2fs00170-023-11906-z&partnerID=40&md5=20ab08169248f2a738ac6318cfd26345},
	abstract = {In this work, a soft sensor–based digital twin (DT) was developed to reduce the startup time in manufacturing plastic tubes and enable real-time product quality monitoring, i.e., the weight per unit length and the inner and outer diameters of the tube. An experimental campaign was conducted on a real tube extrusion line using three polyvinyl chloride (PVC) compounds and different process conditions, and machine learning regression algorithms were trained and tested to create the models of the extruder and the extrusion die the DT is based on. The characterization of the considered material, whose properties were given as input to the digital models, was carried out according to a procedure based only on the data collected by the production line. The DT was tested for the startup of the production of a single-layer tube and allowed to achieve the specified customer requirements (thickness and weight) in a few minutes. The proposed solution thus proved to be a valuable tool for reducing the setup time, thus increasing the efficiency of the process. © 2023, The Author(s).},
	author = {Bovo, Enrico and Sorgato, Marco and Lucchetta, Giovanni},
	year = {2023},
	keywords = {Machine learning, Case-studies, Manufacturing process, Chlorine compounds, Data-driven model, Digital manufacturing, Extrusion, Plastic tube, Polyvinyl chlorides, Rapid startups, Real- time, Soft sensors, Startup time, Tube extrusions, Tubes (components)},
}

@article{machado_literature_2023,
	title = {Literature review of digital twin in healthcare},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170409582&doi=10.1016%2fj.heliyon.2023.e19390&partnerID=40&md5=3b0a9a7a5fd6da02279c410ec7d5ed26},
	abstract = {This article aims to make a bibliometric literature review using systematic scientific mapping and content analysis of digital twins in healthcare to know the evolution, domain, keywords, content type, and kind and purpose of digital twin's implementation in healthcare, so a consolidation and future improvement of existing knowledge can be made and gaps for new studies can be identified. The increase in publications of digital twins in healthcare is quite recent and it is still concentrated in the domain of technology sources. The subject is majorly concentrated in patient's digital twin group and in precision medicine and aspects, issues and/or policies subgroups, although the publications keywords mirror it only at the group side. Digital twins in healthcare are probably stepping out of the infancy phase. On the other hand, digital twins in hospital group and the device and facilities management subgroups are more mature with all knowledge gathered from the manufacturing sector. There is an absence of some publication's types in general, device and care subgroup and no whole body or hospital digital twin was reported. Based on the presented arguments, guidelines for future research were presented: advance in the creation of general frameworks, in subgroups not as much explored, and in groups and subgroups already explored, but that need more advancement to achieve the main goals of a whole human or hospital digital twin with the main issues resolved. © 2023 The Authors},
	author = {Machado, Tatiana Mallet and Berssaneti, Fernando Tobal},
	year = {2023},
}

@article{oliveira_systematic_2023,
	title = {A systematic literature review on the impact of formatting elements on code legibility},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163388981&doi=10.1016%2fj.jss.2023.111728&partnerID=40&md5=1ac99e82c24196970128ea680e222e6b},
	abstract = {Context: Software programs can be written in different but functionally equivalent ways. Even though previous research has compared specific formatting elements to find out which alternatives affect code legibility, seeing the bigger picture of what makes code more or less legible is challenging. Goal: We aim to find which formatting elements have been investigated in empirical studies and which alternatives were found to be more legible for human subjects. Method: We conducted a systematic literature review and identified 15 papers containing human-centric studies that directly compared alternative formatting elements. We analyzed and organized these formatting elements using a card-sorting method. Results: We identified 13 formatting elements (e.g., indentation) and 33 levels of formatting elements (e.g., two-space indentation), which are about formatting styles, spacing, block delimiters, long or complex code lines, and word boundary styles. While some levels were found to be statistically better than other equivalent ones in terms of code legibility, e.g., appropriate use of indentation with blocks, others were not, e.g., formatting layout. For identifier style, we found divergent results, where one study found a significant difference in favor of camel case, while another study found a positive result in favor of snake case. Conclusion: The number of identified papers, some of which are outdated, and the many null and contradictory results emphasize the relative lack of work in this area and underline the importance of more research. There is much to be understood about how formatting elements influence code legibility before the creation of guidelines and automated aids to help developers make their code more legible. © 2023},
	author = {Oliveira, Delano and Santos, Reydne and Madeiral, Fernanda and Masuhara, Hidehiko and Castor, Fernando},
	year = {2023},
	keywords = {Systematic literature review, Codes (symbols), Software project, Empirical studies, Card-sorting, Code legibility, Formatting element, Human subjects, Human-centric, Indentation, Program understandability, Understandability},
}

@article{broekhuizen_ai_2023,
	title = {{AI} for managing open innovation: {Opportunities}, challenges, and a research agenda},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166189921&doi=10.1016%2fj.jbusres.2023.114196&partnerID=40&md5=5f455a87a517d661bcaaa134c340eeaa},
	abstract = {Artificial intelligence (AI) provides ample opportunities for enabling effective knowledge sharing among organizations seeking to foster open innovation. Past research often investigates the capability of AI to perform ‘human’ tasks in structured application fields. Yet, there is a lack of research that systematically analyzes when and how AI can be used for the more complex and unstructured tasks of open innovation (OI). We present a framework for leveraging AI-enabled applications to foster productive OI collaborations. Specifically, we create a 3x3 matrix by aligning the three OI stages (initiation, development, realization) with the three management functions of AI (mapping, coordinating, controlling). This matrix assists in identifying how various AI applications may augment or automate human intelligence, thereby helping to resolve prevailing OI challenges. It provides guidance on how organizations can use AI to establish, execute and govern exchanges across the OI stages. Finally, we lay out an agenda for future research. © 2023 The Authors},
	author = {Broekhuizen, Thijs and Dekker, Henri and de Faria, Pedro and Firk, Sebastian and Nguyen, Dinh Khoi and Sofka, Wolfgang},
	year = {2023},
}

@inproceedings{imamura_towards_2023,
	title = {Towards a {Catalog} of {Heuristics} for the {Design} of {Systems}-of-{Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165978127&doi=10.1145%2f3592813.3592897&partnerID=40&md5=1c68db0b4c5b8832e2973121156381a6},
	abstract = {Context: Systems-of-Systems (SoS) are arrangements of independent systems that are increasingly present in everyday life and can be observed in domains such as healthcare, transport, and Industry 4.0, to mention a few. Problem: A significant concern regarding SoS refers to the constituent systems’ (CS) independence. CS are managed by different organizations that control them independently of SoS. Hence, the design of SoS is challenging as it involves careful investigation, allocation, and integration of CS to ensure proper operation. Solution: This paper provides a catalog of good practices and recommendations, herein referred to as “heuristics”, which can be applied to the SoS design. The main purpose of the catalog is to provide directions on what practitioners should consider during the design phase to ensure the proper operation of the SoS. IS theory: This research is based on the General Systems Theory that allows understanding SoS as a complex system constructed with independent systems. Method: We conducted a systematic mapping study (SMS) to identify which heuristics have been applied to SoS design. The results were discussed in a focus group with professionals to organize the heuristics. Summary of Results: After reaching a consensus on the focus group, we organized a catalog of fifteen heuristics into five categories: initiation, CS, interoperability, emergent behavior, and monitoring. Contributions and Impact in the IS area: The heuristics catalog, which is grounded in the literature, would support researchers and professionals in identifying critical issues during the SoS design phase. © 2023 Copyright held by the owner/author(s).},
	author = {Imamura, Marcio and Ferreira, Francisco and Fernandes, Juliana and Neto, Valdemar Vicente Graciano and dos Santos, Rodrigo Pereira},
	year = {2023},
	keywords = {Systematic mapping studies, Focus groups, System of systems, System-of-systems, Design phase, Electric utilities, Emergent behaviours, General systems theory, Good practices, Heuristic methods, Independent systems, Systems interoperability, Systems-of-systems design},
}

@article{silva_energy_2023,
	title = {Energy awareness and energy efficiency in internet of things middleware: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144045635&doi=10.1007%2fs12243-022-00936-5&partnerID=40&md5=33938e423af15f297f51ad01617f424d},
	abstract = {The Internet of Things (IoT) is characterized by a myriad of physical deices, together with high heterogeneity in both software and hardware. Middleware platforms have been proposed in order to alleviate such heterogeneity, providing relevant services and easing application development. In IoT systems, energy consumption is a key concern due to the proliferation of devices and their limited battery capacity. IoT middleware platforms can play an important role in providing applications with strategies, and support, for energy awareness and energy efficiency. Although there is a significant existing body of work related to IoT middleware, there is, as yet, no complementary overview of the state of the art on how these platforms can contribute to energy efficiency and energy awareness in IoT systems. This paper provides such an overview in the form of a systematic literature review (SLR). The SLR was carried out by following a systematic, rigorous procedure to search, select, and analyze primary studies available in the literature. Our corpus, as presented in this paper, is made up of twenty-two such studies, each presenting strategies and solutions on middleware support for energy efficiency and energy awareness in IoT systems. These strategies mainly focus on network adaptation, task offloading, and concrete implementations. However, most of these studies do not consider energy-aware/efficiency abstractions, and focus on solutions working at the end-user application side. In conclusion, this paper also raises relevant challenges and potential directions for further research resulting from the main SLR findings. © 2022, Institut Mines-Télécom and Springer Nature Switzerland AG.},
	author = {Silva, Pedro Victor Borges Caldas da and Taconet, Chantal and Chabridon, Sophie and Conan, Denis and Cavalcante, Everton and Batista, Thais},
	year = {2023},
	keywords = {Systematic literature review, State of the art, Internet of things, Application development, Battery capacity, Energy efficiency, Energy utilization, Energy-awareness, Green computing, High heterogeneity, Middleware, Middleware platforms, Power management, Software and hardwares, System energy consumption, Work-related},
}

@article{abdulazeem_human_2023,
	title = {Human {Factors} {Considerations} for {Quantifiable} {Human} {States} in {Physical} {Human}–{Robot} {Interaction}: {A} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170345069&doi=10.3390%2fs23177381&partnerID=40&md5=25dd2b033ee1d3ee530c5a69317e90de},
	abstract = {As the global population rapidly ages with longer life expectancy and declining birth rates, the need for healthcare services and caregivers for older adults is increasing. Current research envisions addressing this shortage by introducing domestic service robots to assist with daily activities. The successful integration of robots as domestic service providers in our lives requires them to possess efficient manipulation capabilities, provide effective physical assistance, and have adaptive control frameworks that enable them to develop social understanding during human–robot interaction. In this context, human factors, especially quantifiable ones, represent a necessary component. The objective of this paper is to conduct an unbiased review encompassing the studies on human factors studied in research involving physical interactions and strong manipulation capabilities. We identified the prevalent human factors in physical human–robot interaction (pHRI), noted the factors typically addressed together, and determined the frequently utilized assessment approaches. Additionally, we gathered and categorized proposed quantification approaches based on the measurable data for each human factor. We also formed a map of the common contexts and applications addressed in pHRI for a comprehensive understanding and easier navigation of the field. We found out that most of the studies in direct pHRI (when there is direct physical contact) focus on social behaviors with belief being the most commonly addressed human factor type. Task collaboration is moderately investigated, while physical assistance is rarely studied. In contrast, indirect pHRI studies (when the physical contact is mediated via a third item) often involve industrial settings, with physical ergonomics being the most frequently investigated human factor. More research is needed on the human factors in direct and indirect physical assistance applications, including studies that combine physical social behaviors with physical assistance tasks. We also found that while the predominant approach in most studies involves the use of questionnaires as the main method of quantification, there is a recent trend that seeks to address the quantification approaches based on measurable data. © 2023 by the authors.},
	author = {Abdulazeem, Nourhan and Hu, Yue},
	year = {2023},
	keywords = {human, Humans, Literature reviews, aged, Aged, Birth rates, Domestic services, ergonomics, Ergonomics, Global population, Healthcare services, Human robot interaction, industry, Industry, Life expectancies, Long life, Man machine systems, Manipulators, Physical humanrobot interaction (phri), Population statistics, Robot applications, robotics, Robotics, Robots manipulators, social behavior, Social behavior, Social Behavior, Social behaviour},
}

@article{ferreira_lessons_2023,
	title = {Lessons learned to improve the {UX} practices in agile projects involving data science and process automation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145568139&doi=10.1016%2fj.infsof.2022.107106&partnerID=40&md5=e1f34b13a26ed1d8c5db7eb024963f24},
	abstract = {Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams’ perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users’ routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions. © 2022 Elsevier B.V.},
	author = {Ferreira, Bruna and Marques, Silvio and Kalinowski, Marcos and Lopes, Hélio and Barbosa, Simone D.J.},
	year = {2023},
	keywords = {End-users, Automation, Users' experiences, Agile, Agile Methodologies, Agile teams, Data Science, Decision power, Lean inception, Process automation, Process control, User centered design, User involvement, User participation},
}

@article{kitchenham_segress_2023,
	title = {{SEGRESS}: {Software} {Engineering} {Guidelines} for {REporting} {Secondary} {Studies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132534569&doi=10.1109%2fTSE.2022.3174092&partnerID=40&md5=a3ded056d25379cd7fc5bad8fa40273c},
	abstract = {Context: Several tertiary studies have criticized the reporting of software engineering secondary studies. Objective: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). Method: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. Results: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, ad hoc recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. Conclusions: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs. © 2022 IEEE.},
	author = {Kitchenham, Barbara and Madeyski, Lech and Budgen, David},
	year = {2023},
	keywords = {Software engineering, Systematic Review, Mapping, Risk assessment, Evidence Based Software Engineering, Mapping studies, Systematic, Quality assessment, Guideline, Software, Mixed method, Mixed-method review, PRISMA 2020, Quality reviews, Reporting guideline, Risk of bias, Threat to validity},
}

@article{dakkak_continuous_2023,
	title = {Continuous deployment in software-intensive system-of-systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150259304&doi=10.1016%2fj.infsof.2023.107200&partnerID=40&md5=fcf9069aa453b5d80bc44a98117f42ad},
	abstract = {Context: While continuous deployment is popular among web-based software development organizations, adopting continuous deployment in software-intensive system-of-systems is more challenging. On top of the challenges arising from deploying software to a single software-intensive embedded system, software-intensive system-of-systems (SiSoS) add a layer of complexity as new software undergoes an extensive field validation applied to individual components of the SiSoS, as well as the overall SiSoS, to ensure that both legacy and new functionalities are working as desired. Objectives: This paper aims to study how SiSoS transitions to continuous deployment by exploring how continuous deployment impacts field testing and validation activities, how continuous deployment can be practiced in SiSoS, and to identify the success factors that companies need to consider when transitioning to continuous deployment. Method: We conducted a case study at Ericsson AB focusing on the embedded software of the Third Generation Radio Access Network (3G RAN). The 3G RAN consists of two large-scale software-intensive embedded systems, representing a simple SiSoS composed of two systems. 3G RAN software was the first to transition to continuous deployment and is used as a reference case for other products within Ericsson AB. Results: Software deployment, in addition to field testing and validation, have transitioned from being a discrete activity performed at the end of software development to a continuous process performed in parallel to software development. Further, our study reveals an orchestrating approach for software deployment, which allows pre/post validation of legacy behavior and new features in a shorter release and deployment cadence. Furthermore, we identified the essential success factors that organizations should consider when transitioning to continuous deployment. Conclusion: Transition to continuous deployment, in addition to field testing and validation, shall be considered and planned carefully. In this paper, we provide a set of success factors and orchestration technique that helps organization when transitioning to continuous deployment in the software-intensive embedded system-of-systems context. © 2023 Elsevier B.V.},
	author = {Dakkak, Anas and Bosch, Jan and Olsson, Helena Holmström and Issa Mattos, David},
	year = {2023},
	keywords = {Embedded systems, Software design, Software testing, Success factors, Agile software development, Legacy systems, Continuous software engineerings, Software intensive systems, System of systems, System-of-systems, Continuous deployment, Ericsson, Field testing, Field validation, Software-intensive system-of-system},
}

@article{toledo_algorithmic_2023,
	title = {Algorithmic {Thinking} and {Extension} of {Its} {Definition} for {Trainee} {Software} {Developers}: {A} {Systematic} {Literature} {Mapping}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666630&doi=10.1109%2fRITA.2023.3323784&partnerID=40&md5=193d79d0896f7dc849cea2f0ec49c32d},
	abstract = {This paper exhibits a systematic literature mapping of the considerations required to develop algorithmic thinking in a first course in computer programming (CS1) in university academic programs in computing. In the methodological process of this study, 5 stages were proposed: research questions, search, selection, quality assessment and synthesis extraction. In this way, 5 guiding questions were drawn, 136 articles generated by the search stage were analyzed and the synthesis of 55 documents that met the criteria of this research was concluded, thus compiling the different practices used for the development of algorithmic thinking. In addition, as a result of the systematic literature mapping, a definition of Algorithmic Thinking oriented Software Engineering and didactics is proposed. © 2023 IEEE.},
	author = {Toledo, Javier Alejandro Jiménez and Collazos, César A. and Ortega, Manuel and Ramos, Deixy Ximena},
	year = {2023},
	keywords = {Software engineering, Mapping, Research questions, Quality assessment, Computation theory, Computing, Academic program, Algorithmic thinking, Computational logic, Computational thinkings, Logic programming, Software developer},
}

@article{borstler_double-counting_2023,
	title = {Double-counting in software engineering tertiary studies — {An} overlooked threat to validity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150795598&doi=10.1016%2fj.infsof.2023.107174&partnerID=40&md5=62bd250321bb30d74a7637c85f9bf590},
	abstract = {Context: Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in software engineering. Objective: We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue. Method: We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them. Results: In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies. Conclusions: Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies. © 2023 The Author(s)},
	author = {Börstler, Jürgen and bin Ali, Nauman and Petersen, Kai},
	year = {2023},
	keywords = {Software engineering, Tertiary study, Bias, Research method, Tertiary review, Guideline, Population statistics, Double counting, Empirical, Meta-review, Overview of review, Recommendation, Review of review, Umbrella review},
}

@inproceedings{rokani_fault_2023,
	title = {Fault diagnosis of induction motors using artificial intelligence techniques: {A} systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159667695&doi=10.1063%2f5.0129239&partnerID=40&md5=f5dc44315df865856408a2a1d79eebbd},
	abstract = {This manuscript proposes a systematic review of the latest publications, between 2010-2021, of research articles that investigate Induction Motor Fault Diagnosis (IMFD), using Artificial Intelligence techniques. Artificial Intelligence (AI) is an innovative branch of science and engineering. AI techniques constitute the most cutting-edge method in IMFD. Induction motors are regarded as more extensively used than other electric machines. Therefore, preserving their health is critical. It is vital to prevent incipient faults by monitoring the condition of the (IMs) and using diagnostic techniques. An incipient failure in an IM should be detected as early as possible to interrupt the evolution of the fault and reduce the financial losses and the repair period. This paper aspires to: a) Condense the existing surveys concerning the fault diagnosis in induction motors using AI techniques by searching the benefits and limitations of those surveys, b) Determine the gaps in existing research to recommend ideas for further investigation, c) Implement a background in this realm of AI for novel research projects. Moreover, it is followed a particular review protocol that defines the research questions and the methods applied to conduct the systematic review. © 2023 Author(s).},
	author = {Rokani, V. and Karaisas, P. and Kaminaris, S.D.},
	year = {2023},
}

@inproceedings{ockiya_review_2023,
	title = {A {Review} of {Human} {Factors} in {Remote} {Software} {Project} {Management}: {A} {Progressive} {Look} at {Human} {Based} {Issues} in {Remote} {Software} {Development} {Environments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184518421&doi=10.1145%2f3634848.3634858&partnerID=40&md5=ed5ffbcf2eb102cece0c656199f7585b},
	abstract = {There is a documented high rate of project failure within the software engineering community. Many researchers have discussed what the issues are and how to solve them using novel processes, technology, and tools, but the statistics remain mostly unchanged. We explored these issues from another perspective focusing on human based factors and how it affects remote software teams. Using a systematic literature review approach with selected criteria we explored the issues under several clusters. We found there exists a relationship between human based factors in remote software teams and success/failure, which, if better understood and managed could be used to reduce the challenges experienced by remote teams. We also identified a limitation in the availability of empirically tested data and suggest further research in understanding human based factors in remote teams as a precursor to reducing the rate of failure. © 2023 Owner/Author.},
	author = {Ockiya, Tamunoemi Fancey and Lock, Russell},
	year = {2023},
	keywords = {Software design, Project management, Human engineering, Failure analysis, Agile Methodologies, Engineering community, High rate, Novel process, Process Technologies, Project failures, Remote software development, Software project management, Software teams, Software-development environments},
}

@article{liu_citizen_2023,
	title = {Citizen involvement in digital transformation: a systematic review and a framework},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165768993&doi=10.1108%2fOIR-04-2022-0237&partnerID=40&md5=11a1d93c18925a8a4b0e076a792744ef},
	abstract = {Purpose: The purpose of this paper is to improve the understanding of the factors influencing the success of digital transformation (DT) and problems/challenges in DT as well as the communication methods used to involve citizens, based on a systematic literature review of research articles about citizen involvement in DT published between January 2010 and May 2021. Design/methodology/approach: After establishing inclusion and exclusion criteria, a systematic review of relevant studies was conducted. Out of a total of 547 articles, 33 met the paper selection criteria. Findings: The analysis of the included 33 empirical studies reveals that the factors influencing the success of DT can be described as the opposite side from challenges and problems in DT. These factors and challenges/problems all influence DT and they can be grouped into organisational values, management capabilities, organisational infrastructure, and workforce capabilities. The communication methods for citizen involvement in DT include: (1) communication mediated by human, (2) communication mediated by computers, and (3) mixed communication methods. Originality/value: The study identified specific factors that influence DT supported by citizen involvement, at a more fine-grained level. The findings concerning communication methods extend related studies for citizen involvement by adding town hall meetings and communication methods mediated by computers. Furthermore, this study links the research findings to develop a framework for citizen involvement in DT, assisting in better selecting communication methods to involve citizens for addressing problem areas in DT. Peer review: The peer review history for this article is available at: https://publons.com/publon/10.1108/OIR-04-2022-0237 © 2022, Emerald Publishing Limited.},
	author = {Liu, Caihua and Zowghi, Didar},
	year = {2023},
	keywords = {Digital transformation, Systematic literature review, Systematic Review, Citizen involvement, Communication method, Design/methodology/approach, Inclusion and exclusions, Paper selections, Peer review, Selection criteria},
}

@article{szabo_user-centered_2023,
	title = {User-centered approaches in software development processes: {Qualitative} research into the practice of {Hungarian} companies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137081694&doi=10.1002%2fsmr.2501&partnerID=40&md5=e408c37e01d89f1dc78aa027550e4269},
	abstract = {Integrating user-centered approaches into development processes is one of the main challenges nowadays that derives from different objectives of software engineering (SE) and human-computer interaction (HCI) fields. For SE experts, the main goal is quality code creation, whereas for HCI professionals, it is the continuous product interaction with the users. The major question is what tools and timings can be used together to achieve these goals effectively. Therefore, this article provides comparative, exploratory, and qualitative research about possible solutions on how practitioners transfer HCI values and practices to SE processes. The current practice of software companies was studied by conducting interviews on a sample of 13 Hungarian Information Technology companies to explore the SE processes in respect of several dimensions (applied development models, the integrity of user-centered methods, and the user experience [UX] maturity). According to preliminary expectations, the development processes of the various companies proceed in different steps; nevertheless, they can be well grouped together based on the UX methods applied. The results representing the various user-centered processes can be considered useful for future decision makers of software companies worldwide. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Szabó, Bálint and Hercegfi, Károly},
	year = {2023},
	keywords = {Development process, Decision making, Software design, Computer software, Human computer interaction, Users' experiences, Software process, Surveys, Interview, Hungarians, Qualitative research, User experience (UX), User-centered approach, UX maturity, UX method},
}

@article{binamungu_behaviour_2023,
	title = {Behaviour driven development: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160605559&doi=10.1016%2fj.jss.2023.111749&partnerID=40&md5=0d6c623a12e99c3edeb2a8a0624478da},
	abstract = {Context: Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps. Objective: To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021. Method: By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research. Results: The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects. Conclusion: The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research. © 2023 Elsevier Inc.},
	author = {Binamungu, Leonard Peter and Maro, Salome},
	year = {2023},
	keywords = {Specifications, Software design, Mapping, Systematic mapping studies, Semi-structured, Software requirements, Natural languages, 'current, Scientific literature, Petroleum reservoir evaluation, Behavior driven development, Boolean functions, Natural language specifications, Philosophical aspects, Systematic mapping study in software engineering},
}

@article{alanazi_software_2023,
	title = {Software {Engineering} {Techniques} for {Building} {Sustainable} {Cities} with {Electric} {Vehicles}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167892803&doi=10.3390%2fapp13158741&partnerID=40&md5=2974ae54e3ef1b855a7e8fd9235640e7},
	abstract = {As the process of urbanization continues to accelerate, the demand for sustainable cities has become more critical than ever before. The incorporation of electric vehicles (EVs) is a key component in creating sustainable cities. However, the development of smart cities for EVs entails more than just the installation of charging stations. Software engineering plays a crucial role in realizing smart cities for electric vehicles. This paper examines the role of software engineering in the creation of smart cities for electric vehicles, the techniques utilized in electric vehicle charging infrastructure, the obstacles faced by software engineers, and the future of software engineering in sustainable cities. Specifically, the paper explores the significance of software engineering in integrating EVs into the transportation system, including the design of smart charging and energy management systems, and the establishment of intelligent transportation systems. Additionally, the paper offers case studies to demonstrate successful software engineering implementations for smart cities. Finally, the paper concludes with a discussion of the challenges that software engineers encounter in implementing intelligent transportation systems for EVs and provides future directions for software engineering in sustainable cities. © 2023 by the authors.},
	author = {Alanazi, Fayez and Alenezi, Mamdouh},
	year = {2023},
}

@article{vianna_grey_2023,
	title = {A {Grey} {Literature} {Review} on {Data} {Stream} {Processing} applications testing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160024077&doi=10.1016%2fj.jss.2023.111744&partnerID=40&md5=279db1183abd50513795efc85821d5f3},
	abstract = {Context: The Data Stream Processing (DSP) approach focuses on real-time data processing by applying specific techniques for capturing and processing relevant data for on-the-fly results, i.e. without necessarily requiring prior storage. Like in any other software, testing plays a vital role in the quality assurance of DSP applications. However, testing such kind of software is not a simple task. In this context, some factors that make challenging testing are message temporality, parallelism, data volume, complex infrastructure, variability, and speed of messages. Objective: This work aims to map and synthesize industry knowledge and experience regarding DSP application testing. Specifically, we want to know about challenges, test purposes, test approaches, test data sources, and adopted tools. Method: To achieve the objective, we performed a Grey Literature Review (e.g., blog posts, white papers, discussion lists, lecture themes at technical events, professional social networks, software repositories, and other web-published) on testing DSP applications. We searched the grey literature using Google's regular search engine in addition to specific searches on technical software development content websites. The selected studies were analyzed using qualitative and quantitative techniques. Results: Results are based on evidence from 154 selected sources. The challenges for testing DSP applications are the complexity of DSP applications, test infrastructure complexity, timing, and data acquisition issues. The main test objectives identified are functional suitability, performance efficiency, reliability, and maintainability. The main test approaches reported: Performance Testing, Regression Testing, Property-Based Testing, Chaos Testing, and Contract/Schema Testing. The strategies adopted by practitioners to obtain test data: Historical Data, Production Data Mirroring, Semi-Synthetic Data, and Synthetic Data. We also report 50 tools used in various testing activities, which are used for: automating infrastructure, generating test data, test utilities, dealing with timing issues, load generation, simulation, and others. Furthermore, we identified gaps and opportunities for future scientific work. Conclusion: This work selected and summarized content produced by practitioners regarding DSP application testing. We identified that knowledge, techniques, and tools intrinsic to the practice were not present in the formal literature, so this study helps reduce the gap between industry and academia on this topic. The document has delivered benefits to industry practitioners and academic researchers. © 2023 Elsevier Inc.},
	author = {Vianna, Alexandre and Kamei, Fernando Kenji and Gama, Kiev and Zimmerle, Carlos and Neto, João Alexandre},
	year = {2023},
	keywords = {Software design, Software testing, Application programs, Social networking (online), Literature reviews, Search engines, Data handling, Software testings, Digital storage, Social sciences computing, Application testing, Complex networks, Data acquisition, Data stream, Data streams processing, Grey literature, Processing applications, Synthetic data, Test data, Testing data},
}

@article{ahmad_requirements_2023-1,
	title = {Requirements engineering for artificial intelligence systems: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149281892&doi=10.1016%2fj.infsof.2023.107176&partnerID=40&md5=cf81845e526ed8ccc8d8ddea0bf657f1},
	abstract = {Context: In traditional software systems, Requirements Engineering (RE) activities are well-established and researched. However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE. Existing literature has focused on using AI to manage RE activities, with limited research on RE for AI (RE4AI). Objective: This paper investigates current approaches for specifying requirements for AI systems, identifies available frameworks, methodologies, tools, and techniques used to model requirements, and finds existing challenges and limitations. Method: We performed a systematic mapping study to find papers on current RE4AI approaches. We identified 43 primary studies and analyzed the existing methodologies, models, tools, and techniques used to specify and model requirements in real-world scenarios. Results: We found several challenges and limitations of existing RE4AI practices. The findings highlighted that current RE applications were not adequately adaptable for building AI systems and emphasized the need to provide new techniques and tools to support RE4AI. Conclusion: Our results showed that most of the empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data requirements, and areas such as ethics, trust, and explainability need further research. © 2023 Elsevier B.V.},
	author = {Ahmad, Khlood and Abdelrazek, Mohamed and Arora, Chetan and Bano, Muneera and Grundy, John},
	year = {2023},
	keywords = {Software engineering, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Software-systems, Engineering activities, Machine learning, Machine-learning, 'current, Engineering education, Artificial intelligence systems, Model requirements, System requirements, Tools and techniques},
}

@inproceedings{fischer_becoming_2023,
	title = {Becoming a {Data}-{Driven} {Organization}: {A} {Comparative} {Case} {Study} on {Digital} {Transformation} {Strategies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192539148&partnerID=40&md5=1db34add5835badee7f963d868850262},
	abstract = {In today’s data-centric era, organizations increasingly aim to operate more data-driven and therefore engage in digital transformations toward becoming a data-driven organization (DDO). To govern such transformations, top managers develop digital transformation strategies (DTS) characterized by different organizational ambidexterity approaches. This study analyzes how such DTS influence the process and (intermediate) outcomes of organizations’ digital transformations toward becoming a DDO by studying two organizations undertaking such DDO transformations using the concept of organizational ambidexterity as a theoretical lens. On this empirical basis, we find that DTS characterized by different organizational ambidexterity approaches lead to different transformation processes and (intermediate) outcomes. Thereby, this study contributes to existing academic literature in the field of DDOs and DTS, as such transformation journeys toward becoming a DDO have not been studied in its entirety yet. Furthermore, our paper offers practical guidance for top managers to develop and implement a DTS suitable for their organization. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	author = {Fischer, Hannes},
	year = {2023},
	keywords = {Digital transformation, Metadata, Information systems, Information use, Case-studies, Data driven, Organisational, Data centric, Data-driven organization, Digital transformation strategy, Organizational ambidexterity, Top managers},
}

@article{khan_software_2023,
	title = {Software architecture for quantum computing systems — {A} systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151881159&doi=10.1016%2fj.jss.2023.111682&partnerID=40&md5=949a3516e7bfda8a68429eeb94166d2d},
	abstract = {Quantum computing systems rely on the principles of quantum mechanics to perform a multitude of computationally challenging tasks more efficiently than their classical counterparts. The architecture of software-intensive systems can empower architects who can leverage architecture-centric processes, practices, description languages to model, develop, and evolve quantum computing software (quantum software for short) at higher abstraction levels. We conducted a Systematic Literature Review (SLR) to investigate (i) architectural process, (ii) modelling notations, (iii) architecture design patterns, (iv) tool support, and (iv) challenging factors for quantum software architecture. Results of the SLR indicate that quantum software represents a new genre of software-intensive systems; however, existing processes and notations can be tailored to derive the architecting activities and develop modelling languages for quantum software. Quantum bits (Qubits) mapped to Quantum gates (Qugates) can be represented as architectural components and connectors that implement quantum software. Tool-chains can incorporate reusable knowledge and human roles (e.g., quantum domain engineers, quantum code developers) to automate and customise the architectural process. Results of this SLR can facilitate researchers and practitioners to develop new hypotheses to be tested, derive reference architectures, and leverage architecture-centric principles and practices to engineer emerging and next generations of quantum software. © 2023 The Authors},
	author = {Khan, Arif Ali and Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Mikkonen, Tommi and Abrahamsson, Pekka},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, Software testing, Software architecture, Modeling languages, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Software intensive systems, Architectural process, Architecture-centric, Classical counterpart, Qubits},
}

@inproceedings{mayr_unified_2023,
	title = {Unified {Theory} of {Acceptance} and {Use} of {Technology} ({UTAUT}) for {Intelligent} {Process} {Automation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192543634&partnerID=40&md5=c1660d81fb09bfe882968ce8d157b22d},
	abstract = {Intelligent process automation is a technological innovation that combines symbolic automation tools with machine learning. Intelligent process automation can automate complex tasks that otherwise have to be performed by humans when symbolic automation is not powerful enough. Regardless of the high economic potential for companies, the adoption rate in practice is comparatively low. This could be due to the adoption behavior of the employees. In our work, we iteratively develop a Unified Theory of Acceptance and use of Technology (UTAUT) model for the adoption of intelligent process automation and evaluate it with an empirical study. With our research we want to empower designers to adapt the corresponding tools in the future to increase adoption. The study shows that, in addition to established factors for technology adoption, trust, transparency, and attitude towards technology are primary decision factors. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	author = {Mayr, Alexander and Stahmann, Philip and Nebel, Maximilian and Janiesch, Christian},
	year = {2023},
	keywords = {Information systems, Information use, Technological innovation, Automation, Machine-learning, Technology adoption, Empirical studies, Process control, Adoption behavior, Automation tools, Complex task, Economic potentials, Intelligent process automation, The unified theory of acceptance and use of technology(UTAUT)},
}

@inproceedings{garcia_advances_2023,
	title = {Advances in {Web} {API} testing: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192841057&doi=10.1109%2fENC60556.2023.10508648&partnerID=40&md5=2ce84324710ed395808ffc09f45b2a83},
	abstract = {Web APIs serving as an intermediary for communication between distributed systems has increased recently. It has become critical to test these APIs to ensure their functionality and quality thoroughly. This study aims to systematically map the literature to analyze the techniques, methods, artifacts, and strategies employed during the testing phase of web APIs. Utilizing a systematic mapping study approach (SMS), we identified 42 studies that outlined various tests applicable to these APIs. Further, our analysis uncovered numerous methods, techniques, and strategies. Types of test artifacts such as the API specification, test cases, or test matrices were also found. Finally, testing activities were identified through approaches presented by each study's authors, in which test results were applied and analyzed. The findings will establish the basis for the development of a testing guide, which in turn will support professionals who need to test APIs and who lack specific knowledge on how to carry out this activity due to the lack of standards. © 2023 IEEE.},
	author = {Garcia, Josue Capistran and Hernández, Jorge Octavio Ocharán and Arriaga, Juan Carlos Peréz and Riaño, Hector Javier Limón},
	year = {2023},
	keywords = {Systematic Review, Software testing, Mapping, Systematic mapping studies, Software testings, Testing process, Test case, Web API, Quality assurance, API specifications, Distributed systems, Specification test, Testing phase},
}

@inproceedings{horne_ten_2023,
	title = {Ten regulatory principles to scaffold the design, manufacture, and use of trustworthy autonomous systems, illustrated in a maritime context},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167979891&doi=10.1145%2f3597512.3599701&partnerID=40&md5=696a9887e71dfc7d264d1bc2b56d5fc8},
	abstract = {Autonomous systems are increasingly prevalent around the world, with the benefits related to safety, efficiency, and sustainability attractive in addition to the opportunity to establish entirely new capabilities. In order to operationalise autonomous systems technology it is critical to have in place the legal, regulatory and ethical infrastructure necessary to enable safe and trusted operation. While there is a growing body of literature regarding ethical Artificial Intelligence (AI), there is a need for more academic exploration of legal and regulatory best practice for autonomous systems used in commercial and defence contexts. This paper addresses that literature gap by considering the role of regulation and its relationship with trust in a multi-disciplinary context, before proposing 10 principles to base regulatory development and implementation on. These principles, Trust-centred; Collaborative; Risk-based; Evidence-led; Facilitate experimentation; Systems-focussed; Usable; Consistent; Adaptable and Reviewable, collectively provide a domain and technology agnostic basis for a regulatory framework development and implementation approach that supports the design, manufacture and operation of safe and trusted autonomous systems. The paper concludes by recommending next steps towards the regulation of safe and trusted autonomous systems, including a focus on collaboration and experimentation. © 2023 ACM.},
	author = {Horne, Rachel and Law-Walsh, Caroline and Assaad, Zena and Joiner, Keith},
	year = {2023},
	keywords = {Best practices, Laws and legislation, Autonomous system, Ethical technology, Law 3.0, Regulation, Regulation of autonomy, Regulatory frameworks, Regulatory principles, Risk-based, Scaffolds, Trust and regulation, Trustworthy autonomous system},
}

@article{pereira_junior_systematic_2023,
	title = {Systematic {Literature} {Review} on {Virtual} {Electronics} {Laboratories} in {Education}: {Identifying} the {Need} for an {Aeronautical} {Radar} {Simulator}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163803143&doi=10.3390%2felectronics12122573&partnerID=40&md5=a2d0097e4a068fae9af69afceff35003},
	abstract = {The objective of this work is to propose the development of a virtual electronics laboratory with an aeronautical radar simulator using immersive technologies to help students learn. To verify whether this proposal was viable, the systematic literature review (SLR) methodology was used, whose objective was to verify whether immersive technologies were being used effectively in education and, also, what challenges, opportunities, and benefits they bring to Education 4.0. For this, eight Research Questions (RQs) were formulated to be answered by articles based on the highest SLR scores. The results presented by SLR were as follows: there was an increase in the use of immersive technologies in education, but virtual reality (VR) is still more used in education than AR, despite VR being more expensive than AR; the use of these new technologies brings new challenges, opportunities, and benefits for education; there was an increase in the quality of teaching for complex subjects; and there was an increase in students’ interest in the content presented. © 2023 by the authors.},
	author = {Pereira Júnior, Enderson Luiz and Moreira, Miguel Ângelo Lellis and Portella, Anderson Gonçalves and de Azevedo Junior, Célio Manso and de Araújo Costa, Igor Pinheiro and Fávero, Luiz Paulo and Gomes, Carlos Francisco Simões and dos Santos, Marcos},
	year = {2023},
}

@inproceedings{nikiforova_identification_2023,
	title = {Identification of {High}-{Value} {Dataset} determinants: is there a silver bullet for efficient sustainability-oriented data-driven development?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167867852&doi=10.1145%2f3598469.3598556&partnerID=40&md5=617d9c410c006582371208a8f10e79d1},
	abstract = {Open Government Data (OGD) are seen as one of the trends that has the potential to benefit the economy, improve the quality, efficiency, and transparency of public administration, and change the lives of citizens, and the society as a whole facilitating efficient sustainability-oriented data-driven services. However, the quick achievement of these benefits is closely related to the "value"of the OGD, i.e., how useful, and reusable the data provided by public agencies are for creating value for the above stakeholder. This is where the notion of "high-value datasets"(HVD), defined by the European Commission in Open Data Directive, comes, referring to data that can create the most value for society, the economy, and the environment. This is even more so, considering the proliferation of Artificial Intelligence (AI) and machine learning (ML) applications in various domains. While there are some efforts in that direction, there is still no available framework for identifying country-specific high-value datasets (and their determinants). The objective of the workshop is to raise awareness and build a network of key stakeholders around the HVD issue, to allow each participant to think about how and whether the determination of HVD is taking place in their country, how this can be improved with the help of portal owners, data publishers, data owners, businesses and citizens, what are and can be determinants to be used for identifying HVDs, whether they are SMART. Our main motivation is that, as members of the dg.o community, we can collaboratively answer the above questions, and those raised during the previous two editions of this workshop at ICEGOV2022 and ICOD2022, forming an initial knowledge base, as well as assessing currently used indicators. In this 3rd edition of the workshop, previously obtained results, which make up a list of the most promising indicators, will be discussed, validated and possibly refined through live discussions with the workshop participants following the DELPHI method. © 2023 ACM.},
	author = {Nikiforova, Anastasija and Alexopoulos, Charalampos and Rizun, Nina and Ciesielska, Magdalena},
	year = {2023},
	keywords = {Decision making, Economic and social effects, Data driven, High-value dataset, Open Data, Open datum, Open government data, Sustainable development, Knowledge based systems, Artificial intelligence learning, Determinant, European Commission, Machine learning applications, Public administration, Public agencies, Silver},
}

@article{kitchenham_how_2023,
	title = {How {Should} {Software} {Engineering} {Secondary} {Studies} {Include} {Grey} {Material}?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128295733&doi=10.1109%2fTSE.2022.3165938&partnerID=40&md5=c83dc9fa989c40ab6aa2d2b6f0c71aa3},
	abstract = {Context: Recent papers have proposed the use of grey literature (GL) and multivocal reviews. These papers have raised issues about the practices used for systematic reviews (SRs) in software engineering (SE) and suggested that there should be changes to the current SR guidelines. Objective: To investigate whether current SR guidelines need to be changed to support GL and multivocal reviews. Method: We discuss the definitions of GL and the importance of GL and of industry-based field studies in SE SRs. We identify properties of SRs that constrain the material used in SRs: a) the nature of primary studies; b) the requirements of SRs to be auditable, traceable, and reproducible; and explain why these requirements restrict the use of blogs in SRs. Results: SR guidelines have always considered GL as a possible source of primary studies and have never supported exclusion of field studies that incorporate the practitioners' viewpoint. However, the concept of GL, which was meant to refer to documents that were not formally published, is now being extended to information from sources such as blogs/tweets/Q\&A posts. Thus, it might seem that SRs do not make full use of GL because they do not include such information. However, the unit of analysis for an SR is the primary study. Thus, it is not the source but the type of information that is important. Any report describing a rigorous empirical evaluation is a candidate primary study. Whether it is actually included in an SR depends on the SR eligibility criteria. However, any study that cannot be guaranteed to be publicly available in the long term should not be used as a primary study in an SR. This does not prevent such information from being aggregated in surveys of social media and used in the context of evidence-based software engineering (EBSE). Conclusions: Current guidelines for SRs do not require extensions, but their scope needs to be better defined. SE researchers require guidelines for analysing social media posts (e.g., blogs, tweets, vlogs), but these should be based on qualitative primary (not secondary) study guidelines. SE researchers can use mixed-methods SRs and/or the fourth step of EBSE to incorporate findings from social media surveys with those from SRs and to develop industry-relevant recommendations. © 1976-2012 IEEE.},
	author = {Kitchenham, Barbara and Madeyski, Lech and Budgen, David},
	year = {2023},
	keywords = {Software engineering, Systematic Review, Systematic mapping studies, Social networking (online), Evidence Based Software Engineering, Social sciences computing, Systematic, Guideline, Surveys, Mixed method, Mixed-method review, Grey literature, Blogs, Government, Multivocal review},
}

@article{da_silva_relationship_2023,
	title = {Relationship between ecosystem innovation and performance measurement models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134164589&doi=10.1108%2fIJPPM-06-2021-0349&partnerID=40&md5=ad93977f490e4d1e1c8aba9e3c9501c5},
	abstract = {Purpose: This study examines the relationship between the innovation ecosystem and performance measurement models. Although the innovation ecosystem and measurement models are widely recognized, the existing literature lacks a comprehensive understanding of the relationship between the proposed themes. Furthermore, it does not reveal how studies can be grouped to propose a thematic typology of the relationship. Design/methodology/approach: The authors present a systematic literature review conducted in the Web of Science and Scopus databases, from a textual corpus that aided the proposition of the typology that aims to provide answers regarding the addressed themes. Findings: The results of this review are based on a total of sixty peer-reviewed articles from the innovation ecosystem literature and performance measurement models between 1995 and 2020. The results make several contributions to the literature. First, by integrating evidence from empirical studies, the authors identified a typology formed by three classes: (1) ecosystem agents (2) analytical focus and (3) structured measurement tools. Second, the authors verified the relationship between the themes and discovered the existence of gaps to be filled, with the proposition of three drivers. Third, the authors presented a comprehensive mapping of field studies with a descriptive analysis of the textual corpus. Originality/value: The results of the research provide important implications for researchers, managers and policy makers. Furthermore, the authors suggest directions for future research, including the need to examine the performance of the entire innovation ecosystem, integrating the different agents that exist for performance measurement. © 2022, Emerald Publishing Limited.},
	author = {da Silva, Deoclécio Junior Cardoso and Lopes, Luis Felipe Dias and Santos Costa Vieira da Silva, Luciana and da Silva, Wesley Vieira and Teixeira, Clarissa Stefani and Veiga, Claudimar},
	year = {2023},
}

@article{sworna_nlp_2023,
	title = {{NLP} methods in host-based intrusion detection systems: {A} systematic review and future directions},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174894190&doi=10.1016%2fj.jnca.2023.103761&partnerID=40&md5=006065bd017f3587ee6b0c34803dfda5},
	abstract = {Host-based Intrusion Detection System (HIDS) is an effective last line of defense for defending against cyber security attacks after perimeter defenses (e.g., Network-based Intrusion Detection System and Firewall) have failed or been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among the top two most used security tools by Security Operation Centers (SOC) of organizations. Although effective and efficient HIDS is highly desirable for industrial organizations, the evolution of increasingly complex attack patterns causes several challenges resulting in performance degradation of HIDS (e.g., high false alert rate creating alert fatigue for SOC staff). Since Natural Language Processing (NLP) methods are better suited for identifying complex attack patterns, an increasing number of HIDS are leveraging the advances in NLP that have shown effective and efficient performance in precisely detecting low footprint, zero-day attacks and predicting an attacker's next steps. This active research trend of using NLP in HIDS demands a synthesized and comprehensive body of knowledge of NLP-based HIDS. Despite the drastically growing adoption of NLP in HIDS development, there has been relatively little effort allocated to systematically analyze and synthesize the available peer review literature to understand how NLP is used in HIDS development. The lack of a synthesized and comprehensive body of knowledge on such an important topic motivated us to conduct a Systematic Literature Review (SLR) of the papers on the end-to-end pipeline of the use of NLP in HIDS development. For the end-to-end NLP-based HIDS development pipeline, we identify, taxonomically categorize and systematically compare the state-of-the-art of NLP methods usage in HIDS, attacks detected by these NLP methods, datasets and evaluation metrics which are used to evaluate the NLP-based HIDS. We highlight the relevant prevalent practices, considerations, advantages and limitations to support the HIDS developers. We also outline the future research directions for the NLP-based HIDS development. © 2023 The Authors},
	author = {Sworna, Zarrin Tasnim and Mousavi, Zahra and Babar, Muhammad Ali},
	year = {2023},
	keywords = {Natural language processing systems, Natural languages, Language processing, Natural language processing, System development, Complex networks, Anomaly detection, Computer crime, Cyber security, Cybersecurity, Host-based intrusion detection, Host-based intrusion detection system, Intrusion detection, Pipeline processing systems, Pipelines, Processing method, Security operation center, Zero-day attack},
}

@inproceedings{fortuna_surveying_2023,
	title = {Surveying the {Relevance} of the {Critical} {Success} {Factors} of {Agile} {Transformation} {Initiatives} from a {Project} {Management} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180157252&doi=10.1145%2f3629479.3629515&partnerID=40&md5=ed9d9b9181f0ac254ac6c07e2875a10a},
	abstract = {Background: Agile methods and practices have been consistently adopted in recent years as alternatives to traditional software development processes to address the ever-changing needs of IT organizations. In a previous systematic mapping study, we identified twelve critical success factors of agile transformations from a project management perspective. Objective: In this paper, we investigate how practitioners perceive the relevance of these factors and whether other factors should be considered. Method: We conducted a survey research involving project managers from several organizations undergoing agile transformations. Results: The participants' perceptions provided valuable insights into the relevance of the critical success factors. Additionally, we identified five new critical success factors: organizational ambidexterity, use of tools and automation, breaking down organizational silos, team commitment, and alignment of organizational goals and expectations. These newly identified factors contribute to a more comprehensive understanding of organizations' challenges during an agile transformation. Based on the results and the literature, we formulated three propositions representing recommendations that can foster agile transformation. Conclusions: The evidence gathered in this study indicates that the factors investigated previously are highly relevant. Moreover, organizations should consider them to enhance the chances of success of agile transformation initiatives. © 2023 ACM.},
	author = {Fortuna, Alessandra and Mattos, Claudio Saraiva and Andrade, Álan Júnior Da Cruz and Ramos, Luiz Felipe and Dutra, Eliezer and Santos, Rodrigo Pereira Dos and Santos, Gleison},
	year = {2023},
	keywords = {Software design, Success factors, Agile software development, Organisational, Project management, Agile methods, Agile practices, Agile transformations, Changing needs, Critical success factor, IT organizations, Software development process},
}

@article{erthal_characterization_2023-1,
	title = {Characterization of continuous experimentation in software engineering: {Expressions}, models, and strategies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759787&doi=10.1016%2fj.scico.2023.102961&partnerID=40&md5=fe2272f51ad17194de939829974123e2},
	abstract = {Context: Continuous Experimentation (CE) has become increasingly popular across industry and academic communities. Major software organizations use CE to increase their revenue by adding value to end-users, and researchers are investigating the CE adoption process and usage to expand its success. Given this rapid evolution, observing a shared understanding of CE definitions, processes, and experiment strategies is difficult, potentially jeopardizing new implementations and focused research efforts. Objective: To characterize CE from the perspective of its definitions, processes, and strategies for experimentation available in the technical literature and to evolve the understanding perspectives for “continuous experimentation” and “data-driven development” definitions. Method: To select and analyze sources of information in the technical literature dealing with different aspects of continuous experimentation through a Literature Study using an ad hoc search improved with snowballing (backward and forward). Organize the findings into new perspectives for CE definitions, processes, and experiment strategies. Results: It was possible to identify many different definitions, processes, and experimental strategies used to describe CE in the 72 analyzed empirical papers, making it difficult to decide on their combination to be applied in a real software development project. Therefore, it has been proposed to evolve the CE understanding perspective, to categorize its experiment strategies, and to offer a combined development process for CE combining parts of other processes. Besides, conjectural requirements have been identified, which can contribute to better differentiating requirements and hypotheses in the CE context. Conclusion: Likely, a better understanding of CE is still missing. It can contribute towards organizing a common taxonomy to facilitate the possible choices for the experiment strategies. Therefore, there is space for more investigations on its applicability and value in different categories of software systems, despite all the advancements of CE and its promotion in developing modern software systems. © 2023 Elsevier B.V.},
	author = {Erthal, Vladimir M. and de Souza, Bruno P. and dos Santos, Paulo Sérgio M. and Travassos, Guilherme H.},
	year = {2023},
	keywords = {Software design, Software testing, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Data driven, Data-driven development, A/b testing, Controlled experiment, Engineering expression, Expression modeling, Technical literature},
}

@article{santos_distributed_2023,
	title = {Distributed {Scrum}: {A} {Case} {Meta}-analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180152360&doi=10.1145%2f3626519&partnerID=40&md5=8d7c6e0cd850d803596d4150c118f884},
	abstract = {Distributed Scrum adapts the Scrum project management framework for geographically distributed software teams. Experimentally evaluating the effectiveness of Distributed Scrum is impractical, but many case studies and experience reports describe teams and projects that used Distributed Scrum. This article synthesizes the results of these cases using case meta-analysis, a technique for quantitatively analyzing qualitative case reports. On balance, the evidence suggests that Distributed Scrum has no impact, positive or negative, on overall project success. Consequently, claims by agile consultants who present Distributed Scrum as a recipe for project success should be treated with great caution, while researchers should investigate more varied perspectives to identify the real drivers of success in distributed and global software development. © 2023 held by the owner/author(s).},
	author = {Santos, Ronnie De Souza and Ralph, Paul and Arshad, Arham and Stol, Klaas-Jan},
	year = {2023},
	keywords = {Software design, Human resource management, Project management, Global software engineering, Scra, Meta-analysis, Software teams, Case meta-analyse, Distributed scrums, Distributed software, Distributed software development, Project management frameworks, Project success},
}

@article{dobaj_towards_2023,
	title = {Towards {DevOps} for {Cyber}-{Physical} {Systems} ({CPSs}): {Resilient} {Self}-{Adaptive} {Software} for {Sustainable} {Human}-{Centric} {Smart} {CPS} {Facilitated} by {Digital} {Twins}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175031906&doi=10.3390%2fmachines11100973&partnerID=40&md5=5e0d1126be87b6fd21b7b77f667e9194},
	abstract = {The Industrial Revolution drives the digitization of society and industry, entailing Cyber-Physical Systems (CPSs) that form ecosystems where system owners and third parties share responsibilities within and across industry domains. Such ecosystems demand smart CPSs that continuously align their architecture and governance to the concerns of various stakeholders, including developers, operators, and users. In order to satisfy short- and long-term stakeholder concerns in a continuously evolving operational context, this work proposes self-adaptive software models that promote DevOps for smart CPS. Our architectural approach extends to the embedded system layer and utilizes embedded and interconnected Digital Twins to manage change effectively. Experiments conducted on industrial embedded control units demonstrate the approach’s effectiveness in achieving sub-millisecond real-time closed-loop control of CPS assets and the simultaneous high-fidelity twinning (i.e., monitoring) of asset states. In addition, the experiments show practical support for the adaptation and evolution of CPS through the dynamic reconfiguring and updating of real-time control services and communication links without downtime. The evaluation results conclude that, in particular, the embedded Digital Twins can enhance CPS smartness by providing service-oriented access to CPS data, monitoring, adaptation, and control capabilities. Furthermore, the embedded Digital Twins can facilitate the seamless integration of these capabilities into current and future industrial service ecosystems. At the same time, these capabilities contribute to implementing emerging industrial services such as remote asset monitoring, commissioning, and maintenance. © 2023 by the authors.},
	author = {Dobaj, Jürgen and Riel, Andreas and Macher, Georg and Egretzberger, Markus},
	year = {2023},
}

@book{bhat_engineering_2023,
	title = {Engineering {Challenges} in the {Development} of {Artificial} {Intelligence} and {Machine} {Learning} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179277659&doi=10.1201%2f9781032624983-7&partnerID=40&md5=8d4c838a9eb34b83f7f86e0841ed7615},
	abstract = {In this chapter, we first introduce artificial intelligence and machine learning (AI/ML) as state-of-the-art in engineering software and then outline the major differences between AI/ML and traditional software development. In particular, we categorize AI/ML engineering challenges in different phases. Eventually, different challenges are generalized and categorized. Finally, we observe that software testing, quality assurance, and management of the data are the most challenging issues that engineers/developers are currently facing. © 2024 Taylor \& Francis Group, LLC.},
	author = {Bhat, Mohammad Idrees and Yaqoob, Syed Irfan and Imran, Mohammad},
	year = {2023},
}

@article{giray_use_2023,
	title = {On the use of deep learning in software defect prediction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140965251&doi=10.1016%2fj.jss.2022.111537&partnerID=40&md5=abc6530954079e153d281e71264be476},
	abstract = {Context: Automated software defect prediction (SDP) methods are increasingly applied, often with the use of machine learning (ML) techniques. Yet, the existing ML-based approaches require manually extracted features, which are cumbersome, time consuming and hardly capture the semantic information reported in bug reporting tools. Deep learning (DL) techniques provide practitioners with the opportunities to automatically extract and learn from more complex and high-dimensional data. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of the utilization of DL algorithms for SDP in the literature. Method: We systematically selected a pool of 102 peer-reviewed studies and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: Main highlights include: (1) most studies applied supervised DL; (2) two third of the studies used metrics as an input to DL algorithms; (3) Convolutional Neural Network is the most frequently used DL algorithm. Conclusion: Based on our findings, we propose to (1) develop more comprehensive DL approaches that automatically capture the needed features; (2) use diverse software artifacts other than source code; (3) adopt data augmentation techniques to tackle the class imbalance problem; (4) publish replication packages. © 2022 The Authors},
	author = {Giray, Görkem and Bennin, Kwabena Ebo and Köksal, Ömer and Babur, Önder and Tekinerdogan, Bedir},
	year = {2023},
	keywords = {Systematic literature review, Deep learning, Machine-learning, Defects, Forecasting, Software defect prediction, Convolutional neural networks, Data mining, Machine learning techniques, Quality assurance, Bug reporting, Clustering algorithms, Defect prediction methods, Learning-based approach, Reporting tools, Semantics, Semantics Information},
}

@article{fischer_data-driven_2023,
	title = {Data-{Driven} {Organizations}: {Review}, {Conceptual} {Framework}, and {Empirical} {Illustration}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180313770&doi=10.3127%2fAJIS.V27I0.4425&partnerID=40&md5=e02e898eb6866062fdfeaaa30c52d11d},
	abstract = {With companies and other organizations increasingly striving to become (more) data-driven, there has been growing research interest in the notion of a data-driven organization (DDO). In existing literature, however, different understandings of such an organization emerged. The study at hand sets forth to synthesize the fragmented body of research through a review of existing DDO definitions and implicit understandings of this concept in the information systems and related literatures. Based on the review results and drawing on the established concept of the “knowing organization,” our study identifies five core dimensions of a DDO—namely, data sourcing \& sensemaking, data capabilities, data-driven culture, data-driven decision-making, and data-driven value creation—which we integrate into a conceptual DDO framework. Most notably, the proposed framework suggests that—like its predecessor, the knowing organization—a DDO may draw on an outside-in view; however, it may also draw on an inside-out view, or even combine the two views, thereby setting itself apart from the knowing organization. To illustrate our conceptual DDO framework and demonstrate its usefulness, we apply this framework to three empirical examples. Theoretical and practical contributions as well as directions for future research are discussed. © (2023), (Australasian Association for Information Systems). All Rights Reserved.},
	author = {Fischer, Hannes and Wiener, Martin and Strahringer, Susanne and Kotlarsky, Julia and Bley, Katja},
	year = {2023},
}

@inproceedings{cordeiro_towards_2023,
	title = {Towards a {Framework} {Based} on {Open} {Science} {Practices} for {Promoting} {Reproducibility} of {Software} {Engineering} {Controlled} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166021485&partnerID=40&md5=145a60cab90f6f1a29901242e43e78d6},
	abstract = {Experimentation in Software Engineering has increased in the last decades as a way to provide evidence on theories and technologies. In a controlled experiment life cycle, several artifacts are used/reused and even produced. Such artifacts are mostly in the form of data, which should favor the reproducibility of such experiments. In this context, reproducibility can be defined as the ability to reproduce a study. Different benefits, such as methodology and data reuse, can be achieved from this ability. Despite the recognized benefits, several challenges have been faced by researchers regarding the experiments’ reproducibility capability. To overcome them, we understand that Open Science practices, related to provenance, preservation, and curation, might aid in improving such a capability. Therefore, in this paper, we present the proposal for an open science-based Framework to deal with controlled experiment research artifacts towards making such experiments de facto reproducible. To do so, different models associated with open science practices are planned to be integrated into the Framework. © 2023 CIbSE 2023 - XXVI Ibero-American Conference on Software Engineering. All rights reserved.},
	author = {Cordeiro, André F.R.},
	year = {2023},
	keywords = {Software engineering, Life cycle, Open science, Controlled experiment, Curation, Data reuse, Experiment research, Reproducibilities, Research artefacts},
}

@inproceedings{ris_systemic_2023,
	title = {A {Systemic} {Mapping} of {Methods} and {Tools} for {Performance} {Analysis} of {Data} {Streaming} with {Containerized} {Microservices} {Architecture}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169796889&doi=10.23919%2fCISTI58278.2023.10211834&partnerID=40&md5=d6a237cf4005729c3547af7a0673b592},
	abstract = {With the Internet of Things (IoT) growth and customer expectations, the importance of data streaming and streaming processing has increased. Data Streaming refers to the concept where data is processed and transmitted continuously and in real-time without necessarily being stored in a physical location. Personal health monitors and home security systems are examples of data streaming sources. This paper presents a systematic mapping study of the performance analysis of Data Streaming systems in the context of Containerization and Microservices. The research aimed to identify the main methods, tools, and techniques used in the last five years for the execution of this type of study. The results show that there are still few performance evaluation studies for this system niche, and there are gaps that must be filled, such as the lack of analytical modeling and the disregard for communication protocols' influence. © 2023 ITMA.},
	author = {Ris, Simone and Araujo, Jean and Beserra, David},
	year = {2023},
	keywords = {Mapping, Performance, Containers, Microservice, Internet of things, Performances analysis, Real- time, Analysis of data, Customer expectation, Data reduction, Data streaming, Data transfer, Physical locations, Realibility, Streaming processing},
}

@article{silva_digital_2023,
	title = {The {Digital} {Twin} {Paradigm} {Applied} to {Soil} {Quality} {Assessment}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146668292&doi=10.3390%2fs23021007&partnerID=40&md5=14555c99ca9482a8958de1c8b705d8ab},
	abstract = {This article presents the results regarding a systematic literature review procedure on digital twins applied to precision agriculture. In particular, research and development activities aimed at the use of digital twins, in the context of predictive control, with the purpose of improving soil quality. This study was carried out through an exhaustive search of scientific literature on five different databases. A total of 158 articles were extracted as a result of this search. After a first screening process, only 11 articles were considered to be aligned with the current topic. Subsequently, these articles were categorised to extract all relevant information, using the preferred reporting items for systematic reviews and meta-analyses methods. Based on the obtained results, there are two main conclusions to draw: First, when compared with industrial processes, there is only a very slight rising trend regarding the use of digital twins in agriculture. Second, within the time frame in which this work was carried out, it was not possible to find any published paper on the use of digital twins for soil quality improvement within a model predictive control context. © 2023 by the authors.},
	author = {Silva, Letícia and Rodríguez-Sedano, Francisco and Baptista, Paula and Coelho, João Paulo},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, Scientific literature, Development activity, Model predictive control, Precision agriculture, Precision Agriculture, Predictive control, Research activities, Research and development, soil, Soil, Soil quality assessments, Soils, Soils qualities},
}

@article{schipor_gearwheels_2023,
	title = {{GearWheels}: {A} {Software} {Tool} to {Support} {User} {Experiments} on {Gesture} {Input} with {Wearable} {Devices}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134623880&doi=10.1080%2f10447318.2022.2098907&partnerID=40&md5=5be729e9568048f5de88c0aea18ab9e7},
	abstract = {We introduce GearWheels, a software tool for studies about gesture input with wearables, including smartwatches, rings, and glasses. GearWheels features an event-based asynchronous software architecture design implemented exclusively with web standards, communications protocols, and data formats, which makes it flexible to support many wearables via HTTP and WebSocket communications. GearWheels differentiates from prior software tools for gesture acquisition, elicitation, recognition, and analysis with its web-based, wearable-oriented, experiment-centered architecture design. We demonstrate GearWheels with a device affixed to the index finger, wrist, and the temple of a pair of glasses to illustrate touch stroke-gesture and motion-gesture input acquisition. We also perform a technical evaluation of GearWheels in the form of a simulation experiment, and report the request-response time performance of the software components of GearWheels with off-the-shelf wearables. We release GearWheels as open source software to assist researchers and practitioners in implementing studies about gesture input with wearables. © 2022 Taylor \& Francis Group, LLC.},
	author = {Schipor, Ovidiu-Andrei and Vatavu, Radu-Daniel},
	year = {2023},
	keywords = {Software design, Open source software, Open systems, Communications data, Communications protocols, Computer aided software engineering, Event-based, Gesture input, Glass, HTTP, Network architecture, Software architecture design, Software-tools, User experiments, Wearable devices, Wearable technology, Web standards, Websocket},
}

@article{zapata_systematic_2023,
	title = {Systematic {Mapping} of the {Literature} on the {Conceptual} {Modeling} of {Industry} 4.0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173573848&doi=10.1007%2f978-3-031-34147-2_15&partnerID=40&md5=1b3b1b38bf9c300501453861ddb7a8fa},
	abstract = {The Industry 4.0 concept refers to a new way of producing through the adoption of 4.0 technologies based on solutions focused on interconnectivity, automation, and real-time data. Given the importance of conceptualizing the problem domain and its solution, this paper presents the results of a systematic mapping to identify the state of the art and discover the existing contributions to the conceptual modeling of industry 4.0. A search was carried out in the Scopus, IEEE Xplore, and ACM DL digital libraries from January 2017 to May 2022. It was found that no article describes the model through a language known for this purpose, except for two articles that use Domain Specific Modeling Languages (DSML) and Unified Modeling Language (UML). Of the total number of primary studies, 63.33\% propose a model-based solution, while 13.34\% propose the use of tools, methods, and processes. Finally, 23.33\% present the state of the art. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Zapata, Ayelén and Fransoy, Marcelo and Soto, Salvador and Di Felice, Martín and Panizzi, Marisa},
	year = {2023},
	keywords = {Embedded systems, State of the art, Mapping, Unified Modeling Language, Systematic mapping, Industry 4.0, Digital libraries, Technology-based, Conceptual model, Domain specific modeling languages, Domain-Specific Modelling Languages, Interconnectivity, Problem domain, Real-time data, Specification languages, Systematic mapping of the literature},
}

@book{ritu_software_2023,
	title = {Software {Effort} {Estimation} with {Machine} {Learning} – {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152307515&doi=10.1002%2f9781119896838.ch15&partnerID=40&md5=8e9a36b192b41d691f4c4960256acc6b},
	abstract = {In 1959 the concept of machine learning techniques and algorithm was introduced by Artur Samuel, an IBmer from the United States who made a name for himself in the fields of computer gaming and artificial intelligence. The influence of literature reviews which is done systematically (SLRs), which are the preferred techniques and methods for aggregating effort, is examined in this study. We conducted a systematic literature review using the conventional procedure, which included a manual search of nine periodicals and a few conference proceedings. Eight of the twenty studies that were relevant focused on latest trends in research instead of technique evaluation. Seven LRs dealt with the estimation of effort. The SLR’s quality was best suited with only those in which fields are qualitatively checked not quantitatively. SLRs currently cover a large number of topics, but not all of them. Systematic literature reviews appear to be the most popular among researchers from Asia and Europe, particularly those at the Simula Laboratory. © 2023 Scrivener Publishing LLC.},
	author = {{Ritu} and Bhambri, Pankaj},
	year = {2023},
}

@inproceedings{de_castro_understanding_2023,
	title = {Understanding {Sustainable} {Knowledge}-{Sharing} in {Agile} {Projects}: {Utilizing} {Follow}-the-{Sun} {Technique} ({FTS}) in {Virtual} {Teams}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183565184&doi=10.1016%2fj.procs.2023.10.023&partnerID=40&md5=e8a2113a7c5e0ce18a6d80b3fec2ca8e},
	abstract = {In Agile IT projects, promoting effective knowledge sharing is essential not only for achieving success but also for supporting Sustainable Development Goals (SDGs). However, Companies using virtual teams may face challenges in coordinating work, particularly when teams are distributed across different time zones, ultimately hindering their ability to consistently share knowledge. This can lead to delays and inefficiencies, ultimately impacting the project outcomes and the organization's profitability. To ensure sustainable knowledge sharing, a comprehensive framework is necessary that addresses the environmental, social, economic, and political aspects of the project. This paper proposes a framework that combines the Follow-the-Sun (FTS) technique and the Sustainable Knowledge Sharing Model, enabling 24-hour knowledge sharing in virtual teams and benefiting IT agile projects. © 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)},
	author = {de Castro, Rodrigo Oliveira and Sanin, Cesar and Levula, Andrew and Szczerbicki, Edward},
	year = {2023},
	keywords = {Knowledge management, Sustainable development, Project management, 24-hour knowledge-sharing cycle, Agile IT project, Economic responsibility, Environmental responsibility, Follow-the-sun technique, IT project, Knowledge-sharing, Political responsibility, Social responsibilities, Sustainable knowledge sharing, Sustainable knowledge sharing model, Virtual team},
}

@inproceedings{vasylieva_how_2023,
	title = {How {Agile} {Are} you? {Discussing} {Maturity} {Levels} of {Agile} {Maturity} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320257&doi=10.1109%2fSEAA60479.2023.00049&partnerID=40&md5=328e1188c0b79baca242d53e65b4626e},
	abstract = {With the emergence of agile software development methods, new approaches for determining agile maturity have become necessary. Other than for traditional maturity and capability models like CMMI and ISO/IEC 15504, the field of agile maturity models is not yet settled. Even worse, a common understanding regarding agility in general and the levels of agility in particular is missing. The paper at hand aims to shed light on the field of agile maturity models with a particular focus on maturity levels, their definition, and their evaluation and computation. We conducted a systematic literature review to extract maturity levels and provide an initial harmonization of the levels found. Our findings from analyzing 19 agile maturity models show that there is yet no agreement with regard to the maturity levels. In total, 69 maturity levels have been analyzed for harmonization opportunities. Two major dimensions of maturity levels of agile maturity models could be identified: (1) team-related and (2) general maturity, which is comparable to standard approaches. However, the procedures to assess organizations and processes, if at all present, are to a large extent focused on persons and their personal opinion, which paves the way for future research, e.g., in terms of developing measurement systems for assessing agile maturity. © 2023 IEEE.},
	author = {Vasylieva, Kseniia and Kuhrmann, Marco and Xavier, Meenu Kadavilveetil and Klunder, Jil},
	year = {2023},
	keywords = {Software design, Agile software development, Software process, Agile maturity model, Capability model, Harmonisation, Maturity levels, Maturity model, New approaches, Software development methods},
}

@inproceedings{ntinda_aligning_2023,
	title = {Aligning {Academic} {Efforts} with {Key} {Industries}: {A} {Case} of {Computing} at the {University} of {Namibia}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182994686&doi=10.1109%2fFIE58773.2023.10343344&partnerID=40&md5=f95405a625e2b7d7d2cb3afce77a7778},
	abstract = {Preparing future graduates for the workplace should involve linkage with industry. Ultimately, students gain valuable insight into real-life projects, preparing them for future careers. However, universities in some developing countries lag in their initiatives to promote industry-academia collaboration. In this paper, we explore how to strengthen academic efforts at the University of Namibia (UNAM) with the assistance of industry in Namibia. In the study, we analyse: 1) current practices of industry collaboration worldwide published in ACM and IEEE digital library through a scoping review, and 2) students' capstone projects conducted in the final year of the Bachelor of Science (Honors) in the computing discipline in 2020 - 2022 at UNAM. The analysis from the scoping review found six (6) different University-Industry Collaboration initiatives employed in universities worldwide. Additionally, the review of current students' theses indicates that they are not aligned with all four key industries in Namibia: Mining, Tourism, Fisheries, and Agriculture. Hence, we contextualised the analysis by reflecting upon the economic drivers and demands of the country. The preliminary outcomes of this study allowed us to propose the incorporation of the Conceive, Design, Implement, and Operate model in the computing degree programme that UNAM can adopt in developing an effective curriculum that aligns with the demands of the key relevant industries in Namibia. The aim is to support the development of new talent that will promote the country's economic growth. Reflecting on this process can also benefit other universities in developing countries by assisting them in contextualising their curricula and addressing their local and national requirements. © 2023 IEEE.},
	author = {Ntinda, Maria Ndapewa and Sedano, Carolina Islas and Apiola, Mikko and Sutinen, Erkki},
	year = {2023},
	keywords = {Students, Digital libraries, Curricula, Computing, Bachelor of science, Capstone projects, CDIO model, Computing disciplines, Current practices, Developing countries, Economic analysis, Industry collaboration, Mining, Namibia, Scoping review, University industries},
}

@article{jagstedt_dependencies_2023,
	title = {Dependencies as a barrier for continuous innovation in cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174302916&doi=10.1504%2fIJTM.2023.133915&partnerID=40&md5=09819f40631a7253d848d4f119ce6628},
	abstract = {In the automotive domain, as an example of cyber-physical systems, continuous software deployment is actively explored to deliver increasingly capable features to existing fleets of vehicles. The distributed nature of software coupled with tight hardware integration and potentially tremendous variability between vehicles make ensuring compatibility of updated software a significant challenge – both technically and managerially. While the automotive industry commonly forms larger multi-brand organisations to utilise economies of scale, processes for continuous deployment contradictory assumes a single organisation with full control. This paper sets out to shed light on challenges of adopting continuous deployment in the context of such a multi-brand cyber-physical systems organisation. Following a case study, the paper describes a tension between the managerial perspective concerned with platform strategies, and the engineering perspective responsible for developing products from those platforms. The paper highlights software dependencies as a barrier to continuous innovation of cyber-physical systems in multi-brand organisations. Copyright © 2023 Inderscience Enterprises Ltd.},
	author = {Jagstedt, Siri and Mellegård, Niklas and Lind, Kenneth},
	year = {2023},
	keywords = {Embedded systems, Software design, Agile software development, Continuous integrations, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Continuous deployment, Continuous innovation, Dependency, Economics, Multi-brand organization, Product architecture, Product platforms},
}

@article{iannone_secret_2023,
	title = {The {Secret} {Life} of {Software} {Vulnerabilities}: {A} {Large}-{Scale} {Empirical} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895799&doi=10.1109%2fTSE.2022.3140868&partnerID=40&md5=8645cb8f157905454c978f4ae194b36e},
	abstract = {Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database-pertaining to 1,096 open-source software projects on GitHub-and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues. © 1976-2012 IEEE.},
	author = {Iannone, Emanuele and Guadagni, Roberta and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio},
	year = {2023},
	keywords = {Information management, Software design, Life cycle, Software vulnerabilities, Software-systems, Open source software, Codes (symbols), Source codes, Open systems, Data mining, Software, Empirical Software Engineering, Code, Mining software, Mining software repository, Software development management, Software repositories},
}

@article{mubarkoot_software_2023,
	title = {Software {Compliance} {Requirements}, {Factors}, and {Policies}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141927596&doi=10.1016%2fj.cose.2022.102985&partnerID=40&md5=972aacda02a7c5ba18e90461a46647d6},
	abstract = {Background: Recent statistics reveal that 56\% of software attacks are caused by insider negligence and 26\% are caused by malicious insiders. They also show that 67\% of organizations experience at least 21 incidents per year. Most of these incidents require significant time and effort to contain them. In this regard, ensuring compliance with corporate policies, regulations, and industry best practices is paramount. Purpose: This study investigates software compliance requirements, factors, and policies together with the challenges they address. By taking a wider perspective, this study aims at bringing an understanding of existing research foci, evolving issues, and research directions. Method: The study uses a systematic literature review and keyword analysis, to identify relevant studies that address the derived research questions. Considering scholarly articles published in the last decade, 4,772 results were retrieved and checked through an initial screening. A thorough screening is then conducted to further reduce the results to 77 primary articles. Findings: The requirement on security of end users is gaining more attention. There is an emphasis on the gap between domain and compliance experts on the one side and software engineers on the other side. The review also identified 55 factors (and their underlying theories) that impact behavioral compliance with a majority of them focusing on individuals. Our results also list nineteen policies and compliance challenges they address. No distinction is found between open-source and proprietary software among the reviewed studies. The most mentioned policies are security education, training, and awareness (SETA), compliance automation, and organizational climate. The evolving topics in the field are: theory of workarounds, compliance and privacy by design, policy as code, security stress, and home-office users. Implications: The review provides 9 recommendations, comprising practical implications for decision makers, theoretical implications for future research, and potential enhancement of the underlying theories. © 2022 The Author(s)},
	author = {Mubarkoot, Mohammed and Altmann, Jörn and Rasti-Barzoki, Morteza and Egger, Bernhard and Lee, Hyejin},
	year = {2023},
	keywords = {Systematic literature review, Decision making, Open source software, Impact, Factor, Best practices, Open systems, Corporate policies, Decision theory, Malicious insiders, Policy regulations, Requirement, Software attacks, Software compliance},
}

@article{gardey_ux-painter_2023,
	title = {{UX}-{Painter}: {Fostering} {UX} {Improvement} in an {Agile} {Setting}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149840802&doi=10.1007%2f978-3-031-25648-6_4&partnerID=40&md5=50a565c23e05f522a0b2f5833cbc014b},
	abstract = {It is generally difficult in agile teams, specially those geographically distributed, to keep up with the user experience (UX) issues that emerge on each product increment. UX designers need the help of developers to set up user testing environments and to code improvements to the user interface, while developers are too busy with functionality issues. This paper describes a tool called UX-Painter and shows through a case study, how it may help in the above setting to synchronize UX practices and allow for continuous UX improvement during an agile development. UX-Painter allows designers to set up A/B testing environments, exploring interface design alternatives without the need of programming skills, through predefined transformations called client-side web refactorings. Once a design alternative is selected to be implemented in the application’s codebase, UX-Painter may also facilitate this step, exporting the applied refactorings to different frontend frameworks. Thus, we foster a method where UX backlog items can be systematically tackled and resolved in an agile setting. © 2023, Springer Nature Switzerland AG.},
	author = {Gardey, Juan Cruz and Grigera, Julián and Rossi, Gustavo and Garrido, Alejandra},
	year = {2023},
	keywords = {User interfaces, Users' experiences, Refactorings, User testing, Agile methods, Agile teams, Code improvement, Design alternatives, Show through, Testing environment, Web engineering},
}

@article{nunez-agurto_traffic_2023,
	title = {Traffic {Classification} in {Software}-{Defined} {Networking} by {Employing} {Deep} {Learning} {Techniques}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175975883&doi=10.1007%2f978-3-031-45682-4_6&partnerID=40&md5=9a95f5b968d737eb60ef3bdb967e4d27},
	abstract = {Software-Defined Networking provides a global vision of the network, centralized controller, dynamic routing, dynamic update of the flow table, and traffic analysis. The features of Software-Defined Networking and the integration of Deep Learning techniques allow the introduction of intelligence to optimize, manage and maintain them better. In this context, this work aims to provide a Systematic Literature Review on traffic classification in Software-Defined Networking with Deep Learning techniques. Furthermore, we analyze and synthesize the selected studies based on the categorization of traffic classes and the employed Deep Learning techniques to draw meaningful research conclusions. Finally, we identify new challenges and future research directions on this topic. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Nuñez-Agurto, Daniel and Fuertes, Walter and Marrone, Luis and Benavides-Astudillo, Eduardo and Vásquez-Bermúdez, Mitchell},
	year = {2023},
	keywords = {Systematic literature review, Deep learning, Learning algorithms, Learning systems, Centralized controllers, Controller dynamics, Dynamic routing, Global vision, Learning techniques, Routing dynamics, Software defined networking, Software-defined networkings, Traffic classification},
}

@article{volden_wayfinding_2023,
	title = {Wayfinding and {Navigation} in the {Outdoors}: {Quantitative} and {Data} {Driven} {Development} of {Personas} and {Requirements} for {Wayfinding} in {Nature}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173041138&doi=10.1007%2f978-3-031-35129-7_14&partnerID=40&md5=77edf2f35a0d4ce956149ab12f497864},
	abstract = {Persona development in human-centered design processes is mostly done in a qualitative process involving procedures like interviews, focus-groups and workshops. These are methods that are criticized for being prone to biases, as not being based on rigorous empirical data and for using small and possibly non-representative populations. Quantitative approaches are an alternative or supplement to qualitative methods. Through a survey (n = 693) we have investigated how people navigate and find their way in the nature. The questionnaire contains questions on demographics, activities, and wayfinding behaviors when out in the nature. The study’s aim was twofold: First we wanted to investigate the use of a quantitative approach for exploring user behaviors and attitudes when having access to a sufficiently large data material. Secondly, we wanted to provide for persona-development for way-finding systems used in the nature. The methods applied in this study is a combination of Principal Component Analyses (PCA) and Cluster Analyses (CA). Based on these methods three factors where identified, which again lead to three clusters of respondents. The study concludes that when having access to quantitative data as we managed to have in this study, the combination of PCA and CA is an efficient and precise way to describe requirements and develop Personas. Results also indicate significant effects of demographic variables like age and gender for technology preferences. as well as for confidence in abilities when navigating and finding the way in nature. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Volden, Frode and Wattne, Ole E.},
	year = {2023},
	keywords = {Behavioral research, Data driven, Design-process, Population statistics, Cluster analysis, Cluster analyze, Human-centred designs, Principal component analysis, Principal-component analysis, Qualitative process, Quantitative approach, Quantitative persona development, Way finding, Wayfinding and navigations},
}

@inproceedings{souza_santos_lgbtqia_2023,
	title = {{LGBTQIA}+ ({In}) {Visibility} in {Computer} {Science} and {Software} {Engineering} {Education}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165153862&doi=10.1109%2fCHASE58964.2023.00026&partnerID=40&md5=91575c7fa526b69d70f2e73d63079186},
	abstract = {Modern society is diverse, multicultural, and multifaceted. Because of these characteristics, we are currently observing an increase in the debates about equity, diversity, and inclusion in different areas, especially because several groups of individuals are underrepresented in many environments. In computer science and software engineering, it seems counterintuitive that these areas, which are responsible for creating technological solutions and systems for billions of users around the world, do not reflect the diversity of the society to which it serves. In trying to solve this diversity crisis in the software industry, researchers started to investigate strategies that can be applied to increase diversity and improve inclusion in academia and the software industry. However, the lack of diversity in computer science and related courses, including software engineering, is still a problem, in particular when some specific groups are considered. LGBTQIA+ students, for instance, face several challenges to fit into technology courses, even though most students in universities right now belong to Generation Z, which is described as open-minded to aspects of gender and sexuality. In this study, we aimed to discuss the state-of-art of publications about the inclusion of LGBTQIA+ students in computer science education. Using a mapping study, we identified eight studies published in the past six years that focused on this public. We present strategies developed to adapt curricula and lectures to be more inclusive to LGBTQIA+ students and discuss challenges and opportunities for future research. © 2023 IEEE.},
	author = {Souza Santos, Ronnie De and Stuart-Verner, Brody and De Magalhaes, Cleyton V. C.},
	year = {2023},
	keywords = {Computer software, Engineering education, Students, Software industry, Computer Science Education, Education computing, Computer science and software engineerings, Computer Science course, Computer-related course, Diversity, Inclusions, LGBTQIA+, Software engineering education, Technological solution, Technological system},
}

@article{pantoja_yepez_training_2023,
	title = {Training software architects suiting software industry needs: {A} literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174223101&doi=10.1007%2fs10639-023-12149-x&partnerID=40&md5=857c26b2b7109dcd5a963c38e629e347},
	abstract = {The ability to define, evaluate, and implement software architectures is a fundamental skill for software engineers. However, teaching software architecture can be challenging as it requires students to be involved in real-context projects with high degrees of complexity. This involves making trade-off decisions among several quality attributes. Furthermore, the academic perception of software architecture differs from the industrial viewpoint. To address this issue, a study was conducted to identify and analyze the strategies, challenges, and course experiences used for teaching software architectures. The study analyzed 56 articles reporting on teaching experiences focused specifically on software architectures or focused on software engineering in general but discussing software architecture. The main contributions of this work include identifying strategies used in educating software architecture students aligned with the needs of the software industry. These strategies include short design projects, large development projects, and projects with actual clients. Additionally, the study compared curriculum contents in software development and architecture courses and identified recurring topics such as architecture patterns, quality attributes, and architectural views. This study also recognizes the set of skills that students of software architecture should develop during training, such as leadership and negotiation. The challenges in software architecture training were discussed, such as instructors’ lack of experience in actual projects, the abstract and fuzzy nature of software architectures, and the difficulty of involving clients and industry experts. Evaluation methods commonly used in training software architects, such as surveys, pre-test/post-test, and quality metrics on architectural artifacts, were identified and described. Overall, this study guides researchers and educators in improving their software architecture courses by incorporating strategies reported by the literature review. These strategies can bring architecture courses closer to the needs and conditions of the software industry. © 2023, The Author(s).},
	author = {Pantoja Yépez, Wilson Libardo and Hurtado Alegría, Julio Ariel and Bandi, Ajay and Kiwelekar, Arvind W.},
	year = {2023},
}

@inproceedings{heyn_automotive_2023,
	title = {Automotive {Perception} {Software} {Development}: {An} {Empirical} {Investigation} into {Data}, {Annotation}, and {Ecosystem} {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165140236&doi=10.1109%2fCAIN58948.2023.00011&partnerID=40&md5=73c99440d36cf7a3c3c1d5b7444837f6},
	abstract = {Software that contains machine learning algorithms is an integral part of automotive perception, for example, in driving automation systems. The development of such software, specifically the training and validation of the machine learning components, requires large annotated datasets. An industry of data and annotation services has emerged to serve the development of such data-intensive automotive software components. Wide-spread difficulties to specify data and annotation needs challenge collaborations between OEMs (Original Equipment Manufacturers) and their suppliers of software components, data, and annotations.This paper investigates the reasons for these difficulties for practitioners in the Swedish automotive industry to arrive at clear specifications for data and annotations. The results from an interview study show that a lack of effective metrics for data quality aspects, ambiguities in the way of working, unclear definitions of annotation quality, and deficits in the business ecosystems are causes for the difficulty in deriving the specifications. We provide a list of recommendations that can mitigate challenges when deriving specifications and we propose future research opportunities to overcome these challenges. Our work contributes towards the on-going research on accountability of machine learning as applied to complex software systems, especially for high-stake applications such as automated driving. © 2023 IEEE.},
	author = {Heyn, Hans-Martin and Habibullah, Khan Mohammad and Knauss, Eric and Horkoff, Jennifer and Borg, Markus and Knauss, Alessia and Li, Polly Jing},
	year = {2023},
	keywords = {Data, Ecosystems, Specifications, Software design, Application programs, Learning algorithms, Automation, Machine learning, Machine-learning, Machine learning algorithms, Software-component, Automotive industry, Automotives, Accountability, Annotation, Data annotation, Empirical investigation, Large dataset, Machine components, Requirements specifications},
}

@article{neves_data_2023,
	title = {Data privacy in the {Internet} of {Things} based on anonymization: {A} review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161049485&doi=10.3233%2fJCS-210089&partnerID=40&md5=31fb26c942cd9ccd8d3943c25a607885},
	abstract = {The Internet of Things (IoT) has shown rapid growth in recent years. However, it presents challenges related to the lack of standardization of communication produced by different types of devices. Another problem area is the security and privacy of data generated by IoT devices. Thus, with the focus on grouping, analyzing, and classifying existing data security and privacy methods in IoT, based on data anonymization, we have conducted a Systematic Literature Review (SLR). We have therefore reviewed the history of works developing solutions for security and privacy in the IoT, particularly data anonymization and the leading technologies used by researchers in their work. We also discussed the challenges and future directions for research. The objective of the work is to give order to the main approaches that promise to provide or facilitate data privacy using anonymization in the IoT area. The study's results can help us understand the best anonymization techniques to provide data security and privacy in IoT environments. In addition, the findings can also help us understand the limitations of existing approaches and identify areas for improvement. The results found in most of the studies analyzed indicate a lack of consensus in the following areas: (i) with regard to a solution with a standardized methodology to be applied in all scenarios that encompass IoT; (ii) the use of different techniques to anonymize the data; and (iii), the resolution of privacy issues. On the other hand, results made available by the k-anonymity technique proved efficient in combination with other techniques. In this context, data privacy presents one of the main challenges for broadening secure domains in applying privacy with anonymity. © 2023 - IOS Press. All rights reserved.},
	author = {Neves, Flávio and Souza, Rafael and Sousa, Juliana and Bonfim, Michel and Garcia, Vinicius},
	year = {2023},
	keywords = {Systematic literature review, Data privacy, Privacy, Internet of things, Anonymization, Data anonymization, Data security and privacy, Dataflow, K-Anonymity, Problem areas, Rapid growth, Security and privacy},
}

@article{denecke_developing_2023,
	title = {Developing a {Technical}-{Oriented} {Taxonomy} to {Define} {Archetypes} of {Conversational} {Agents} in {Health} {Care}: {Literature} {Review} and {Cluster} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147143364&doi=10.2196%2f41583&partnerID=40&md5=acb31e1475488e8316c9d3c5a54d84e0},
	abstract = {Background: The evolution of artificial intelligence and natural language processing generates new opportunities for conversational agents (CAs) that communicate and interact with individuals. In the health domain, CAs became popular as they allow for simulating the real-life experience in a health care setting, which is the conversation with a physician. However, it is still unclear which technical archetypes of health CAs can be distinguished. Such technical archetypes are required, among other things, for harmonizing evaluation metrics or describing the landscape of health CAs. Objective: The objective of this work was to develop a technical-oriented taxonomy for health CAs and characterize archetypes of health CAs based on their technical characteristics. Methods: We developed a taxonomy of technical characteristics for health CAs based on scientific literature and empirical data and by applying a taxonomy development framework. To demonstrate the applicability of the taxonomy, we analyzed the landscape of health CAs of the last years based on a literature review. To form technical design archetypes of health CAs, we applied a k-means clustering method. Results: Our taxonomy comprises 18 unique dimensions corresponding to 4 perspectives of technical characteristics (setting, data processing, interaction, and agent appearance). Each dimension consists of 2 to 5 characteristics. The taxonomy was validated based on 173 unique health CAs that were identified out of 1671 initially retrieved publications. The 173 CAs were clustered into 4 distinctive archetypes: a text-based ad hoc supporter; a multilingual, hybrid ad hoc supporter; a hybrid, single-language temporary advisor; and, finally, an embodied temporary advisor, rule based with hybrid input and output options. Conclusions: From the cluster analysis, we learned that the time dimension is important from a technical perspective to distinguish health CA archetypes. Moreover, we were able to identify additional distinctive, dominant characteristics that are relevant when evaluating health-related CAs (eg, input and output options or the complexity of the CA personality). Our archetypes reflect the current landscape of health CAs, which is characterized by rule based, simple systems in terms of CA personality and interaction. With an increase in research interest in this field, we expect that more complex systems will arise. The archetype-building process should be repeated after some time to check whether new design archetypes emerge. © 2023 Journal of Medical Internet Research. All rights reserved.},
	author = {Denecke, Kerstin and May, Richard},
	year = {2023},
	keywords = {human, Humans, human computer interaction, Review, artificial intelligence, Artificial Intelligence, cluster analysis, Cluster Analysis, Communication, controlled study, data privacy, data processing, Delivery of Health Care, health care, health care delivery, intelligence, internet access, interpersonal communication, k means clustering, language, Language, machine learning, personality, sentiment analysis, taxonomy},
}

@article{tsui_detect_2023,
	title = {Detect and {Interpret}: {Towards} {Operationalization} of {Automated} {User} {Experience} {Evaluation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169436114&doi=10.1007%2f978-3-031-35702-2_6&partnerID=40&md5=71662b96b384aa4acc41fbb5146fa2f3},
	abstract = {The evaluation of user experience (UX) with software products is widely recognized as a critical aspect of supporting a product lifecycle. However, existing UX evaluation methods tend to require high levels of human involvement in data collection and analysis. This makes the ongoing UX monitoring particularly challenging, especially given the increasing number of products, growing user base and associated data. Thus, there is a strong demand in developing UX evaluation systems that are able to automatically track UX and provide insights on required design improvements. The few existing frameworks for such automated systems can help identify user-centric metrics for UX evaluation, but mostly focus on providing recommendations on best practices of determining metrics and tend to reflect only parts of the UX. Moreover, these frameworks predominantly rely on high-level UX concepts, but do not necessarily allow measurements to reveal the underlying causes of UX challenges. In this paper, we demonstrate how the above-mentioned challenges can be addressed through a combination of data gathering and analysis paths employed by the traditional UX evaluation methods. Our paper contributes to the field by providing a review of existing automated UX evaluation approaches and common UX evaluation data collection methods, and offering a two-tier measurement approach for developing automated UX evaluation system, which augments the reflective power of traditional UX evaluation methods. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Tsui, Angeline Sin Mei and Kuzminykh, Anastasia},
	year = {2023},
	keywords = {Life cycle, Automation, Users' experiences, Evaluation methods, Data acquisition, Automatic UX evaluation, Data collection, Evaluation method and technique, Evaluation of users, Method and technique, Product life cycles, Software products, User experience evaluations},
}

@article{heikkinen_continual_2023,
	title = {Continual {Service} {Improvement}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172415273&doi=10.1007%2f978-3-031-43703-8_3&partnerID=40&md5=68142092a8a39278b60b3241c351fb51},
	abstract = {Continual Service Improvement (CSI) is an ongoing activity to identify and improve organization practices and services to align them with changing needs. CSI is one of the core elements of IT Service Management (ITSM) frameworks. However, as a research topic it is still an emerging research area of service science. This study explores implementation of CSI and its seven-step improvement process in the context of ITSM. The goal of this paper is to present results of systematic literature review increasing understanding about the CSI and seven-step improvement process, and provide topics for future research. A Systematic Literature Review (SLR) was carried out to analyse CSI-related academic articles. Our main finding is that CSI-related terminology needs clarification and consistency both in academia and in practice to guide the future CSI research for example clarify roles and internal practices of CSI; provide a staged approach for continual improvement; and identify models that support improving and automating the seven-step improvement process. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Heikkinen, Sanna and Jäntti, Marko and Tukiainen, Markku},
	year = {2023},
	keywords = {Systematic literature review, Continual service improvement, IT service management, IT services, ITIL, Service improvement, Service management, Seven-step improvement process},
}

@article{ouhaichi_research_2023,
	title = {Research trends in multimodal learning analytics: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151456109&doi=10.1016%2fj.caeai.2023.100136&partnerID=40&md5=4c1ed5486737fbf780207ba5b51fa068},
	abstract = {Understanding and improving education are critical goals of learning analytics. However, learning is not always mediated or aided by a digital system that can capture digital traces. Learning in such environments can be studied by recording, processing, and analyzing different signals, including video and audio, so that traces of actors’ actions and interactions are captured. Multimodal Learning Analytics refers to analyzing these signals through the use and integration of these multiple modes. However, a need exists to evaluate how research is conducted in the emerging field of multimodal learning analytics to aid and evaluate how these systems work. With the growth of multimodal learning analytics, research trends and technologies are needed to support its development. We conducted a systematic mapping study based on established systematic literature practices to identify multimodal learning analytics research types, methodologies, and trending research themes. Most mapped papers presented different solutions and used evaluation-based research methods to demonstrate an increasing interest in multimodal learning analytics technologies. In addition, we identified 14 topics under four themes––learning context, learning process, systems and modality, and technologies––that can contribute to the growth of multimodal learning analytics. © 2023 The Authors},
	author = {Ouhaichi, Hamza and Spikol, Daniel and Vogel, Bahtijar},
	year = {2023},
}

@inproceedings{baron_evidence_2023,
	title = {Evidence {Profiles} for {Validity} {Threats} in {Program} {Comprehension} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171794365&doi=10.1109%2fICSE48619.2023.00162&partnerID=40&md5=eb9f911cd27ddf564d8b8e6fbcac370a},
	abstract = {Searching for clues, gathering evidence, and reviewing case files are all techniques used by criminal investigators to draw sound conclusions and avoid wrongful convictions. Medicine, too, has a long tradition of evidence-based practice, in which administering a treatment without evidence of its efficacy is considered malpractice. Similarly, in software engineering (SE) research, we can develop sound methodologies and mitigate threats to validity by basing study design decisions on evidence. Echoing a recent call for the empirical evaluation of design decisions in program comprehension experiments, we conducted a 2-phases study consisting of systematic literature searches, snowballing, and thematic synthesis. We found out (1) which validity threat categories are most often discussed in primary studies of code comprehension, and we collected evidence to build (2) the evidence profiles for the three most commonly reported threats to validity. We discovered that few mentions of validity threats in primary studies (31 of 409) included a reference to supporting evidence. For the three most commonly mentioned threats, namely the influence of programming experience, program length, and the selected comprehension measures, almost all cited studies (17 of 18) did not meet our criteria for evidence. We show that for many threats to validity that are currently assumed to be influential across all studies, their actual impact may depend on the design and context of each specific study. Researchers should discuss threats to validity within the context of their particular study and support their discussions with evidence. The present paper can be one resource for evidence, and we call for more meta-studies of this type to be conducted, which will then inform design decisions in primary studies. Further, although we have applied our methodology in the context of program comprehension, our approach can also be used in other SE research areas to enable evidence-based experiment design decisions and meaningful discussions of threats to validity. © 2023 IEEE.},
	author = {Baron, Marvin Munoz and Wyrich, Marvin and Graziotin, Daniel and Wagner, Stefan},
	year = {2023},
	keywords = {Software engineering, Design, Study design, Software engineering research, Empirical evaluations, Evidence-based practices, Empirical Software Engineering, Threat to validity, Case files, Design decisions, Evaluation of designs, Program comprehension},
}

@inproceedings{dallegrave_action_2023,
	title = {Action {Research} for {Industry} {Academia} {Collaboration} : {A} replication {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169786536&doi=10.23919%2fCISTI58278.2023.10211674&partnerID=40&md5=947725c1c9cf30739a4b22bde3653ba7},
	abstract = {Collaboration between industry and academic communities requires considerable work but has the power to foster innovation. This relationship with joint trust promotes knowledge exchange that helps develop more qualified researchers and professionals. The Action Research (AR) method combines theory and practice, and studies involving industry-academia collaboration (IAC) have shown encouraging results. Nevertheless, further investigation is required to verify the effects of applying this method. This research investigates the perceptions of academic master's and doctoral program students and professionals involved in projects that applied the AR method as a strategy to foster IAC. This article replicates a case study with different projects that conducted an AR in software companies. This study indicated high satisfaction among students (83\%) when using action research in the course. All students considered the practical knowledge very relevant and would like to use the method again in other opportunities throughout their academic and professional life. This investigation showed that conducting IAC projects using the AR method within the industry in an educational context was challenging. That occurred due to the lack of experience in using empirical methods. Also, the professional's unavailability delayed the results and, consequently, the activities in the project that already had a very tight schedule. © 2023 ITMA.},
	author = {Dallegrave, Tamara and Santos, Wylliams Barbosa},
	year = {2023},
	keywords = {Industry-academia collaboration, Knowledge management, Students, Power, Research method, Replication study, Replication, Academic community, Action research, Collaborative practice research, Collaborative practices, Practice researches},
}

@article{sadeghiani_sayings_2023,
	title = {Sayings and doings become ‘practice’ through ‘practice thirdness’: pivot in recipes for practice},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162251678&doi=10.1080%2f08985626.2023.2225044&partnerID=40&md5=057becbb87581d7172bf60a6275f5339},
	abstract = {The abductive logic behind the practice lens allows practice researchers to contextualize theorizing and emphasize non-generalizability of their findings. However, scholars are critical of this non-generalizability flaw. In this conceptual paper, we aim to go beyond such criticisms and constructively discuss how this flaw might be resolved. In doing so, we theorize ‘practice thirdness’ as the shared understanding of knowing how to do practice, at local and universal levels, and provide a framework for discussing the generalizability of practice. We take ‘pivot’, at the heart of the Lean Startup as our case, and based on different interpretations of this practice, we argue what entrepreneurs have said and what scholars have interpreted of what entrepreneurs have said do not show what they have actually done. Therefore, despite the formation of practice local thirdness, i.e. practice thirdness in a particular context, in the case of pivot, still, we need academic conversation to reach practice universal thirdness, i.e. practice thirdness across different contexts. We suggest that practice researchers take a neopragmatic lens for studying practice patterns across different contexts. Also, we argue why practice researchers should be open to other methods besides the commonly recommended (non)participant observation. Moreover, we propose a model for communicating and generalizing practice based on Peirce’s triadic model of semiosis and Nonaka and Takeuchi’s model of knowledge management. © 2023 Informa UK Limited, trading as Taylor \& Francis Group.},
	author = {Sadeghiani, Ayoob and Anderson, Alistair and Ahmadi, Sadra and Shokouhyar, Sajjad and Hajipour, Bahman},
	year = {2023},
	keywords = {knowledge, modeling, research work, theoretical study},
}

@article{saarikallio_quality_2023,
	title = {Quality culture boosts agile transformation—{Action} research in a business-to-business software business},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137236655&doi=10.1002%2fsmr.2504&partnerID=40&md5=f87d77293e772685a8659d46dc1dbc29},
	abstract = {Agile methodologies are sometimes adopted, with the assumption that benefits can be attained by only using a set of best practices, which can sometimes work to a degree. In this paper, a case is discussed where a software-producing organization of seven teams achieved significant improvements. The goal of the research was to answer two questions: how an already agile organization could improve its performance further and what is the impact of promoting quality aspects? The questions were answered by implementing interventions based on prior literature and data emerging from semi-structured interviews. The context was an established business with a complex revenue stream structure, meaning the mix of various project/service/product based work rendered the adoption of agile methods a challenge. Action research comprising three rounds of interventions was conducted to improve the organization and its quality culture while enforcing code review practices. Interventions resulted in a significant improvement in quality, as measured by reported defects. Therefore, it is suggested that agile methods are not sufficient on their own to take software business forward unless a quality-focused culture is simultaneously achieved through a mindset change and organizational structures to enforce quality practices. The paper contributes to research on the managerial practices of software business and agile transformation by providing empirical support to introducing formal quality improvement to the agile mix as a method for practitioners to improve organizations with complex business models and multiple teams. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Saarikallio, Matti and Tyrväinen, Pasi},
	year = {2023},
	keywords = {Computer software, Human resource management, Empirical, Agile adoptions, B2B, Business models, Development method, Hybrid development method, Increment planning event, Mixed business model, Quality, Revenue streams, Scaled agile, Team coordination},
}

@article{nikiforova_towards_2023,
	title = {Towards {High}-{Value} {Datasets} {Determination} for {Data}-{Driven} {Development}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172028920&doi=10.1007%2f978-3-031-41138-0_14&partnerID=40&md5=568da301604faa0be07050ee6fd28679},
	abstract = {Open government data (OGD) is seen as a political and socio-economic phenomenon that promises to promote civic engagement and stimulate public sector innovations in various areas of public life. To bring the expected benefits, data must be reused and transformed into value-added products or services. This, in turn, sets another precondition for data that are expected to not only be available and comply with open data principles, but also be of value, i.e., of interest for reuse by the end-user. This refers to the notion of “high-value dataset” (HVD), recognized by the European Data Portal as a key trend in the OGD area in 2022. While there is a progress in this direction, e.g., the Open Data Directive, incl. identifying 6 key categories, a list of HVDs and arrangements for their publication and re-use, they can be seen as “core”/“base” datasets aimed at increasing interoperability of public sector data with a high priority, contributing to the development of a more mature OGD initiative. Depending on the specifics of a region and country - geographical location, social, environmental, economic issues, cultural characteristics, (under)developed sectors and market specificities, more datasets can be recognized as of high value for a particular country. However, there is no standardized approach to assist chief data officers in this, and there is a clear lack of conceptualizations for the determination of HVD and systematic oversight. In this paper, we present a systematic review of existing literature on the HVD determination, which is expected to form an initial knowledge base for this process, including used approaches and indicators to determine them, data, stakeholders. © 2023, IFIP International Federation for Information Processing.},
	author = {Nikiforova, Anastasija and Rizun, Nina and Ciesielska, Magdalena and Alexopoulos, Charalampos and Miletić, Andrea},
	year = {2023},
	keywords = {Data driven, Open Data, Open data ecosystem, Open datum, Open government data, Public values, Knowledge based systems, Public administration, High-value data, Public sector, Stakeholder, Value data},
}

@article{gabriele_human-car_2023,
	title = {Human-{Car} {Interface}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138724816&doi=10.1007%2f978-3-031-12547-8_50&partnerID=40&md5=3637be87138725a152cd4fa146cc5323},
	abstract = {A systematic literature review, or SLR, seeks to structure the review carried out in the defined areas in a replicable and auditable method, in order to facilitate and objectify both the search for answers to research questions and their accessibility by peers. In this study, we present an SLR carried out in November 2021 by the PRISMA method, on interaction and interface design focused on the automotive User Experience, having these three research questions: (RQ1) What are the objects of study of the articles? (RQ2) Which methods are used to analyze the object of study? (RQ3) What are the samples size of the surveys carried out? At the end of the Screening, 20 articles were selected to answer the research questions, and some data deserve attention, such as the 60\% that didn't identify the use of UX assessment questionnaires or the 35\% that had incomplete demographic data. We also saw that the objects of study are concentrated in 3 major areas and that the methodology used is, for the most part, similar in structure. The lack of studies carried out in South America prompted us to develop a research project focused on the Brazilian User. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Gabriele, Felipe and Martins, Laura},
	year = {2023},
}

@article{asdecker_dirty_2023,
	title = {A {Dirty} {Little} {Secret}? {Conducting} a {Systematic} {Literature} {Review} {Regarding} {Overstocks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165980153&doi=10.1007%2f978-3-031-38145-4_13&partnerID=40&md5=9d35ba41edd77488927f63ed1bf14f32},
	abstract = {Due to numerous media reports, overstocks in supply chains have recently attracted attention alongside the public sustainability debate. The goal of this paper is to aggregate the current body of knowledge and develop a better understanding regarding (1) the quantification of overstocks (what?), (2) the management approaches used (how?), and (3) the motives of managing overstocks (why?). The review synthesizes 48 relevant publications that were systematically gathered from three of the leading scientific databases. Based on the results of the review, a research agenda is derived that identifies ten particularly promising avenues for future investigations. Furthermore, the review shows that the existing knowledge about overstocks and the way they are managed is not only limited, but also very fragmented. A holistic perspective is missing, which motivates this paper to call for a conceptualization in the sense of an “overstock management” function. To initiate this process, a definition of the term is proposed. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Asdecker, Björn and Tscherner, Manette and Kurringer, Nikolas and Felch, Vanessa},
	year = {2023},
}

@article{marques_gamification_2023,
	title = {Gamification for agile: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161850209&doi=10.1504%2fIJASM.2023.130838&partnerID=40&md5=5bd2dad7d6ee9d8c774d0e1ed8fe714e},
	abstract = {Gamification has been used in software engineering to motivate practitioners to adopt agile. This study assesses the state of the art regarding the use of gamification in agile projects. A systematic literature review was followed by searching for peer-reviewed papers and dissertations on the topic and assessing their quality. Overall, 225 studies were found, but only 12 selected. Most studies focused on the Scrum framework, and the completion of stories/tasks was the practice subject to gamification more times. While the impact of gamification initiatives was positive, these studies lacked a proper empirical validation of the proposed gamification solutions. Despite the novelty of this field, there seems to be potential in the use of gamification to improve agile projects, but future studies should address the gaps identified in this analysis and provide more detail when reporting their results, namely regarding the discussion of the impact, benefits, and challenges of gamification. Copyright © 2023 Inderscience Enterprises Ltd.},
	author = {Marques, Rita and da Silva, Miguel Mira and Gonçalves, Daniel},
	year = {2023},
}

@article{abdullah_controlling_2023,
	title = {Controlling {Automatic} {Experiment}-{Driven} {Systems} {Using} {Statistics} and {Machine} {Learning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186763994&doi=10.1007%2f978-3-031-36889-9_9&partnerID=40&md5=9e42d9434fbe1e6f36858bb2983e5e75},
	abstract = {Experiments are used in many modern systems to optimize their operation. Such experiment-driven systems are used in various fields, such as web-based systems, smart-* systems, and various selfadaptive systems. There is a class of these systems that derive their data from running simulations or another type of computation, such as in digital twins, online planning using probabilistic model-checking, or performance benchmarking. To obtain statistically significant results, these systems must repeat the experiments multiple times. As a result, they consume extensive computation resources. The GraalVM benchmarking project detects performance changes in the GraalVM compiler. However, the benchmarking project has an extensive usage of computational resources and time. The doctoral research project proposed in this paper focuses on controlling the experiments with the goal of reducing computation costs. The plan is to use statistical and machine learning approaches to predict the outcomes of experiments and select the experiments yielding more useful information. As an evaluation, we are applying these methods to the GraalVM benchmarking project; the initial results confirm that these methods have the potential to significantly reduce computation costs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
	author = {Abdullah, Milad},
	year = {2023},
	keywords = {Model checking, Machine learning, Machine-learning, Online systems, Cost reduction, Benchmarking, Computation costs, Driven system, Experiment-driven system, On-line planning, Running simulations, Self-adaptive system, Smart System, Statistics learning, Web-based system},
}

@inproceedings{southier_systematic_2023,
	title = {Systematic {Mapping} {Review} on {Log} {Preparation} for {Process} {Mining}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164696308&doi=10.1109%2fCSCWD57460.2023.10152649&partnerID=40&md5=fff1faec33e5ffca044fa44095cb3bfa},
	abstract = {Process Mining (PM) is a research discipline that helps organizations track and optimize processes to support their business. Further, it focuses on providing process analysis techniques and tools, and several of its applications have been described in the literature. The start point for PM is using event logs generated by information systems to analyze processes. These event logs need to be extracted from databases and prepared for use because the quality of the event logs used as input is critical to the success of any PM effort. In this article, we present a systematic mapping review to provide the reader with highlights of the state-of-the-art techniques for event log preparation. Based on the retrieved studies, we identified six main categories of log preparation techniques: extraction, cleaning, repair, non-adequate granularity, quality evaluation, and privacy. The results are explored quantitatively and qualitatively. All results are made available through spreadsheets and charts. We believe this paper is a starting point for researchers to identify the studies that would help them prepare event logs for PM. © 2023 IEEE.},
	author = {Southier, Luiz Fernando Puttow and De Freitas, Sheila Cristiana and Pizzini, Adriano and Santos, Eduardo Alves Portela and Scalabrin, Edson Emilio},
	year = {2023},
	keywords = {Information systems, Information use, Mapping, It focus, Systematic mapping, Quality control, Data mining, Business Process, Business process management, Enterprise resource management, Event logs, Log preparation, Log preprocessing, Process analysis tool, Process management, Process mining},
}

@inproceedings{jena_systematic_2023,
	title = {Systematic {Literature} {Review} on {Object} {Oriented} {Software} {Testing} {Techniques}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159096103&doi=10.1109%2fICIDCA56705.2023.10100236&partnerID=40&md5=bec544b3c28fab8be6514087f8c26751},
	abstract = {The software industry is quickly adopting the object-oriented paradigm. OO is now widely recognized as the ideal paradigm for designing complex systems. Since flaws might be added over the course of the software development process, the finished result requires to be tested. Many software testing approaches available to test object-oriented software are thoroughly surveyed in this study. Because of principles like inheritance, polymorphism, and others, software built using object-oriented technology presents testing issues. It is crucial to trace where each object is formed and where that definition is referenced to monitor how each object behaves over its lifetime. This means that the capacity of test cases to uncover flaws is crucial to testing. Although the creation of test cases is essential for the testing process therefore it is the primary focus for study in the analysis of software testing. The entire process of thoroughly analyzing and categorizing primary studies took four months. The test cases' efficacy guarantees the system's quality while lowering the risk of system failure. Although there are several methods to approach this problem, it is obvious that a full study of the issue and a step-by-step resolution are needed. For various strategies utilized in the OO system, authors have examined and reviewed several articles. A complete study of the various OO-based Testing Techniques has been examined and reviewed by this methodical review of the literature. The recommendations made in the review can be used by future scholars to close the research gaps. It was discovered from this review that using the right software testing methods can lower the likelihood of system failure. © 2023 IEEE.},
	author = {Jena, Divya and Kumari, Ankita and Titoria, Jhanvi and {Ankita} and Rathee, Nisha and Kumar, Brijesh},
	year = {2023},
	keywords = {Systematic literature review, Software design, Software testing, Software testings, Object oriented programming, Test case, Software industry, Biomimetics, Class testing, Nature inspired technique, Object-Oriented Software Testing, Software testing techniques, System failures, Testing technique},
}

@article{iftikhar_catalog_2023,
	title = {A {Catalog} of {Source} {Code} {Metrics} – {A} {Tertiary} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161231906&doi=10.1007%2f978-3-031-31488-9_5&partnerID=40&md5=e8e0766c41b52cfff333dbcc80900e58},
	abstract = {Context: A large number of source code metrics are reported in the literature. It is necessary to systematically collect, describe and classify source code metrics to support research and practice. Objective: We aim to utilize existing secondary studies to develop a catalog of source code metrics together with their descriptions. The catalog will also provide information about which units of code (e.g., operators, operands, lines of code, variables, parameters, code blocks, or functions) are used to measure the internal quality attributes and the scope on which they are collected. Method: We conducted a tertiary study to identify secondary studies reporting source code metrics. We have classified the source code metrics according to the measured internal quality attributes, the units of code used in the measures, and the scope at which the source code metrics are collected. Results: From 711 secondary studies, we identified 52 relevant secondary studies. We reported 423 source code metrics together with their descriptions and the internal quality attributes they measure. Source code metrics predominantly incorporate function as a unit of code to measure internal quality attributes. In contrast, several source code metrics use more than one unit of code when measuring internal quality attributes. Nearly 51\% of the source code metrics are collected at the class scope, while almost 12\% and 15\% of source code metrics are collected at module and application levels, respectively. Conclusions: Researchers and practitioners can use the extensive catalog to assess which source code metrics meet their individual needs based on the description and classification scheme presented. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Iftikhar, Umar and Ali, Nauman Bin and Börstler, Jürgen and Usman, Muhammad},
	year = {2023},
	keywords = {Tertiary study, Codes (symbols), Code quality, Computer programming languages, Internal quality, Quality attributes, Source code metrics, Code measurement, Internal quality attribute, Line of codes, Number of sources, Variable-parameters},
}

@inproceedings{adinegoro_comparison_2023,
	title = {Comparison of {UI}/{UX} {Development} {Using} {Design} {Thinking} vs {Lean} {UX} : {A} {Comparative} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181532115&doi=10.1109%2fICE3IS59323.2023.10335225&partnerID=40&md5=4108ff0d37fc6ba7cb414bfaa977fe1c},
	abstract = {The User Experience (UX) development method plays an important role in ensuring the successful design of the user experience of an application that is effective, satisfying and meets user needs. This study compares two popular methods that are often used in developing UX, namely Design Thinking and Lean UX. The purpose of this study is to evaluate the advantages, disadvantages and focus of each method in developing UX designs. The results of this study indicate that the Design Thinking method focuses more on understanding the user or is more user-centered, while Lean UX focuses more on flexibility and fast iteration as well as continuous experimentation or testing. Based on this, it is interesting to further investigate how a team determines which method to use in their UX design project, considering that both methods have their own strengths and weaknesses, and there are various factors that influence the UX design development stage, such as resources, time, project scale, and others. © 2023 IEEE.},
	author = {Adinegoro, Rafi and Suakanto, Sinung and Fakhrurroja, Hanif and Hardiyanti, Margareta},
	year = {2023},
	keywords = {Design, User interfaces, Users' experiences, Iterative methods, Design thinking, Development method, Comparatives studies, Design development, Design programs, Development stages, Lean UX, User experience, User need, User-centred, Well testing},
}

@article{li_reproducible_2023,
	title = {Reproducible {Searches} in {Systematic} {Reviews}: {An} {Evaluation} and {Guidelines}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165868236&doi=10.1109%2fACCESS.2023.3299211&partnerID=40&md5=4c299fd1736b7dade073a0f83544d7d7},
	abstract = {[Context:] The Systematic Review is promoted as a more reliable way of producing a high-quality review of prior research. But there are a range of threats that can undermine the reliability and quality of such reviews. One threat is the reproducibility of automated searches. [Objectives:] To evaluate the state-of-practice of reproducible searches in secondary studies, and to consider ways to improve the reproducibility of searches. [Method:] We re-run the searches of 621 secondary studies and analyse the outcomes of those (attempted) re-runs. We use the outcomes, and our experience of re-running the searches, to propose ways to improve the reproducibility of automated searches. [Results:] With the 621 studies, more than 50\% of the literal search strings (ignoring other settings) are not reusable; about 87\% of the searches (e.g., with settings) cannot be repeated; and around 94\% of the searches (including all elements of the search) are irreproducible. We propose guidelines for automated search, directing particular attention at the formulation of search strings. [Conclusion:] While some aspects of automated search are beyond the direct control of researchers (e.g., variations in features, constraints and performance of search engines), many aspects can be effectively managed through more careful formulation and execution of the search strings themselves, and of the search settings. While the results of our evaluation are disappointing there are many simple, concrete steps that researchers can make to improve the reproducibility of their searches. © 2013 IEEE.},
	author = {Li, Zheng and Rainer, Austen},
	year = {2023},
	keywords = {Systematic Review, Search engines, Evidence Based Software Engineering, Automation, Systematic, Quality control, Guideline, Software reliability, Reproducibilities, Automated searches, Reproducibility of result, Search problem, Secondary study},
}

@article{silva_systematic_2023,
	title = {Systematic {Literature} {Review} of the {Use} of {Virtual} {Reality} in the {Inclusion} of {Children} with {Autism} {Spectrum} {Disorders} ({ASD})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172211707&doi=10.1007%2f978-3-031-40113-8_49&partnerID=40&md5=25bb4d5c9139fb13f06871578bff30cf},
	abstract = {Virtual reality (VR) technologies have been evolving in recent decades, allowing simulating real-life situations in controlled and safe virtual environments, where they reveal increasingly realistic details. There is an increase in the number of publications on virtual reality interventions in different areas, especially in Education, particularly in interventions with children diagnosed with Autism Spectrum Disorders (ASD). The lack of social skills prevents these children diagnosed with ASD to respond appropriately and adapt to the most diverse daily social situations. On this basis, VR has revealed a set of evidences that present promising results and show great acceptance among the diversified population with ASD. In order to understand how VR may contribute to the improvement of skills, allowing their inclusion, we conducted a systematic review of the literature. We present considerations on the selected studies, identifying the main gaps and pointing out possible directions for future research. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Silva, Rui Manuel and Carvalho, Diana and Martins, Paulo and Rocha, Tânia},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, E-learning, Diseases, Virtual reality, Autism spectrum disorders, Children with autisms, Main Gap, Social skills, Virtual reality technology},
}

@article{liu_information_2023,
	title = {Information quality of conversational agents in healthcare},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159121148&doi=10.1177%2f02666669231172434&partnerID=40&md5=0f4aa9701b5b8b1e355e46fa0ce52f4a},
	abstract = {Artificial Intelligence has found applications in a wide range of fields, including conversational agents designed for healthcare services. The quality of healthcare services greatly depends on the quality of the information provided by the agents. Achieving quality-assured information from conversational agents to support effective decision-making remains as a significant challenge in healthcare. Although prior review studies have shown an interest in investigating the information quality (IQ) of conversational agents in healthcare, no systematic review has been performed to present IQ definitions, factors influencing IQ, and IQ impacts. We conducted a systematic review of 45 articles published up to 2021 to investigate IQ definitions, factors influencing IQ, and IQ impacts in the context of conversational agents applied in healthcare. The findings of this review are integrated into a conceptual framework for the IQ research program in the context of conversational agents in healthcare, which has not been received attention in the literature, guiding future research directions. The present study also discusses implications for both researchers and practitioners to enhance the agents’ IQ and improve the quality of health-related services. © The Author(s) 2023.},
	author = {Liu, Caihua and Zowghi, Didar and Peng, Guochao and Kong, Shufeng},
	year = {2023},
}

@inproceedings{de_souza_using_2023,
	title = {Using {Experimentation} to {Evaluate} {Security} {Requirements} in {IoT} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184815056&doi=10.1109%2fSIoT60039.2023.10390013&partnerID=40&md5=470cbe05f85e03a3fc7e6a659e834c21},
	abstract = {Security requirements are critical success factors for Internet of Things (IoT) software systems due to how they can mitigate vulnerabilities, for instance, prevent unauthorized access to system and device data by third parties, assuring the final quality of the software system. Then, problems related to security requirements and vulnerabilities must be addressed in the early stage of IoT development projects. In this way, Continuous Experimentation (CE) is a promising software construction practice to observe alternative security and vulnerability solutions. Thus, this paper evaluates security requirements based on the vulnerabilities of IoT software systems using such CE. First, we identified an evidence-based set of IoT vulnerability issues to be assessed. Thus, this work reports an exploratory study using CE to mitigate some vulnerabilities in IoT software systems, indicating that not all security requirements can be worked out with CE. Therefore, further studies are necessary to categorize the IoT software systems vulnerabilities that can be mitigated using continuous experimentation. © 2023 IEEE.},
	author = {De Souza, Bruno Pedraca and De Paiva, Bruno Dantas and Travassos, Guilherme Horta},
	year = {2023},
	keywords = {Computer software, Software-systems, Internet of things, Empirical Software Engineering, Cryptography, Development programmes, Device data, IS critical success factors, Security requirements, Security vulnerabilities, Third parties, Unauthorized access, Vulnerability},
}

@article{buchgeher_using_2023,
	title = {Using {Architecture} {Decision} {Records} in {Open} {Source} {Projects} - {An} {MSR} {Study} on {GitHub}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162914500&doi=10.1109%2fACCESS.2023.3287654&partnerID=40&md5=04585b93cdd9a9d4124142acf3977fb7},
	abstract = {Architecture decision records (ADRs) have been proposed as a resource-efficient means for capturing architectural design decisions (ADDs), and have received attention not only from researchers but also from practitioners. We conducted a mining software repositories (MSR) study, in which we analyzed the use of ADRs in open source repositories at GitHub. Our results show that the adoption of ADRs is still low, although the number of repositories using ADRs is increasing every year. About 50\% of all repositories with ADRs contain just one to five ADRs suggesting that the concept has been tried but not yet definitively adopted. In repositories that use ADRs more systematically, we observed that recording decisions is a team activity conducted by two or more users over a longer period of time. In most repositories the template proposed by Michael Nygrad is used. We, finally, provide an interpretation of the obtained results and discuss open future research challenges by elaborating on implications of the study's findings as well as on recommendations on how to further increase the adoption of ADRs. © 2013 IEEE.},
	author = {Buchgeher, Georg and Schoberl, Stefan and Geist, Verena and Dorninger, Bernhard and Haindl, Philipp and Weinreich, Rainer},
	year = {2023},
	keywords = {Decision making, Software design, Open source software, Knowledge management, Software architecture, Open systems, Open-source softwares, Mining software, Mining software repository, Software development management, Software repositories, Secondary study, Architecture decision record, Architecture decisions, Github, Open source projects, Software architecture knowledge managements},
}

@inproceedings{alshahwan_software_2023,
	title = {Software {Testing} {Research} {Challenges}: {An} {Industrial} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161898000&doi=10.1109%2fICST57152.2023.00008&partnerID=40&md5=1d3e066d49cf0d1adbddebd59727ba72},
	abstract = {There have been rapid recent developments in automated software test design, repair and program improvement. Advances in artificial intelligence also have great potential impact to tackle software testing research problems. In this paper we highlight open research problems and challenges from an industrial perspective. This perspective draws on our experience at Meta Platforms, which has been actively involved in software testing research and development for approximately a decade. As we set out here, there are many exciting opportunities for software testing research to achieve the widest and deepest impact on software practice. With this overview of the research landscape from an industrial perspective, we aim to stimulate further interest in the deployment of software testing research. We hope to be able to collaborate with the scientific community on some of these research challenges. © 2023 IEEE.},
	author = {Alshahwan, Nadia and Harman, Mark and Marginean, Alexandru},
	year = {2023},
	keywords = {Artificial intelligence, Software testing, Automation, Repair, Industrial research, Software testings, Automated program repair, Automated remediation, Automated software engineering, Genetic improvements, Research challenges, Research problems, Test designs, Test projects, Test repair},
}

@inproceedings{karras_divide_2023,
	title = {Divide and {Conquer} the {EmpiRE}: {A} {Community}-{Maintainable} {Knowledge} {Graph} of {Empirical} {Research} in {Requirements} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666312&doi=10.1109%2fESEM56168.2023.10304795&partnerID=40&md5=89b0f483a3169d34b96c76308bbd4fe1},
	abstract = {[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews. © 2023 IEEE.},
	author = {Karras, Oliver and Wernlein, Felix and Klunder, Jil and Auer, Soren},
	year = {2023},
	keywords = {Literature reviews, Requirement engineering, Requirements engineering, 'current, Empirical research, Research communities, Divide-and-conquer, Infrastructure, Knowledge graph, Knowledge graphs, Long-term goals, Technical infrastructure},
}

@inproceedings{olsson_all_2023,
	title = {All data is equal or is some data more equal? {On} strategic data collection and use in the embedded systems domain},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326861&doi=10.1109%2fSEAA60479.2023.00056&partnerID=40&md5=0daed7f6ec9ae9aa3afd9e4c555cb505},
	abstract = {Effective collection and use of data is key for companies across domains and it is only increasing in importance. For companies in the embedded systems domain, data constitutes the basis not only for quality assurance and diagnostics of their systems but also for new service development and innovation. For these companies, data is an enabler for continuous delivery of customer value and hence, a key asset for entirely new and recurring revenue streams. However, effective use of data requires careful collection of different kinds of data depending on the purpose and context for which it is intended to be used. In this paper, we identify the challenges that companies experience in their contemporary data practices and we outline the kinds of data that companies need to collect as they evolve through different maturity stages. In addition, we provide concrete guidance on the specific data to collect during each maturity stage. © 2023 IEEE.},
	author = {Olsson, Helena Holmstrom and Bosch, Jan},
	year = {2023},
	keywords = {Embedded systems, Embedded-system, Quality assurance, Data acquisition, Data collection, Data collectio, Data exploitatio, Data practices, Maturity stages, New service development, Service innovation, Strategic data, System domain},
}

@article{suzanna_continuous_2023,
	title = {Continuous {Software} {Engineering} for {Augmented} {Reality}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799327&doi=10.14569%2fIJACSA.2023.0140719&partnerID=40&md5=783a913267516c101f5c106d990de433},
	abstract = {Continuous software engineering is a new trend that has attracted increasing attention from the research community in recent years. In software engineering there are “continuous” stages that are used depending on the number of artifact repositories such as databases, meta data, virtual machines, networks and servers, various logs, and reports. Augmented Reality (AR) technology is currently growing rapidly. We can find this technology in various fields of life, but unfortunately sustainable software engineering for Augmented Reality is not found. The method shown in previous research is a general method in software engineering so that a theory is needed for sustainable software engineering for AR considering that AR is not just an ordinary application but there are 3D elements and specific components that must be met so that it can be called AR. The main idea behind this research is to find a continuous pattern from the stages of the existing method so far. For example, in general the stages of system development are planning, analysis, design, implementation and maintenance. Then after the application has been built, does it finish there? As we know software always grows and develops according to human needs. Therefore, there are continuous stages that must be patterned so that the life cycle process can be maintained. In this paper we present our initial findings about the continuous stages of continuous software engineering namely continuous planning, continuous analysis, continuous design, continuous programming, continuous integration, and continuous maintenance. © 2023, Science and Information Organization. All Rights Reserved.},
	author = {{Suzanna} and {Sasmoko} and Gaol, Ford Lumban and Oktavia, Tanty},
	year = {2023},
	keywords = {Life cycle, Application programs, Augmented reality, Continuous integrations, Continuous software engineerings, Research communities, Computer programming, Continuous analysis, Continuous design, Continuous maintenance, Continuous planning, Continuous programming, Maintenance, Method in software engineering, Sustainable softwares},
}

@article{stradowski_exploring_2023,
	title = {Exploring the challenges in software testing of the {5G} system at {Nokia}: {A} survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138479890&doi=10.1016%2fj.infsof.2022.107067&partnerID=40&md5=0095b98ee47f2ca332b69f85235a5d54},
	abstract = {Context: The ever-growing size and complexity of industrial software products pose significant quality assurance challenges to engineering researchers and practitioners, despite the constant effort to increase knowledge and improve the processes. 5G technology developed by Nokia is one example of such a grand and highly complex system with improvement potential. Objective: The following paper provides an overview of the current quality assurance processes used by Nokia to develop the 5G technology and provides insight into the most prominent challenges by an evaluation of perceived importance, urgency, and difficulty to understand the future opportunities. Method: Nokia mode of operation, briefly introduced in this paper, has been subjected to extensive analysis by a selected group of experienced test-oriented professionals to define the most critical areas of concern. Secondly, the identified problems were evaluated by Nokia gNB system-level test professionals in a dedicated survey. Results: The questionnaire was completed by 312 out of 2935 (10.63\%) possible respondents. The challenges are seen as the most important and urgent: customer scenario testing, performance testing, and competence ramp-up. Challenges seen as the most difficult to solve are low occurrence failures, hidden feature dependencies, and hardware configuration-specific problems. Conclusions: Our research identified several improvement areas in the quality assurance processes used to develop the 5G technology by determining the most important and urgent problems that at the same time have a low perceived difficulty. Such initiatives are attractive from a business perspective. On the other hand, challenges seen as the most impactful yet difficult may be of interest to the academic research community. © 2022 The Author(s)},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	keywords = {Software testing, Computer software selection and evaluation, Quality control, 5G mobile communication systems, Surveys, Quality assurance, \% reductions, 5g technology, Efficiency improvement, Engineering challenges, Quality assurance process, Software engineering challenge, Software quality assurance, System level testing, Test effort reduction, Test efforts},
}

@book{scutari_pragmatic_2023,
	title = {The {Pragmatic} {Programmer} for {Machine} {Learning}: {Engineering} {Analytics} and {Data} {Science} {Solutions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183218511&doi=10.1201%2f9780429292835&partnerID=40&md5=8d4692d72420d4b30ed26d6a7d295edb},
	abstract = {Machine learning has redefined the way we work with data and is increasingly becoming an indispensable part of everyday life. The Pragmatic Programmer for Machine Learning: Engineering Analytics and Data Science Solutions discusses how modern software engineering practices are part of this revolution both conceptually and in practical applictions. Comprising a broad overview of how to design machine learning pipelines as well as the state-of-the-art tools we use to make them, this book provides a multi-disciplinary view of how traditional software engineering can be adapted to and integrated with the workflows of domain experts and probabilistic models. From choosing the right hardware to designing effective pipelines architectures and adopting software development best practices, this guide will appeal to machine learning and data science specialists, whilst also laying out key high-level principlesin a way that is approachable for students of computer science and aspiring programmers. © 2023 Marco Scutari and Mauro Malvestio.},
	author = {Scutari, Marco and Malvestio, Mauro},
	year = {2023},
}

@inproceedings{denecke_investigating_2023,
	title = {Investigating conversational agents in healthcare: {Application} of a technical-oriented taxonomy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164261105&doi=10.1016%2fj.procs.2023.01.413&partnerID=40&md5=1ec35be7bce132042a4312cfb84fbe07},
	abstract = {Conversational agents (CA) are increasingly applied to realize health applications that collect patient data, provide information or even deliver health interventions. We developed a taxonomy focusing on technical characteristics of health CA with the purpose of creating a reporting guideline towards health CA and of building technical-oriented archetypes. The taxonomy comprises 18 dimensions which can be grouped into four perspectives. In this work, we wanted to find out whether the taxonomy is complete and can be applied appropriately by researcher to describe the technical characteristics of their health CA. Through a literature review, we identified 103 unique health CA for which publications have been published in 2021 and 2022. We contacted the corresponding or first authors of those papers asking for providing the information along our taxonomy for the CA described in their paper. For this purpose, our taxonomy was transformed into a questionnaire. To study applicability and understandability of the taxonomy, we also extracted the requested information from the papers using the taxonomy and compared the results to those of the participants. 95 E-Mails could be delivered. 26 persons out of 95 replied to our request resulting in a return rate of 27.3\%. Results show that the majority of CA is simple in terms of CA personality; visualized as avatar or without embodiment. Systems are mainly rule-based, domain-specific and support one language. We recognized several differences between replies given by the participants and what has been extracted from the publications on the CA by us. We conclude that in order to apply the taxonomy as reporting guideline clear definitions must be given for the single characteristics. Some additional characteristics have to be added. © 2023 Elsevier B.V.. All rights reserved.},
	author = {Denecke, Kerstin and May, Richard},
	year = {2023},
	keywords = {Literature reviews, Taxonomies, Chatbots, Understandability, Conversational agents, Health care application, Health chatbot, Health interventions, Hospital data processing, Patient data, Patient treatment, Simple++, Technology},
}

@article{lagos_electric_2023,
	title = {Electric {Vehicles} and the {Use} of {Demand} {Projection} {Models}: {A} {Systematic} {Mapping} of {Studies}; [{Vehículos} eléctricos y el uso de modelos de proyección de demanda: un mapeo sistemático de estudios]},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145909076&doi=10.15446%2fing.investig.99251&partnerID=40&md5=185c4ab25d57790fba1e7cc50593cd3d},
	abstract = {In today’s world, electric vehicles have become a real solution to the problem of pollution caused by petrol and diesel-powered vehicles. However, incorporating them successfully into the global vehicle park poses new challenges. Some of these challenges have to do with meeting the electricity demand, providing the physical installations for charging, and the size and capacity of the electric grid required to deliver the necessary supply. Solving these new problems requires determining or projecting the electrical and/or physical requirements involved, but there is no single model or methodology to do this, nor any single document which summarizes the existing information. To address this situation, this work presents the result of a systematic mapping study that seeks to provide organized information about the (mathematical) models for the demand arising from electric vehicles, as well as to answer a series of questions posed for this research. The results obtained show that there is a wide variety of models used to determine demand requirements –of either physical or electrical elements– in which mathematical modelling and operations research tools are normally used. Other results indicate that demand models are mainly focused on the electrical requirements rather than on physical ones, and that, in most cases, the type of vehicle for which the demand is studied is not mentioned. © Universidad Nacional de Colombia.},
	author = {Lagos, Dafne and Mancilla, Rodrigo and Reinecke, Carolina and Leal, Paola},
	year = {2023},
}

@article{ciesla_analysis_2023,
	title = {{AN} {ANALYSIS} {OF} {THE} {IMPLEMENTATION} {OF} {ACCESSIBILITY} {TOOLS} {ON} {WEBSITES}; [{ANALIZA} {IMPLEMENTACJI} {NARZĘDZI} {DOSTĘPNOŚCI} {NA} {STRONACH} {WWW}]},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180501621&doi=10.35784%2fiapgos.4459&partnerID=40&md5=a147574e2d2512c8186ffed285bd14dc},
	abstract = {The websites of higher education institutions, due to the fact that they are addressed to multiple stakeholder groups, not only need to have an appropriately designed information structure but must also be useful. Additionally, in the case of public universities, their services are expected to be accessible to the widest possible audience, especially for people with disabilities. The accessibility tools used on websites should be quickly located, easily identifiable and user-friendly. So far, no standards have been developed regarding these issues, and therefore, there are various solutions on the web. The objective of this study is to analyze various implementations of accessibility tools on university websites in terms of their location, form of presentation and ways that enable access to them. A study was conducted in which web interfaces were evaluated with the participation of users. The experiment consisted of two parts: the first one used the eye tracking technique, whereas in the second one, a survey was conducted. The research material was prototypes of websites from four different universities. Each website had two versions differing in implementation of accessibility tools. In the study, 35 participants were divided into two groups of people. Each group was shown one of the two sets of website prototypes and the users were tasked with finding and activating a specific accessibility tool. After exploring the websites, each participant completed a questionnaire that pertained to their opinions regarding aspects such as appearance, placement and a way to access tools dedicated to people with disabilities. The obtained data, processed to the form of heatmaps and fixation maps, were subjected to a qualitative analysis. The survey results and eye tracking data were analyzed quantitatively. On the basis of performed analyzes it can be concluded that the following factors have an impact on the reduction in efficiency and productivity of users: placement of accessibility tools on university websites in a place other than the upper right corner, an indirect access to t hese tools or their non-standard appearance. © 2023, Politechnika Lubelska. All rights reserved.},
	author = {Cieśla, Marcin and Dzieńkowski, Mariusz},
	year = {2023},
}

@inproceedings{ansyah_usability_2023,
	title = {Usability {Testing} of {User} {Experience} and {User} {Interface} {Design} on {Mobile} {Map} {Applications}: {A} {Comparative} {Study} of {User} {Perception} and {Interaction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180361940&doi=10.1109%2fICTS58770.2023.10330882&partnerID=40&md5=76aa72c4a0d6b18a15a12a75a0cdd186},
	abstract = {Map applications play a crucial role in everyday life, assisting users in finding locations and optimizing travel routes. This study aims to evaluate the usability of map applications, focusing on UX testing using GOMS and SUS techniques and UI testing using the A/B Testing method. The results show the effectiveness of the three applications (GMaps, Petal, and Waze) in completing the given tasks. GMaps demonstrated the fastest task completion time and the highest SUS score, followed by Petal and Waze. GMaps' advantage may be due to most respondents being accustomed to using Android and the native applications of this operating system. Furthermore, A/B Testing of UI elements revealed a nearly balanced preference between GMaps and Petal, indicating Petal's potential despite not being an Android-native application. Based on these findings, the development of map applications with good usability can be achieved by combining the strengths of each tested application. © 2023 IEEE.},
	author = {Ansyah, Adi Surya Suwardi and Masruri, Muhammad Zahid and Rochimah, Siti},
	year = {2023},
	keywords = {User interfaces, Users' experiences, Usability engineering, Comparatives studies, Android (operating system), GOMS, Mobile-map application, SUS, Travel routes, Usability testing, User interaction, User interface designs, User perceptions},
}

@article{kantsepolsky_exploring_2023,
	title = {Exploring {Quantum} {Sensing} {Potential} for {Systems} {Applications}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151562593&doi=10.1109%2fACCESS.2023.3262506&partnerID=40&md5=65f96769350f4d726502220c429403ef},
	abstract = {The current rise of quantum technology is compelled by quantum sensing research. Thousands of research labs are developing and testing a broad range of sensor prototypes. However, there is a lack of knowledge about specific applications and real-world use cases where the benefits of these sensors will be most pronounced. This study presents a comprehensive review of quantum sensing state-of-practice. It also provides a detailed analysis of how quantum sensing overcomes the existing limitations of sensor-driven systems' precision and performance. Based on the review of over 500 quantum sensor prototype reports, we determined four groups of quantum sensors and discussed their readiness for commercial usage. We concluded that quantum magnetometry and quantum optics are the most advanced sensing technologies with empirically proven results. In turn, quantum timing and kinetics are still in the early stages of practical validation. In addition, we defined four systems domains in which quantum sensors offer a solution for existing limitations of conventional sensing technologies. These domains are 1) GPS-free positioning and navigating services, 2) time-based operations, 3) topological visibility, and 4) environment detection, prediction, and modeling. Finally, we discussed the current constraints of quantum sensing technologies and offered directions for future research. © 2013 IEEE.},
	author = {Kantsepolsky, Boris and Aviv, Itzhak and Weitzfeld, Roye and Bordo, Eliyahu},
	year = {2023},
	keywords = {Quantum optics, Current rise, Extraterrestrial measurements, Magnetometers, Magnetometry, Quantum information science, Quantum sensing, Quantum sensing technology, Quantum sensors, Quantum system, Sensing technology, Sensitivity, Superconducting magnets, System applications, Temperature measurement},
}

@article{danilovaite_acoustic_2023,
	title = {Acoustic {Analysis} for {Vocal} {Fold} {Assessment}—{Challenges}, {Trends}, and {Opportunities}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149948719&doi=10.1007%2f978-3-031-24453-7_8&partnerID=40&md5=f4509bb5138c74c510919fefff4baa37},
	abstract = {The goal of this study was a review of trends in non-invasive vocal fold assessment to identify the significance of acoustic analysis within the scope of proposed methods. A review protocol for selected relevant studies was developed using systematic review guidelines. A classification scheme was applied to process the selected relevant study set, data were extracted and mapped in a systematic map. A systematic map was used to synthesize data for a quantitative summary of the main research question. A tabulated summary was created to summarize supporting topics. Results show that non-invasive vocal fold assessment is influenced by general computer science trends. Machine learning techniques dominate studies and publications, i.e., 51\% of the set used at least one method to detect and classify vocal fold pathologies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Danilovaitė, Monika and Tamulevičius, Gintautas},
	year = {2023},
}

@inproceedings{pauzi_descriptive_2023,
	title = {From {Descriptive} to {Predictive}: {Forecasting} {Emerging} {Research} {Areas} in {Software} {Traceability} {Using} {NLP} from {Systematic} {Studies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160566966&doi=10.5220%2f0011964100003464&partnerID=40&md5=f46987b89f92b2b7189014ceb75f3cad},
	abstract = {Systematic literature reviews (SLRs) and systematic mapping studies (SMSs) are common studies in any discipline to describe and classify past works, and to inform a research field of potential new areas of investigation. This last task is typically achieved by observing gaps in past works, and hinting at the possibility of future research in those gaps. Using an NLP-driven methodology, this paper proposes a meta-analysis to extend current systematic methodologies of literature reviews and mapping studies. Our work leverages a Word2Vec model, pre-trained in the software engineering domain, and is combined with a time series analysis. Our aim is to forecast future trajectories of research outlined in systematic studies, rather than just describing them. Using the same dataset from our own previous mapping study, we were able to go beyond descriptively analysing the data that we gathered, or to barely 'guess' future directions. In this paper, we show how recent advancements in the field of our SMS, and the use of time series, enabled us to forecast future trends in the same field. Our proposed methodology sets a precedent for exploring the potential of language models coupled with time series in the context of systematically reviewing the literature. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	author = {Pauzi, Zaki and Capiluppi, Andrea},
	year = {2023},
	keywords = {Software engineering, Systematic Review, Mapping, Systematic mapping studies, Natural language processing systems, Natural languages, Forecasting, Mapping studies, Language processing, Natural language processing, Research areas, Engineering research, Software traceability, Systematic study, Time series analysis, Times series},
}

@inproceedings{chadbourne_applications_2023,
	title = {Applications of {Causality} and {Causal} {Inference} in {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770994&doi=10.1109%2fSERA57763.2023.10197835&partnerID=40&md5=4585f442f1486a0ef77c40af2d43d737},
	abstract = {Causal inference is a study of causal relationships between events and the statistical study of inferring these relationships through interventions and other statistical techniques. Causal reasoning is any line of work toward determining causal relationships, including causal inference. This paper explores the relationship between causal reasoning and various fields of software engineering. This paper aims to uncover which software engineering fields are currently benefiting from the study of causal inference and causal reasoning, as well as which aspects of various problems are best addressed using this methodology. With this information, this paper also aims to find future subjects and fields that would benefit from this form of reasoning and to provide that information to future researchers. This paper follows a systematic literature review, including; the formulation of a search query, inclusion and exclusion criteria of the search results, clarifying questions answered by the found literature, and synthesizing the results from the literature review. Through close examination of the 45 found papers relevant to the research questions, it was revealed that the majority of causal reasoning as related to software engineering is related to testing through root cause localization. Furthermore, most causal reasoning is done informally through an exploratory process of forming a Causality Graph as opposed to strict statistical analysis or introduction of interventions. Finally, causal reasoning is also used as a justification for many tools intended to make the software more human-readable by providing additional causal information to logging processes or modeling languages. © 2023 IEEE.},
	author = {Chadbourne, Patrick and Eisty, Nasir U.},
	year = {2023},
	keywords = {Systematic literature review, Software testing, Application programs, Modeling languages, Inclusion and exclusions, Causal inferences, Causal reasoning, Causal relationships, Causality graph, Engineering fields, Professional aspects, Search queries, Statistical study, Statistical techniques},
}

@article{pinciroli_modeling_2023,
	title = {Modeling more software performance antipatterns in cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180204421&doi=10.1007%2fs10270-023-01137-x&partnerID=40&md5=4d0a00e3145240ff506af59818085186},
	abstract = {The design of cyber-physical systems (CPS) is challenging due to the heterogeneity of software and hardware components that operate in uncertain environments (e.g., fluctuating workloads), hence they are prone to performance issues. Software performance antipatterns could be a key means to tackle this challenge since they recognize design problems that may lead to unacceptable system performance. This manuscript focuses on modeling and analyzing a variegate set of software performance antipatterns with the goal of quantifying their performance impact on CPS. Starting from the specification of eight software performance antipatterns, we build a baseline queuing network performance model that is properly extended to account for the corresponding bad practices. The approach is applied to a CPS consisting of a network of sensors and experimental results show that performance degradation can be traced back to software performance antipatterns. Sensitivity analysis investigates the peculiar characteristics of antipatterns, such as the frequency of checking the status of resources, that provides quantitative information to software designers to help them identify potential performance problems and their root causes. Quantifying the performance impact of antipatterns on CPS paves the way for future work enabling the automated refactoring of systems to remove these bad practices. © 2023, The Author(s).},
	author = {Pinciroli, Riccardo and Smith, Connie U. and Trubiani, Catia},
	year = {2023},
	keywords = {Embedded systems, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Sensitivity analysis, Performances analysis, Anti-patterns, Model-based OPC, Model-based performance analyse, Performance impact, Software modeling, Software performance, Software performance antipattern},
}

@article{zervogianni_user-based_2023,
	title = {A user-based information rating scale to evaluate the design of technology-based supports for autism},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159588062&doi=10.1007%2fs10209-023-00995-y&partnerID=40&md5=0737802f2d1107ce78721c9b82cf6935},
	abstract = {The present study aimed to merge expertise from evidence-based practice and user-centered design to develop a rating scale for considering user input and other sources of information about end-users in studies reporting on the design of technology-based support for autism. We conducted a systematic review of the relevant literature to test the reliability and validity of the scale. The scale demonstrated acceptable reliability and validity based on a randomized sample of 211 studies extracted from the output of the systematic review. The scale can help provide a more complete assessment of the quality of the design process of technology-based supports for autism and be beneficial to autistic people, their families, and related professionals in making informed decisions regarding such supports. © 2023, The Author(s).},
	author = {Zervogianni, Vanessa and Fletcher-Watson, Sue and Herrera, Gerardo and Goodwin, Matthew S. and Triquell, Elise and Pérez-Fuster, Patricia and Brosnan, Mark and Grynszpan, Ouriel},
	year = {2023},
	keywords = {Software engineering, Systematic Review, Evidence Based Software Engineering, Diseases, Evidence-based practices, Technology-based, User centered design, Autism, Digital technologies, Rating scale, Reliability and validity, Sources of informations, User input},
}

@article{hanna_web_2023,
	title = {Web applications testing techniques: a systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153523073&doi=10.1504%2fIJWET.2022.129250&partnerID=40&md5=a26478fab291606ae9d407737eef2aaa},
	abstract = {Due to the importance of web application testing techniques for detecting faults and assessing quality attributes, many research papers were published in this field. For this reason, it became essential to analyse, classify and summarise the research in the field. To achieve this goal, this research conducted a systematic mapping study on 98 research papers in the field of web applications testing published between 2008 and 2021. The results showed that the most commonly used web applications testing techniques in literature are model-based testing and security testing. Besides, the most commonly used models in model-based testing are finite-state machines. The most targeted vulnerability in security testing is SQL injection. Test automation is the most targeted testing goal in both model-based and security testing. For other web applications testing techniques, the main goals of testing were test automation, test coverage, and assessing security quality attributes. Copyright © 2022 Inderscience Enterprises Ltd.},
	author = {Hanna, Samer and Ahmad, Amro Al-Said},
	year = {2023},
	keywords = {Model checking, Software testing, Mapping, Systematic mapping studies, Automation, Model based testing, Test-coverage, Testing technique, Security testing, SMS, Test Automation, Testing purpose, Web application testing, Web application testing technique},
}

@inproceedings{sukmandhani_recent_2023,
	title = {Recent {Trends} for {Text} {Summarization} in {Scientific} {Documents}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186658985&doi=10.1109%2fICCED60214.2023.10425025&partnerID=40&md5=ee6395ea43c82ca2bb7a21bae1fc9482},
	abstract = {Text summarization is a natural language processing (NLP) technique in artificial intelligence that has been studied in recent years. Every document containing text is tested to get a good summary result. In producing a good summary, proper accuracy is required; therefore, various techniques and methods are used, and another challenge is the use of language in the documents being tested. Various types of documents have been used as research for text summarization, including scientific documents, so researchers are interested in studying them more intensely to get the state of the art in research. This research was conducted to review previous research using literature studies from various publications that published research on text summarization in scientific documents. The researcher collected data from several publishers who had published this type of research from 1988 to 2023 quarter 1 and obtained three hundred and eleven articles, then the researcher analyzed the data using the Kitchenham method to get the right literature to answer the research question. Thirty-seven articles were selected for further in-depth analysis. In addition, the researcher found some interesting findings beyond the research question, including other techniques used to solve the text summarization problem and other types of output that are not commonly produced so that it can increase knowledge in researching text summarization. © 2023 IEEE.},
	author = {Sukmandhani, Arief Agus and Arifin, Yulyani and Zarlis, Muhammad and Budiharto, Widodo},
	year = {2023},
	keywords = {State of the art, Natural language processing systems, Natural languages, Literature studies, Research questions, Recent trends, In-depth analysis, Kitchenham, Language processing techniques, Scientific documents, Text processing, Text Summarisation},
}

@inproceedings{malik_chess_2023,
	title = {{CHESS}: {A} {Framework} for {Evaluation} of {Self}-{Adaptive} {Systems} {Based} on {Chaos} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166300952&doi=10.1109%2fSEAMS59076.2023.00033&partnerID=40&md5=320addb57c1eb68c0bc46ef60c86add5},
	abstract = {There is an increasing need to assess the correct behavior of self-adaptive and self-healing systems due to their adoption in critical and highly dynamic environments. However, there is a lack of systematic evaluation methods for self-adaptive and self-healing systems. We proposed CHESS, a novel approach to address this gap by evaluating self-adaptive and self-healing systems through fault injection based on chaos engineering (CE).The artifact presented in this paper provides an extensive overview of the use of CHESS through two microservice-based case studies: a smart office case study and an existing demo application called Yelb. It comes with a managing system service, a self-monitoring service, as well as five fault injection scenarios covering infrastructure faults and functional faults. Each of these components can be easily extended or replaced to adopt the CHESS approach to a new case study, help explore its promises and limitations, and identify directions for future research. © 2023 IEEE.},
	author = {Malik, Sehrish and Naqvi, Moeen Ali and Moonen, Leon},
	year = {2023},
	keywords = {Software testing, Case-studies, Adaptive systems, Self-adaptive system, Artifact, Chaos engineerings, Dynamic environments, Evaluation, Fault injection, Resilience, Self-healing, Self-healing materials, Self-healing systems},
}

@article{nunes_public_2023,
	title = {Public {Policies} for {Renewable} {Energy}: {A} {Review} of the {Perspectives} for a {Circular} {Economy}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145769847&doi=10.3390%2fen16010485&partnerID=40&md5=69befd59d2f418f67bae9cf3954f42a8},
	abstract = {The development and implementation of public policies towards renewable energies are crucial in order to address the contemporary challenges faced by humanity. The 3Rs (reduce, reuse, and recycle), as a circular economic practice, are often cited as one of the best solutions for sustainable development. Therefore, this study analyzed public policies for renewable energy from the perspective of the circular economy. Accordingly, a systematic review of the literature was carried out with respect to the beneficiaries and convergences of circularities, with a focus on public policies for renewable energies. The sample had public policies classified into three types (distributive, redistributive, and regulatory policies). The results showed that the first studies began in 1999, with a significant increase in publications during the 2010s, in which Germany was the country with the greatest contribution. The analyses associated with space showed the countries committed to the use of renewable energies and the 3Rs of the circular economy to reduce greenhouse gas emissions. The economic analyses revealed that the circular economy for the generation of renewable energy has a positive economic return in terms of social well-being and the mitigation of environmental degradation. There is a barrier to the circular economy’s development posed by the cost of its implementation in the private sector and the resistance to raising awareness in society, requiring strong public sector engagement in decision making and the constant evaluation of public policies. It is concluded that the circular economy facilitates more efficient, productive structures and public policies, promoting alternatives for energy security and sustainability for the world energy matrix. © 2023 by the authors.},
	author = {Nunes, Anna Manuella Melo and Coelho Junior, Luiz Moreira and Abrahão, Raphael and Santos Júnior, Edvaldo Pereira and Simioni, Flávio José and Rotella Junior, Paulo and Rocha, Luiz Célio Souza},
	year = {2023},
	keywords = {Systematic Review, Decision making, Reuse, Sustainable development, Economic analysis, Bibliometric, Bio economy, Circular economy, Circularity, Classifieds, Clean energy, Energy policy, Energy security, Energy transitions, Gas emissions, Greenhouse gases, Renewable energies},
}

@article{garcia-mireles_profile_2023,
	title = {A {Profile} of {Practices} for {Reporting} {Systematic} {Reviews}: {A} {Conference} {Case}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142725070&doi=10.1007%2f978-3-031-20322-0_3&partnerID=40&md5=cf245e893be2c48982af13f33f268194},
	abstract = {Several criticisms about the quality of reporting systematic reviews (SRs) have been published and new guidelines propose to use standardized instruments to report them. To identify practices for reporting SRs, I reviewed 32 SRs published in the International Conference on Software Process Improvement (CIMPS). Well reported practices are related to the execution of the automatic database search process and the identification of selection criteria. However, issues arise related to the completeness of the search process, procedures for dealing with inconsistencies during selection, extraction, and classification of data. Besides, validity threats only are addressed by a third of SRs. As a conclusion, the identification of reporting practices can help SR authors to identify both strengths and opportunities areas for conducting and reporting SRs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {García-Mireles, Gabriel Alberto},
	year = {2023},
}

@article{fathullah_methodological_2023,
	title = {Methodological {Investigation}: {Traditional} and {Systematic} {Reviews} as {Preliminary} {Findings} for {Delphi} {Technique}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166663606&doi=10.1177%2f16094069231190747&partnerID=40&md5=d2ca2c58a0c1b475d56ad690f25c431c},
	abstract = {The Delphi method has been used as a way to reach consensus among experts established in the 1950s. The method was originally conceived to be used as a forecasting instrument for business in a mixed-method study. The Delphi method is usually conducted with elements of anonymity, iteration, controlled feedback and statistical group response. Delphi method had traditionally used expert opinions through methods such as interviews and brainstorming for the initial point of studies conducted. However, with the expanding volumes of research papers and articles in this age, can techniques such as Traditional Literature Review (TLR) and Systematic Literature Review (SLR) also be viable technique to be an initial point of studies for a Delphi study. As such this study aims 1) to examine and discover whether TLR and SLR are appropriate techniques to be used in a Delphi study and 2) construct a Delphi study process overview. This study adopts a methodology in which seven articles that have used TLR and SLR in their Delphi study will be analyzed. The results shows that TLR and SLR are appropriate techniques to be used in a Delphi study and that there are two types of processes to incorporate them into the study. We have also constructed a Delphi process overview through our analysis. We hope that the results of this study will be able help researchers who are interested in doing a Delphi study to know on what are the benefits of TLR and SLR in these studies along with how to incorporate them. © The Author(s) 2023.},
	author = {Fathullah, Muhammad Afif and Subbarao, Anusuyah and Muthaiyah, Saravanan},
	year = {2023},
}

@article{ramos_gutierrez_when_2023,
	title = {When business processes meet complex events in logistics: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139865511&doi=10.1016%2fj.compind.2022.103788&partnerID=40&md5=2a8944612f473febd716772e7f8b2474},
	abstract = {Logistics processes are attracting growing attention because of the globalisation of the market. Its growing complexity and the need for reducing costs have provoked the seek of new solutions based on the processing of the complex events that the business processes produce. Event-Driven Business Process Management (EDBPM) is a discipline that studies the integration of business processes and complex events. The analysis of the maturity level of the approaches and gaps to point out future lines of research could help not only logistics organisations, but also academia. Logistics organisation could benefit from producing more environmentally friendly and optimal solutions in transport, and academia could benefit from revealing open problems. Thus, this study aims to identify current approaches, frameworks, and tools that integrate business processes and complex events in the logistics domain. To do so, we follow a systematic approach to do a mapping study that captures and synthesises the approaches, frameworks, and tools that integrate these two fields. As a result, 10,978 articles were gathered and 169 of them were selected for extraction. We have classified the selected studies according to several criteria, including the business process life cycle in which they are being applied, the business process modelling language, and the event process modelling language, among others. Our synthesis reveals the open challenges and the most relevant frameworks and tools. However, there is no mature enough framework or tool ready to be used in companies, and a promising research must provide solutions that cover all phases in the process life cycle. © 2022 The Author(s)},
	author = {Ramos Gutiérrez, Belén and Reina Quintero, Antonia M. and Parody, Luisa and Gómez López, María Teresa},
	year = {2023},
	keywords = {Systems engineering, Systematic literature review, Life cycle, Mapping, Systematic mapping studies, Modeling languages, Business Process, Enterprise resource management, Administrative data processing, Complex event processing, Complex events, Event Processing, Event-driven, Event-driven business process, Process engineering, Process life cycles},
}

@inproceedings{gomes_engagement_2023,
	title = {Engagement, {Participation}, and {Liveness}: {Understanding} {Audience} {Interaction} in {Technology}-{Based} {Events}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160820251&doi=10.5220%2f0011848600003467&partnerID=40&md5=c623e829f3cc5063272a4783ea66b092},
	abstract = {Technologies have been changing how the audience participates in different events. This participation is distinct in each type of event. For example, in educational settings, polls with clickers and word clouds are usually used to involve the audience. For music festivals and other musical performances, organizers opt out of providing led sticks, necklaces and wristbands. Different uses for the smartphones, such as using them as lanterns aiming at obtaining crowd effect, are other ordinary and spontaneous ways of interaction. Recently, more research has been published in journals and scientific conferences discussing the use of these technologies, with techniques for fostering interaction and collaboration. Therefore, we conducted a literature review using forward and backward snowballing, looking for articles about how researchers use new technologies to increase audience experience in different contexts of events and what concepts are raised from that perspective. As a result, we propose a taxonomy of those concepts related to audience experience through three lenses: engagement, participation, and liveness. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	author = {Gomes, Genildo and Conte, Tayana and Castro, Thaís and Gadelha, Bruno},
	year = {2023},
	keywords = {Engagement, Technology-based, Audience interaction, Audience participation, Educational settings, Event, Liveness, Music, Musical performance, Smart phones, Word clouds},
}

@inproceedings{lopez-tenorio_comparing_2023,
	title = {Comparing {Machine} {Learning} for {SQL} {Injection} {Detection} in {Web} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188434335&doi=10.1109%2fISCMI59957.2023.10458664&partnerID=40&md5=568c3adc519c4cd0bac1461be4d6ecc9},
	abstract = {This work analyzes the machine learning techniques most used in SQL injection (SQLi) detection in order to make a comparison in terms of precision, as well as characterize the data with which the models for SQLi detection are generated. For the analysis, a systematic literature review is developed to extract the data reported from the state-of-the-art. A total of 31 primary studies are selected, of which 22 address the analysis and exploring ML techniques for SQLi detection; 20 conduct experiments to test the models in terms of performance and accuracy; and 14 explore the characteristics of the data with which ML models are prepared. In 22 of the 31 papers, 5 ML algorithms for classification problems stand out: Decision Tree, K-Nearest Neighbors, Naive Bayes, Random Forest, and Support Vector Machine. Decision Tree is the most used algorithm for detecting SQLi, appearing in 18 of 31 papers. The t-student test is applied for samples of unequal variances. The results demonstrate a marginal difference between techniques, although Random Forest is one of the techniques with the greatest consistency in accuracy. © 2023 IEEE.},
	author = {Lopez-Tenorio, Brandom and Dominguez-Isidro, Saul and Cortes-Verdin, Maria Karen and Perez-Arriaga, Juan Carlos},
	year = {2023},
	keywords = {Systematic literature review, State of the art, Performance, Learning systems, Machine-learning, Decision trees, Support vector machines, Machine learning techniques, Nearest neighbor search, Nearest-neighbour, Random forests, SQL injection, Web system, Work analysis},
}

@inproceedings{martinez_fault_2023,
	title = {Fault {Tree} {Analysis} and {Failure} {Modes} and {Effects} {Analysis} for {Systems} with {Artificial} {Intelligence}: {A} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183473600&doi=10.1109%2fICSRS59833.2023.10381456&partnerID=40&md5=090510ddac7703ca68c93042075ad314},
	abstract = {Reliability engineering has well-established analysis techniques to design critical systems that will be safe to operate. Two already field-proven techniques are the FTA (Fault Tree Analysis) and the FMECA (Failure Modes, Effects, and Criticality Analysis). These techniques, recommended or required by several supervisory authorities or independent assessments, will continue to be the main assets for the analysis of potential failures and faults. This mapping study revisits FMECA and FTA from the perspective of how they are used when dealing with systems with Artificial Intelligence (AI) components. After the literature database search and selection, 24 primary sources were leveraged to map them regarding their context, scope, considerations, and maturity. The diversity of safety-critical application domains and functions, and the need of more evidences from the evaluations of the proposed approaches, suggest that this field requires a pressing attention. The extracted considerations can be relevant elements of industrial guidelines. © 2023 IEEE.},
	author = {Martinez, Jabier and Eguia, Alexander and Urretavizcaya, Imanol and Amparan, Estibaliz and Negro López, Pablo},
	year = {2023},
	keywords = {Artificial intelligence, Mapping, Search engines, Mapping studies, Analysis techniques, Critical systems, Failure (mechanical), Failure mode and effects analysis, Failure modes, Failure modes effects and criticality analysis, Fault tree analyses (FTA), Fault tree analysis, FMEA, Independent assessment, Reliability (engineering), Reliability analysis, Safety factor, Supervisory authority},
}

@article{trieflinger_elevating_2023,
	title = {Elevating {Software} {Quality} {Through} {Product} {Discovery} {Techniques}: {Key} {Findings} from a {Grey} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172412219&doi=10.1007%2f978-3-031-43703-8_4&partnerID=40&md5=90c881cce119c4f33fbb22896447dde3},
	abstract = {In the era of digital transformation, the notion of software quality transcends its traditional boundaries, necessitating an expansion to encompass the realms of value creation for customers and the business. Merely optimizing technical aspects of software quality can result in diminishing returns. Product discovery techniques can be seen as a powerful mechanism for crafting products that align with an expanded concept of quality—one that incorporates value creation. Previous research has shown that companies struggle to determine appropriate product discovery techniques for generating, validating, and prioritizing ideas for new products or features to ensure they meet the needs and desires of the customers and the business. For this reason, we conducted a grey literature review to identify various techniques for product discovery. First, the article provides an overview of different techniques and assesses how frequently they are mentioned in the literature review. Second, we mapped these techniques to an existing product discovery process from previous research to provide concrete guidelines for establishing product discovery in their organizations. The analysis shows, among other things, the increasing importance of techniques to structure the problem exploration process and the product strategy process. The results are interpreted regarding the importance of the techniques to practical applications and recognizable trends. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	author = {Trieflinger, Stefan and Weiss, Lukas and Münch, Jürgen},
	year = {2023},
	keywords = {Digital transformation, Product management, Literature reviews, Users' experiences, Computer software selection and evaluation, Product discovery, Software Quality, Grey literature, Technical aspects, Traditional boundaries, Value creation},
}

@article{usman_quality_2023,
	title = {A {Quality} {Assessment} {Instrument} for {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152967598&doi=10.37190%2fe-Inf230105&partnerID=40&md5=66143efbcfdadca0a7c250438942e4e3},
	abstract = {Background: Systematic literature reviews (SLRs) have become a standard practice as part of software engineering (SE) research, although their quality varies. To build on the reviews, both for future research and industry practice, they need to be of high quality. Aim: To assess the quality of SLRs in SE, we put forward an appraisal instrument for SLRs. Method: A well-established appraisal instrument from research in healthcare was used as a starting point to develop the instrument. It is adapted to SE using guidelines, checklists, and experiences from SE. The first version was reviewed by four external experts on SLRs in SE and updated based on their feedback. To demonstrate its use, the updated version was also used by the authors to assess a sample of six selected systematic literature studies. Results: The outcome of the research is an appraisal instrument for quality assessment of SLRs in SE. The instrument includes 15 items with different options to capture the quality. The instrument also supports consolidating the items into groups, which are then used to assess the overall quality of an SLR. Conclusion: The presented instrument may be helpful support for an appraiser in assessing the quality of SLRs in SE. © 2023 The Authors. Published by Wrocław University of Science and Technology Publishing House.},
	author = {Usman, Muhammad and Ali, Nauman Bin and Wohlin, Claes},
	year = {2023},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Tertiary study, Industrial research, Software engineering research, Quality assessment, AMSTAR 2, Assessment instruments, Critical appraisal, Industry practices, Standard practices},
}

@inproceedings{silva_are_2023,
	title = {Are safety-critical systems really survivable to attacks?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161903777&doi=10.1109%2fSysCon53073.2023.10131114&partnerID=40&md5=f78a5de334951fc6c0a773986807fa84},
	abstract = {Safety-Critical Systems (SCS) stand for those systems designed to tackle events that could potentially cause human injury or loss of life, significant property damage, financial loss, or damage to the environment, among others. Modern SCS are in transport, infrastructure, medicine, nuclear engineering, recreation, and many other fields. Nevertheless, SCS are prone to various attacks aiming to explore their inherent complexity and broad attack surface to jeopardize essential services and assets. Survivability is key to protecting SCS through integrating preventive, reactive, and tolerant defenses. This paper stands out by analyzing the survivability aspects of SCS under attack through a systematic literature review of recently published articles. Based on the review, we devise a classification to separate the studies that focus on survivability for SCS from those that deal with related aspects, such as resistance, recognition, or tolerance to attacks. Further, we expose literature limitations indicating why there is still no guaranteed survivability of SCS in the presence of attacks. © 2023 IEEE.},
	author = {Silva, Helber and Vieira, Marco and Neto, Augusto},
	year = {2023},
	keywords = {Safety engineering, Systematic literature review, Safety critical systems, Security systems, Attack, Financial loss, Human injury, Loss of life, Losses, Mission critical systems, Property damage, Survivability, Transport infrastructure},
}

@article{silva_extended_2023,
	title = {Extended {Remote} {Laboratories}: {A} {Systematic} {Review} of the {Literature} {From} 2000 to 2022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159716536&doi=10.1109%2fACCESS.2023.3271524&partnerID=40&md5=13a741dc197bddbc5b16f6dd86a3df80},
	abstract = {Remote Laboratories (RLs) break barriers in education since they provide real experimentation anytime and anywhere. However, their greatest drawback is the lack of immersion. However, Extended Reality (XR) can overcome this shortcoming through the integration of AR, or VR techniques, into remote experimentation, thus developing Extended Reality Remote Laboratories (XRLs), which can be Augmented Reality Remote Laboratories (ARLs), or Virtual Reality Remote Laboratories (VRLs). Our study consists in a systematic review concerning the state-of-the-art of XRLs during the period 2000-2022. Findings from the systematic review generated two main results. First, a thorough analysis that reports: a timeline of publications, studies per country, most influential universities, most popular journals and conferences, most cited publications, description of ARLs, description of VRLs, and most notable publications. Secondly, a classification of the XRLs encountered during research, and a proposed architecture for creating XRLs, in order to guide developers wishing to integrate XR into their experiments. © 2013 IEEE.},
	author = {Silva, Isabela Nardi Da and Garcia-Zubia, Javier and Hernandez-Jayo, Unai and Alves, Joao Bosco Da Mota},
	year = {2023},
	keywords = {Systematic Review, Augmented reality, Virtual reality, Extended reality, Laboratories, Metaverses, Pandemic, Remote experimentation, Remote laboratories, STEM, X reality},
}

@article{pelaez_practice_2024-1,
	title = {A practice for specifying user stories in multimedia system design: {An} approach to reduce ambiguity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370340&doi=10.55612%2fs-5002-060-009&partnerID=40&md5=9a590e19db567e98bcfa6de5b1317058},
	abstract = {Various agile approaches have guided the development of solutions based on a minimum viable product for the use of user stories (USs). These approaches provide a generic process for specifying USs for a wide variety of solutions. This study presents an original proposal for a practical approach to the specification of USs for multimedia systems, expressed using the Essence graphical notation language. The practice includes a set of activities, techniques, and tools that can be used by professionals to define the USs of a multimedia system, and which can help to mitigate potential ambiguity problems such as vagueness and insufficiency in formulation. To explore its application, a case study was carried out with the participation of two groups of professionals: an experimental group, and a control group. The results for the analysis factors are promising, and show that by carrying out the activities that make up the practice, a work team can achieve the specification of USs at three levels of concreteness, which contribute to the reduction of problems of vagueness and insufficiency in the USs of a multimedia system. Using this approach, the USs can be guaranteed to meet the value proposition of the multimedia system that will be implemented. © (2024), (ASLERD). All Rights Reserved.},
	author = {Peláez, C.A. and Solano, A.},
	year = {2024},
	note = {Publisher: ASLERD},
	keywords = {User stories, Ambiguity, Multimedia systems, Specification},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{olsson_strategic_2024-1,
	title = {Strategic {Digital} {Product} {Management} in the {Age} of {AI}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188683557&doi=10.1007%2f978-3-031-53227-6_24&partnerID=40&md5=ff21f9d7826ae4b6d04a0c34aee99441},
	abstract = {The role of software product management is key for building, implementing and managing software products. However, although there is prominent research on software product management (SPM) there are few studies that explore how this role is rapidly changing due to digitalization and digital transformation of the software-intensive industry. In this paper, we study how key trends such as DevOps, data and artificial intelligence (AI), and the emergence of digital ecosystems are rapidly changing current SPM practices. Whereas earlier, product management was concerned with predicting the outcome of development efforts and prioritizing requirements based on these predictions, digital technologies require a shift towards experimental ways-of-working and hypotheses to be tested. To support this change, and to provide guidelines for future SPM practices, we first identify the key challenges that software-intensive embedded systems companies experience with regards to current SPM practices. Second, we present an empirically derived framework for strategic digital product management (SPM4AI) in which we outline what we believe are key practices for SPM in the age of AI. © The Author(s) 2024.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Olsson, H.H. and Bosch, J.},
	year = {2024},
	keywords = {Artificial intelligence, Data, Digital ecosystem, Digital products, Digital transformation, Digitalization, Ecosystems, Embedded systems, Information management, Management IS, Management practises, Metadata, Product management, Software product management, Strategic digital product management, DevOps, Digital ecosystems},
	annote = {Export Date: 23 June 2024},
}

@article{costa_multicriteria_2024-1,
	title = {Multicriteria {Decision}-{Making} in {Public} {Security}: {A} {Systematic} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195897347&doi=10.3390%2fmath12111754&partnerID=40&md5=4022e1fcd0721b69f61fec3f77e49eda},
	abstract = {The Multiple Criteria Decision-Making/Analysis (MCDM/A) methods have been widely used in several management contexts. In public security, their use enhances managerial decision-making by considering the decision-maker’s preference structure and providing a multidimensional view of problems. However, methodological support for their applications in this field lacks clarity, including selecting appropriate methods, addressing pertinent problematics, and identifying alternatives and criteria. To address this gap, this article conducts a Systematic Literature Review (SLR) to diagnose the state of the art and identify the main directions of the research in multicriteria models applied to public security management. The research methodology involves five main research questions, and the extraction and analysis of data from 51 articles selected through a structured filtering process. The analysis includes identifying the number of publications and citations, as well as listing the MCDM/A approaches and issues employed. Furthermore, the criteria used and the number of criteria considered are discussed, as well as the method employed. Finally, the identification of the main research directions in MCDM/A models applied to public security is presented. The findings suggest that prioritization and classification are common problematics, social criteria are frequently considered, and the AHP method is widely used, often employing fuzzy sets and hybrid models. © 2024 by the authors.},
	author = {Costa, J. and Silva, M.},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {decision making, multicriteria models, public security, systematic literature review},
	annote = {Export Date: 23 June 2024},
}

@article{kanwal_systematic_2024-1,
	title = {Systematic review on contract-based safety assurance and guidance for future research},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178994069&doi=10.1016%2fj.sysarc.2023.103036&partnerID=40&md5=42770677fc83293a0709b4a44140a836},
	abstract = {The safety requirements are often described via specifications called contracts. To verify that the system fulfils certain safety requirements, for instance, in the assume-guarantee contract specification, the key safety indicators are organized, so that if certain assumptions hold then the respective behaviour is guaranteed. Safety contracts provide a means of exposing potential incompatibilities early in the development process, selecting components to reuse, certifying systems, and identifying uncertainty sources during the operational phase. There exist several studies on contract-based safety assurance, however, there is not any systematic study in this field. For this, a first Systematic Literature Review (SLR) is carried out to obtain an overview of the various contract-based safety assurance concepts, problems, proposed solutions, and their usefulness. In our study, the identification and selection of the primary studies were based on a well-planned search strategy. The search process identified a total of 2881 studies published between 1969 and 2021, out of which 66 studies were selected through a multi-stage process according to our predefined SLR protocol. This SLR aims to highlight the state-of-the-art of contract-based safety assurance and identify potential gaps for future research. Based on research topics in selected studies, we identified the following main categories: contract type, analysis techniques for system safety, compliance with standards, development stage, domain, level of automation, type of study and evaluation, and tool support. The findings of the systematic review not only highlight that the contracts are even more important for advanced safety-critical systems but also strategies to exploit their full potential should be considered in future studies. The suggestions revealed for future research include the usage of contracts for adapting new behaviour, defining system boundaries, interacting with other systems, managing risk during operation, dynamic/runtime safety assurance, and integration of safety with security. © 2023 The Authors},
	author = {Kanwal, S. and Muram, F.U. and Javed, M.A.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Assume-guarantee reasoning, Contract specifications, Contract-based assurance, Development process, Key safety indicator, Model checking, Regulatory compliance, Risk management, Safety assurance, Safety engineering, Safety indicator, Safety requirements, Specifications, Systematic literature review, Systematic Review, Key safety indicators, Safety, Systematic review},
	annote = {Export Date: 23 June 2024},
}

@article{quin_b_2024-1,
	title = {A/{B} testing: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186599305&doi=10.1016%2fj.jss.2024.112011&partnerID=40&md5=eebe13c6f2c48ed90e79c2ef28c186f3},
	abstract = {A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user's point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing. © 2024 The Author(s)},
	author = {Quin, F. and Weyns, D. and Galster, M. and Silva, C.C.},
	year = {2024},
	note = {Publisher: Elsevier Inc.},
	keywords = {Systematic literature review, A/B test engineering, A/B testing, Controlled experimentation, Data driven decision, Decision making, Decisions makings, End-users, Hypothesis testing, Software design, Software testing, State of the art, Test engineering},
	annote = {Export Date: 23 June 2024},
}

@article{stupar_model-based_2023-1,
	title = {Model-based cloud service deployment optimisation method for minimisation of application service operational cost},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148332611&doi=10.1186%2fs13677-023-00389-8&partnerID=40&md5=f64e50ae46a6dd7b46e8606ff5d35f41},
	abstract = {Many currently existing cloud cost optimisation solutions are aimed at cloud infrastructure providers, and they often deal only with specific types of application services. Unlike infrastructure providers, the providers of cloud applications are often left without a suitable cost optimisation solution, especially concerning the wide range of different application types. This paper presents an approach that aims to provide an optimisation solution for the providers of applications hosted in the cloud environments, applicable at the early phase of a cloud application lifecycle and for a wide range of application services. The focus of this research is the development of the method for identifying optimised service deployment option in available cloud environments based on the model of the service and its context, intending to minimise the operational cost of the cloud service while fulfilling the requirements defined by the service level agreement. A cloud application context metamodel is proposed that includes parameters related to both the application service and the cloud infrastructure relevant for the cost and quality of service. By using the proposed optimisation method, knowledge is gained about the effects of the cloud application context parameters on the service cost and quality of service, which is then used to determine the optimal service deployment option. The service models are validated using cloud applications deployed in laboratory conditions, and the optimisation method is validated using the simulations based on the proposed cloud application context metamodel. The experimental results based on two cloud application services demonstrate the ability of the proposed approach to provide relevant information about the impact of cloud application context parameters on service cost and quality of service and use this information for reducing service operational cost while preserving the acceptable service quality level. The results indicate the applicability and relevance of the proposed approach for cloud applications in the early service lifecycle phase since application providers can gain valuable insights regarding service deployment decision without acquiring extensive datasets for the analysis. © 2023, The Author(s).},
	author = {Stupar, I. and Huljenic, D.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Application contexts, Cloud application context, Cloud applications, Cloud services, Cost management, Cost optimization method, Costs, Costs Optimization, Distributed database systems, Iaa, Life cycle, Operational cost, Optimization method, Quality of service, Quality-of-service, Saa, Service modeling, Cloud application, Cloud service, Cost optimisation method, IaaS, SaaS, Service model},
	annote = {Export Date: 23 June 2024},
}

@article{gomes_systematic_2024-1,
	title = {Systematic {Literature} {Review} on {Hybrid} {Robotic} {Vehicles}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188890772&doi=10.3390%2frobotics13030034&partnerID=40&md5=c19609c850e3513650980723f5d1a341},
	abstract = {Autonomous vehicles are a continuously rising technology in several industry sectors. Examples of these technologies lie in the advances in self-driving cars and can be linked to extraterrestrial exploration, such as NASA’s Mars Exploration Rovers. These systems present a leading methodology allowing for increased task performance and capabilities, which are no longer limited to active human support. However, these robotic systems may vary in shape, size, locomotion capabilities, and applications. As such, this report presents a systematic literature review (SLR) regarding hybrid autonomous robotic vehicles focusing on leg–wheel locomotion. During this systematic review of the literature, a considerable number of articles were extracted from four different databases. After the selection process, a filtered sample was reviewed. A brief description of each document can be found throughout this report. © 2024 by the authors.},
	author = {Gomes, D.F. and Pinto, V.H.},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {robotics, autonomous, hybrid, legged–wheeled, vehicle},
	annote = {Export Date: 23 June 2024},
}

@article{gutierrez-fernandez_variability_2024-1,
	title = {Variability management and software product line knowledge in software companies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196035889&doi=10.1016%2fj.jss.2024.112114&partnerID=40&md5=01f89174d622cf80d622addaf2be7c35},
	abstract = {Software product line engineering aims to systematically generate similar products or services within a given domain to reduce cost and time to market while increasing reuse. Various studies recognize the success of product line engineering in different domains. Software variability have increased over the years in many different domains such as mobile applications, cyber–physical systems or car control systems to just mention a few. However, software product line engineering is not as widely adopted as other software development technologies. In this paper, we present an empirical study conducted through a survey distributed to many software development companies. Our goal is to understand their need of software variability management and the level of knowledge the companies have regarding software product line engineering. The survey was answered by 127 participants from more than a hundred of different software development companies. Our study reveals that most of companies manage a catalog of similar products in a way or another (e.g. clone-and-own, common modules that are statically imported,etc.), they mostly document the features of products using text or spreed sheet based documents and more than 66\% of companies identify a base product from which they derive other similar products. We also found a correlation between the lack of Software Product Line (SPL) knowledge and the absence of reuse practices. Notably, this is the first study that explore software variability needs regardless of a company's prior knowledge of SPL. The results encourages further research to understand the reason for the limited knowledge and application of software product line engineering practices, despite the growing demand of variability management. © 2024 The Authors},
	author = {Gutiérrez-Fernández, A.M. and Chacón-Luna, A.E. and Benavides, D. and Fuentes, L. and Rabiser, R.},
	year = {2024},
	note = {Publisher: Elsevier Inc.},
	keywords = {Software design, Application programs, Computer software reusability, Cost engineering, Different domains, Product line engineering, Reduce costs, Reduce time, Reuse, Software company, Software Product Line, Software variabilities, Time to market, Variability management, Software product lines},
	annote = {Export Date: 23 June 2024},
}

@article{espinosa_predictive_2024-1,
	title = {Predictive models for health outcomes due to {SARS}-{CoV}-2, including the effect of vaccination: a systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182467732&doi=10.1186%2fs13643-023-02411-1&partnerID=40&md5=4558aa7f912a743649e02aff8975d10a},
	abstract = {Background: The interaction between modelers and policymakers is becoming more common due to the increase in computing speed seen in recent decades. The recent pandemic caused by the SARS-CoV-2 virus was no exception. Thus, this study aims to identify and assess epidemiological mathematical models of SARS-CoV-2 applied to real-world data, including immunization for coronavirus 2019 (COVID-19). Methodology: PubMed, JSTOR, medRxiv, LILACS, EconLit, and other databases were searched for studies employing epidemiological mathematical models of SARS-CoV-2 applied to real-world data. We summarized the information qualitatively, and each article included was assessed for bias risk using the Joanna Briggs Institute (JBI) and PROBAST checklist tool. The PROSPERO registration number is CRD42022344542. Findings: In total, 5646 articles were retrieved, of which 411 were included. Most of the information was published in 2021. The countries with the highest number of studies were the United States, Canada, China, and the United Kingdom; no studies were found in low-income countries. The SEIR model (susceptible, exposed, infectious, and recovered) was the most frequently used approach, followed by agent-based modeling. Moreover, the most commonly used software were R, Matlab, and Python, with the most recurring health outcomes being death and recovery. According to the JBI assessment, 61.4\% of articles were considered to have a low risk of bias. Interpretation: The utilization of mathematical models increased following the onset of the SARS-CoV-2 pandemic. Stakeholders have begun to incorporate these analytical tools more extensively into public policy, enabling the construction of various scenarios for public health. This contribution adds value to informed decision-making. Therefore, understanding their advancements, strengths, and limitations is essential. © 2024, The Author(s).},
	author = {Espinosa, O. and Mora, L. and Sanabria, C. and Ramos, A. and Rincón, D. and Bejarano, V. and Rodríguez, J. and Barrera, N. and Álvarez-Moreno, C. and Cortés, J. and Saavedra, C. and Robayo, A. and Franco, O.H.},
	year = {2024},
	note = {Publisher: BioMed Central Ltd},
	keywords = {Article, comorbidity, compartment model, coronavirus disease 2019, COVID-19, data base, high income country, hospitalization, human, Humans, Joanna Briggs Institute critical appraisal checklist, low income country, mathematical model, outcome assessment, pandemic, Pandemics, predictive model, public health, SARS-CoV-2, SARS-CoV-2 Alpha, SARS-CoV-2 Delta, SARS-CoV-2 Gamma, SARS-CoV-2 Omicron, Severe acute respiratory syndrome coronavirus 2, susceptible exposed infectious recovered model, susceptible infected recovered model, systematic review, United States, vaccination, Vaccination, Outcome Assessment, Health Care},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{simoes_it_2024-1,
	title = {{IT} {Workforce} {Outsourcing} {Benefits}, {Challenges} and {Success} {Factors} - {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194866459&doi=10.1145%2f3658271.3658305&partnerID=40&md5=cb10e64c7faf72b7af9a9bc6c3436200},
	abstract = {Context: Organizations are highly dependent on a competent IT workforce. IT workforce outsourcing has become a strategic choice of organizations where IT structures are increasingly needed to make management processes more efficient, cost-effective, innovative, and flexible. Problem: The adoption of IT workforce outsourcing presents benefits, challenges, and success factors that require proper management. Despite their direct or indirect impact on IT workforce management, workforce performance, and the quality of products and services, many organizations may not be fully aware of these factors. Solution: We identified the benefits, challenges, and success factors associated with IT workforce outsourcing in the academic literature. IS Theory: We based the study on the General Systems Theory as our findings highlight intertwined relations between organizations, processes, and people involved in IT workforce outsourcing. Method: We executed a systematic mapping study using Engineering Village and Scopus digital databases and complemented the results with backward and forward snowballing. Summary of Results: Based on 32 studies, we identified 13 benefits, 24 challenges, and 18 success factors. These encompass issues related to aspects such as outsourcing strategy, service capacity, human resources, contractual aspects, and outsourcing planning. Contributions and Impact in the IS Area: We compared the results with a prior field study involving industry practitioners engaged in IT workforce outsourcing. We highlight the importance of hearing from both practitioners and the academic literature. Based on the results, academia can propose related research while practitioners can propose actions to improve the adoption of outsourcing practices, risk reduction, and the relationship between contractors and suppliers, among others.  © 2024 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Simões, C.A. and Santos, G.},
	year = {2024},
	keywords = {Academic literature, Audition, Benefit, Challenge, Cost effectiveness, Information systems, Information use, IT structures, IT Workforce, IT workforce outsourcing, Management process, Mapping, Outsourcing, Strategic choice, Success factors, Systematic mapping studies, Benefits, Challenges, IT Workforce Outsourcing, Success Factors, Systematic Mapping Study},
	annote = {Export Date: 23 June 2024},
}

@article{laureate_systematic_2023-1,
	title = {A systematic review of the use of topic models for short text social media analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154596265&doi=10.1007%2fs10462-023-10471-x&partnerID=40&md5=9739bfcc768a46989a8b24b876659c8b},
	abstract = {Recently, research on short text topic models has addressed the challenges of social media datasets. These models are typically evaluated using automated measures. However, recent work suggests that these evaluation measures do not inform whether the topics produced can yield meaningful insights for those examining social media data. Efforts to address this issue, including gauging the alignment between automated and human evaluation tasks, are hampered by a lack of knowledge about how researchers use topic models. Further problems could arise if researchers do not construct topic models optimally or use them in a way that exceeds the models’ limitations. These scenarios threaten the validity of topic model development and the insights produced by researchers employing topic modelling as a methodology. However, there is currently a lack of information about how and why topic models are used in applied research. As such, we performed a systematic literature review of 189 articles where topic modelling was used for social media analysis to understand how and why topic models are used for social media analysis. Our results suggest that the development of topic models is not aligned with the needs of those who use them for social media analysis. We have found that researchers use topic models sub-optimally. There is a lack of methodological support for researchers to build and interpret topics. We offer a set of recommendations for topic model researchers to address these problems and bridge the gap between development and applied research on short text topic models. © 2023, The Author(s).},
	author = {Laureate, C.D.P. and Buntine, W. and Linger, H.},
	year = {2023},
	note = {Publisher: Springer Nature},
	keywords = {Systematic Review, Applied research, Evaluation measures, LDA, Short texts, Social media, Social media analysis, Social media datum, Social networking (online), Topic Modeling, Twitter, NLP, Short text, Topic model},
	annote = {Export Date: 23 June 2024},
}

@article{elder_survey_2024-1,
	title = {A {Survey} on {Software} {Vulnerability} {Exploitability} {Assessment}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191395395&doi=10.1145%2f3648610&partnerID=40&md5=4d9c74ed045e6b2d7675c61936db9cb8},
	abstract = {Knowing the exploitability and severity of software vulnerabilities helps practitioners prioritize vulnerability mitigation efforts. Researchers have proposed and evaluated many different exploitability assessment methods. The goal of this research is to assist practitioners and researchers in understanding existing methods for assessing vulnerability exploitability through a survey of exploitability assessment literature. We identify three exploitability assessment approaches: assessments based on original, manual Common Vulnerability Scoring System, automated Deterministic assessments, and automated Probabilistic assessments. Other than the original Common Vulnerability Scoring System, the two most common sub-categories are Deterministic, Program State based, and Probabilistic learning model assessments. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author = {Elder, S. and Rahman, M.R. and Fringer, G. and Kapoor, K. and Williams, L.},
	year = {2024},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Assessment approaches, Common vulnerability scoring systems, Deterministic programs, Deterministics, Exploitability, Probabilistic assessments, Program state, Software vulnerabilities, State based, Vulnerability mitigation, software vulnerability},
	annote = {Export Date: 23 June 2024},
}

@article{bekkemoen_explainable_2024-1,
	title = {Explainable reinforcement learning ({XRL}): a systematic literature review and taxonomy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178051880&doi=10.1007%2fs10994-023-06479-7&partnerID=40&md5=8a5a3e37497d50b95590c9e028c76928},
	abstract = {In recent years, reinforcement learning (RL) systems have shown impressive performance and remarkable achievements. Many achievements can be attributed to combining RL with deep learning. However, those systems lack explainability, which refers to our understanding of the system’s decision-making process. In response to this challenge, the new explainable RL (XRL) field has emerged and grown rapidly to help us understand RL systems. This systematic literature review aims to give a unified view of the field by reviewing ten existing XRL literature reviews and 189 XRL studies from the past five years. Furthermore, we seek to organize these studies into a new taxonomy, discuss each area in detail, and draw connections between methods and stakeholder questions (e.g., “how can I get the agent to do \_?”). Finally, we look at the research trends in XRL, recommend XRL methods, and present some exciting research directions for future research. We hope stakeholders, such as RL researchers and practitioners, will utilize this literature review as a comprehensive resource to overview existing state-of-the-art XRL methods. Additionally, we strive to help find research gaps and quickly identify methods that answer stakeholder questions. © 2023, The Author(s).},
	author = {Bekkemoen, Y.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Systematic literature review, Decision making, Decision-making process, Deep learning, Explainability, Explainable artificial intelligence, Explanation, Interpretability, Literature reviews, Performance, Reinforcement learning, Reinforcement learning systems, Reinforcement learnings, Taxonomies},
	annote = {Export Date: 23 June 2024},
}

@article{shuraida_impact_2024-1,
	title = {The {Impact} of {Feature} {Exploitation} and {Exploration} on {Mobile} {Application} {Evolution} and {Success}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195242164&doi=10.17705%2f1jais.00844&partnerID=40&md5=5e1d823044298ee989de4f14e0ac2639},
	abstract = {Mobile device applications are the largest segment of IS with an estimated 5 billion users. Yet despite their widespread and growing use, there is little research examining how these mobile applications evolve with each new release update. To ensure market success, developers need to satisfy their user base by incorporating users’ reviews and feedback on the one hand and exploring new features and content that allow them to stay competitive on the other. Drawing on the organizational learning and innovation literature, the findings of the present study suggest that a mix of these two activities of exploitation and exploration in consequent app updates is likely to result in the app’s success. We further contribute to this body of work by examining the influence of users’ online review characteristics on exploitation and exploration activities in app development. The findings suggest that users’ convergence on similar issues (review concurrence) is likely to favor an orientation prioritizing exploitation over exploration activities, while the number of user reviews (review volume) has a curvilinear relationship with it. © 2024 by the Association for Information Systems.},
	author = {Shuraida, S. and Gao, Q. and Safadi, H. and Jain, R.},
	year = {2024},
	note = {Publisher: Association for Information Systems},
	keywords = {Exploitation, Exploitation and explorations, Feature exploitation, Mobile application development, Mobile application evolution, Mobile applications, Mobile computing, Online user review, Online users, Technological innovation, User reviews, Exploration, Mobile Application Development, Mobile Application Evolution, Online User Reviews, Technological Innovation},
	annote = {Export Date: 23 June 2024},
}

@article{dos_santos_automatic_2024-1,
	title = {Automatic user story generation: a comprehensive systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195041962&doi=10.1007%2fs41060-024-00567-0&partnerID=40&md5=7b9c1924d2789134c475860878667716},
	abstract = {User stories are the lifeblood of agile software development due to their semi-structured format and ease of implementation. However, the variety of data sources and textual formats used to document software requirements bring a challenge for software development teams. They often need to read and comprehend the client’s needs from different sources and convert them into user stories manually. This process demands time, and it is also prone to errors. As an alternative to remedy this issue, there are studies concerning the automatic generation of user stories. We conducted a systematic literature review (SLR) to identify and analyze existing approaches for automatically generating user stories. We investigated which type of corpora were used for training and testing, which Natural Language Processing (NLP) or Machine Learning (ML) techniques are employed to reach this goal, and how researchers are evaluating the quality of the user stories generated. Our SLR followed established guidelines and investigated state-of-the-art research from prominent academic publishers such as ACM, IEEE Xplore, and ScienceDirect. Studies published until April 2024 were included, with a focus on those addressing the research questions proposed. Our findings indicate a critical shortage of publicly available corpora hindering advancements in this field, especially in the current era of ML. The team also found there is a broad variety of techniques being employed on this topic. Finally, the studies need to pay more attention to guidelines for evaluating user stories quality. The automatic user story generation remains in its early stages. We highlight some opportunities for contribution and discuss the direction of future works. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.},
	author = {dos Santos, C.A. and Bouchard, K. and Minetto Napoleão, B.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic literature review, Software design, Agile software development, Data-source, Learning algorithms, Learning systems, Natural language processing systems, Requirement engineering, Requirements engineering, Semi-structured, Software requirements, Story generations, Text generations, Textual format, User stories, Text generation, User story},
	annote = {Export Date: 23 June 2024},
}

@article{liu_design_2024-1,
	title = {Design for dependability — {State} of the art and trends},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185411375&doi=10.1016%2fj.jss.2024.111989&partnerID=40&md5=7f5b36b728954f77c7c91606f55da869},
	abstract = {This paper presents an overview of design for dependability as a process involving three distinct but interrelated activities: risk analysis, risk mitigation, and risk assessment. Although these activities have been the subject of numerous works, few of them address the issue of their integration into rigorous design flows. Moreover, most existing results focus on dependability for small-size safety-critical systems with specific static architectures. They cannot be applied to large systems, such as autonomous systems with dynamic heterogeneous architectures and AI components. The overwhelming complexity and lack of interpretability of AI present challenges to model-based techniques and require empirical approaches. Furthermore, it is impossible to cope with all potential risks at design time; run-time assurance techniques are necessary to cost-effectively achieve the desired degree of dependability. The paper synthesizes the state of the art showing particularly the impact of new trends stemming from the integration of AI components in design flows. It argues that these trends will have a profound impact on design methods and the level of dependability. It advocates the need for a new theoretical basis for dependability engineering that allows the integration of traditional model-based approaches and data-driven techniques in the search for trade-offs between efficiency and dependability. © 2024},
	author = {Liu, H. and Huang, C. and Sun, K. and Yin, J. and Wu, X. and Wang, J. and Zhang, Q. and Zheng, Y. and Nigam, V. and Liu, F. and Sifakis, J.},
	year = {2024},
	note = {Publisher: Elsevier Inc.},
	keywords = {Safety engineering, State of the art, AI systems, Dependable AI system, Design, Design flows, Design for dependability, Economic and social effects, Integration, Risk analysis, Risk assessment, Risk mitigation, Risks assessments, Run-time assurance, Runtimes, Safety critical systems, Search engines, Dependable AI systems},
	annote = {Export Date: 23 June 2024},
}

@incollection{travassos_tertiary_2024-1,
	title = {A {Tertiary} {Study} on the {Convergence} of {Human}–{Computer} {Interaction} and {Artificial} {Intelligence}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191293638&doi=10.1007%2f978-3-031-53957-2_1&partnerID=40&md5=0c7138f0d7563d7b36599adb4b058895},
	abstract = {The emergence of technologies like artificial intelligence (AI) has strengthened computational solutions by using powerful algorithms that can support learning behaviors from available data and replicating such behaviors in software. A possible behavior regards the interaction of humans with computers. In this context, Human–Computer Interaction (HCI) promotes the design and evaluation of intuitive and high-usability interactive systems for human users. However, it is not clear yet the convergence between HCI and AI and how such a convergence can improve the computational solutions to benefit the end users despite the many secondary studies regarding HCI and AI available in the technical literature. Therefore, it is necessary to characterize the convergence between HCI and AI revealed in secondary studies, aiming at a better understanding of the challenges and implications of conveying the convergence of these two areas of interest from the perspective of research by undertaking a tertiary study to acquire knowledge from secondary studies published in the Scopus database until 2022. Thirty-eight secondary studies from 17 countries provided evidence of the possible convergence of HCI and AI. The main purposes of converging these areas are to foster user trust and satisfaction, to improve communication and user experience, to increase learnability and performance, and to enhance environmental observation and services. These findings have been observed in 26 problem domains and 23 system domains, performed with many distinct instruments. The convergence of HCI and AI is of worldwide interest, with a growing use of HCI methods and criteria to support developing and improving AI software systems. However, such convergence does not concern specific areas but concerns AI software systems characteristics, human use, and their interaction. Promoting the convergence of HCI and AI is challenging without a clear vision of the target software system. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	publisher = {Springer Nature},
	author = {Travassos, G.H. and Felizardo, K.R. and Morandini, M. and Kolski, C.},
	year = {2024},
	keywords = {Artificial intelligence, Systematic literature review, Computational solutions, Computer software, Contemporary software system engineering, Design and evaluations, Evidence Based Software Engineering, Human computer interaction, Intelligence software, Learning behavior, Software-systems, Support learning, Tertiary study, User interfaces, Artificial Intelligence, Contemporary Software Systems Engineering, Evidence-Based Software Engineering, Human–Computer Interaction, Systematic Literature Review, Tertiary Study},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{pedrosa_immersive_2024-1,
	title = {Immersive {Learning} {Environments} for {Self}-regulation of {Learning}: {A} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176965303&doi=10.1007%2f978-3-031-47328-9_36&partnerID=40&md5=afd01b0fe3773e626ee444d9ca7af52f},
	abstract = {Self-regulation of learning (SRL) plays a decisive role in learning success but characterizing learning environments that facilitate development of SRL skills constitutes a great challenge. Given the growing interest in Immersive Learning Environments (ILE), we sought to understand how ILE are built with attention to SRL, via a literature review of pedagogical uses, practices and strategies with ILE that have an explicit focus on SRL. From a final corpus of 25 papers, we collected 134 extracts attesting use of ILE for SRL. We classified and mapped them using the Beck, Morgado \& O’Shea framework and its three dimensions of the immersion phenomenon: system, narrative and challenge. There is a predominance of uses of ILE for SRL aligned with Challenge-based immersion: Skill Training, Collaboration, Engagement, and Interactive Manipulation and Exploration. In contrast, uses aligned with System-based immersion (Emphasis, Accessibility, Seeing the Invisible) were not identified. There were few cases of use of Narrative-based immersion. Uses combining the three dimensions of immersive had residual prevalence. We concluded that there is greater tendency in studies of SRL in ILE to enact active roles (aligned with the Challenge dimension of immersion). The low prevalence of Narrative immersion and System immersion evidence gaps in the diversity of pedagogical uses of ILE to develop SRL, which indicate opportunities for research and creation of innovative educational practices. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Pedrosa, D. and Morgado, L. and Beck, D.},
	year = {2024},
	keywords = {Literature reviews, Computer aided instruction, Deregulation, Educational practice, Educational strategy, Educational use, Immersion, Immersive learning, Learning environments, Pedagogical use, Self regulation, Self-regulated learning, Educational Practices, Educational Strategies, Educational Uses, Self-regulated Learning},
	annote = {Export Date: 23 June 2024},
}

@article{jordanov_containerized_2024-1,
	title = {Containerized {Microservices} for {Mobile} {Applications} {Deployed} on {Cloud} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195214138&doi=10.3991%2fijim.v18i10.45929&partnerID=40&md5=e3396147196473e57712536d54efef0c},
	abstract = {This study explores the transformative role of containerized microservices in the sphere of mobile application development, especially within public cloud ecosystems. It focuses on how technologies such as Docker and Kubernetes contribute to improving deployment, scalability, and overall management of mobile applications, with an emphasis on containerizing backend services. We analyze their efficiency in streamlining deployment processes, focusing on how they improve the application’s performance and reliability. Additionally, we examine various alternative deployment strategies, such as blue-green, rolling, and canary releases, to emphasize their effectiveness in minimizing risks and facilitating smooth transitions in dynamic cloud environments. The study takes a comprehensive approach to achieve this goal, which includes a systematic review of existing literature, a thorough examination of relevant use cases, and an assessment of open-source technologies. Our findings reveal not only the practical benefits of these strategies but also their strategic application, offering important insights for software engineers and decision-makers. This study emphasizes the significance of integrating and optimizing containerized microservices in mobile app development to achieve more efficient, scalable, and manageable application lifecycles on cloud-based platforms. © 2024 by the authors of this article.},
	author = {Jordanov, J. and Simeonidis, D. and Petrov, P.},
	year = {2024},
	note = {Publisher: International Federation of Engineering Education Societies (IFEES)},
	keywords = {Decision making, Life cycle, Application programs, Mobile application development, Mobile applications, Mobile computing, Cloud systems, Containerization, Containers, Deployment process, It focus, Microservice, Open source software, Performance and reliabilities, Public clouds, Virtualization, Virtualizations, containerization, microservices, mobile application, public cloud, virtualization},
	annote = {Export Date: 23 June 2024},
}

@article{kompella_innovations_2024-1,
	title = {Innovations, strategic organizational actions, and sailing-ship effect: illustrated with an {IT} product},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182492367&doi=10.1108%2fJSTPM-08-2022-0125&partnerID=40&md5=19fae5223ac33e5337300c31268e3cfb},
	abstract = {Purpose: In socio-technical transition theory, resistance by existing technology and regime resistance plays a key role. The resistance is in the form of intentional improvements; eventually, the regime destabilizes and adopts the new technology, referred to as the sailing-ship effect. Researchers used a structural view and examined it as a strategic action and its relationship with new technology (competitive/symbiotic) in non-fast-changing sailing systems. This study uses a microlevel view and examines it in a fast-changing where products/services are developed by integrating existing technology with new product innovations; their success depends on addressing technical/market uncertainty. This study examines the sailing-ship effect in a fast-changing system and contributes to the socio-technical transition theory. Design/methodology/approach: The authors need to examine the phenomena of the sailing-ship effect in its setting, and a case-study method is appropriate. The selected case provided diverse analytic and heuristic perspectives to examine the phenomena; therefore, it was a single case study. Findings: In an IT scenario, the strategic actions decide and realize agility and competitive advantage by formulating appropriate goals with required budgets and coevolutionary changes to resources at product, process and organizational levels, addressing technical/market uncertainty. Moreover, the agility displayed by strategic actions determines the relationship with new technology, which is interspersed. Finally, it provided insights into struggle, navigation and negotiations, forming strategic actions to display the sailing-ship effect. Research limitations/implications: The study selected a Banking Financial Services and Insurance product of an IT Services company. As start-ups exhibit inherent (emergent) agility, the authors can examine agility as a combination of emergent and strategic actions by selecting a start-up. Practical implications: The study highlights the strategic actions specific to an IT services company. It developed its product and services by steering clear from IT innovations such as native cloud and continuous deployment. It improved its products/services with necessary organizational changes and achieved the desired agility and competitive advantage. Therefore, organizations devise appropriate strategic actions to combat the sailing-ship effect apart from setting goals and selecting IT innovations. Originality/value: The study expands the socio-technical transition theory by selecting a fast-changing system. It provided insights into the relationship between existing and new technology and the strategic actions necessary to manage technical and market uncertainty and achieve the desired competitive advantage, or the sailing-ship effect. © 2024, Emerald Publishing Limited.},
	author = {Kompella, L.},
	year = {2024},
	note = {Publisher: Emerald Publishing},
	keywords = {Microservices, Process innovations, Product innovations, Sailing-ship effect, Socio-technical transitions},
	annote = {Export Date: 23 June 2024},
}

@article{chen_understanding_2024-1,
	title = {Understanding and evaluating software reuse costs and benefits from industrial cases—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189036914&doi=10.1016%2fj.infsof.2024.107451&partnerID=40&md5=c4671380bf15db91cc961767f9aa77df},
	abstract = {Context: Software reuse costs and benefits have been investigated in several primary studies, which have been aggregated in multiple secondary studies as well. However, existing secondary studies on software reuse have not critically appraised the evidence in primary studies. Moreover, there has been relatively less focus on how software reuse costs and benefits were measured in the primary studies, and the aggregated evidence focuses more on software reuse benefits than reuse costs. Objective: This study aims to cover the gaps mentioned in the context above by synthesizing and critically appraising the evidence reported on software reuse costs and benefits from industrial cases. Method: We used a systematic literature review (SLR) to conduct this study. The results of this SLR are based on a final set of 30 primary studies. Results: We identified nine software reuse benefits and six software reuse costs, in which better quality and improved productivity were investigated the most. The primary studies mostly used defect-based and development time-based metrics to measure reuse benefits and costs. Regarding the reuse practices, the results show that software product lines, verbatim reuse, and systematic reuse were the top investigated ones, contributing to more reuse benefits. The quality assessment of the primary studies showed that most of them are either of low (20\%) or moderate (67\%) quality. Conclusion: Based on the number and quality of the studies, we conclude that the strength of evidence for better quality and improved productivity as reuse benefits is high. There is a need to conduct more high quality studies to investigate, not only other reuse costs and benefits, but also how relatively new reuse-related practices, such as InnerSource and microservices architecture, impact software reuse. © 2024 The Author(s)},
	author = {Chen, X. and Usman, M. and Badampudi, D.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Costs, Computer software reusability, Reuse, Cost and benefits, Development time, Evaluating software, Software reuse benefit, Software reuse cost, Software-reuse, Software reuse, Software reuse benefits, Software reuse costs},
	annote = {Export Date: 23 June 2024},
}

@article{stojanov_tertiary_2023-1,
	title = {A {Tertiary} {Study} on {Microservices}: {Research} {Trends} and {Recommendations}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182954403&doi=10.1134%2fS0361768823080200&partnerID=40&md5=aea0b73069a88175c9a499a8ffd106c5},
	abstract = {Abstract: The development and adoption of microservices, as one of the most promising directions for developing heterogeneous distributed software systems, have been driven by dynamic changes in business and technology. In addition to the development of new applications, a significant aspect of microservices is the migration from legacy monolithic systems to microservice architectures. Such development trends are accompanied by an increase in the number of primary and secondary publications addressing microservices, highlighting the need to systematize research at a higher level. The objective of this study is to comprehensively analyze secondary studies in the field of microservices from the following five aspects: (1) publishing trends, (2) quality trends of secondary studies, (3) research trends, (4) domains of implementation, and (5) future research directions. The study follows the guidelines for conducting a systematic literature review. The findings were derived from 44 secondary studies published in the period from January 2016 to January 2023. These studies were organized and analyzed to address the five proposed research questions pertaining to the study objectives. The findings suggest that the most promising research directions are related to the development, implementation, and validation of new approaches, methods, and tools that encompass all the phases of the life cycle. Additionally, these research directions have applications in a variety of business and human life domains. Recommendations for further literature reviews relate to improvement of quality assessment of selected studies, more detailed review of architecture quality attributes, inquiry of human factor issues, and certain maintenance and operation issues. From the methodological aspect, recommendations relate to using social science qualitative methods for more detailed analysis of selected studies, and inclusion of gray literature that will bring the real experience of experts from industry. © 2023, Pleiades Publishing, Ltd.},
	author = {Stojanov, Z. and Hristoski, I. and Stojanov, J. and Stojkov, A.},
	year = {2023},
	note = {Publisher: Pleiades Publishing},
	keywords = {Systematic literature review, Life cycle, Tertiary study, Microservice, Development trends, Distributed software system, Dynamic changes, Future research directions, Legacy systems, Monolithic systems, New applications, Research trends, systematic literature review, microservices, tertiary study},
	annote = {Export Date: 23 June 2024},
}

@incollection{sharp_humans_2023-1,
	title = {Humans in the loop: {People} at the heart of systems development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195009972&doi=10.1007%2f978-3-031-45304-5_23&partnerID=40&md5=745be4cd63797d17ef996d3e9070fc1d},
	abstract = {Despite increased automation in the process, people are (still) at the heart of software systems development. This chapter adopts a sociotechnical perspective and explores three areas that characterize the role of humans in software systems development: people as creators, people as users, and people in partnership with systems. Software is created by specialist developers such as software engineers and non-specialists such as "makers." Software developers build communities and operate within several cultures (e.g., professional, company, and national), all of which affect both the development process and the resulting product. Software is used by people. Users also operate within communities and cultures which influence product use, and how systems are used feeds back into future systems development. People and systems are interdependent: they work in partnership to achieve a wide range of goals. However, software both supports what people want to do and shapes what can be done. © The Author(s) 2024. All rights reserved.},
	publisher = {Springer Nature},
	author = {Sharp, H.},
	year = {2023},
	annote = {Export Date: 23 June 2024},
}

@article{umar_advances_2024-1,
	title = {Advances in automated support for requirements engineering: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183775500&doi=10.1007%2fs00766-023-00411-0&partnerID=40&md5=13e42fd60463aa53bb491243c6804cfd},
	abstract = {Requirements Engineering (RE) has undergone several transitions over the years, from traditional methods to agile approaches emphasising increased automation. In many software development projects, requirements are expressed in natural language and embedded within large volumes of text documents. At the same time, RE activities aim to define software systems' functionalities and constraints. However, manually executing these tasks is time-consuming and prone to errors. Numerous research efforts have proposed tools and technologies for automating RE activities to address this challenge, which are documented in published works. This review aims to examine empirical evidence on automated RE and analyse its impact on the RE sub-domain and software development. To achieve our goal, we conducted a Systematic Literature Review (SLR) following established guidelines for conducting SLRs. We aimed to identify, aggregate, and analyse papers on automated RE published between 1996 and 2022. We outlined the output of the support tool, the RE phase covered, levels of automation, development approach, and evaluation approaches. We identified 85 papers that discussed automated RE from various perspectives and methodologies. The results of this review demonstrate the significance of automated RE for the software development community, which has the potential to shorten development cycles and reduce associated costs. The support tools primarily assist in generating UML models (44.7\%) and other activities such as omission of steps, consistency checking, and requirement validation. The analysis phase of RE is the most widely automated phase, with 49.53\% of automated tools developed for this purpose. Natural language processing technologies, particularly POS tagging and Parser, are widely employed in developing these support tools. Controlled experimental methods are the most frequently used (48.2\%) for evaluating automated RE tools, while user studies are the least employed evaluation method (8.2\%). This paper contributes to the existing body of knowledge by providing an updated overview of the research literature, enabling a better understanding of trends and state-of-the-art practices in automated RE for researchers and practitioners. It also paves the way for future research directions in automated requirements engineering. © Crown 2024.},
	author = {Umar, M.A. and Lano, K.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic literature review, Software design, Natural language processing systems, Requirement engineering, Requirements engineering, Agile approaches, Automated requirement engineering, Automated support, Automation, Computational linguistics, Engineering activities, Natural languages, Software development projects, Support tool, Unified Modeling Language, Automated RE, Support},
	annote = {Export Date: 23 June 2024},
}

@article{song_when_2024-1,
	title = {When debugging encounters artificial intelligence: state of the art and open challenges},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186138673&doi=10.1007%2fs11432-022-3803-9&partnerID=40&md5=9eede2fdcbf17988a79b522100b5aa0c},
	abstract = {Both software debugging and artificial intelligence techniques are hot topics in the current field of software engineering. Debugging techniques, which comprise fault localization and program repair, are an important part of the software development lifecycle for ensuring the quality of software systems. As the scale and complexity of software systems grow, developers intend to improve the effectiveness and efficiency of software debugging via artificial intelligence (artificial intelligence for software debugging, AI4SD). On the other hand, many artificial intelligence models are being integrated into safety-critical areas such as autonomous driving, image recognition, and audio processing, where software debugging is highly necessary and urgent (software debugging for artificial intelligence, SD4AI). An AI-enhanced debugging technique could assist in debugging AI systems more effectively, and a more robust and reliable AI approach could further guarantee and support debugging techniques. Therefore, it is important to take AI4SD and SD4AI into consideration comprehensively. In this paper, we want to show readers the path, the trend, and the potential that these two directions interact with each other. We select and review a total of 165 papers in AI4SD and SD4AI for answering three research questions, and further analyze opportunities and challenges as well as suggest future directions of this cross-cutting area. © Science China Press 2024.},
	author = {Song, Y. and Xie, X. and Xu, B.},
	year = {2024},
	note = {Publisher: Science China Press},
	keywords = {Safety engineering, Software design, State of the art, Life cycle, Computer software, Software-systems, Artificial intelligence techniques, Current fields, Fault localization, Hot topics, Image recognition, Machine learning, Machine-learning, Program debugging, Program repair, Repair, Software debugging, Software development life-cycle, artificial intelligence, machine learning, fault localization, program repair, software debugging},
	annote = {Export Date: 23 June 2024},
}

@article{liu_conversation-based_2024-1,
	title = {Conversation-based hybrid {UI} for the repertory grid technique: {A} lab experiment into automation of qualitative surveys},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183466132&doi=10.1016%2fj.ijhcs.2024.103227&partnerID=40&md5=9c4644f20ae1abe675a64043de47f49c},
	abstract = {A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language. © 2024 The Authors},
	author = {Liu, Y. and Martens, J.-B.},
	year = {2024},
	note = {Publisher: Academic Press},
	keywords = {Automation, Chatbots, Graphical user interfaces, Grid techniques, Hybrid UI, Hybrid User Interfaces, Lab. experiment, Qualitative survey automation, Qualitative surveys, Repertory grid technique, Repertory grids, Users' experiences, Chatbot},
	annote = {Export Date: 23 June 2024},
}

@article{diaz_how_2024-1,
	title = {How can feature usage be tracked across product variants? {Implicit} {Feedback} in {Software} {Product} {Lines}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186517167&doi=10.1016%2fj.jss.2024.112013&partnerID=40&md5=d2166267177422aa1cf29c0b37bd4de1},
	abstract = {Implicit feedback involves gathering information about software usage to grasp how and when the software is utilized. This study investigates the integration of implicit feedback mechanisms into Software Product Line Engineering (SPLE). While common in product-based development for identifying bugs, usability issues, and informing requirement prioritization, its adoption in SPLs has been limited due to the unique challenges posed by SPLE, such as the emphasis on Domain Engineering over Application Engineering, and the need for systematic reuse of shared assets. We propose a novel approach to incorporate feedback practices into Domain Engineering, thereby shifting the focus from individual product variants to the SPL platform, and specifically moving from product-based feedback to feature-based feedback. Based on a case study, we suggest that product derivation includes a second step that injects the trackers at the time of derivation, using a Feedback Model that complements the Configuration Model for feedback analysis.To test this approach, we introduce FEACKER, an extension to pure::variants as the variability manager. FEACKER injects trackers when the product variant is derived. The findings are validated through a Technology Acceptance Model (TAM) evaluation and a focus group discussion, providing insights into the feasibility, acceptance, and potential impact of platform-based feedback in SPLE. The results indicate agreement on the benefits of conducting feedback analysis at the platform level and the perception that FEACKER seamlessly extends the capabilities of pure::variants. © 2024 The Authors},
	author = {Díaz, O. and Medeiros, R. and Al-Hajjaji, M.},
	year = {2024},
	note = {Publisher: Elsevier Inc.},
	keywords = {Software design, Product line engineering, Software Product Line, Computer software, Program debugging, Application engineering, Continuous development, Domain engineering, Feedback analysis, Feedback mechanisms, Implicit feedback, Product variants, Requirements prioritization, Software Product Lines},
	annote = {Export Date: 23 June 2024},
}

@article{cauz_text_2024-1,
	title = {Text readability in augmented reality: a multivocal literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186594558&doi=10.1007%2fs10055-024-00949-6&partnerID=40&md5=2be8cca4f03f6481cf9e0d914a2d6e02},
	abstract = {Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations. © The Author(s) 2024.},
	author = {Cauz, M. and Clarinval, A. and Dumas, B.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Literature reviews, 'current, Augmented reality, Contextual constraints, Decision trees, Game Engine, Industrial research, Legibility, Mixed reality, Optical see-through, Readability, State of research, Text, Augmented Reality, Mixed Reality},
	annote = {Export Date: 23 June 2024},
}

@article{rico_experiences_2024-1,
	title = {Experiences from conducting rapid reviews in collaboration with practitioners — {Two} industrial cases},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178453626&doi=10.1016%2fj.infsof.2023.107364&partnerID=40&md5=1973bcd1a7948080b1e54df8a4a4d573},
	abstract = {Context: Evidence-based software engineering (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice. Objective: This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and industry. Through the lens of two case studies, we delve into the practical application and experience of conducting RRs. Methods: We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We collected data through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis. Results: We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners. Conclusion: The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange. © 2023 The Author(s)},
	author = {Rico, S. and Ali, N.B. and Engström, E. and Höst, M.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software engineering, Systematic Review, Literature reviews, Evidence Based Software Engineering, Industrial research, Industry-academia collaboration, Knowledge exchange, Knowledge management, Literature studies, Rapid review, Research relevance, Study design, Systematic method, Systematic review, Rapid reviews},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{klymenko_integration_2024-1,
	title = {On the {Integration} of {Privacy}-{Enhancing} {Technologies} in the {Process} of {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980857&doi=10.5220%2f0012632500003690&partnerID=40&md5=11923a80a832d27738266388d3b58f9d},
	abstract = {The class of technologies known as Privacy-Enhancing Technologies (PETs) has been receiving rising attention in the academic sphere. In practice, however, the adoption of such technologies remains low. Beyond the actual implementation of a PET, it is not clear where along the process of software engineering PETs should be considered, and which activities must take place to facilitate their implementation. In this light, we aim to investigate the placement of PETs in the software engineering process, specifically from the perspective of privacy requirements engineering. To do this, we conduct a systematic literature review and interview 10 privacy experts, exploring the integration of PETs into the software engineering process, as well as identifying associated challenges along with their potential solutions. We systematize our findings in a unified process diagram that illustrates the roles and activities involved in the implementation of PETs in software systems. In addition, we map the identified solution concepts to the diagram, highlighting which stages of the software engineering process are vital in tackling the corresponding challenges and supporting the adoption of PETs. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	publisher = {Science and Technology Publications, Lda},
	author = {Klymenko, A. and Meisenbacher, S. and Favaro, L. and Matthes, F.},
	year = {2024},
	keywords = {Software engineering, Systematic literature review, Requirement engineering, Requirements engineering, Software-systems, Data privacy, Engineering education, Privacy engineerings, Privacy enhancing technologies, Privacy requirements, Process diagram, Software engineering process, Solution concepts, Unified process, Privacy Engineering, Privacy Requirements, Privacy-Enhancing Technologies, Requirements Engineering},
	annote = {Export Date: 23 June 2024},
}

@article{cacciuttolo_sensor_2024-1,
	title = {Sensor {Technologies} for {Safety} {Monitoring} in {Mine} {Tailings} {Storage} {Facilities}: {Solutions} in the {Industry} 4.0 {Era}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194092250&doi=10.3390%2fmin14050446&partnerID=40&md5=b1e13d13f95a8659defb3f6d305755f9},
	abstract = {The recent tailings storage facility (TSF) dam failures recorded around the world have concerned society in general, forcing the mining industry to improve its operating standards, invest greater economic resources, and implement the best available technologies (BATs) to control TSFs for safety purposes and avoid spills, accidents, and collapses. In this context, and as the era of digitalization and Industry 4.0 continues, monitoring technologies based on sensors have become increasingly common in the mining industry. This article studies the state of the art of implementing sensor technologies to monitor structural health and safety management issues in TSFs, highlighting advances and experiences through a review of the scientific literature on the topic. The methodology applied in this article adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and utilizes scientific maps for data visualization. To do so, three steps were implemented: (i) a quantitative bibliometric analysis, (ii) a qualitative systematic review of the literature, and (iii) a mixed review to integrate the findings from (i) and (ii). As a result, this article presents the main advances, gaps, and future trends regarding the main characteristics of the sensor technologies applied to monitor TSF structural health and safety management in the era of digitalization. According to the results, the existing research predominantly investigates certain TSF sensor technologies, such as wireless real-time monitoring, remote sensors (RS), unmanned aerial vehicles (UAVs), unmanned survey vessels (USVs), artificial intelligence (AI), cloud computing (CC), and Internet of Things (IoT) approaches, among others. These technologies stand out for their potential to improve the safety management monitoring of mine tailings, which is particularly significant in the context of climate change-related hazards, and to reduce the risk of TSF failures. They are recognized as emerging smart mining solutions with reliable, simple, scalable, secure, and competitive characteristics. © 2024 by the authors.},
	author = {Cacciuttolo, C. and Guzmán, V. and Catriñir, P. and Atencio, E.},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {Industry 4.0, Internet of Things, mine tailings, real-time monitoring, remote sensing, risks, safety, sensors, wireless},
	annote = {Export Date: 23 June 2024},
}

@article{sumardi_effect_2024-1,
	title = {{THE} {EFFECT} {OF} {ISLAMIC} {ATTRIBUTES} {TO} {CONSUMER} {SATISFACTION}: {A} {META}-{ANALYSIS}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191822486&doi=10.30892%2fgtg.53201-1215&partnerID=40&md5=f4f1dd059b68da375bbab257d3c39e60},
	abstract = {In the growing halal industry, there are differences of opinion among researchers about the effect of Islamic attributes on consumer satisfaction. Therefore, this paper aims to evaluate the effect of Islamic attributes on consumer satisfaction. The Prisma flow diagram indicated 23 papers and consists of 59 studies to analyze with JASP Software. The study identifies significant authors, dominant publishers, methodology, and theories commonly employed in this topic. The result proves that catering to Muslim needs through Islamic attributes can significantly enhance consumer satisfaction and th e presence of other variables as moderators will strengthen tourist satisfaction. © 2024 Editura Universitatii din Oradea. All rights reserved.},
	author = {Sumardi, R.S. and Mahomed, A.S.B. and Aziz, Y.A.},
	year = {2024},
	note = {Publisher: Editura Universitatii din Oradea},
	keywords = {diagram, hotel industry, Islamism, life satisfaction, meta-analysis, software, Islamic attributes, Muslim-friendly hotel, Prisma flow diagram, satisfaction},
	annote = {Export Date: 23 June 2024},
}

@article{pretorius_when_2024-1,
	title = {When rationality meets intuition: {A} research agenda for software design decision-making},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189632288&doi=10.1002%2fsmr.2664&partnerID=40&md5=7a7dd1858ceb779f6079fb0c96876131},
	abstract = {As society's reliance on software systems escalates over time, so too does the cost of failure of these systems. Meanwhile, the complexity of software systems, as well as of their designs, is also ever-increasing, influenced by the proliferation of new tools and technologies to address intended societal needs. The traditional response to this complexity in software engineering and software architecture has been to apply rationalistic approaches to software design through methods and tools for capturing design rationale and evaluating various design options against a set of criteria. However, research from other fields demonstrates that intuition may also hold benefits for making complex design decisions. All humans, including software designers, use intuition and rationality in varying combinations. The aim of this article is to provide a comprehensive overview of what is known and unknown from existing research regarding the use and performance consequences of using intuition and rationality in software design decision-making. To this end, a systematic literature review has been conducted, with an initial sample of 3909 unique publications and a final sample of 26 primary studies. We present an overview of existing research, based on the literature concerning intuition and rationality use in software design decision-making and propose a research agenda with 14 questions that should encourage researchers to fill identified research gaps. This research agenda emphasizes what should be investigated to be able to develop support for the application of the two cognitive processes in software design decision-making. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Pretorius, C. and Razavian, M. and Eling, K. and Langerak, F.},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Application programs, Software-systems, Behavioral research, Design decision-making, Intuition, Rationality, Research agenda, Software design decisions, Software Evolution, Software process, systematic literature review, decision-making, intuition, rationality, research agenda, software design},
	annote = {Export Date: 23 June 2024},
}

@article{hannousse_twenty-two_2024-1,
	title = {Twenty-two years since revealing cross-site scripting attacks: {A} systematic mapping and a comprehensive survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191018882&doi=10.1016%2fj.cosrev.2024.100634&partnerID=40&md5=f261951f45b0df8972c9f9c044dc6651},
	abstract = {Cross-site scripting (XSS) is one of the major threats menacing the privacy of data and the navigation of trusted web applications. Since its disclosure in late 1999 by Microsoft security engineers, several techniques have been developed with the aim of securing web navigation and protecting web applications against XSS attacks. XSS has been and is still in the top 10 list of web vulnerabilities reported by the Open Web Applications Security Project (OWASP). Consequently, handling XSS attacks has become one of the major concerns of several web security communities. Despite the numerous studies that have been conducted to combat XSS attacks, the attacks continue to rise. This motivates the study of how the interest in XSS attacks has evolved over the years, what has already been achieved to prevent these attacks, and what is missing to restrain their prevalence. In this paper, we conduct a systematic mapping and a comprehensive survey with the aim of answering all these questions. We summarize and categorize existing endeavors that aim to handle XSS attacks and develop XSS-free web applications. The systematic mapping yielded 157 high-quality published studies. By thoroughly analyzing those studies, a comprehensive taxonomy is drawn out outlining various techniques used to prevent, detect, protect, and defend against XSS attacks and vulnerabilities. The study of the literature revealed a remarkable interest bias toward basic (84.71\%) and JavaScript (81.63\%) XSS attacks as well as a dearth of vulnerability repair mechanisms and tools (only 1.48\%). Notably, existing vulnerability detection techniques focus solely on single-page detection, overlooking flaws that may span across multiple pages. Furthermore, the study brought to the forefront the limitations and challenges of existing attack detection and defense techniques concerning machine learning and content-security policies. Consequently, we strongly advocate the development of more suitable detection and defense techniques, along with an increased focus on addressing XSS vulnerabilities through effective detection (hybrid solutions) and repair strategies. Additionally, there is a pressing need for more high-quality studies to overcome the limitations of promising approaches such as machine learning and content-security policies while also addressing diverse XSS attacks in different languages. Hopefully, this study can serve as guidance for both the academic and practitioner communities in the development of XSS-free web applications. © 2024 Elsevier Inc.},
	author = {Hannousse, A. and Yahiouche, S. and Nait-Hamoud, M.C.},
	year = {2024},
	note = {Publisher: Elsevier Ireland Ltd},
	keywords = {Mapping, Machine learning, Machine-learning, Repair, Data privacy, Cross site scripting, Defense techniques, High quality, Network security, Security systems, Systematic mapping, WEB application, Web applications, WEB security, XSS attack, XSS vulnerability, Cross-site scripting, Survey, Web security, XSS attacks, XSS vulnerabilities},
	annote = {Export Date: 23 June 2024},
}

@article{silva_using_2024-1,
	title = {Using {Hypotheses} to {Manage} {Technical} {Uncertainty} and {Architecture} {Evolution} in a {Software} {Start}-up},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195806801&doi=10.1109%2fMS.2024.3383628&partnerID=40&md5=e5c7d0c12e832411fc2d35cc91e1d614},
	abstract = {This article presents the case of a start-up applying a technique named ArchHypo that uses hypotheses to express uncertainties related to the software architecture. Ten months after identifying the hypotheses, it was assessed how the usage of this technique impacted their decisions.  © 1984-2012 IEEE.},
	author = {Silva, K. and Melegati, J. and Wang, X. and Ferreira, M. and Guerra, E.},
	year = {2024},
	note = {Publisher: IEEE Computer Society},
	keywords = {Computer software, Uncertainty},
	annote = {Export Date: 23 June 2024},
}

@article{khan_revolutionizing_2024-1,
	title = {Revolutionizing software developmental processes by utilizing continuous software approaches},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179315668&doi=10.1007%2fs11227-023-05818-8&partnerID=40&md5=b3d09e613cf9ddb415938502bdd55553},
	abstract = {The development of smart and innovative software applications in various disciplines has inspired our lives by providing various cutting-edge technologies spanning from online to smart and efficient systems. The proliferation of innovative internet-enabled tools has transformed the nation into a globalized world where individuals can participate on various platforms, collaborate in activities, communicate on issues, and exchange information safely and consistently. Coordination and cooperation are essential in software development. It gathers all software developers in one space, encouraging them to discuss goals and work rationally to accomplish the project goal. In recent years, continuous software development and deployment have become increasingly common in software engineering. Continuous software engineering (CSE) is a method that involves a variety of strategies to increase the regularity of novel and modified software versions. CSE enables a continuous learning and improvement process through rapid software update iteration by combining continuous integration and delivery. Continuous integration is a method that has arisen in order to remove gaps between development and deployment. Software engineers must handle uncertainty and alter stakeholders' requirements, which is possible through continuous software developmental strategies that manage the overall software cycle and produce high-quality software applications. The proposed study is a systematic review related to continuous software development and deployment and focuses on achieving four aims: (1) To explore the impacts of continuous development on software, (2) to pinpoint various tools used to carry out this process, (3) to highlight the challenges faced in adopting continuous approaches for development and (4) to analyze the phases of continuous software engineering. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	author = {Khan, H.U. and Afsar, W. and Nazir, S. and Noor, A. and Kundi, M. and Maashi, M. and Alshahrani, H.M.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Software engineering, Software design, Application programs, Computer software reusability, Automation, Automated systems, Continuous integrations, Continuous software, Continuous software engineerings, Cutting edge technology, Iterative methods, Online systems, Reusable softwares, Software applications, Software approach, Software deployment, Software upgradation, Reusable software},
	annote = {Export Date: 23 June 2024},
}

@article{taskeen_research_2023-1,
	title = {A research landscape on software defect prediction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150786328&doi=10.1002%2fsmr.2549&partnerID=40&md5=d3fda2148e5228abe618078eb6140777},
	abstract = {Software defect prediction is the process of identifying defective files and modules that need rigorous testing. In the literature, several secondary studies including systematic reviews, mapping studies, and review studies have been reported. However, no research work such as a tertiary study that combines secondary studies has focused on providing a landscape of software defect prediction useful to understand the body of knowledge. Motivated by this, we intend to perform a tertiary study by following a systematic literature review protocol to provide a research landscape of the targeted domain. We synthesize the quality of the secondary studies and investigate the employed techniques and the performance evaluation measures for evaluating the software defect prediction model. Furthermore, this study aims at exploring different datasets employed in the reported experimentation. Moreover, the current study intends at highlighting the research trends, gaps, and opportunities in the targeted research domain. The results indicate that none of the reported defect prediction techniques can be regarded as the best; however, the reported techniques performed better in different testing situations. In addition, machine learning (ML)-based techniques perform better than traditional statistical techniques mainly due to the potential of discovering the defects and generating generalized results. Moreover, the obtained results highlight the need for further work in the domain of ML-based techniques. Furthermore, publicly available datasets should be considered for experimentation or replication purposes. The potential future work can focus on data quality, ethical ML, cross-project defect prediction, early defect prediction process, class imbalance problem, and model overfitting. © 2023 John Wiley \& Sons Ltd.},
	author = {Taskeen, A. and Khan, S.U.R. and Felix, E.A.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Systematic literature review, Systematic Review, Software testing, Mapping, Systematic mapping studies, Tertiary study, Machine-learning, Body of knowledge, Defect prediction, Defects, Forecasting, Mapping studies, Performances evaluation, Software defect prediction, systematic literature review, tertiary study, software defect prediction, systematic mapping study},
	annote = {Export Date: 23 June 2024},
}

@article{rodriguez_ux_2023-1,
	title = {{UX} debt in an agile development process: evidence and characterization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173738867&doi=10.1007%2fs11219-023-09652-2&partnerID=40&md5=d1b89ddbc5b6fa37cb681b4f6d464e97},
	abstract = {The metaphor of technical debt (TD) has generated a conceptual framework on factors that weaken the quality of software and accumulate a repair cost. However, user-related aspects like user experience (UX) receive little consideration among TD types, for reasons like the substantial focus on code TD, some dynamics inherent to agile processes, and an apparent lack of cumulative cost over time. This article has two main goals: first, to present evidence of the existence of UXDebt as a type of TD, with a cumulative cost for the development team as well as stakeholders; second, to propose a definition and characterization of UXDebt that may serve as a frame for further research on methods and tools for continuous management within agile processes. For the first goal, we have compiled evidence on the current state of UXDebt from three sources: a literature review, a survey among software engineering professionals in agile teams, and the analysis of UX issues in GitHub. All sources have evidenced some form of UXDebt; surveyed practitioners have recognized its poor management with a cost for the entire team that accumulates over time. Moreover, issue tracking systems allow to visualize and measure a technical form of UXDebt. For the second goal, we have defined a conceptual model that characterizes UXDebt in terms of both technical and non-technical aspects. On the technical side, we propose the notion of UX smells which allows us to discuss concrete management activities. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Rodriguez, A. and Gardey, J.C. and Grigera, J. and Rossi, G. and Garrido, A.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Software engineering, Development process, A/B testing, Users' experiences, Agile development, Agile process, Cumulative cost, Human resource management, Odors, Refactorings, Technical debts, User testing, UX smell, User experience, Refactoring, Technical debt, UX smells},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{unkelos-shpigel_revise_2023-1,
	title = {Revise {That} {Again}: {Are} {You} {Motivated}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180004904&doi=10.1145%2f3617553.3617885&partnerID=40&md5=ff5086d61dda21c9c557267e97ae7636},
	abstract = {Requirements engineering (RE) presents several challenges stemming from the required collaboration and knowledge transfer between analysists, developers, and customers. Motivation theories have been used occasionally to analyze and encourage motivation and engagement of stakeholders in RE tasks. In recent years, gamification techniques have been used in software engineering tasks, and specifically, in RE tasks in order to promote stakeholder engagement. However, existing research works seldom offer a rigorous method for designing gamification environments for RE tasks. This paper describes a socio-Technical environment, which was built for requirements elicitation and specification. This environment allows researchers and team managers to decide on different mechanisms to gamify the current RE task in practice. The environment was evaluated by experts and was further tested with the participation of students in two proof of concept studies for demonstrating its functionality, yielding some anecdotic results.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Unkelos-Shpigel, N. and Berencwaig, B. and Kas, S.},
	year = {2023},
	keywords = {Software engineering, Requirement engineering, Requirements engineering, Knowledge management, Human resource management, Engagement, Engineering tasks, Gamification, Knowledge transfer, Motivation, Motivation and engagements, Motivation theories, Sociotechnical, Stakeholder engagement, Technical environments, Requirements Engineering},
	annote = {Export Date: 23 June 2024},
}

@article{ros_theory_2024-1,
	title = {A theory of factors affecting continuous experimentation ({FACE})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179585225&doi=10.1007%2fs10664-023-10358-z&partnerID=40&md5=c9efd3f16565fe8e41091c8792e51a17},
	abstract = {Context: Continuous experimentation (CE) is used by many companies with internet-facing products to improve their business models and software solutions based on user data. Some companies deliberately adopt a systematic experiment-driven approach to software development while some companies use CE in a more ad-hoc fashion. Objective: The goal of this study is to identify factors for success in CE that explain the variations in the utility and efficacy of CE between different companies. Method: We conducted a multi-case study of 12 companies involved with CE and performed 27 interviews with practitioners at these companies. Based on that empirical data, we then built a theory of factors at play in CE. Results: We introduce a theory of Factors Affecting Continuous Experimentation (FACE). The theory includes three factors, namely 1) processes and infrastructure for CE, 2) the user problem complexity of the product offering, and 3) incentive structures for CE. The theory explains how these factors affect the effectiveness of CE and its ability to achieve problem-solution and product-market fit. Conclusions: Our theory may inspire practitioners to assess an organisation’s potential for adopting CE and to identify factors that pose challenges in gaining value from CE practices. Our results also provide a basis for defining practitioner guidelines and a starting point for further research on how contextual factors affect CE and how these may be mitigated. © 2023, The Author(s).},
	author = {Ros, R. and Bjarnason, E. and Runeson, P.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {A/B testing, Business software, Case-studies, Continuous experimentation, Data driven, Data-driven development, Empirical research, Facing products, Multi-case study, Theory building},
	annote = {Export Date: 23 June 2024},
}

@article{erat_emotion_2024-1,
	title = {Emotion recognition with {EEG}-based brain-computer interfaces: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186412474&doi=10.1007%2fs11042-024-18259-z&partnerID=40&md5=da6a586436c707ff0124ec5644173527},
	abstract = {Electroencephalography (EEG)-based Brain-Computer Interface (BCI) systems for emotion recognition have the potential to assist the enrichment of human–computer interaction with implicit information since they can enable understanding of the cognitive and emotional activities of humans. Therefore, these systems have become an important research topic today. This study aims to present trends and gaps on this topic by performing a systematic literature review based on the 216 published scientific literature gathered from various databases including ACM, IEEE Xplore, PubMed, Science Direct, and Web of Science from 2016 to 2020. This review gives an overview of all the components of EEG based BCI system from the signal stimulus module which includes the employed device, signal stimuli, and data processing modality, to the signal processing module which includes signal acquisition, pre-processing, feature extraction, feature selection, classification algorithms, and performance evaluation. Thus, this study provides an overview of all components of an EEG-based BCI system for emotion recognition and examines the available evidence in a clear, concise, and systematic way. In addition, the findings are aimed to inform researchers about the issues on what are research trends and the gaps in this field and guide them in their research directions. © The Author(s) 2024.},
	author = {Erat, K. and Şahin, E.B. and Doğan, F. and Merdanoğlu, N. and Akcakaya, A. and Durdu, P.O.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Systematic literature review, Affective brain-computer interface, Affective Computing, Biomedical signal processing, Brain computer interface, Classification (of information), Cognitive systems, Data handling, Electroencephalography, Electrophysiology, Emotion, Emotion recognition, Emotion Recognition, Feature extraction, Implicit informations, Interface system, Research topics, Scientific literature, Speech recognition, Web of Science, Affective brain-computer interface (aBCI), Affective computing, Brain-computer interface, Human–computer interaction},
	annote = {Export Date: 23 June 2024},
}

@book{bratianu_knowledge_2024-1,
	title = {Knowledge translation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190356698&doi=10.1108%2f9781803828893&partnerID=40&md5=7ecf8b9fc80a0ff97fa83a5e6a870d8d},
	abstract = {Knowledge translation is a topic that originated in the field of health sciences where the need to move research to practice is of critical importance. In parallel, the field of knowledge sciences has developed a research base around knowledge transfer, knowledge sharing, knowledge exchange, knowledge articulation and knowledge absorption. This book brings these two important tracks together and synthesizes the fragmented literatures. It also draws from essential work on human communication and considers how these concepts are affected by the knowledge economy. The book raises awareness of the critical role that knowledge translation plays in every academic field of study, and in everyday life. To demonstrate this role, the book presents a grounding model that readers can use to better see their knowledge translation challenges and opportunities. Drawing on the author teams experience in a range of domains and sectors, the book explores knowledge translation in the fields of manufacturing, infectious diseases, automated call centers, regulatory development and compliance, financial lending, transportation safety, and doctor-patient discourse. All rights reserved.},
	publisher = {Emerald Group Publishing Ltd.},
	author = {Bratianu, C. and Garcia-Perez, A. and Dal Mas, F. and Bedford, D.},
	year = {2024},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{costa_testing_2024-1,
	title = {Testing on {Dynamically} {Adaptive} {Systems}: {Challenges} and {Trends}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193917885&doi=10.5220%2f0012555900003690&partnerID=40&md5=15527034acfbfd71c1daf5ea77036493},
	abstract = {Dynamically Adaptive Systems (DAS) are systems capable of modifying themselves automatically according to the surrounding environment. Traditional testing approaches are ineffective for these systems due to their dynamic aspects, making fault detection complex. Although various testing approaches have been proposed for DASs, there is no up-to-date overview of the approaches, challenges, and trends. This research therefore presents the results of a systematic literature review to identify the challenges, approaches and trends in testing dynamically adaptable systems. For this objective, 25 articles between 2020 and 2023 were analyzed to answer our research questions. As a result, approaches and their characteristics were identified, such as what type of system they can be applied to, what activity is included in the testing process, and at what level of testing. We also highlighted challenges that are still being faced and trends in testing dynamically adaptive systems. For a more in-depth analysis of the results related to the challenges, grounded theory procedures were applied to organize them and encourage future research that seeks to overcome and mitigate them. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	publisher = {Science and Technology Publications, Lda},
	author = {Costa, I.D.N. and Santos, I.S. and Andrade, R.M.C.},
	year = {2024},
	keywords = {Systematic literature review, Systematic Review, Software testing, Adaptable system, Adaptive systems, Dynamic aspects, Dynamically adaptive systems, Fault detection, Faults detection, Research questions, Software testings, Surrounding environment, Testing process, Adaptive Systems, Software Testing},
	annote = {Export Date: 23 June 2024},
}

@article{barcelos_requirements_2024-1,
	title = {Requirements engineering in industry 4.0: {State} of the art and directions to continuous requirements engineering},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186634667&doi=10.1002%2fsys.21753&partnerID=40&md5=f62bda5bb7a47f95a40a286270f546c2},
	abstract = {The 4th Industrial Revolution, also known as Industry 4.0, intends to transform manufacturing processes into smart factories with full digitalization and intelligent, decentralized, and flexible production. In this scenario, Industry 4.0 systems (i.e., software-intensive systems that automate smart factories) have required rigorous and continuous development, but smart factory companies often have difficulty dealing with Requirements Engineering (RE) where requirements continuously change and emerge at runtime to support the changeability of complex production processes. Such requirements encompass engineering (e.g., mechanical, electrical, electronic, production/manufacturing) and business areas and involve the vertical and horizontal integration of heterogeneous manufacturing systems. There is also a lack of a panorama of how Industry 4.0 projects have performed with RE activities. The main goal of this paper is to present the state-of-the-art research concerning RE in Industry 4.0 and draw attention to the next most urgent steps. For this, we selected and examined studies that address RE for Industry 4.0, noting that much of this literature is recent but does not fully address the complexity and dynamism of the requirements for Industry 4.0. Grounded on these studies and our academic and industry experience, we highlight the need for Continuous Requirements Engineering (CRE) for Industry 4.0. Significance and Practitioner Points: The main implications of this paper are: (i) For researchers: It offers the state of the art of RE in the context of Industry 4.0 and points out several important open issues that require an urgent investigation through new research topics; and (ii) For practitioners: It provides directions for new or even existing Industry 4.0 projects on how to deal with RE activities aiming to overcome the several challenges to perform them. © 2024 Wiley Periodicals, Inc.},
	author = {Barcelos, L.V. and Antonino, P.O. and Nakagawa, E.Y.},
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {State of the art, Requirement engineering, Requirements engineering, Engineering activities, Continuous requirement engineering, Decentralized production, Flexible production, Industrial revolutions, Industry 4.0, Manufacturing process, Smart factory, continuous requirements engineering, requirements engineering, smart factory},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{nayak_gdpr_2024-1,
	title = {{GDPR} {Compliant} {ChatGPT} {Playground}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196073971&doi=10.1109%2fICETCS61022.2024.10543557&partnerID=40&md5=abdead7dc470268c7eadcc3ac9c14abb},
	abstract = {ChatGPT is an AI based conversational tool developed by OpenAPI on the design principles of Large Language Model (LLM) and a publicly accessible tool. ChatGPT has increasingly becoming popular tool for enabling applications involving interactive and contextual search across corporate companies, profit/non-profit organizations, educational/medical institutions, and researcher's community to name a few.The corporate companies are abided by GDPR (General Data Protection Regulation) compliance checks and restricted to share confidential, personal or sensitive information's (hereinafter referred as critical data) into public domains. As ChatGPT server is hosted outside corporate boundaries, to fulfil GDPR compliance check, the corporate companies must have necessary systems in place of any data leaving outside of corporate boundaries.We are proposing a novel solution in identifying GDPR noncompliant DPP (Data Privacy and Protection) entities from the prompt query given to ChatGPT. To achieve this, from the corporate documents we first manually tag critical entities from 'named entity tagging tool' in building the corporate specific DPP entities knowledgebase, build custom NER (Named Entity Recognition) model on top of prebuilt corporate specific DPP entities knowledgebase, an ChatGPT playground interface to accept any user's prompt query, before firing the query to ChatGPT we validate the user's prompt query against custom NER model to detect if any corporate specific DPP entities are present, warn the user by highlighting the corporate specific DPP entities if present to facilitate user in negating the same, we enabled feedback loop from the user for the highlighted corporate specific DPP entities to improvise the custom NER model and logging all the input prompt queries fired to ChatGPT to enable corporate auditing process by using techniques from Natural Language Processing (NLP), Information Extraction, Information Retrieval (IR) and Custom NER model. © 2024 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nayak, S.P. and Pasumarthi, S. and Rajagopal, B. and Verma, A.K.},
	year = {2024},
	keywords = {Natural language processing systems, Natural languages, ChatGPT, Compliance control, Custom named entity recognition model, General data protection regulations, Information extraction, Information retrieval, Language model, Language processing, Large language model, Named entity recognition, Natural language processing, Recognition models, Sensitive data, NLP, Custom NER model, DPP, GDPR, Information Extraction, Information Retrieval, LLM},
	annote = {Export Date: 23 June 2024},
}

@article{rainer_reporting_2024-1,
	title = {Reporting case studies in systematic literature studies—{An} evidential problem},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195473840&doi=10.1016%2fj.infsof.2024.107501&partnerID=40&md5=b030b447b2cf73523aec750252a31d2a},
	abstract = {Context: The term and label, “case study”, is not used consistently by authors of primary studies in software engineering research. It is not clear whether this problem also occurs for systematic literature studies (SLSs). Objective: To investigate the extent to which SLSs in/correctly use the term and label, “case study”, when classifying primary studies. Methods: We systematically collect two sub-samples (2010–2021 \& 2022) comprising a total of eleven SLSs and 79 primary studies. We examine the designs of these SLSs, and then analyse whether the SLS authors and the primary-study authors correctly label the respective primary study as a “case study”. Results: 76\% of the 79 primary studies are misclassified by SLSs (with the two sub-samples having 60\% and 81\% misclassification, respectively). For 39\% of the 79 studies, the SLSs propagate a mislabelling by the original authors, whilst for 37\%, the SLSs introduce a new mislabel, thus making the problem worse. SLSs rarely present explicit definitions for “case study” and when they do, the definition is not consistent with established definitions. Conclusions: SLSs are both propagating and exacerbating the problem of the mislabelling of primary studies as “case studies”, rather than – as we should expect of SLSs – correcting the labelling of primary studies, and thus improving the body of credible evidence. Propagating and exacerbating mislabelling undermines the credibility of evidence in terms of its quantity, quality and relevance to both practice and research. © 2024 The Author(s)},
	author = {Rainer, A. and Wohlin, C.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Systematic mapping studies, Literature studies, Case-studies, Credible evidence, Labelings, Misclassifications, Software engineering research, Sub-samples, Systematic review, Case study, Systematic mapping study},
	annote = {Export Date: 23 June 2024},
}

@incollection{waqas_using_2024-1,
	title = {Using {LowCode} and {NoCode} {Tools} in {DevOps}: {A} {Multivocal} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186469250&doi=10.1007%2f978-3-031-50590-4_5&partnerID=40&md5=c5ae8e1a062bce7898965cd9cbe9abcd},
	abstract = {DevOps and Low-code/No-code tools are two trends that are gaining interest in both academic and industry environments. This study analyzed the usage of low-code and no-code tools in DevOps. It examined the available tools, their applications, and the associated concerns or limitations. To do so, a Multivocal Literature Review (MVLR) was conducted to include academic and grey literature. The results reveal the utilization of a range of tools across multiple aspects of DevOps, including application delivery, workflow automation, process management, continuous integration, deployment, monitoring and logging. Despite their potential benefits, concerns and limitations such as script standardization, security risks, limited customization, vendor lock-in, and scalability issues persist. This review, alongside identified concerns, emphasizes the increasing adoption and benefits of Low-code/No-code tools in DevOps. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Waqas, M. and Ali, Z. and Sánchez-Gordón, M. and Kristiansen, M.},
	year = {2024},
	keywords = {Multivocal literature review, DevOps tools, Low-code, No-code},
	annote = {Export Date: 23 June 2024},
}

@article{lin_shape_2024-1,
	title = {Shape or size matters? {Towards} standard reporting of tensile testing parameters for human soft tissues: systematic review and finite element analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190110729&doi=10.3389%2ffbioe.2024.1368383&partnerID=40&md5=481782653b6ae903cfe0c77a6615b1b3},
	abstract = {Material properties of soft-tissue samples are often derived through uniaxial tensile testing. For engineering materials, testing parameters (e.g., sample geometries and clamping conditions) are described by international standards; for biological tissues, such standards do not exist. To investigate what testing parameters have been reported for tensile testing of human soft-tissue samples, a systematic review of the literature was performed using PRISMA guidelines. Soft tissues are described as anisotropic and/or hyperelastic. Thus, we explored how the retrieved parameters compared against standards for engineering materials of similar characteristics. All research articles published in English, with an Abstract, and before 1 January 2023 were retrieved from databases of PubMed, Web of Science, and BASE. After screening of articles based on search terms and exclusion criteria, a total 1,096 articles were assessed for eligibility, from which 361 studies were retrieved and included in this review. We found that a non-tapered shape is most common (209 of 361), followed by a tapered sample shape (92 of 361). However, clamping conditions varied and were underreported (156 of 361). As a preliminary attempt to explore how the retrieved parameters might influence the stress distribution under tensile loading, a pilot study was performed using finite element analysis (FEA) and constitutive modeling for a clamped sample of little or no fiber dispersion. The preliminary FE simulation results might suggest the hypothesis that different sample geometries could have a profound influence on the stress-distribution under tensile loading. However, no conclusions can be drawn from these simulations, and future studies should involve exploring different sample geometries under different computational models and sample parameters (such as fiber dispersion and clamping effects). Taken together, reporting and choice of testing parameters remain as challenges, and as such, recommendations towards standard reporting of uniaxial tensile testing parameters for human soft tissues are proposed. Copyright © 2024 Lin, Pirrung, Niestrawska, Ondruschka, Pinter, Henyš and Hammer.},
	author = {Lin, A.C. and Pirrung, F. and Niestrawska, J.A. and Ondruschka, B. and Pinter, G. and Henyš, P. and Hammer, N.},
	year = {2024},
	note = {Publisher: Frontiers Media SA},
	keywords = {Aspect ratio, Aspect-ratio, Bone, Dogbone, Dumbbell, Finite element method, Geometry, Histology, Human soft tissue, ISO material testing standard, ISO Standards, Load testing, Material testing, Non-tapered, Rectangular, Soft tissue, Stress analysis, Stress concentration, Tapered, Tensile stress, Tensile testing, Testing standards, Tissue engineering, aspect ratio, dog bone, dumbbell, human soft tissue, non-tapered, rectangular, tapered},
	annote = {Export Date: 23 June 2024},
}

@article{cunha_insight_2024-1,
	title = {An insight into the capabilities of professionals and teams in agile software development: {An} update of the systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186425034&doi=10.24138%2fjcomss-2023-0172&partnerID=40&md5=0fb0b3eb775f0b92eb3fb70e8c650c20},
	abstract = {Agile Software Development (ASD) confronts the challenge of effectively measurement and predicting the capabilities of software engineers and teams to improve individual performance, team efficiency, and project success. This study delves into exploring and identifying gaps and research prospects in assessing and predicting human capabilities within ASD. Thus, we conducted a Systematic Literature Review, building upon a prior review from 2001 to 2016 by different authors. To encompass primary studies published after 2016, we extended to 2022. Further, our study extends the scope of the previous SLR with a new research question to identify key attributes in publications focused on agile team formation. Our findings disclosed new attributes for evaluating and predicting the capabilities of professionals engaged in ASD, such as Openness to Creativity and Agile Adaptation. These attributes boost individual performance, contribute to ameliorated team productivity, and facilitate the precise composition of teams. Moreover, this study expands our prior study, providing more details on capability identification and research design, extends the analysis of attributes and prediction models, provides a more granular discussion of discoveries and comparisons with prior review, and more indepth discussion about practical implications and thoroughly examines study validity. We observed that technical metrics were more prevalent than social and innovative aspects. Also, the study identified the prediction of agile capabilities as an emerging research domain necessitating further scrutiny due to the scarcity of existing models. The majority of studies (78\%) supplied detailed metric descriptions, facilitating the evolution of the capabilities repository and supporting future investigations in this domain. Ultimately, these findings can aid agile practitioners in formulating team composition decisions based on individuals’ and teams’ appraised and foreseen abilities. © 2024, Croatian Communications and Information Society. All rights reserved.},
	author = {Cunha, F. and Perkusich, M. and Guimaraes, E. and Santos, R. and Rique, T. and Albuquerque, D. and Perkusich, A. and Almeida, H. and Gorgônio, K.C.},
	year = {2024},
	note = {Publisher: Croatian Communications and Information Society},
	keywords = {Software engineering, Systematic literature review, Agile software development, Agile teams, Capability mea-surement, Capability prediction, Individual capability, Software development, Team capability},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{tseng_unlocking_2024-1,
	title = {Unlocking the {Potential} of {Open} {Government} {Data}: {Exploring} the {Strategic}, {Technical}, and {Application} {Perspectives} of {High}-{Value} {Datasets} {Opening} in {Taiwan}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195261385&doi=10.1145%2f3657054.3657089&partnerID=40&md5=39e4f20be1e5ec31effab47f6affbaed},
	abstract = {Today, data has an unprecedented value as it forms the basis for data-driven decision-making, including serving as an input for AI models, where the latter is highly dependent on the availability of the data. However, availability of data in an open data format creates a little added value, where the value of these data, i.e., their relevance to the real needs of the end user, is key. This is where the concept of “high-value dataset” (HVD) comes into play, which has become popular in recent years. Defining and opening HVD is a process consisting of a set of interrelated steps, the implementation of which may vary from one country or region to another. Therefore, there has recently been a call to conduct research in a country or region setting considered to be of greatest national value. So far, only a few studies have been conducted at the regional or national level, most of which consider only one step of the process, such as identifying HVD or measuring their impact. With this study, we answer this call and examine the national case of Taiwan by exploring the entire lifecycle of HVD opening. As such, the aim of the paper is to understand and evaluate the life cycle of high-value dataset publishing in one of the world’s leading producers of information and communication technology (ICT) products - Taiwan. To do this, we conduct a qualitative study with exploratory interviews with representatives from government agencies in Taiwan responsible for HVD opening, exploring the entire HVD lifecycle. As such, we examine (1) strategic aspects related to the HVD determination process, (2) technical aspects, and (3) application aspects. © 2024 Copyright held by the owner/author(s).},
	publisher = {Association for Computing Machinery},
	author = {Tseng, H.-L. and Nikiforova, A.},
	year = {2024},
	keywords = {Ecosystems, Decision making, Life cycle, e-government, High-value dataset, High-value dataset”, Impact, OGD, Open Data, Open data ecosystem, Open datum, Open government data, Public data, Public data ecosystem, Public values, SDG, Sustainable development, Sustainable development goal, high-value dataset, HVD, impact, open data ecosystem, open government data, public data ecosystem, public value, sustainable development goal},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{chen_murs_2023-1,
	title = {{MuRS}: {Mutant} {Ranking} and {Suppression} using {Identifier} {Templates}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180550494&doi=10.1145%2f3611643.3613901&partnerID=40&md5=9ee43fb4c7f4d5a78da0b97896dcccb4},
	abstract = {Diff-based mutation testing is a mutation testing approach that only mutates lines affected by a code change under review. This approach scales independently of the code-base size and introduces test goals (mutants) that are directly relevant to an engineer's goal such as fixing a bug, adding a new feature, or refactoring existing functionality. Google's mutation testing service integrates diff-based mutation testing into the code review process and continuously gathers developer feedback on mutants surfaced during code review. To enhance the developer experience, the mutation testing service uses a number of manually-written rules that suppress not-useful mutants - mutants that have consistently received negative developer feedback. However, while effective, manually implementing suppression rules requires significant engineering time. This paper proposes and evaluates MuRS, an automated approach that groups mutants by patterns in the source code under test and uses these patterns to rank and suppress future mutants based on historical developer feedback on mutants in the same group. To evaluate MuRS, we conducted an A/B testing study, comparing MuRS to the existing mutation testing service. Despite the strong baseline, which uses manually-written suppression rules, the results show a statistically significantly lower negative feedback ratio of 11.45\% for MuRS versus 12.41\% for the baseline. The results also show that MuRS is able to recover existing suppression rules implemented in the baseline. Finally, the results show that statement-deletion mutant groups received both the most positive and negative developer feedback, suggesting a need for additional context that can distinguish between useful and not-useful mutants in these groups. Overall, MuRS is able to recover existing suppression rules and automatically learn additional, finer-grained suppression rules from developer feedback. © 2023 Owner/Author.},
	publisher = {Association for Computing Machinery, Inc},
	author = {Chen, Z. and Salawa, M. and Vijayvergiya, M. and Petrović, G. and Ivanković, M. and Just, R.},
	year = {2023},
	keywords = {Software testing, Refactorings, Automated approach, Code changes, Code review, Codes (symbols), Developer feedback, Engineering time, Google+, Mutation testing, Review process, Source codes, Verification, Code Review, Developer Feedback, Mutation Testing},
	annote = {Export Date: 23 June 2024},
}

@article{bourai_deep_2024-1,
	title = {Deep learning-assisted medical image compression challenges and opportunities: systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191883296&doi=10.1007%2fs00521-024-09660-8&partnerID=40&md5=13b002c543c49b3905dbde4e4d088646},
	abstract = {Over the preceding decade, there has been a discernible surge in the prominence of artificial intelligence, marked by the development of various methodologies, among which deep learning emerges as a particularly auspicious technique. The captivating attribute of deep learning, characterised by its capacity to glean intricate feature representations from data, has served as a catalyst for pioneering approaches and methodologies spanning a multitude of domains. In the face of the burgeoning exponential growth in digital medical image data, the exigency for adept image compression methodologies has become increasingly pronounced. These methodologies are designed to preserve bandwidth and storage resources, thereby ensuring the seamless and efficient transmission of data within medical applications. The critical nature of medical image compression accentuates the imperative to confront the challenges precipitated by the escalating deluge of medical image data. This review paper undertakes a comprehensive examination of medical image compression, with a predominant focus on sophisticated, research-driven deep learning techniques. It delves into a spectrum of approaches, encompassing the amalgamation of deep learning with conventional compression algorithms and the application of deep learning to enhance compression quality. Additionally, the review endeavours to explicate these fundamental concepts, elucidating their inherent characteristics, merits, and limitations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	author = {Bourai, N.E.H. and Merouani, H.F. and Djebbar, A.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic Review, Deep learning, Learning systems, ]+ catalyst, 3d predictor, Digital storage, Exponential growth, Feature representation, Image compression, Lossless compression, Lossy compressions, Medical applications, Medical image compression, Medical images datum, Medical imaging, 3D Predictors, Lossy compression},
	annote = {Export Date: 23 June 2024},
}

@article{krafft_black-box_2024-1,
	title = {Black-{Box} {Testing} and {Auditing} of {Bias} in {ADM} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194259767&doi=10.1007%2fs11023-024-09666-0&partnerID=40&md5=c0ad5488881bfd7adbca316b7f704c7c},
	abstract = {For years, the number of opaque algorithmic decision-making systems (ADM systems) with a large impact on society has been increasing: e.g., systems that compute decisions about future recidivism of criminals, credit worthiness, or the many small decision computing systems within social networks that create rankings, provide recommendations, or filter content. Concerns that such a system makes biased decisions can be difficult to investigate: be it by people affected, NGOs, stakeholders, governmental testing and auditing authorities, or other external parties. Scientific testing and auditing literature rarely focuses on the specific needs for such investigations and suffers from ambiguous terminologies. With this paper, we aim to support this investigation process by collecting, explaining, and categorizing methods of testing for bias, which are applicable to black-box systems, given that inputs and respective outputs can be observed. For this purpose, we provide a taxonomy that can be used to select suitable test methods adapted to the respective situation. This taxonomy takes multiple aspects into account, for example the effort to implement a given test method, its technical requirement (such as the need of ground truth) and social constraints of the investigation, e.g., the protection of business secrets. Furthermore, we analyze which test method can be used in the context of which black box audit concept. It turns out that various factors, such as the type of black box audit or the lack of an oracle, may limit the selection of applicable tests. With the help of this paper, people or organizations who want to test an ADM system for bias can identify which test methods and auditing concepts are applicable and what implications they entail. © The Author(s) 2024.},
	author = {Krafft, T.D. and Hauer, M.P. and Zweig, K.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {Artificial intelligence, Decision making, Social networking (online), Taxonomies, Machine learning, Machine-learning, Behavioral research, Algorithmics, Auditing, Automated decision making, Bias, Black boxes, Black-box testing, Computing system, Decision-making systems, Small decisions, Social sciences computing, Test method, Automated decision-making, Black box, Testing},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{alshehhi_6dvf_2024-1,
	title = {{6DVF}: {A} {Framework} for the {Development} and {Evaluation} of {Mobile} {Data} {Visualisations}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193993437&doi=10.5220%2f0012692900003687&partnerID=40&md5=1273a2dbb4880051c8da6756c713353f},
	abstract = {Mobile apps, in particular tracking apps, rely heavily on data visualisations to empower end-users to make decisions about their health, sport, finance, household, and more. This has prompted app designers and developers to invest more effort in delivering quality visualisations. Many frameworks, including the Visualisation and Design Framework and Google Material Design, have been developed to guide the creation of informative and well-designed charts. However, our study reveals the need to incorporate additional aspects in the design process of such data visualisations to address user characterisation and needs, the nature of data to visualise, and the experience on small smart screens. In this paper, we introduce the Six-Dimensions Data Visualization Framework (6DVF), specifically designed for data visualisation on mobile devices. The 6DVF encompasses user characteristics and needs, data attributes, chart styling, interaction, and the mobile environment. We conducted two evaluation studies to measure the effectiveness and practicality of our 6DVF in guiding designers, pinpointing areas for improvement—especially in data completeness and usability for end-users. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	publisher = {Science and Technology Publications, Lda},
	author = {Alshehhi, Y.A. and Ahmad, K. and Abdelrazek, M. and Bonti, A.},
	year = {2024},
	keywords = {End-users, Design, Data visualization, Evaluation methods, Human-centered computing, Mobile app, Mobile data, Visualization, Visualization design guideline, Visualization designs, Visualization evaluation method, Visualization framework, Visualization technique, Human-Centered Computing, Visualisation Design Guidelines, Visualisation Evaluation Methods, Visualisation Techniques},
	annote = {Export Date: 23 June 2024},
}

@article{ramadhan_combining_2024-1,
	title = {Combining intelligent tutoring systems and gamification: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167368098&doi=10.1007%2fs10639-023-12092-x&partnerID=40&md5=43507a0e5e0cd0e0cd9b55014c46e64e},
	abstract = {One of the Information and Communication Technology (ICT) developments used in the learning process is the Intelligent Tutoring System (ITS), and gamification can overcome boredom, lack of interest or motivation, and monotony when using the ITS. In this study, the application of ITS equipped with Gamification is called ITS + G. Currently, several studies have built the ITS + G. However, there has not been a Systematic Literature Review (SLR) that synthesizes the characteristics of the ITS and Gamification combination. Several previous SLRs have been carried out and discussed the ITS only and several other SLRs discussed gamification only. Therefore, this SLR focused on the characteristics of ITS and gamification as a unit. This study succeeded in synthesizing that ITS + G has the potential to be applied to both STEM and non-STEM subjects. Three main game elements are mostly used in ITS + G: levels, points, and progress bars, which are supported for several reasons. Several techniques that have been used to measure the success of ITS + G are synthesized. Several positive impacts of ITS + G are revealed. Some negative impacts that need to be considered and studied in future research are also noticed. The results of this study could be the basis for ITS + G research in the future and increase the repertoire of knowledge related to ITS and Gamification. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	author = {Ramadhan, A. and Warnars, H.L.H.S. and Razak, F.H.A.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Systematic literature review, Intelligent Tutoring System. Gamification},
	annote = {Export Date: 23 June 2024},
}

@article{russo_towards_2024-1,
	title = {Towards a {Comprehensive} {Framework} for the {Multidisciplinary} {Evaluation} of {Organizational} {Maturity} on {Business} {Continuity} {Program} {Management}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152037415&doi=10.1080%2f19393555.2023.2195577&partnerID=40&md5=d074612c8a1ed3cb1576ba883e6469fd},
	abstract = {Organizational dependency on Information and Communication Technology (ICT) drives the preparedness challenge to cope with business process disruptions. Business Continuity Management (BCM) encompasses effective planning to enable business functions to resume to an acceptable state of operation within a defined timeframe. This paper presents a systematic literature review that communicates the strategic guidelines to streamline the organizational processes in the BCM program, culminating in the Business Continuity Plan design, according to the organization’s maturity. The systematic literature review methodology follows the Evidence-Based Software Engineering protocol assisted by the Parsifal tool, using the EbscoHost, ScienceDirect, and Scopus databases, ranging from 2000 to February 2021. International Standards and Frameworks guide the BCM program implementation, however, there is a gap in communicating metrics and what needs to be measured in the BCM program. The major paper result is the confirmation of the identified gap, through the analysis of the studies that, according to the BCM components, report strategic guidelines to streamline the BCM program. The analysis quantifies and discusses the contribution of the studies on each BCM component to design a framework supported by metrics, that allows assessing the organization’s preparedness in each BCM component, focusing on Information Systems and ICT strategies. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	author = {Russo, N. and Reis, L. and Silveira, C. and Mamede, H.S.},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Software engineering, Information management, Systematic literature review, Information systems, Information use, Business continuity, Business continuity management, Business continuity plans, Information and Communication Technologies, Management components, Management projects, Organisational, Organizational maturity, Program management, Business continuity plan, information and communication technology, information systems, measurement, organizational maturity},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{cannon_exploring_2024-1,
	title = {Exploring {Knowledge}-{Based} {Systems} for {Commercial} {Mortgage} {Underwriting}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181982238&doi=10.1007%2f978-3-031-50385-6_9&partnerID=40&md5=d52fdc852e47217b293e25ba18fad7c1},
	abstract = {While the residential mortgage industry has benefited from automated mortgage applications and underwriting, the commercial mortgage industry still relies heavily on manual underwriting. The paper aims to present the state of research in the domain of knowledge-based systems (KBS) and commercial mortgage underwriting and to identify the challenges in commercial mortgage underwriting by conducting a systematic literature review (SLR). The SLR uses the review process outlined by Kitchenham and includes a hybrid coding approach to analyze the data. The paper finds that KBS and ontologies to automate commercial mortgage underwriting were not studied yet. It also identifies several challenges in mortgage underwriting in data, process, and underwriting categories. The findings of this paper can be used for further research in the field of commercial mortgage underwriting and KBS. KBS appear suitable to address the identified challenges and can be integrated into information system automation or artificial intelligence (AI) implementations for commercial mortgage underwriting. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Cannon, K.P. and Preis, S.J.},
	year = {2024},
	keywords = {Systematic literature review, State of research, Review process, Commercial mortgage underwriting, Domain of knowledge, Hybrid coding, Knowledge based systems, Knowledge-based systems, Mortgage industry, Ontology, Ontology's, Underwriting challenge, commercial mortgage underwriting, knowledge-based system, underwriting challenges},
	annote = {Export Date: 23 June 2024},
}

@article{iqbal_exploring_2024-1,
	title = {Exploring issues of story-based effort estimation in {Agile} {Software} {Development} ({ASD})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189756864&doi=10.1016%2fj.scico.2024.103114&partnerID=40&md5=5c6fe43964d8140912f49f9bc15524e9},
	abstract = {Context: Effort estimation based on user stories plays a pivotal role in agile software development, where accurate predictions of project efforts are vital for success. While various supervised ML tools attempt to estimate effort, the prevalence of estimation errors presents significant challenges, as evidenced by the CHAOS report by the Standish Group, which highlights incorrect estimations contributing to a substantial percentage of failed agile projects. Objectives: This research delves into the domain of user story-based effort estimation in agile software development, aiming to explore the issues arising from inaccurate estimations. The primary goal is to uncover these issues comprehensively and propose potential solutions, thus enhancing the efficacy of the user story-based estimation method. Methods: To achieve the research objectives, a systematic literature review (SLR) is conducted, surveying a wide range of sources to gather insights into issues surrounding user story-based effort estimation. The review encompasses diverse estimation methods, user story attributes, and the array of challenges that can result from inaccurate estimations. Results: The SLR reveals a spectrum of issues undermining the accuracy of user story-based effort estimation. It identifies internal factors like communication, team expertise, and composition as crucial determinants of estimation reliability. Consistency in user stories, technical complexities, and task engineering practices also emerge as significant contributors to estimation inaccuracies. The study underscores the interconnectedness of these issues, emphasizing the need for a standardized protocol to minimize inaccuracies and enhance estimation precision. Conclusion: In light of the findings, it becomes evident that addressing the multi-dimensional factors influencing user story-based effort estimation is imperative for successful agile software development. The study underscores the interplay of various aspects, such as team dynamics, task complexity, and requirement engineering, in achieving accurate estimations. By recognizing these challenges and implementing recommended solutions, software development processes can avoid failures and enhance their prospects of success in the agile paradigm. © 2024 Elsevier B.V.},
	author = {Iqbal, M. and Ijaz, M. and Mazhar, T. and Shahzad, T. and Abbas, Q. and Ghadi, Y. and Ahmad, W. and Hamam, H.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Software design, User stories, Agile, Effort Estimation, Issues and challenges, Ml, User story effort estimation, User story, Effort estimation, Systematic Literature Review (SLR)},
	annote = {Export Date: 23 June 2024},
}

@article{verbeke_safeguarding_2024-1,
	title = {Safeguarding {Users} of {Consumer} {Mental} {Health} {Apps} in {Research} and {Product} {Improvement} {Studies}: an {Interview} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183317493&doi=10.1007%2fs12152-024-09543-8&partnerID=40&md5=3cfcc42befa8c56721a1a2b252cfaa03},
	abstract = {Mental health-related data generated by app users during the routine use of Consumer Mental Health Apps (CMHAs) are being increasingly leveraged for research and product improvement studies. However, it remains unclear which ethical safeguards and practices should be implemented by researchers and app developers to protect users during these studies, and concerns have been raised over their current implementation in CMHAs. To better understand which ethical safeguards and practices are implemented, why and how, 17 app developers and researchers were interviewed who had been involved in using CMHA data for studies. Interviewees discussed the impact on stakeholder interests, sufficiency thresholds and procedural alterations of informed consent, data protection, gathering app user perspectives and representing users in app design and study conduct, and ensuring adequate support. Although the reasoning behind how and why these ethical safeguards and practices should be implemented showed considerable variability and several gaps, interviewees converged on various general lines of reasoning. This allowed for the development of a coherent and nuanced account that could prove useful for future CMHA studies and which could stimulate further normative investigation of the role of ethical safeguards and practices in these studies. © 2024, The Author(s), under exclusive licence to Springer Nature B.V.},
	author = {Verbeke, K. and Jain, C. and Shpendi, A. and Borry, P.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.},
	keywords = {human, article, consumer, informed consent, interview, mental health, reasoning, research ethics, role playing, Safety, Consumer mental health apps, Data protection, Inclusive, Informed consent, Research ethics},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{vyakaranam_preliminary_2024-1,
	title = {Preliminary {Study}: {Speech} {Emotion} {Recognition} in {Online} {Teaching} {From} the {Perspective} of {Educators} {Especially} {Late} {Deafened}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191480308&doi=10.1109%2fICoSEIT60086.2024.10497503&partnerID=40&md5=a411b6e9dde2de86d370a3b44da0cd85},
	abstract = {An effective Human Computer Interaction (HCI) for recognizing human emotions automatically is known to as affective computing. Affective computing is achieved through programming, which is meant to detect emotions from facial expressions or verbal utterances. These detected emotions can be further used in many applications. Speech emotion recognition (SER) is one such affective computing application where the detected emotions can be used effectively, particularly in online education. Such an application would allow for the teaching strategies to be tailored for the students, on the basis of their understanding of the ongoing class, which is reflected in their verbal feedback. This valuable application of technology will enhance and support education for all, especially the educators who have issues with hearing verbal feedback from students. The main objective of this study is to find out from educators (with and without hearing impairment) on the necessity of knowing student emotions to enhance teaching outcomes for effective teaching and also the benefits of having an automatic detection system which would display student emotions in an online classroom setting. The outcome of this research clearly indicated how a preliminary user requirement study highlights the need and benefits of implementing a SER system. © 2024 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Vyakaranam, A. and Ramayah, B. and Maul, T.},
	year = {2024},
	keywords = {Audition, Human computer interaction, Affective Computing, Emotion Recognition, Speech recognition, Computing applications, E-learning, Facial Expressions, Hearing impaired, On-line education, Online teaching, Recognizing Human Emotion, Speech emotion recognition, Student emotions, Students, Teaching strategy, human computer interaction, affective computing, hearing impaired, speech emotion recognition},
	annote = {Export Date: 23 June 2024},
}

@article{alfayez_technical_2024-1,
	title = {Technical debt ({TD}) through the lens of {Twitter}: {A} survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150285686&doi=10.1002%2fsmr.2547&partnerID=40&md5=61dbe64fbea9285e8238a45ee0af5365},
	abstract = {Technical debt (TD) is a metaphor used to refer to the added software system costs acquired from taking shortcuts. Unfortunately, large amounts of TD can lead to serious consequences, and, thus, the management of TD is essential. Due to TD being a relatively new subject of study, many aspects of TD remain ambiguous. Fortunately, Twitter has been proven to hold a wealth of information on many subjects. As such, this survey study aims to gain a better understanding on how interest in TD has evolved over time and how TD is addressed on Twitter. A total of 128,897 TD-related tweets were scrapped from Twitter and analyzed using a number of proxy measures and Latent Dirichlet Allocation (LDA). The results revealed that interest in TD on Twitter has been generally increasing since the platform's early stages. Furthermore, TD-related tweets were found to revolve around 11 distinct categories. The TD in games category was discovered to be the most popular category, followed by TD communication and TD repayment. The results highlight that TD is a diverse and overarching topic that contains many potential avenues for further exploration. Software engineering researchers, practitioners, and educators can utilize this study to help steer their TD-related future efforts. © 2023 John Wiley \& Sons Ltd.},
	author = {Alfayez, R. and Winn, R. and Ding, Y. and Alfayez, G. and Boehm, B.},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Software engineering, Social networking (online), Topic Modeling, Twitter, Software-systems, Technical debts, Large amounts, Proxy measure, Statistics, Survey study, System costs, Through the lens, Wealth of information, survey study, technical debt, topic modeling},
	annote = {Export Date: 23 June 2024},
}

@article{gamon-sanz_industries_2024-1,
	title = {Industries, frameworks, and key drivers of lean startup: a systematic literature review},
	shorttitle = {Sectores, marcos de trabajo y factores clave del lean startup: una revisión sistemática de la literatura},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194199657&doi=10.18845%2fte.v18i2.7137&partnerID=40&md5=7a8ba0c27daa55420998a5bf874847ce},
	abstract = {Lean Startup provides an iterative, hypothesis-driven approach to business creation and product development, promoting data-driven decision-making by involving potential users and customers during the development cycle. Despite increased academic attention, the debate on the benefits of Lean Startup is open. To contribute to the understanding of how organisations incorporate Lean Startup principles, this study conducts a systematic literature review and identifies the industries in which its application has been analysed, the adaptation models followed by organisations, and the key drivers for its adoption in new and established organisations. The study's findings contribute to advancing knowledge on the adoption of Lean Startup principles, tools, and techniques in organisations. The results also help to the academic debate surrounding the usefulness and application of Lean Startup principles across different organisational contexts. © 2024 Business School, Instituto Tecnologico de Costa Rica. All rights reserved.},
	author = {Gamón-Sanz, A. and Alegre, J. and Chiva, R.},
	year = {2024},
	note = {Publisher: Business School, Instituto Tecnologico de Costa Rica},
	keywords = {systematic literature review, continuous experimentation, innovation, Lean Startup, product development},
	annote = {Export Date: 23 June 2024},
}

@article{borstler_acceptance_2024-1,
	title = {Acceptance behavior theories and models in software engineering — {A} mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190986067&doi=10.1016%2fj.infsof.2024.107469&partnerID=40&md5=aeb245400acdd1eab3fe1df63e4db457},
	abstract = {Context: The adoption or acceptance of new technologies or ways of working in software development activities is a recurrent topic in the software engineering literature. The topic has, therefore, been empirically investigated extensively. It is, however, unclear which theoretical frames of reference are used in this research to explain acceptance behaviors. Objective: In this study, we explore how major theories and models of acceptance behavior have been used in the software engineering literature to empirically investigate acceptance behavior. Method: We conduct a systematic mapping study of empirical studies using acceptance behavior theories in software engineering. Results: We identified 47 primary studies covering 56 theory uses. The theories were categorized into six groups. Technology acceptance models (TAM and its extensions) were used in 29 of the 47 primary studies, innovation theories in 10, and the theories of planned behavior/ reasoned action (TPB/TRA) in six. All other theories were used in at most two of the primary studies. The usage and operationalization of the theories were, in many cases, inconsistent with the underlying theories. Furthermore, we identified 77 constructs used by these studies of which many lack clear definitions. Conclusions: Our results show that software engineering researchers are aware of some of the leading theories and models of acceptance behavior, which indicates an attempt to have more theoretical foundations. However, we identified issues related to theory usage that make it difficult to aggregate and synthesize results across studies. We propose mitigation actions that encourage the consistent use of theories and emphasize the measurement of key constructs. © 2024 The Author(s)},
	author = {Börstler, J. and Ali, N.B. and Petersen, K. and Engström, E.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software design, Mapping, Behavioral research, Mapping studies, Acceptance behavior, Fitness, Innovations diffusion, TAM, Technology adoption, Theory and models, Theory use in software engineering, TPB, TRA, Innovation diffusion},
	annote = {Export Date: 23 June 2024},
}

@article{parashar_machine_2024-1,
	title = {Machine {Learning} for {Prediction} of {Cardiovascular} {Disease} and {Respiratory} {Disease}: {A} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182185184&doi=10.1007%2fs42979-023-02529-y&partnerID=40&md5=690bb2c326e3d41a6ad69c7ffb6e6e89},
	abstract = {Cardiovascular (CVD) and respiratory diseases (RD) have been in the active domain for machine learning (ML) researchers as these diseases significantly contribute to mortality in humans. Some studies suggest that CVD problems such as cerebrovascular problems, dysrhythmia, inflammatory heart disease, ischemic heart disease (IHD), and RD related problems remain high even after COVID-19 infection clears up. To the best of our knowledge, this is the only study that surveyed these two diseases. This paper’s goal is to explore the existing state of the art in the application of ML in the detection, categorization, and prediction of disorders related to CVD and RD. The review highlights ML algorithms used in prediction of CVD and RD related diseases, datasets used by the articles, technique used for feature selection, features selected for the study, dataset used in the article was unimodal or multimodal, and performance of the algorithm. In CVD category, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used Feature Selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. In case of RDs, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used feature selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. The intent of this review is to stimulate the interest of scientists in this challenging field and to acquaint them with current advances in the field. To design a system that predict CVD or RD in a patient using uni or multi modal datasets, approaches such as data cleaning, feature selection, region of interest (ROI) identification, and classification are applied. This article provided details related to publicly available datasets, most used classification algorithm with performance metric. © 2024, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	author = {Parashar, G. and Chaudhary, A. and Pandey, D.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Machine learning, Cardiovascular disease, Heart disease prediction, Lung disease prediction, Respiratory disease},
	annote = {Export Date: 23 June 2024},
}

@article{pessoa_journey_2024-1,
	title = {A {Journey} to {Identify} {Users}' {Classification} {Strategies} to {Customize} {Game}-{Based} and {Gamified} {Learning} {Environments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173024793&doi=10.1109%2fTLT.2023.3317396&partnerID=40&md5=1f2cf8f37cdfb1825e95025ee65cc6ef},
	abstract = {Game designers and researchers have sought to create gameful environments that consider user preferences to increase engagement and motivation. In this sense, it is essential to identify the most suitable game elements for users' profiles. Designers and researchers must choose strategies to classify users into predefined profiles and select the most appropriate game elements for each user. This activity may challenge designers, learning designers, and researchers since they must base their choice on personal aspects that require a deep understanding. Therefore, this article aims to assist game designers, learning designers, and researchers in selecting user classification strategies to customize and personalize game-based and gamified learning environments. By conducting systematic literature mapping, we consolidate the most common strategies and explore their applications in games and gamification. Our analysis, based on 25 publications, reveals that we can classify the strategies according to user interaction, user personality, learning style, and motivation for learning. Strategies based on user interactions emerge as the most popular, while questionnaires and log data systems are commonly used instruments for identifying user profiles. The findings of this SLM offer valuable knowledge for game designers and researchers to define the criteria that will be used to evaluate the effect of games and gamified environments in educational contexts. © 2008-2011 IEEE.},
	author = {Pessoa, M. and Lima, M. and Pires, F. and Haydar, G. and Melo, R. and Rodrigues, L. and Oliveira, D. and Oliveira, E. and Galvao, L. and Gadelha, B. and Isotani, S. and Gasparini, I. and Conte, T.},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Software design, Computer aided instruction, Users' experiences, Gamification, Motivation, Computer games, Distal outcome, Game, Game design, Game elements, Game-based Learning, Job analysis, Learning performance, Psychology, Systematic, Task analysis, User classification, User profile, User type, gamification, Distal outcomes, game elements, game-based learning, games, learning performance, user classification, user type},
	annote = {Export Date: 23 June 2024},
}

@article{rahman_systematic_2024-1,
	title = {A {Systematic} {Literature} {Review} on {Software} {Maintenance} {Offshoring} {Decisions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191657835&doi=10.1016%2fj.infsof.2024.107475&partnerID=40&md5=0ecbbd1c575abec8a390ebb7b093a2cc},
	abstract = {Context: Over the last decades, the rapid expansion of the internet has prompted an increasing number of organizations that have taken their work global and have outsourced their information technology (IT) activities to specialized suppliers. The longest part of the software life cycle includes software maintenance, which consumes 60-70\% of the total IT budget. Therefore, organizations have adopted offshoring strategies to reduce maintenance costs and free up resources to focus on their core competencies. Offshore outsourcing decision-making involves technical, social, and other influencing factors; however, there is a limited understanding of the key factors associated with offshoring software maintenance within the global software development context. Objective: This work presents the factors that have influenced the decision-making process of offshoring software maintenance. Further, this research sheds light on decision-making by identifying the models, frameworks, and software tools used within this context. Method: A systematic literature review is conducted, delving into the factors related to the decision-making and analyzing the models, frameworks and tools supporting offshoring software maintenance. Results: This study identifies the top 10 key factors concerning the decision-making process, namely human communication, cost reduction, organizational and employee maturity, project management practices, IT infrastructure support, language constraints, knowledge-based support, changes in requirements, legal issues and cultural diversity. In addition, the models, frameworks, and tools used in the decision-making process of software maintenance are analyzed, and research gaps are identified. Conclusion: The findings reveal that the software industry lacks effective and efficient models tailored explicitly for software offshoring within the global software development landscape. Overall, this study provides valuable insights into the decision-making dynamics of software maintenance offshoring by identifying key factors and research gaps that can pave the way for developing more effective decision support systems. © 2024 Elsevier B.V.},
	author = {Rahman, H.U. and da Silva, A.R. and Alzayed, A. and Raza, M.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Life cycle, Outsourcing, Decision-making process, Computer software, Knowledge based systems, Budget control, Cost reduction, Decision support systems, Factor, Global software development, Key factors, Modelling framework, Modelling tools, Off-shoring, Offshore oil well production, Project management, Research gaps, Factors, Offshoring, Software maintenance},
	annote = {Export Date: 23 June 2024},
}

@article{ataei_application_2023-1,
	title = {Application of microservices patterns to big data systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158161899&doi=10.1186%2fs40537-023-00733-4&partnerID=40&md5=b9d883d26f0ffb65d220eb10e5ae0f8d},
	abstract = {The panorama of data is ever evolving, and big data has emerged to become one of the most hyped terms in the industry. Today, users are the perpetual producers of data that if gleaned and crunched, have the potential to reveal game-changing patterns. This has introduced an important shift regarding the role of data in organizations and many strive to harness to power of this new material. Howbeit, institutionalizing data is not an easy task and requires the absorption of a great deal of complexity. According to the literature, it is estimated that only 13\% of organizations succeeded in delivering on their data strategy. Among the root challenges, big data system development and data architecture are prominent. To this end, this study aims to facilitate data architecture and big data system development by applying well-established patterns of microservices architecture to big data systems. This objective is achieved by two systematic literature reviews, and infusion of results through thematic synthesis. The result of this work is a series of theories that explicates how microservices patterns could be useful for big data systems. These theories are then validated through expert opinion gathering with 7 experts from the industry. The findings emerged from this study indicates that big data architectures can benefit from many principles and patterns of microservices architecture. © 2023, The Author(s).},
	author = {Ataei, P. and Staegemann, D.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic literature review, Microservice, Architecture, Big data, Big data architecture, Computer architecture, Data architectures, Data engineering, Data systems, Microservice pattern, Power, System development, Microservices, Data architecture, Microservices patterns},
	annote = {Export Date: 23 June 2024},
}

@article{larsen_statistical_2024-1,
	title = {Statistical {Challenges} in {Online} {Controlled} {Experiments}: {A} {Review} of {A}/{B} {Testing} {Methodology}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174052217&doi=10.1080%2f00031305.2023.2257237&partnerID=40&md5=6c9c5ad886579d9d80d246f2a4580971},
	abstract = {The rise of internet-based services and products in the late 1990s brought about an unprecedented opportunity for online businesses to engage in large scale data-driven decision making. Over the past two decades, organizations such as Airbnb, Alibaba, Amazon, Baidu, Booking.com, Alphabet’s Google, LinkedIn, Lyft, Meta’s Facebook, Microsoft, Netflix, Twitter, Uber, and Yandex have invested tremendous resources in online controlled experiments (OCEs) to assess the impact of innovation on their customers and businesses. Running OCEs at scale has presented a host of challenges requiring solutions from many domains. In this article we review challenges that require new statistical methodologies to address them. In particular, we discuss the practice and culture of online experimentation, as well as its statistics literature, placing the current methodologies within their relevant statistical lineages and providing illustrative examples of OCE applications. Our goal is to raise academic statisticians’ awareness of these new research opportunities to increase collaboration between academia and the online industry. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	author = {Larsen, N. and Stallrich, J. and Sengupta, S. and Deng, A. and Kohavi, R. and Stevens, N.T.},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {A/B testing, Literature review, Online controlled experiments, Randomized controlled trials, Treatment effect estimation},
	annote = {Export Date: 23 June 2024},
}

@article{zhao_identifying_2024-1,
	title = {Identifying the primary dimensions of {DevSecOps}: {A} multi-vocal literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191655409&doi=10.1016%2fj.jss.2024.112063&partnerID=40&md5=950cf6baeee3cf5ed2efac499a8425da},
	abstract = {Context: Security as a key non-functional requirement of software development is often ignored and devalued in DevOps programs, with security seen as an inhibitor to high velocity required in DevOps implementation. Hence, the DevSecOps approach as a security-orientated expansion to DevOps, has aimed to integrate security into DevOps implementation by promoting collaboration among development, operation and security teams. DevSecOps is a topical concept and rapidly emerging area of practice in both academic and industrial settings. Objective: We reviewed both the white and grey literature to identify recent researches and practical trends of DevSecOps, aiming to: (a) review, document and analyze the current state of DevSecOps in the existing literature; (b) investigate the application of DevSecOps in Global Software Engineering (GSE) contexts. Method: A Multi-vocal Literature Review on DevSecOps and its global application was conducted, by executing a dual-track strategy including white (104 studies) and grey (43 studies) literature from 2012 to 2021. A Thematic Analysis was performed to identify, synthesize and analyze the themes within data for reporting the MLR results. Results: Through the Multi-vocal Literature Review and Thematic Analysis, this paper identifies five major aspects of DevSecOps (Definitions, Challenges, Practices, Tools/Technologies, and Metrics/Measurement); collects related themes of each aspect; and generates a Challenge-Practice-Tool-Metric (CPTM) model by integrating the themes of the latter four aspects within a lifecycle model. Moreover, an unexplored area relating to the global application of DevSecOps has been identified. Conclusion: Based on MLR results, a CPTM (Challenge-Practice-Tool-Metric) model is built to reveal the current status of DevSecOps. The model provides a breakdown and a broad landscape of DevSecOps, from which researchers and practitioners may select an area of focus to improve their knowledge or practice. With DevSecOps spanning the many stages of the lifecycle, we believe the model will enable emphases and absences such as global aspects to be investigated. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2024 The Author(s)},
	author = {Zhao, X. and Clear, T. and Lal, R.},
	year = {2024},
	note = {Publisher: Elsevier Inc.},
	keywords = {Software design, Life cycle, Application programs, Literature reviews, Devsecop, Global applications, Global software engineering, METRIC model, Multivocal literature review, Non-functional requirements, Open science, Security, Thematic analysis, DevOps, DevSecOps},
	annote = {Export Date: 23 June 2024},
}

@article{gorkovenko_data-enhanced_2023-1,
	title = {Data-{Enhanced} {Design}: {Engaging} {Designers} in {Exploratory} {Sensemaking} with {Multimodal} {Data}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181706477&doi=10.57698%2fv17i3.01&partnerID=40&md5=d19ee20c1a2b039444e080ad65da687a},
	abstract = {Research in the wild can reveal human behaviors, contexts, and needs around products that are difficult to observe in the lab. Telemetry data from the use of physical products can help facilitate in the wild research, in particular by suggesting hypotheses that can be explored through machine learning models. This paper explores ways for designers without strong data skills to engage with multimodal data to develop a contextual understanding of product use. This study is framed around a lightweight version of a data enhanced design research process where multimodal telemetry data was captured by a GoPro camera attached to a bicycle. This was combined with the video data and conversation with the rider to carry out an exploratory sensemaking process and generate design research questions that could potentially be addressed through data capture, annotation, and machine learning. We identify a range of ways that designers could make use of the data for ideation and developing context through annotating and exploring the data. Participants used data and annotation practices to connect the micro and macro, spot interesting moments, and frame questions around an unfamiliar problem. The work follows the designers’ questions, methods, and explorations, both immediate concerns and speculations about working at larger scales with machine learning models. This points to the possibility of tools that help designers to engage with machine learning, not just for optimization and refinement, but for creative ideation in the early stages of design processes. © 2023 Gorkovenko, Jenkins, Vaniea, \& Murray-Rust.},
	author = {Gorkovenko, K. and Jenkins, A. and Vaniea, K. and Murray-Rust, D.},
	year = {2023},
	note = {Publisher: Chinese Institute of Design},
	keywords = {Cycling, Design Research, GoPro Data, Multimodal Data Annotation},
	annote = {Export Date: 23 June 2024},
}

@article{birkeland_research_2024-1,
	title = {Research areas and methods of interest in {European} intraday electricity market research—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189751189&doi=10.1016%2fj.segan.2024.101368&partnerID=40&md5=feb6e626b69abee383d73de8104038fa},
	abstract = {This paper establishes a robust foundation for the expansion of European intraday electricity market research through a systematic literature review. The review encompasses 132 primary studies from various libraries, categorizing them based on research area, methodologies, dataset context, and dataset date. The resulting taxonomy identifies six major research groups: Bidding, Market Modeling, Price Forecasting, Market Design, Forecast Errors, and Market Abuse. The analysis of the review results leads to actionable recommendations for future European intraday electricity market research. These recommendations include the utilization of close-to-live datasets to accurately reflect the impacts of the energy transition, the exploration of market abuse in the energy market industry, and the broadening of national electricity market studies beyond Germany. This systematic review aims to benefit various stakeholders, including academic researchers, industry participants, and European regulators, by providing a structured and objective compilation of existing research and offering insights into the identified gaps within the intraday electricity market research landscape. © 2024 The Author(s)},
	author = {Birkeland, D. and AlSkaif, T.},
	year = {2024},
	note = {Publisher: Elsevier Ltd},
	keywords = {Systematic literature review, Forecasting, Electric industry, Intraday electricity market, Market abuse, Market bidding, Market model, Market researches, Power markets, Price forecasting, Research areas, Research groups, Research method, Intraday electricity markets},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{ramzan_test-driven_2024-1,
	title = {Test-{Driven} {Development} ({TDD}) in {Small} {Software} {Development} {Teams}: {Advantages} and {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190268315&doi=10.1109%2fICACS60934.2024.10473291&partnerID=40&md5=20eefd4834bd0370b52239da636512c5},
	abstract = {In the context of small software development teams, this research article gives a thorough investigation of the adoption of test-driven development (TDD) approaches. It aims to highlight the benefits that TDD offers, such as improved code quality through modularization and proactive defect spotting which results in effective debugging and development processes. It also discusses the complex issues that arise when TDD is implemented in smaller teams, such as the learning curve and resource constraints. This study significantly advances the understanding of how TDD can be used to optimize software development techniques in organizations or software houses having small development teams. It also explores the possible advantages and challenges. © 2024 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ramzan, H.A. and Ramzan, S. and Kalsum, T.},
	year = {2024},
	keywords = {Development process, Software design, Software testing, Code quality, Debugging process, Development approach, Improved efficiency, Learning curves, Modular construction, Modularizations, Small team, Software development teams, Test driven development, improved efficiency, small teams, software development, TDD},
	annote = {Export Date: 23 June 2024},
}

@article{ganeshkumar_discovery_2024-1,
	title = {Discovery, development, and deployment of a user-centered point-of-care digital information system to treat and track hypertension and diabetes patients under {India} {Hypertension} {Control} {Initiative} 2019–2022, {India}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362882&doi=10.1177%2f20552076241250153&partnerID=40&md5=0f65a3332b2783d660a88b13a123271d},
	abstract = {Background: Hypertension affects 28.5\% of Indians aged 18–69. Real-time registration and follow-up of persons with hypertension are possible with point-of-care digital information systems. We intend to describe herein the experiences of discovering, developing, and deploying a point-of-care digital information system for public health facilities under the India Hypertension Control Initiative. Methods: We have adopted an agile and user-centered approach in each phase in selected states of India since 2017. A multidisciplinary team adopted a hybrid approach with quantitative and qualitative methods, such as contextual inquiries, usability testing, and semi-structured interviews with healthcare workers, to document and monitor utility and usability. Results: During the discovery phase, we adopted a storyboard technique to understand the requirement of a digital information system. The participatory approach in discovery phase co-designed the information system with the nurses and doctors at Punjab state of India. Simple, which is the developed information system, has a front-end Android mobile application for healthcare workers and a backend dashboard for program managers. As of October 2022, over 24,31,962 patients of hypertension and 8,99,829 diabetes were registered in the information system of 10,017 health facilities. The median duration of registering a new patient was 50 seconds, and for recording a follow-up visit was 14 seconds in the app. High satisfaction was reported in 100 app users’ quarterly interviews. Conclusion: Simple was implemented by administering a user-centered approach and agile techniques. It demonstrated high utility and usability among users, highlighting the benefits of a user-centered approach for effective digital health solutions. © The Author(s) 2024.},
	author = {Ganeshkumar, P. and Bhatnagar, A. and Burka, D. and Durgad, K. and Krishna, A. and Das, B. and Chandak, M. and Sharma, M. and Shivasankar, R. and Pathni, A.K. and Kunwar, A. and Kaur, P.},
	year = {2024},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {digital information system, Hypertension, India Hypertension Control Initiative, Simple app},
	annote = {Export Date: 23 June 2024},
}

@article{pranajaya_examining_2024-1,
	title = {Examining the influence of financial inclusion on investment decision: {A} bibliometric review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183975319&doi=10.1016%2fj.heliyon.2024.e25779&partnerID=40&md5=84031b0918cf4b2a700171cfb937cf8c},
	abstract = {This study delves into the contemporary landscape of potential financial inclusion in investment decision-making, leveraging bibliometric research methods. Analyzing 161 publications from the Scopus database (2006–2023), the authors employ performance analysis and scientific mapping tools, including VOSviewer and Biblioshiny R studio. Through co-citation analysis, bibliographic coupling, co-occurrence of keywords analysis, thematic mapping, and thematic evolution analysis, the study uncovers the essential characteristics of the research field. The Result underscores that Innovative financial technologies are positioned as enablers of financial inclusion, with fintech's potential to drive positive social impact. The findings underscore that fostering financial literacy, addressing challenges in fintech adoption, and supporting entrepreneurship are crucial for maximizing the benefits of financial technologies. Overall, the study advocates for a comprehensive approach that combines financial inclusion, individual attitudes, and expertise, and fintech innovation to enhance access to financial services and expand investment opportunities for a more inclusive and prosperous economic landscape. However, the study acknowledges limitations, such as reliance on a single database and exclusion of specific keywords, urging a more inclusive approach to ensure a comprehensive understanding of relevant literature in this dynamic field. © 2024 The Authors},
	author = {Pranajaya, E. and Alexandri, M.B. and Chan, A. and Hermanto, B.},
	year = {2024},
	note = {Publisher: Elsevier Ltd},
	keywords = {Research trends, Bibliometric analysis, Financial inclusion, Investment decision},
	annote = {Export Date: 23 June 2024},
}

@article{moguel-sanchez_bots_2023-1,
	title = {Bots in {Software} {Development}: {A} {Systematic} {Literature} {Review} and {Thematic} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182955496&doi=10.1134%2fS0361768823080145&partnerID=40&md5=ce1e41e228545c5e52caa01baa3d1388},
	abstract = {Abstract: Modern Software Engineering thrives with innovative tools that aid developers in creating better software grounded on quality standards. Software bots are an emerging and exciting trend in this regard, supporting numerous software development activities. As an emerging trend, few studies describe and analyze different bots in software development. This research presents a systematic literature review covering the state of the art of applied and proposed bots for software development. Our study spans literature from 2003 to 2022, with 82 different bots applied in software development activities, covering 83 primary studies. We found four bot archetypes: chatbots which focus on direct communication with developers to aid them, analysis bots that display helpful information in different tasks, repair bots for resolving software defects, and development bots that combine aspects of other bot technologies to provide a service to the developer. The primary benefits of using bots are increasing software quality, providing useful information to developers, and saving time through the partial or total automation of development activities. However, drawbacks are reported, including limited effectiveness in task completion, high coupling to third-party technologies, and some prejudice from developers toward bots and their contributions. We discovered that including Bots in software development is a promising field of research in software engineering that has yet to be fully explored. © 2023, Pleiades Publishing, Ltd.},
	author = {Moguel-Sánchez, R. and Martínez-Palacios, C.S.S. and Ocharán-Hernández, J.O. and Limón, X. and Sánchez-García, A.J.},
	year = {2023},
	note = {Publisher: Pleiades Publishing},
	keywords = {Systematic literature review, Software design, State of the art, Chatbots, Thematic analysis, Bot, Botnet, Computer software selection and evaluation, Development activity, Development bot, Emerging trends, Quality standard, Thematic synthesis, software development, bots, development bots, thematic synthesis},
	annote = {Export Date: 23 June 2024},
}

@article{dominguez_role_2024-1,
	title = {The role of ontologies in smart contracts: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192928073&doi=10.1016%2fj.jii.2024.100630&partnerID=40&md5=25e13e4895435b8b01c9f92b71783473},
	abstract = {The aim of this systematic literature review is to provide a comprehensive understanding of how ontologies address current Smart Contract challenges, identify application scenarios, and present tools and technologies associated with their use. This systematic literature review (SLR), following Kitchenham's methodology, analyses peer-reviewed articles from 2015 to August 2022 from databases such as Scopus, IEEE, Science Direct, Springer Link and ACM. Of the 501 publications identified, 21 are selected for in-depth review based on inclusion, exclusion and quality assessment criteria. The results of this SLR show that ontologies provide solutions to the challenges faced by Smart Contracts mainly at the creation stage. They allow the terms of the contract and the roles of the parties to be defined. Ontologies also enable the development of Smart Contract templates. This facilitates their use by people without technical programming expertise. Despite these potential solutions to the challenges that Smart Contracts face throughout their lifecycle, they lack verification. This increases the vulnerabilities to which Smart Contracts are exposed. Developing validation and verification tools could facilitate using ontologies to create Smart Contracts for different real-world cases. © 2024 Elsevier Inc.},
	author = {Dominguez, J.A. and Gonnet, S. and Vegetti, M.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Life cycle, 'current, Ontology, Ontology's, Application scenario, Assessment criteria, Block-chain, Blockchain, Inclusion-exclusion, Quality assessment, Smart contract, Tools and technologies, Validation and verification},
	annote = {Export Date: 23 June 2024},
}

@article{khoshnevis_search-based_2024-1,
	title = {Search-based approaches to optimizing software product line architectures: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188512328&doi=10.1016%2fj.infsof.2024.107446&partnerID=40&md5=4dd032426e87fee8e0653cb59b8215a1},
	abstract = {Context: Software product line architecture (PLA) plays an important role in developing software product lines (SPLs) and other configurable systems. Search-based (SB) approaches can optimize the design of PLAs according to a given set of metrics as fitness functions. Although this area has been explored by researchers, there is a lack of synthesis of search-based PLA (SBPLA) research. A comprehensive review would offer valuable insights into previous contributions and identify areas for further research. Objective: The objective of this work is to identify and summarize quality-assessed peer-reviewed studies on search-based PLA design from the aspects of the research scope, problems, contributions, evaluation, and open issues. Methods: We conducted a systematic literature review based on Kitchenham's methodology. Based on a predefined search protocol we identified related studies limited to the ones published between 2000 and 2022 in journals and conference proceedings. Results: Out of 686 initial search results, 34 papers were finally selected after a set of deep search, and criteria application activities. We provided a taxonomy of optimization problems in SBPLA and found that PLA remodularization and refactoring were the two categories most emphasized by the researchers. We also provided several other categorizations regarding contributions, research design, open issues, and other subjects of interest. Conclusions: The interest in SBPLA design has been growing since 2014. PLA cloning and re-engineering problems have never been addressed in the literature. Performing subjective evaluation with the participation of experts from the industry will be profitable, as a complementary method to objective experimental evaluation, and therefore carrying out quanti-qualitative research. © 2024 Elsevier B.V.},
	author = {Khoshnevis, S. and Ardestani, O.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Software design, Software Product Line, Computer software, Configurable systems, Metrics as fitness functions, Product line architecture, Quality control, Search-based, Search-based software architecture, Search-based software engineering, Software architecture, Software product line architecture},
	annote = {Export Date: 23 June 2024},
}

@article{galbin-nasui_bug_2024-1,
	title = {Bug reports priority classification models. {Replication} study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189942059&doi=10.1007%2fs10515-024-00432-1&partnerID=40&md5=084ec887cec6bbb2f20eb711594e7b6a},
	abstract = {Bug tracking systems receive a large number of bugs on a daily basis. The process of maintaining the integrity of the software and producing high-quality software is challenging. The bug-sorting process is usually a manual task that can lead to human errors and be time-consuming. The purpose of this research is twofold: first, to conduct a literature review on the bug report priority classification approaches, and second, to replicate existing approaches with various classifiers to extract new insights about the priority classification approaches. We used a Systematic Literature Review methodology to identify the most relevant existing approaches related to the bug report priority classification problem. Furthermore, we conducted a replication study on three classifiers: Naive Bayes (NB), Support Vector Machines (SVM), and Convolutional Neural Network (CNN). Two sets of experiments are performed: first, our own NLTK implementation based on NB and CNN, and second, based on Weka implementation for NB, SVM, and CNN. The dataset used consists of several Eclipse projects and one project related to database systems. The obtained results are better for the bug priority P3 for the CNN classifier, and overall the quality relation between the three classifiers is preserved as in the original studies. The replication study confirmed the findings of the original studies, emphasizing the need to further investigate the relationship between the characteristics of the projects used as training and those used as testing. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
	author = {Galbin-Nasui, A. and Vescan, A.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {Program debugging, BN, Bug priority prediction, Bug reports, Bug tracking system, Classification approach, Classification models, Convolutional neural network, Convolutional neural networks, Naive bayes, Replication study, Support vector machines, Support vectors machine, Bug report, CNN, SVM},
	annote = {Export Date: 23 June 2024},
}

@article{biazotto_technical_2024-1,
	title = {Technical debt management automation: {State} of the art and future perspectives},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180417326&doi=10.1016%2fj.infsof.2023.107375&partnerID=40&md5=db74021970a8b325b1de4b98f924dcab},
	abstract = {Context: Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system's maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most. Objectives: The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM. Methods: We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation. Results: We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges. Conclusion: The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts. © 2023 The Author(s)},
	author = {Biazotto, J.P. and Feitosa, D. and Avgeriou, P. and Nakagawa, E.Y.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {State of the art, Mapping, Systematic mapping studies, Automation, 'current, Technical debts, Codes (symbols), Future perspectives, Management activities, Management automations, Optimal decisions, Technical debt management, Technical debt, Systematic mapping study, Tools},
	annote = {Export Date: 23 June 2024},
}

@article{chueca_consolidation_2024-1,
	title = {The consolidation of game software engineering: {A} systematic literature review of software engineering for industry-scale computer games},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172685092&doi=10.1016%2fj.infsof.2023.107330&partnerID=40&md5=ef2b467462eb9ad92be33ef6e585775f},
	abstract = {Context: Game Software Engineering (GSE) is a branch of Software Engineering (SE) that focuses on the development of video game applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE. Objective: This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field. Method: A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE. Results: The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work. Conclusion: The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge. © 2023 Elsevier B.V.},
	author = {Chueca, J. and Verón, J. and Font, J. and Pérez, F. and Cetina, C.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, State of the art, Application programs, Human computer interaction, 'current, Computer games, Best practices, Engineering research, Game software, Game software engineering, Industry-scale, SLR, Video-games, Volume difference, Video games},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{soto_automated_2024-1,
	title = {Automated {Diagnosis} of {Prostate} {Cancer} {Using} {Artificial} {Intelligence}. {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175991217&doi=10.1007%2f978-3-031-46813-1_6&partnerID=40&md5=3de2e59f800c9fe45cbfe0f5dbe022d5},
	abstract = {Prostate cancer is one of the most preventable causes of death. Periodic testing, seconded by precursors such as living habits, heritage, and exposure to specific materials, help healthcare providers achieve early detection, a desirable scenario that positively correlates with survival. However, the currently available diagnosing mechanisms have a great opportunity of improvement in terms of invasiveness, sensitivity and timing before patients reach advanced stages with a significant probability of metastasis. Supervised artificial intelligence enables early diagnosis and excludes patients from unpleasant biopsies. In this work, we gathered information about methodologies, techniques, metrics, and benchmarks to accomplish early prostate cancer detection, including pipelines with associated patents and knowledge transfer mechanisms, intending to find the reasons precluding the solutions from being massively adopted in the standards of care. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Soto, S. and Pollo-Cattaneo, M.F. and Yepes-Calderon, F.},
	year = {2024},
	keywords = {Artificial intelligence, Systematic literature review, Knowledge management, Automated diagnosis, Automatic pathology diagnose, Causes of death, Diagnosis, Diseases, Early diagnosis, Health care providers, Invasiveness, Pathology, Periodic testing, Prostate cancers, Specific materials, Urology, Automatic pathology diagnosis, diagnosis, Prostate cancer},
	annote = {Export Date: 23 June 2024},
}

@article{jui_fairness_2024-1,
	title = {Fairness issues, current approaches, and challenges in machine learning models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183668121&doi=10.1007%2fs13042-023-02083-2&partnerID=40&md5=c5b3de8f74c7dc7e2d5d8886887b03d0},
	abstract = {With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness. © 2024, The Author(s).},
	author = {Jui, T.D. and Rivas, P.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Decision making, Decision-making process, Learning algorithms, Machine learning, Machine-learning, 'current, Mapping studies, Bias reduction, Fair prediction, Machine learning algorithms, Machine learning models, Model fairness, Training data, AI, Ethics},
	annote = {Export Date: 23 June 2024},
}

@book{kumar_ethical_2024-1,
	title = {The ethical frontier of {AI} and data analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189601406&doi=10.4018%2f979-8-3693-2964-1&partnerID=40&md5=c591996a91637e79d3e1dd27cc10b011},
	abstract = {In the advancing fields of artificial intelligence (AI) and data science, a pressing ethical dilemma arises. As technology continues its relentless march forward, ethical considerations within these domains become increasingly complex and critical. Bias in algorithms, lack of transparency, data privacy breaches, and the broader societal repercussions of AI applications are demanding urgent attention. This ethical quandary poses a formidable challenge for researchers, academics, and industry professionals alike, threatening the very foundation of responsible technological innovation. Navigating this ethical minefield requires a comprehensive understanding of the multifaceted issues at hand. The Ethical Frontier of AI and Data Analysis is an indispensable resource crafted to address the ethical challenges that define the future of AI and data science. Researchers and academics who find themselves at the forefront of this challenge are grappling with the evolving landscape of AI and data science ethics. Underscoring the need for this book is the current lack of clarity on ethical frameworks, bias mitigation strategies, and the broader societal implications, which hinder progress and leave a void in the discourse. As the demand for responsible AI solutions intensifies, the imperative for this reliable guide that consolidates, explores, and advances the dialogue on ethical considerations grows exponentially. Tailored for researchers, academics, and professionals, this publication serves as a beacon of ethical excellence. With a comprehensive exploration of bias, fairness, transparency, and accountability, it guides readers through the intricate web of ethical considerations. From foundational philosophical frameworks to real-world case studies, the book offers a roadmap to not only understand but actively shape the ethical trajectory of AI and data science. It is more than a book; it serves as a transformative tool for those seeking to align technological innovation with ethical standards and societal values. © 2024 by IGI Global. All rights reserved.},
	publisher = {IGI Global},
	author = {Kumar, R. and Joshi, A. and Sharan, H.O. and Peng, S.-L. and Dudhagara, C.R.},
	year = {2024},
	annote = {Export Date: 23 June 2024},
}

@article{iftikhar_tertiary_2024-1,
	title = {A tertiary study on links between source code metrics and external quality attributes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174715019&doi=10.1016%2fj.infsof.2023.107348&partnerID=40&md5=b6a93845d92de37d2044158738593673},
	abstract = {Context: Several secondary studies have investigated the relationship between internal quality attributes, source code metrics and external quality attributes. Sometimes they have contradictory results. Objective: We synthesize evidence of the link between internal quality attributes, source code metrics and external quality attributes along with the efficacy of the prediction models used. Method: We conducted a tertiary review to identify, evaluate and synthesize secondary studies. We used several characteristics of secondary studies as indicators for the strength of evidence and considered them when synthesizing the results. Results: From 711 secondary studies, we identified 15 secondary studies that have investigated the link between source code and external quality. Our results show : (1) primarily, the focus has been on object-oriented systems, (2) maintainability and reliability are most often linked to internal quality attributes and source code metrics, with only one secondary study reporting evidence for security, (3) only a small set of complexity, coupling, and size-related source code metrics report a consistent positive link with maintainability and reliability, and (4) group method of data handling (GMDH) based prediction models have performed better than other prediction models for maintainability prediction. Conclusions: Based on our results, lines of code, coupling, complexity and the cohesion metrics from Chidamber \& Kemerer (CK) metrics are good indicators of maintainability with consistent evidence from high and moderate-quality secondary studies. Similarly, four CK metrics related to coupling, complexity and cohesion are good indicators of reliability, while inheritance and certain cohesion metrics show no consistent evidence of links to maintainability and reliability. Further empirical studies are needed to explore the link between internal quality attributes, source code metrics and other external quality attributes, including functionality, portability, and usability. The results will help researchers and practitioners understand the body of knowledge on the subject and identify future research directions. © 2023 The Author(s)},
	author = {Iftikhar, U. and Ali, N.B. and Börstler, J. and Usman, M.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Tertiary study, Forecasting, Data handling, Codes (symbols), Code quality, Computer programming languages, Evidence, External quality, Internal quality, Maintainability, Object oriented programming, Products quality, Quality attributes, Quality modeling, Reliability, Source code metrics, Tertiary review, Product quality, Quality models},
	annote = {Export Date: 23 June 2024},
}

@article{ferrari_transforming_2023-1,
	title = {On transforming model-based tests into code: {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169886627&doi=10.1002%2fstvr.1860&partnerID=40&md5=5329e23c5dcfa2e46c74b6c32b471ebf},
	abstract = {Model-based test design is increasingly being applied in practice and studied in research. Model-based testing (MBT) exploits abstract models of the software behaviour to generate abstract tests, which are then transformed into concrete tests ready to run on the code. Given that abstract tests are designed to cover models but are run on code (after transformation), the effectiveness of MBT is dependent on whether model coverage also ensures coverage of key functional code. In this article, we investigate how MBT approaches generate tests from model specifications and how the coverage of tests designed strictly based on the model translates to code coverage. We used snowballing to conduct a systematic literature review. We started with three primary studies, which we refer to as the initial seeds. At the end of our search iterations, we analysed 30 studies that helped answer our research questions. More specifically, this article characterizes how test sets generated at the model level are mapped and applied to the source code level, discusses how tests are generated from the model specifications, analyses how the test coverage of models relates to the test coverage of the code when the same test set is executed and identifies the technologies and software development tasks that are on focus in the selected studies. Finally, we identify common characteristics and limitations that impact the research and practice of MBT: (i) some studies did not fully describe how tools transform abstract tests into concrete tests, (ii) some studies overlooked the computational cost of model-based approaches and (iii) some studies found evidence that bears out a robust correlation between decision coverage at the model level and branch coverage at the code level. We also noted that most primary studies omitted essential details about the experiments. © 2023 John Wiley \& Sons Ltd.},
	author = {Ferrari, F.C. and Durelli, V.H.S. and Andler, S.F. and Offutt, J. and Saadatmand, M. and Müllner, N.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Model checking, Specifications, Systematic literature review, Software design, Software testing, Codes (symbols), Abstracting, Concretes, Model based testing, Model specifications, Model-based test, Test case, Test case generation, Test case transformation, Test coverage criteria, Test sets, Test-coverage, systematic literature review, model-based testing, test case generation, test case transformation, test coverage criteria},
	annote = {Export Date: 23 June 2024},
}

@article{karcher_quality_2024-1,
	title = {Quality methods in virtual and augmented reality with a focus on education: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187115518&doi=10.1007%2fs11301-023-00403-y&partnerID=40&md5=1d4ec757b2a3f73d0c92a57fa035ed24},
	abstract = {With the goal of developing a unified approach for implementation of training for quality methods—with the help of innovative assistance systems—the current state of research is determined within the scope of this work. These quality methods include Quality Management Systems such as Lean Management and Six Sigma. A systematic literature search is conducted to determine the current state of research on Augmented and Virtual Reality data glasses, which are considered here as innovative assistance systems. This search extends without restriction to the date of data collection at the beginning of the year 2022, as Augmented and Virtual Reality data glasses are considered to be particularly immersive technologies. Based on the databases Scopus and Web of Science, an extended systematic literature review was used for the research. By answering the research question and classifying the implemented research work, an overview of the current state of virtual and augmented reality research will be given. This makes it clear that further research is needed, especially with regard to the training of quality methods, to develop specific models and action guidelines. © The Author(s) 2024.},
	author = {Karcher, A. and Arnold, D. and Kuhlenkötter, B.},
	year = {2024},
	note = {Publisher: Springer Nature},
	keywords = {Systematic literature review, Augmented reality, Virtual reality, I25, Lean management, Quality methods, Six Sigma},
	annote = {Export Date: 23 June 2024},
}

@article{wairimu_evaluation_2024-1,
	title = {On the {Evaluation} of {Privacy} {Impact} {Assessment} and {Privacy} {Risk} {Assessment} {Methodologies}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184332904&doi=10.1109%2fACCESS.2024.3360864&partnerID=40&md5=847d81293ca4d943618bb7001110f885},
	abstract = {Assessing privacy risks and incorporating privacy measures from the onset requires a comprehensive understanding of potential impacts on data subjects. Privacy Impact Assessments (PIAs) offer a systematic methodology for such purposes, which are closely related to Data Protection Impact Assessments (DPIAs), particularly outlined in Article 35 of the General Data Protection Regulation (GDPR). The core of a PIA is a Privacy Risk Assessment (PRA). PRAs can be integrated as part of full-fledged PIAs or independently developed to support PIA processes. Although these methodologies have been identified as essential enablers of privacy by design, their effectiveness has been criticized because of the lack of evidence of their rigorous and systematic evaluation. Hence, we conducted a Systematic Literature Review (SLR) to identify published PIA and PRA methodologies and assess how and to what extent they have been scientifically validated or evaluated. We found that these methodologies are rarely evaluated for their performance in practice, and most of them have only been validated in limited studies. Most validation evidence is found with PRA methodologies. Of the evaluated methodologies, PIAs were the most evaluated, where case studies were the predominant evaluation method. These evaluated methodologies can be easily transferred to an industrial setting or used by practitioners, as they provide evidence of their use in practice. In addition, the findings in this study can be used to inform researchers of the current state-of-the-art, and practitioners can understand the benefits and current limitations of the methodologies and adopt evidence-based practices. © 2013 IEEE.},
	author = {Wairimu, S. and Iwaya, L.H. and Fritsch, L. and Lindskog, S.},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Risk management, Risk assessment, Data privacy, General data protection regulations, Systematic, Data protection impact assessments, Guideline, Impact assessments, Maturity, Privacy, Privacy impact assessment, Privacy risks, Risks management, Threat modeling, Validity, data protection impact assessment, general data protection regulation, maturity, privacy, privacy by design, privacy risks, review, threat modeling, validity},
	annote = {Export Date: 23 June 2024},
}

@article{oliveira_mobile_2024-1,
	title = {Mobile {Health} from {Developers}’ {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180425615&doi=10.1007%2fs42979-023-02455-z&partnerID=40&md5=8f2aa4333b07b435a23d001076ad88c5},
	abstract = {This paper extends an initial investigation of eHealth from the developers’ perspective. In this extension, our focus is on mobile health data. Despite the significant potential of this development area, few studies try to understand the challenges faced by these professionals. This perspective is relevant to identify the most used technologies and future perspectives for research investigation. Using a KDD-based process, this work analyzed eHealth and mHealth discussions from Stack Overflow (SO) to comprehend this developers’ community. We got and processed 6082 eHealth and 1832 mHealth questions. The most discussed topics include manipulating medical images, electronic health records with the HL7 standard, and frameworks to support mobile health (mHealth) development. Concerning the challenges faced by these developers, there is a lack of understanding of the DICOM and HL7 standards, the absence of data repositories for testing, and the monitoring of health data in the background using mobile and wearable devices. Our results also indicate that discussions have grown mainly on mHealth, primarily due to monitoring health data through wearables and about how to optimize resource consumption during health-monitoring. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	author = {Oliveira, P.A.M. and Junior, E.C. and Andrade, R.M.C. and Santos, I.S. and Neto, P.A.S.},
	year = {2024},
	note = {Publisher: Springer},
	keywords = {mHealth, eHealth, Mining software repositories, Stack overflow},
	annote = {Export Date: 23 June 2024},
}

@article{papatheocharous_context_2024-1,
	title = {Context factors perceived important when looking for similar experiences in decision-making for software components: {An} interview study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190424140&doi=10.1002%2fsmr.2668&partnerID=40&md5=6ed3d43b32c8b6b926652c2475b95b30},
	abstract = {During software evolution, decisions related to components' origin or source significantly impact the quality properties of the product and development metrics such as cost, time to market, ease of maintenance, and further evolution. Thus, such decisions should ideally be supported by evidence, i.e., using previous experiences and information from different sources, even own previous experiences. A hindering factor to such reuse of previous experiences is that these decisions are highly context-dependent and it is difficult to identify when previous experiences come from sufficiently similar contexts to be useful in a current setting. Conversely, when documenting a decision (as a decision experience), it is difficult to know which context factors will be most beneficial when reusing the experience in the future. An interview study is performed to identify a list of context factors that are perceived to be most important by practitioners when using experiences to support decision-making for component sourcing, using a specific scenario with alternative sources of experiences. We observed that the further away (from a company or an interviewee) the experience evidence is, as is the case for online experiences, the more context factors are perceived as important by practitioners to make use of the experience. Furthermore, we discuss and identify further research to make this type of decision-making more evidence-based. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Papatheocharous, E. and Wohlin, C. and Badampudi, D. and Carlson, J. and Wnuk, K.},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Decision making, Decisions makings, Open source software, Software Evolution, Components off the shelves, Context factors, Decision experience, Experience source, In-house, Interview study, Open systems, Open-source softwares, Software-component, decision-making, components off-the-shelf, context factors, decision experience, experience source, in-house, open-source software},
	annote = {Export Date: 23 June 2024},
}

@article{liang_co-design_2024-1,
	title = {Co-design personal sleep health technology for and with university students},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191036091&doi=10.3389%2ffdgth.2024.1371808&partnerID=40&md5=8f5ef4e6d87e2f109fef9327baa66073},
	abstract = {University students often experience sleep disturbances and disorders. Personal digital technologies present a great opportunity for sleep health promotion targeting this population. However, studies that engage university students in designing and implementing digital sleep health technologies are scarce. This study sought to understand how we could build digital sleep health technologies that meet the needs of university students through a co-design process. We conducted three co-design workshops with 51 university students to identify design opportunities and to generate features for sleep health apps through workshop activities. The generated ideas were organized using the stage-based model of self-tracking so that our findings could be well-situated within the context of personal health informatics. Our findings contribute new design opportunities for sleep health technologies targeting university students along the dimensions of sleep environment optimization, online community, gamification, generative AI, materializing sleep with learning, and personalization. 2024 Liang, Melcer, Khotchasing and Hoang.},
	author = {Liang, Z. and Melcer, E. and Khotchasing, K. and Hoang, N.H.},
	year = {2024},
	note = {Publisher: Frontiers Media SA},
	keywords = {Article, human, adult, cohort analysis, data analysis, depersonalization, female, gamification, heart rate, human experiment, learning, male, nervous system function, personal sleep health technology, sleep disorder, sleep education, sleep environment, sleep hygiene, sleep literacy education, sleep quality, university student, mHealth, co-design, participatory design, personal health informatics, sleep health, sleep tracking},
	annote = {Export Date: 23 June 2024},
}

@article{von_scherenberg_data_2024-1,
	title = {Data {Sovereignty} in {Information} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186488145&doi=10.1007%2fs12525-024-00693-4&partnerID=40&md5=81079ebc4fe9e7a98618fa6d1beea7fc},
	abstract = {Data has become a strategic asset for societal prosperity and economic competitiveness. There has long been an academic consensus that the value of data unfolds during its use. Consequently, many stakeholders have called for expanding the use and reuse of data, including the public and open variety, as well as that from private data providers. However, citizens and organizations want self-determination over their data use, that is, data sovereignty. This fundamentals paper applies a literature review to conceptualize the term in Information Systems (IS) research by summarizing current findings and definitions to add further structure to the field. It contributes to the current research streams by introducing a core conceptual model consisting of seven interacting core aspects, involving trust between data providers and consumers for data assets, supported by data infrastructure and contractual agreements on all data lifecycle stages. We evaluate and discuss this conceptual model through recent field examples and provide an overview of future research opportunities. © The Author(s) 2024.},
	author = {von Scherenberg, F. and Hellmeier, M. and Otto, B.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Information systems, Literature review, Conceptualization, Data sovereignty, L86, M15},
	annote = {Export Date: 23 June 2024},
}

@article{kosasih_review_2024-1,
	title = {A review of explainable artificial intelligence in supply chain management using neurosymbolic approaches},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177643935&doi=10.1080%2f00207543.2023.2281663&partnerID=40&md5=73d8d50d41b48d9e9b68eb4ff7b68865},
	abstract = {Artificial Intelligence (AI) has emerged as a complementary technology in supply chain research. However, the majority of AI approaches explored in this context afford little to no explainability, which is a significant barrier to a broader adoption of AI in supply chains. In recent years, the need for explainability has been a strong impetus for research in hybrid AI methodologies that combine neural architectures with logic-based reasoning, which are collectively referred to as Neurosymbolic AI. The aim of this paper is to provide a comprehensive overview of supply chain management literature that employs approaches within the neurosymbolic AI spectrum. To that end, a systematic review is conducted, followed by bibliometric, descriptive and thematic analyses on the identified studies. Our findings indicate that researchers have primarily focused on the limited subset of neurofuzzy approaches, while some supply chain applications, such as performance evaluation and sustainability, and sectors such as pharmaceutical and construction have received less attention. To help address these gaps, we propose five pillars of neurosymbolic AI research for supply chains and provide four use cases of applying unexplored neurosymbolic AI approaches to address typical problems in supply chain management, including a discussion of prerequisites for adopting such technologies. We envision that the findings and contributions of this survey will help encourage further research in neurosymbolic AI for supply chains and increase adoption of such technologies within supply chain practice. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
	author = {Kosasih, E.E. and Papadakis, E. and Baryannis, G. and Brintrup, A.},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Artificial intelligence, Systematic Review, Explainability, Based reasonings, Bibliometrics analysis, Descriptive analysis, Hybrid artificial intelligences, Neural architectures, Neural-networks, Neurosymbolic artificial intelligence, Spectra's, Supply chain management, artificial intelligence, explainability, hybrid AI, neural networks, neurosymbolic AI, Supply chain},
	annote = {Export Date: 23 June 2024},
}

@article{malcher_what_2023-1,
	title = {What do we know about requirements management in software ecosystems?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173886355&doi=10.1007%2fs00766-023-00407-w&partnerID=40&md5=30247f2698bd677da0e7068f7c8ff3fd},
	abstract = {Among the activities in requirements engineering (RE), requirements management ensures that requirements are tracked throughout their life cycle, changes are controlled, and inconsistencies are corrected. Requirements management has become increasingly critical in new ways of developing software and emerging contexts such as software ecosystems (SECO). The changing nature of the SECO introduces complexity in requirements management and results in varied flows of emergent requirements, making managing requirements in SECO challenging. Hence, understanding how requirements management is performed in SECO can help requirements managers improve their practices. This work aims to characterize requirements management in SECO. We have conducted a systematic mapping study (SMS) to achieve this goal. We selected 29 studies using a hybrid search strategy (database search and snowballing). We defined nine characteristics of requirements management in SECO that differentiate it from requirements management in traditional software development. We identified four types of approaches to support requirements management in SECO: tool, method, model, and practice. We found that only three selected studies present an assessment of their approaches. Finally, we characterize requirements management in SECO as an open, informal, collaborative, and decentralized process involving multi-party actors susceptible to power relations. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Malcher, P. and Silva, E. and Viana, D. and Santos, R.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Ecosystems, Management IS, Software design, Life cycle, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Search engines, Database searches, Hybrid search strategies, Managing requirements, Method model, Requirement management, Software ecosystems, Support requirements, Systematic mapping study, Requirements management, Software ecosystem},
	annote = {Export Date: 23 June 2024},
}

@article{zhao_unraveling_2024-1,
	title = {Unraveling quantum computing system architectures: {An} extensive survey of cutting-edge paradigms},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179094225&doi=10.1016%2fj.infsof.2023.107380&partnerID=40&md5=fd9ef3dd6d9f7e6bae0e8fa8439e9741},
	abstract = {Context: The convergence of physics and computer science in the realm of quantum computing systems has sparked a profound revolution within the computer industry. However, despite such promise, the existing focus on quantum software systems primarily centers on the generation of quantum source code, inadvertently overlooking the pivotal role of the overall software architecture. Objectives: In order to provide comprehensive guidance to researchers and practitioners engaged in quantum software development, employing an architecture-centered development model, an extensive literature review was conducted pertaining to existing research on quantum software architecture. The analysis encompasses a detailed examination of the characteristics exhibited by these studies and the identification of prospective challenges that lie ahead in the field of quantum software architecture. Methods: We have closely examined instances of quantum software engineering, quantum modeling languages, quantum design patterns, and quantum communication security to gain insights into the distinctive attributes associated with various software architecture approaches. Results: Our findings underscore the critical significance of prioritizing software architecture in the development of robust and efficient quantum software systems. Through the synthesis of these multifaceted aspects, both researchers and practitioners can devise quantum software solutions that are inherently architecture-centric. Conclusion: The software architecture of quantum computing systems plays a pivotal role in determining their ultimate success and usability. Given the ongoing advancements in quantum computing technology, the migration of traditional software architecture development methods to the domain of quantum software development holds significant importance. © 2023 Elsevier B.V.},
	author = {Zhao, X. and Xu, X. and Qi, L. and Xia, X. and Bilal, M. and Gong, W. and Kou, H.},
	year = {2024},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software design, Computer software, Software-systems, Source codes, Software architecture, Computer industry, Cutting edges, Development model, Modeling languages, Quantum communication, Quantum computers, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Systems architecture, Quantum computing},
	annote = {Export Date: 23 June 2024},
}

@article{gwenhure_gamification_2024-1,
	title = {Gamification of {Cybersecurity} {Awareness} for {Non}-{IT} {Professionals}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188663020&doi=10.17083%2fijsg.v11i1.719&partnerID=40&md5=c675878ca8dc3cf7e3db076c1d3b4664},
	abstract = {This literature review delves into research on the gamification of cybersecurity awareness for non-IT professionals, aiming to provide an accurate report on known and unknown information regarding three key questions: the impact of gamification on cybersecurity awareness interest and engagement, measurable results related to game elements and their connection to specific learning goals, and the long-term effectiveness of gamified cybersecurity. Examining five relevant papers, the findings confirm short-term effectiveness and indicate that the incorporation of various game elements, such as storytelling, team leaderboards, and interactive scenarios, results in increased knowledge, improved engagement, and positive behavior changes aligned with specific cybersecurity awareness learning goals. However, the review also identifies recurring gaps in evaluating individual game elements and customizing gamification strategies for non-IT professionals. Highlighting a critical gap in understanding long-term effectiveness, we argue for further empirical studies to consider habituation effects, emphasizing the need for a nuanced understanding of gamification's impact on cybersecurity awareness over an extended period. Thus, the review contributes to the existing body of knowledge by emphasizing the necessity for empirical studies focusing on sustained, long-term effectiveness and habituation effects in gamified cybersecurity initiatives. © 2024, Serious Games Society. All rights reserved.},
	author = {Gwenhure, A.K. and Rahayu, F.S.},
	year = {2024},
	note = {Publisher: Serious Games Society},
	keywords = {Gamification, Game elements, Cybersecurity, Cybersecurity awareness, non-IT Professionals},
	annote = {Export Date: 23 June 2024},
}

@article{villarrubia_designscrumagility_2024-1,
	title = {{DesignScrum}–{An} agility educational resource powered by creativity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181492975&doi=10.1002%2fspe.3308&partnerID=40&md5=4783fe66f11d0583de6f94a416b680d6},
	abstract = {Agile methods have been widely adopted by the industry and its teaching has seen a surge, particularly in the software development field. However, these methods have a number of limitations which affect product outcomes, such as the fact that many software development companies now use Scrum to get developers to work without interruption between iterations, rather than to maintain a sustainable rhythm. Agile experts have stated the importance of incorporating creativity into Scrum, and although there are several agile resources that help with the learning process, it seems essential to approach such learning from a practical point of view. Furthermore, none of these resources introduce creativity. In this paper, we present an educational resource in the form of a serious game that allows you to acquire all the key concepts of agile and creative methods. The game is based on the use of LEGO pieces to simulate a real project, applying the key concepts of the Scrum and Design Thinking frameworks in a gamified way. It was assessed in a professional training centre of computer science by using surveys through which participants evaluated their previous knowledge of agile and creativity methods. We analysed the improvement of these competences, as well as the general level of satisfaction with the game. After the game, the results showed that the participants' knowledge of the Scrum and Design Thinking frameworks had improved and that they were very satisfied with the whole experience. © 2024 The Authors. Software: Practice and Experience published by John Wiley \& Sons Ltd.},
	author = {Villarrubia, C. and Vara, J.M. and Granada, D. and Gómez-Macías, C. and Pérez-Blanco, F.J.},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Software design, Learning systems, Gamification, Agile, gamification, Agile methods, Agile resource, Creativity, Design thinking, Educational resource, Personnel training, Practice and experience, Scra, Serious games, Software practices, agile, creativity, design thinking, education, scrum},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{langner_challenges_2024-1,
	title = {Challenges for capturing data within data-driven design processes},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194079420&doi=10.1017%2fpds.2024.212&partnerID=40&md5=b2cc9c1ee795f82f17c520d338fdb953},
	abstract = {Cyber-Physical-Systems provide extensive data gathering opportunities along the lifecycle, enabling data-driven design to improve the design process. However, its implementation faces challenges, particularly in the initial data capturing stage. To identify those, a comprehensive approach combining a systematic literature review and an industry survey was applied. Four groups of interrelated challenges were identified as most relevant to practitioners: data selection, data availability in systems, knowledge about data science processes and tools, and guiding users in targeted data capturing. © 2024 Proceedings of the Design Society. All rights reserved.},
	publisher = {Cambridge University Press},
	author = {Langner, C. and Paliyenko, Y. and Müller, B. and Roth, D. and Guertler, M.R. and Kreimeyer, M.},
	year = {2024},
	keywords = {Embedded systems, Systematic literature review, Life cycle, Design, Capturing data, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Data capturing, Data gathering, Data-driven design, Design-process, Industry surveys, Internet of thing, Internet of things, cyber-physical systems, data science, data-driven design, internet of things (IoT)},
	annote = {Export Date: 23 June 2024},
}

@article{minhas_lessons_2023-1,
	title = {Lessons learned from replicating a study on information-retrieval-based test case prioritization},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174265778&doi=10.1007%2fs11219-023-09650-4&partnerID=40&md5=746a6f83a8dbadb4bea7db51dccd8466},
	abstract = {Replication studies help solidify and extend knowledge by evaluating previous studies’ findings. Software engineering literature showed that too few replications are conducted focusing on software artifacts without the involvement of humans. This study aims to replicate an artifact-based study on software testing to address the gap related to replications. In this investigation, we focus on (i) providing a step-by-step guide of the replication, reflecting on challenges when replicating artifact-based testing research and (ii) evaluating the replicated study concerning the validity and robustness of the findings. We replicate a test case prioritization technique proposed by Kwon et al. We replicated the original study using six software programs, four from the original study and two additional software programs. We automated the steps of the original study using a Jupyter notebook to support future replications. Various general factors facilitating replications are identified, such as (1) the importance of documentation; (2) the need for assistance from the original authors; (3) issues in the maintenance of open-source repositories (e.g., concerning needed software dependencies, versioning); and (4) availability of scripts. We also noted observations specific to the study and its context, such as insights from using different mutation tools and strategies for mutant generation. We conclude that the study by Kwon et al. is partially replicable for small software programs and could be automated to facilitate software practitioners, given the availability of required information. However, it is hard to implement the technique for large software programs with the current guidelines. Based on lessons learned, we suggest that the authors of original studies need to publish their data and experimental setup to support the external replications. © 2023, The Author(s).},
	author = {Minhas, N.M. and Irshad, M. and Petersen, K. and Börstler, J.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Software testing, Open source software, Software testings, Information retrieval, Replication study, Open systems, Prioritization techniques, Regression testing, Replication, SIR, Software artefacts, Software project, Technique, Test case prioritization},
	annote = {Export Date: 23 June 2024},
}

@article{idlahcen_exploring_2024-1,
	title = {Exploring data mining and machine learning in gynecologic oncology},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183390317&doi=10.1007%2fs10462-023-10666-2&partnerID=40&md5=e20c182acb99fcbd45f1023721557013},
	abstract = {Gynecologic (GYN) malignancies are gaining new and much-needed attention, perpetually fueling literature. Intra-/inter-tumor heterogeneity and “frightened” global distribution by race, ethnicity, and human development index, are pivotal clues to such ubiquitous interest. To advance “precision medicine” and downplay the heavy burden, data mining (DM) is timely in clinical GYN oncology. No consolidated work has been conducted to examine the depth and breadth of DM applicability as an adjunct to GYN oncology, emphasizing machine learning (ML)-based schemes. This systematic literature review (SLR) synthesizes evidence to fill knowledge gaps, flaws, and limitations. We report this SLR in compliance with Kitchenham and Charters’ guidelines. Defined research questions and PICO crafted a search string across five libraries: PubMed, IEEE Xplore, ScienceDirect, SpringerLink, and Google Scholar—over the past decade. Of the 3499 potential records, 181 primary studies were eligible for in-depth analysis. A spike (60.53\%) corollary to cervical neoplasms is denoted onward 2019, predominantly featuring empirical solution proposals drawn from cohorts. Medical records led (23.77\%, 53 art.). DM-ML in use is primarily built on neural networks (127 art.), appoint classification (73.19\%, 172 art.) and diagnoses (42\%, 111 art.), all devoted to assessment. Summarized evidence is sufficient to guide and support the clinical utility of DM schemes in GYN oncology. Gaps persist, inculpating the interoperability of single-institute scrutiny. Cross-cohort generalizability is needed to establish evidence while avoiding outcome reporting bias to locally, site-specific trained models. This SLR is exempt from ethics approval as it entails published articles. © 2024, The Author(s).},
	author = {Idlahcen, F. and Idri, A. and Goceri, E.},
	year = {2024},
	note = {Publisher: Springer Nature},
	keywords = {Systematic literature review, Machine learning, Machine-learning, Diagnosis, Data mining, Female reproductive system, Global distribution, Gynecologic AI, Gynecologic malignancies, Human development index, Neoplasm, Oncology, Reproductive systems, Tumor heterogeneity, Tumors},
	annote = {Export Date: 23 June 2024},
}

@article{koch_cloud-based_2023-1,
	title = {Cloud-{Based} {Reinforcement} {Learning} in {Automotive} {Control} {Function} {Development}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172153714&doi=10.3390%2fvehicles5030050&partnerID=40&md5=916bf16f7660e18b017ab960469db241},
	abstract = {Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process. Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner. Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry. To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library. It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training. We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario. The evolved control strategy produces a smooth trajectory with energy savings of up to 14\%. The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework. © 2023 by the authors.},
	author = {Koch, L. and Roeser, D. and Badalian, K. and Lieb, A. and Andert, J.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {automation, cloud simulation, co-simulation, function development, reinforcement learning},
	annote = {Export Date: 23 June 2024},
}

@article{hernandez_requirements_2023-1,
	title = {Requirements management in {DevOps} environments: a multivocal mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146221014&doi=10.1007%2fs00766-023-00396-w&partnerID=40&md5=4b84297f3c4a81569ac37e71e82d3cce},
	abstract = {Attention is currently being focused on DevOps, which aims to reduce software development time by means of close collaboration between the development and operations areas. However, little effort has been made to determine the role of requirements management in DevOps. The objective of this study is to help both researchers and practitioners by providing an overview of the best practices regarding requirements engineering in DevOps and identifying which areas still need to be investigated. A multivocal mapping study has, therefore, been carried out in order to study which methodologies, techniques and tools are used to support requirements management in DevOps environments. After applying the review protocol, 37 papers from formal literature and 14 references from grey literature were selected for analysis. The general conclusions obtained after analysing these papers were that, within DevOps, more attention should be paid to: (1) the reuse of requirements in order to identify systems and software artefacts that can serve as a basis for the specification of new projects; (2) the communication of requirements between the different areas of an organisation and the stakeholders of a project; (3) the traceability of requirements in order to identify the relationship with other requirements, artefacts, tasks and processes; (4) non-functional requirements in order to identify the requirements of the operations area in the early phases of a project; and finally (5) specific requirements tools that should be seamlessly integrated into the DevOps toolchain. All these issues must be considered without ignoring the agile and continuous practices of development, operations and business teams. More effort must also be made to validate new methodologies in industry so as to assess and determine their strengths and weaknesses. © 2023, The Author(s).},
	author = {Hernández, R. and Moros, B. and Nicolás, J.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Software design, Computer software reusability, Mapping, Requirement engineering, Requirements engineering, Development time, Mapping studies, Best practices, Requirement management, Agile requirement engineering, Agile requirements, Development and operations, Environmental management, Multivocal mapping study, DevOps, Requirements management, Agile requirements engineering},
	annote = {Export Date: 23 June 2024},
}

@article{spinellis_open_2023-1,
	title = {Open reproducible scientometric research with {Alexandria3k}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178516434&doi=10.1371%2fjournal.pone.0294946&partnerID=40&md5=81c76085ccde3fc925309b30cdec682b},
	abstract = {Considerable scientific work involves locating, analyzing, systematizing, and synthesizing other publications, often with the help of online scientific publication databases and search engines. However, use of online sources suffers from a lack of repeatability and transparency, as well as from technical restrictions. Alexandria3k is a Python software package and an associated command-line tool that can populate embedded relational databases with slices from the complete set of several open publication metadata sets. These can then be employed for reproducible processing and analysis through versatile and performant queries. We demonstrate the software’s utility by visualizing the evolution of publications in diverse scientific fields and relationships among them, by outlining scientometric facts associated with COVID-19 research, and by replicating commonly-used bibliometric measures and findings regarding scientific productivity, impact, and disruption. Copyright: © 2023 Diomidis Spinellis.},
	author = {Spinellis, D.},
	year = {2023},
	note = {Publisher: Public Library of Science},
	keywords = {Metadata, Article, coronavirus disease 2019, human, software, Alexandria3k software, bibliometrics, Bibliometrics, dynamics, factual database, journal impact factor, measurement repeatability, metadata, methodology, online analytical processing, productivity, proof of concept, publication, Research Design, scientometrics, search engine, Search Engine, Databases, Factual},
	annote = {Export Date: 23 June 2024},
}

@article{soares_trends_2023-1,
	title = {Trends in continuous evaluation of software architectures},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147748180&doi=10.1007%2fs00607-023-01161-1&partnerID=40&md5=ff7020687f6de6e3c5a34f6dda65d4d7},
	abstract = {The software industry is increasingly facing the need for continuous deployment of systems. This leads to the adoption of continuous activities, including planning, integration, and deployment (a.k.a. Continuous Software Engineering (CSE)). At the same time, systems should exhibit high-quality architectures, which are often achieved through architecture evaluation methods. However, there is little insight of how such evaluation happens in the context of CSE. To cover this gap, we investigate in this work the state of the art of continuous evaluation of software architectures in CSE, including agile processes like SCRUM. For this, we systematically examine the literature to collect and summarize evidence. Our results show a diversity of means for evaluating architectures in continuous mode to support the continuous evolution of systems. We also found how such evaluation has been incorporated within continuous development processes and agile processes like SCRUM and Crystal. We finally derive the main trends and open issues in the area, aiming to support the community to better understand and further consolidate the field of continuous evaluation of software architectures. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.},
	author = {Soares, R.C. and Capilla, R. and dos Santos, V. and Nakagawa, E.Y.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {State of the art, High quality, Continuous software engineerings, Agile process, Evaluation methods, Quality control, Software architecture, Agile manufacturing systems, Architecture evaluation, Continuous architecture evaluation, Software industry, Time systems, Continuous software engineering},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{zellmer_product-structuring_2023-1,
	title = {Product-{Structuring} {Concepts} for {Automotive} {Platforms}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175948175&doi=10.1145%2f3579027.3608988&partnerID=40&md5=f19a532ed313f3874c3d38986b4968ef},
	abstract = {The products of the automotive industry are facing one of the biggest changes: becoming digital smart devices on wheels. Driven by the rising amount of vehicle functions, electronic control units, and software, today's vehicles are becoming cyber-physical systems that are increasingly complex and hard to manage over their life cycle. To handle these challenges, the automotive industry is adopting and integrating methods like software product-line engineering, electrics/electronics platforms, and product generation. While these concepts are widely recognized in their respective research areas and various domains, there is limited research regarding the practical effectiveness of implementing these concepts in a software-driven automotive context. In this paper, we investigate existing product-structuring concepts and methods that consider both hardware and software artifacts, and their applicability to the automotive as well as other cyber-physical industries. For this purpose, we conducted a systematic mapping study to capture a comprehensive overview of existing product-structuring concepts and methods, based on which we discuss how the state-of-the-art can or cannot help solve the challenges of the automotive industry. Specifically, we analyze the practical applicability of the existing solutions to help practitioners apply them and to guide future research.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Zellmer, P. and Holsten, L. and Leich, T. and Krüger, J.},
	year = {2023},
	keywords = {Embedded systems, Life cycle, Mapping, Systematic mapping studies, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Big changes, Digital devices, Electric electronics, Electric lines, Lifecycle management, Product structuring, Product-structuring concept, Productline, automotive, cyber-physical system, electrics/electronics, life-cycle management, product line, product-structuring concept},
	annote = {Export Date: 23 June 2024},
}

@article{ali_intelligent_2023-1,
	title = {Intelligent {Decision} {Support} {Systems}—{An} {Analysis} of {Machine} {Learning} and {Multicriteria} {Decision}-{Making} {Methods}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187262298&doi=10.3390%2fapp132212426&partnerID=40&md5=dbf125e709aa73f98b50e57c1475067f},
	abstract = {Context: The selection and use of appropriate multi-criteria decision making (MCDM) methods for solving complex problems is one of the challenging issues faced by decision makers in the search for appropriate decisions. To address these challenges, MCDM methods have effectively been used in the areas of ICT, farming, business, and trade, for example. This study explores the integration of machine learning and MCDM methods, which has been used effectively in diverse application areas. Objective: The objective of the research is to critically analyze state-of-the-art research methods used in intelligent decision support systems and to further identify their application areas, the significance of decision support systems, and the methods, approaches, frameworks, or algorithms exploited to solve complex problems. The study provides insights for early-stage researchers to design more intelligent and cost-effective solutions for solving problems in various application domains. Method: To achieve the objective, literature from the years 2015 to early 2020 was searched and considered in the study based on quality assessment criteria. The selected relevant literature was studied to respond to the research questions proposed in this study. To find answers to the research questions, pertinent literature was analyzed to identify the application domains where decision support systems are exploited, the impact and significance of the contributions, and the algorithms, methods, and techniques which are exploited in various domains to solve decision-making problems. Results: Results of the study show that decision support systems are widely used as useful decision-making tools in various application domains. The research has collectively studied machine learning, artificial intelligence, and multi-criteria decision-making models used to provide efficient solutions to complex decision-making problems. In addition, the study delivers detailed insights into the use of AI, ML and MCDM methods to the early-stage researchers to start their research in the right direction and provide them with a clear roadmap of research. Hence, the development of Intelligent Decision Support Systems (IDSS) using machine learning (ML) and multicriteria decision-making (MCDM) can assist researchers to design and develop better decision support systems. These findings can help researchers in designing more robust, efficient, and effective multicriteria-based decision models, frameworks, techniques, and integrated solutions. © 2023 by the authors.},
	author = {Ali, R. and Hussain, A. and Nazir, S. and Khan, S. and Khan, H.U.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {artificial intelligence, machine learning, decision support system (DSS), intelligent decision support systems (IDSS), multi-criteria, multi-criteria decision making (MCDM)},
	annote = {Export Date: 23 June 2024},
}

@article{yang_automatic_2023-1,
	title = {Automatic {Essay} {Evaluation} {Technologies} in {Chinese} {Writing}—{A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174165294&doi=10.3390%2fapp131910737&partnerID=40&md5=a2e9a5b11c5c1af33cd825ae61aeaf0e},
	abstract = {Automatic essay evaluation, an essential application of natural language processing (NLP) technology in education, has been increasingly employed in writing instruction and language proficiency assessment. Because automatic Chinese Essay Evaluation (ACEE) has made some breakthroughs due to the rapid development of upstream Chinese NLP technology, many evaluation tools have been applied in teaching practice and high-risk evaluation processes. However, the development of ACEE is still in its early stages, with many technical bottlenecks and challenges. This paper systematically explores the current research status of corpus construction, feature engineering, and scoring models in ACEE through literature to provide a technical perspective for stakeholders in the ACEE research field. Literature research has shown that constructing the ACEE public corpus is insufficient and lacks an effective platform to promote the development of ACEE research. Various shallow and deep features can be extracted using statistical and NLP techniques in ACEE. However, there are still substantial limitations in extracting grammatical errors and features related to syntax and traditional Chinese Literary style. For the construction of scoring models, existing studies have shown that traditional machine learning and deep learning methods each have advantages in different corpora and feature selections. The deep learning model, which exhibits strong adaptability and multi-task joint learning potential, has broader development space regarding model scalability. © 2023 by the authors.},
	author = {Yang, H. and He, Y. and Bu, X. and Xu, H. and Guo, W.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {systematic literature review, automated essay evaluation, Chinese writing, natural language process},
	annote = {Export Date: 23 June 2024},
}

@article{sadeghiani_what_2023-1,
	title = {What pivot is: {Touching} an elephant in the dark},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85155452238&doi=10.1016%2fj.digbus.2023.100056&partnerID=40&md5=4b53b6bc91a08e0cc741882f699640c0},
	abstract = {The term ‘pivot’ first appeared in the practical literature, demonstrating its role in the Lean Startup, and immediately emerged in the academic literature. However, it suffered from conflicting perceptions and practice-academy divide. Moreover, recently, a growing number of scholars introduced it as a response to the pandemic crisis, at times as an umbrella term. In this study, we first test the concept ‘pivot’ against clarity criteria and find its problematic issues based on a qualitative content analysis of the literature; then, using a multi-case study, we get a critical distance from the pivot's origin; parsimoniously reposition it as ‘substitution’, differentiate it from ‘pivoting’ as its process theory; and discuss it in relation to vision, strategy, and business model. Finally, we propose a conceptual basis and some rules for operationalization. © 2023 The Authors},
	author = {Sadeghiani, A. and Anderson, A.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Business model, Construct clarity, Pivot, Pivoting, Startup, Strategy},
	annote = {Export Date: 23 June 2024},
}

@incollection{furtado_controlled_2023-1,
	title = {Controlled experimentation of software product lines},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160474401&doi=10.1007%2f978-3-031-18556-4_19&partnerID=40&md5=dd6f8e961dba036421d866a0cbda7a31},
	abstract = {The process of experimentation is one of several scientific methods that can provide evidence for a proof of a theory. This process is counterpoint to the real world observation method, thus providing a reliable body of knowledge. However, in the experimentation for emerging areas and in the consolidation process in scientific and industrial communities, such as the software product line (SPL), there has been a constant lack of adequate documentation of experiments that makes it difficult to repeat, replicate, and reproduce studies in SPL. Therefore, this chapter presents a set of guidelines for the quality assessment of SPL experiments with its conceptual model to support the understanding of the proposed guidelines, as well as an ontology for SPL experiments, called OntoExper-SPL, in addition to support the teaching experimentation in SPL. Thus, these points aim to improve the planning, conduction, analysis, sharing, and documentation of SPL experiments, supporting the construction of a reliable and reference body of knowledge in such a context in addition to enabling improvement in the teaching of SPL experiments. © Springer Nature Switzerland AG 2023. All rights reserved.},
	publisher = {Springer International Publishing},
	author = {Furtado, V.R. and Vignando, H. and Luz, C.D. and Steinmacher, I.F. and Kalinowski, M. and OliveiraJr, E.},
	year = {2023},
	annote = {Export Date: 23 June 2024},
}

@article{yin_predicting_2023-1,
	title = {Predicting {Changes} in {User}-{Driven} {Requirements} {Using} {Conditional} {Random} {Fields} in {Agile} {Software} {Development}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112600382&doi=10.1109%2fTEM.2021.3083513&partnerID=40&md5=ce8def99d83c5673583af52f67d3b7a6},
	abstract = {Agile development encourages requirements change. The accurate predictions of changes in user requirements could help software evolve in the right direction to increase user satisfaction. Previous research on requirement predictions mostly relies on historical defects or user feedback. In this article, we aim to predict requirement changes based on user-system interactions in agile software development. We focus on the user-system interaction behaviors used to infer user intentions and predict requirement changes to drive the incremental iterations of agile development. Through a prototype system with two incremental iterations, an embedded program in the system captures the user runtime interaction behavior data. We utilize the conditional random fields to explore the user potential intentions and infer the emerging requirements accordingly. The increased accuracy of results in the iterations proves the effectiveness of our approach in predicting user requirement changes.  © 1988-2012 IEEE.},
	author = {Yin, M. and Meng, D. and Zhu, D. and Wang, Y. and Jiang, J.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Software design, Software testing, Agile software development, Behavioral research, Forecasting, Digital storage, Agile manufacturing systems, Behaviour patterns, Conditional random field, Hidden Markov models, Hidden-Markov models, Interactive computer systems, Micromechanical device, Prototype, Random fields, Real - Time system, Real time systems, Requirements change, Software, User driven, User intention inference, User-driven development, User's intentions, behavior pattern, conditional random fields (CRFs), requirement changes, user intention inference, user-driven development},
	annote = {Export Date: 23 June 2024},
}

@article{innocente_framework_2023-1,
	title = {A framework study on the use of immersive {XR} technologies in the cultural heritage domain},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121296&doi=10.1016%2fj.culher.2023.06.001&partnerID=40&md5=eb7e017ab19913950cb9d4779e742030},
	abstract = {Most cultural promotion and dissemination are nowadays performed through the digitization of heritage sites and museums, a necessary requirement to meet the new needs of the public. Augmented Reality (AR), Mixed Reality (MR), and Virtual Reality (VR) have the potential to improve the experience quality and educational effect of these sites by stimulating users’ senses in a more natural and vivid way. In this respect, head-mounted display (HMD) devices allow visitors to enhance the experience of cultural sites by digitizing information and integrating additional virtual cues about cultural artifacts, resulting in a more immersive experience that engages the visitor both physically and emotionally. This study contributes to the development and incorporation of AR, MR, and VR applications in the cultural heritage domain by providing an overview of relevant studies utilizing fully immersive systems, such as headsets and CAVE systems, emphasizing the advantages that they bring when compared to handheld devices. We propose a framework study to identify the key features of headset-based Extended Reality (XR) technologies used in the cultural heritage domain that boost immersion, sense of presence, and agency. Furthermore, we highlight core characteristics that favor the adoption of these systems over more traditional solutions (e.g., handheld devices), as well as unsolved issues that must be addressed to improve the guests’ experience and the appreciation of the cultural heritage. An extensive search of Google Scholar, Scopus, IEEE Xplore, ACM Digital Library, and Wiley Online Library databases was conducted, including papers published from January 2018 to September 2022. To improve review reporting, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were used. Sixty-five papers met the inclusion criteria and were classified depending on the study's purpose: education, entertainment, edutainment, touristic guidance systems, accessibility, visitor profiling, and management. Immersive cultural heritage systems allow visitors to feel completely immersed and present in the virtual environment, providing a stimulating and educational cultural experience that can improve the quality and learning purposes of cultural visits. Nonetheless, the analyzed studies revealed some limitations that must be faced to give a further impulse to the adoption of these technologies in the cultural heritage domain. © 2023 Consiglio Nazionale delle Ricerche (CNR)},
	author = {Innocente, C. and Ulrich, L. and Moos, S. and Vezzetti, E.},
	year = {2023},
	note = {Publisher: Elsevier Masson s.r.l.},
	keywords = {Augmented reality (AR), Cultural heritage, Digital heritage, Head-mounted display (HMD), Mixed reality (MR), Virtual reality (VR)},
	annote = {Export Date: 23 June 2024},
}

@book{liu_space-air-ground_2023-1,
	title = {Space-air-ground integrated network security},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169763096&doi=10.1007%2f978-981-99-1125-7&partnerID=40&md5=0650c0f66edd4c9e68a8c9fea0b9de9a},
	abstract = {This book focuses on security science and technology, data and information security, and mobile and network security for space-air-ground integrated networks (SAGINs). SAGIN are expected to play an increasingly important role in providing real-time, flexible, and integrated communication and data transmission services in an efficient manner. Today, SAGINs have been widely developed for a range of applications in navigation, environmental monitoring, traffic management, counter-terrorism, etc. However, security becomes a major concern, since the satellites, spacecrafts, and aircrafts are susceptible to a variety of traditional/specific network-based attacks, including eavesdropping, session hijacking, and illegal access. In this book, we review the theoretical foundations of SAGIN security. We also address a range of related security threats and provide cutting-edge solutions in the aspect of ground network security, airborne network security, space network security, and provide future trends in SAGIN security. The book goes from an introduction to the topic's background, to a description of the basic theory, and then to cutting-edge technologies, making it suitable for readers at all levels including professional researchers and beginners. To gain the most from the book, readers should have taken prior courses in information theory, cryptography, network security, etc. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023. All rights reserved.},
	publisher = {Springer Nature},
	author = {Liu, J. and Bai, L. and Jiang, C. and Zhang, W.},
	year = {2023},
	keywords = {Network security, Airborne network security, Communication security, Ground network security, SAGIN security, Space network security, UAV communications},
	annote = {Export Date: 23 June 2024},
}

@article{stradowski_industrial_2023-1,
	title = {Industrial applications of software defect prediction using machine learning: {A} business-driven systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150041533&doi=10.1016%2fj.infsof.2023.107192&partnerID=40&md5=adef39458fbdf72dbe9232624eef7f22},
	abstract = {Context: Machine learning software defect prediction is a promising field of software engineering, attracting a great deal of attention from the research community; however, its industry application tents to lag behind academic achievements. Objective: This study is part of a larger project focused on improving the quality and minimising the cost of software testing of the 5G system at Nokia, and aims to evaluate the business applicability of machine learning software defect prediction and gather lessons learnt. Methods: The systematic literature review was conducted on journal and conference papers published between 2015 and 2022 in popular online databases (ACM, IEEE, Springer, Scopus, Science Direct, and Google Scholar). A quasi-gold standard procedure was used to validate the search, and SEGRESS guidelines were used for transparency, reporting, and replicability. Results: We have selected and analysed 32 publications out of 397 found by our automatic search (and seven by snowballing). We have identified highly relevant evidence of methods, features, frameworks, and datasets used. However, we found a minimal emphasis on practical lessons learnt and cost consciousness — both vital from a business perspective. Conclusion: Even though the number of machine learning software defect prediction studies validated in the industry is increasing (and we were able to identify several excellent papers on studies performed in vivo), there is still not enough practical focus on the business aspects of the effort that would help bridge the gap between the needs of the industry and academic research. © 2023 The Authors},
	author = {Stradowski, S. and Madeyski, L.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic literature review, Software testing, Costs, Application programs, Machine learning, Machine-learning, Defects, Forecasting, Software defect prediction, 5G mobile communication systems, Cost minimization, Effort and cost minimization, Industry applications, Lesson learnt, Machine learning software, Real-world, Research communities, Industry, Effort and cost minimisation},
	annote = {Export Date: 23 June 2024},
}

@article{watson_augmented_2023-1,
	title = {Augmented {Behavioral} {Annotation} {Tools}, with {Application} to {Multimodal} {Datasets} and {Models}: {A} {Systematic} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171380178&doi=10.3390%2fai4010007&partnerID=40&md5=8cad8eef1ab5b05d19d8bea8f2461e3d},
	abstract = {Annotation tools are an essential component in the creation of datasets for machine learning purposes. Annotation tools have evolved greatly since the turn of the century, and now commonly include collaborative features to divide labor efficiently, as well as automation employed to amplify human efforts. Recent developments in machine learning models, such as Transformers, allow for training upon very large and sophisticated multimodal datasets and enable generalization across domains of knowledge. These models also herald an increasing emphasis on prompt engineering to provide qualitative fine-tuning upon the model itself, adding a novel emerging layer of direct machine learning annotation. These capabilities enable machine intelligence to recognize, predict, and emulate human behavior with much greater accuracy and nuance, a noted shortfall of which have contributed to algorithmic injustice in previous techniques. However, the scale and complexity of training data required for multimodal models presents engineering challenges. Best practices for conducting annotation for large multimodal models in the most safe and ethical, yet efficient, manner have not been established. This paper presents a systematic literature review of crowd and machine learning augmented behavioral annotation methods to distill practices that may have value in multimodal implementations, cross-correlated across disciplines. Research questions were defined to provide an overview of the evolution of augmented behavioral annotation tools in the past, in relation to the present state of the art. (Contains five figures and four tables). © 2023, Multidisciplinary Digital Publishing Institute (MDPI). All rights reserved.},
	author = {Watson, E. and Viana, T. and Zhang, S.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {machine learning, annotation, behavior, foundation models},
	annote = {Export Date: 23 June 2024},
}

@article{xu_systematic_2023-1,
	title = {A systematic mapping study on machine learning methodologies for requirements management},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145104779&doi=10.1049%2fsfw2.12082&partnerID=40&md5=d8c1d596f27bfb9f05d19106fa83be54},
	abstract = {Requirements management (RM) plays an important role in requirements engineering. The development of machine learning (ML) is in full swing, and many ML software management techniques had been used to improve the performance of RM methods. However, as no research study is known that exists systematically to summarise the ML methods used in RM. To fill this gap, this paper adopts the systematic mapping study to survey the state-of-the-art ML methods for RM primary studies and were finally selected in this mapping, which was published on 36 conferences and journals. The 24 factors affecting the ML method of RM are determined, of which 9, 11 and 4 are the three parts of RM, namely requirements baseline maintenance, requirements traceability and requirements change management separately. The 18 objectives of the ML method for RM are summarised, of which 6, 7 and 5 are the three parts of RM. The eight ML methods used in RM and their time sequence are summarised. The 18 evaluation indexes for RM in the ML method are determined, and the performance of these methods on these parameters is analysed. The research direction of this paper is of great significance to the research of researchers in demand management. © 2022 The Authors. IET Software published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	author = {Xu, C. and Li, Y. and Wang, B. and Dong, S.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Inc},
	keywords = {Management IS, Mapping, Systematic mapping studies, Performance, Requirement engineering, Requirements engineering, Machine learning, Machine-learning, Requirement management, Machine learning software, Full-swing, Machine learning methods, On-machines, machine learning, systematic mapping study, requirement management},
	annote = {Export Date: 23 June 2024},
}

@article{stradowski_machine_2023-1,
	title = {Machine learning in software defect prediction: {A} business-driven systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144824314&doi=10.1016%2fj.infsof.2022.107128&partnerID=40&md5=c3aa54434eb54f1abaa22279bb12f8ef},
	abstract = {Context: Machine learning is a valuable tool in software engineering allowing fair defect prediction capabilities at a relatively small expense. However, although the practical usage of machine learning in defect prediction has been studied over many years, there is not sufficient systematic effort to analyse its potential for business application. Objective: The following systematic mapping study aims to analyse the current state-of-the-art in terms of machine learning software defect prediction modelling and to identify and classify the emerging new trends. Notably, the analysis is done from a business perspective, evaluating the opportunities to adopt the latest techniques and methods in commercial settings to improve software quality and lower the cost of development life cycle. Method: We created a broad search universe to answer our research questions, performing an automated query through the Scopus database to identify relevant primary studies. Next, we evaluated all found studies using a classification scheme to map the extent of business adoption of machine learning software defect prediction based on the keywords used in the publications. Additionally, we use PRISMA 2020 guideline to validate reporting. Results: After the application of the selection criteria, the remaining 742 primary studies included in Scopus until February 23, 2022 were mapped to classify and structure the research area. The results confirm that the usage of commercial datasets is significantly smaller than the established datasets from NASA and open-source projects. However, we have also found meaningful emerging trends considering business needs in analysed studies. Conclusions: There is still a considerable amount of work to fully internalise business applicability in the field. Performed analysis has shown that purely academic considerations dominate in published research; however, there are also traces of in vivo results becoming more available. Notably, the created maps offer insight into future machine learning software defect prediction research opportunities. © 2022 Elsevier B.V.},
	author = {Stradowski, S. and Madeyski, L.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Life cycle, Mapping, Systematic mapping studies, Open source software, Machine learning, Machine-learning, Defect prediction, Defects, Forecasting, Software defect prediction, Computer software selection and evaluation, Quality control, Cost minimization, Effort and cost minimization, Machine learning software, Business applicability, Business applications, Cost benefit analysis, NASA, Prediction capability, Query processing, Systematic mapping study, Effort and cost minimisation},
	annote = {Export Date: 23 June 2024},
}

@article{medeiros_visualizations_2023-1,
	title = {Visualizations for the evolution of {Variant}-{Rich} {Systems}: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140215376&doi=10.1016%2fj.infsof.2022.107084&partnerID=40&md5=448386c9479f057b5f1f362d53b945fa},
	abstract = {Context: Variant-Rich Systems (VRSs), such as Software Product Lines or variants created through clone \& own, aim at reusing existing assets. The long lifespan of families of variants, and the scale of both the code base and the workforce make VRS maintenance and evolution a challenge. Visualization tools are a needed companion. Objective: We aim at mapping the current state of visualization interventions in the area of VRS evolution. We tackle evolution in both functionality and architecture. Three research questions are posed: What sort of analysis is being conducted to assess VRS evolution? (Analysis perspective); What sort of visualizations are displayed? (Visualization perspective); What is the research maturity of the reported interventions? (Maturity perspective). Methods: We performed a systematic mapping study including automated search in digital libraries, expert knowledge, and snowballing. Results: The study reports on 41 visualization approaches to cope with VRS evolution. Analysis wise, feature identification and location is the most popular scenario, followed by variant integration towards a Software Product Line. As for visualization, nodelink diagram visualization is predominant while researchers have come up with a wealth of ingenious visualization approaches. Finally, maturity wise, almost half of the studies are solution proposals. Most of the studies provide proof-of-concept, some of them also include publicly available tools, yet very few face proof-of-value. Conclusions: This study introduces a comparison framework where to frame future studies. It also points out distinct research gaps worth investigating as well as shortcomings in the evidence about relevance and contextual considerations (e.g., scalability). © 2022 The Author(s)},
	author = {Medeiros, R. and Martinez, J. and Díaz, O. and Falleri, J.-R.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software design, Software Product Line, Mapping, Systematic mapping studies, Computer software, Product variants, Mapping studies, Visualization, Digital libraries, Evolution, Lifespans, System evolution, System maintenance, Variant-rich system, Visualization tools, Maintenance, Software product lines, Mapping study, Variant-rich systems},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{claderon-blas_medical_2023-1,
	title = {Medical {Recommender} {Systems}: a {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192802746&doi=10.1109%2fENC60556.2023.10508695&partnerID=40&md5=eb26de296c721791c678cf1e70075fea},
	abstract = {Medical recommender systems are applications in the field of health. These systems use Artificial Intelligence techniques to provide personalized recommendations to healthcare professionals and patients, based on available and relevant patient information. Software engineering is essential in developing medical recommender systems, as these systems must be accurate, reliable, and secure for use in clinical settings. This work presents a Systematic Literature Review based on the Kitchenham and Charters guide, in order to explore the Artificial Intelligence techniques used in this type of system, which can be incorporated or improved by software developers who participate in this type of project. Twelve primary studies were selected, where mainly machine learning approaches were identified (algorithms based on decision trees, neural networks, Bayesian classifiers and clustering such as k-means), matrix approaches, based on rules, among others. Precision, Recall, and Root Mean Square Error (RMSE) were the main measures used to evaluate the performance of these systems. Finally, the studies propose always increasing the sample size of the tests carried out, including relevant patient information such as social networks and clinical information, as well as exploring other algorithms and approaches that allow improving the results of the recommendation. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Claderón-Blas, J.A. and Cerdan, M.A. and Sánchez-García, A.J. and Domingue-Isidro, S.},
	year = {2023},
	keywords = {Systematic literature review, Learning algorithms, Artificial intelligence techniques, Machine learning, Decision trees, Software, Bayesian networks, Clinical settings, Health care professionals, K-means clustering, Mean square error, Medical recommende system, Metric, Patient information, Personalized recommendation, Recommender systems, System use, Artificial Intelligence, Medical Recommender System, Metrics},
	annote = {Export Date: 23 June 2024},
}

@article{muhammad_human_2023-1,
	title = {Human factors in developing automated vehicles: {A} requirements engineering perspective},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166738978&doi=10.1016%2fj.jss.2023.111810&partnerID=40&md5=a7e6d5d79bc57f87ebe0adaee8fd4182},
	abstract = {Automated Vehicle (AV) technology has evolved significantly both in complexity and impact and is expected to ultimately change urban transportation. Due to this evolution, the development of AVs challenges the current state of automotive engineering practice, as automotive companies increasingly include agile ways of working in their plan-driven systems engineering—or even transition completely to scaled-agile approaches. However, it is unclear how knowledge about human factors (HF) and technological knowledge related to the development of AVs can be brought together in a way that effectively supports today's rapid release cycles and agile development approaches. Based on semi-structured interviews with ten experts from industry and two experts from academia, this qualitative, exploratory case study investigates the relationship between HF and AV development. The study reveals relevant properties of agile system development and HF, as well as the implications of these properties for integrating agile work, HF, and requirements engineering. According to the findings, which were evaluated in a workshop with experts from academia and industry, a culture that values HF knowledge in engineering is key. These results promise to improve the integration of HF knowledge into agile development as well as to facilitate HF research impact and time to market. © 2023 The Author(s)},
	author = {Muhammad, A.P. and Knauss, E. and Bärgman, J.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Requirement engineering, Requirements engineering, Automation, 'current, Agile development, Agile, Agile manufacturing systems, Automated vehicles, Automotive companies, Engineering perspective, Engineering practices, Human engineering, Property, Urban transportation, Vehicle technology, Vehicles, Human factors},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{de_souza_challenges_2023-1,
	title = {On {Challenges} and {Opportunities} of {Using} {Continuous} {Experimentation} in the {Engineering} of {Contemporary} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166029896&doi=10.1145%2f3592813.3592927&partnerID=40&md5=b03d1df07b3686a2afc6fb07af95d244},
	abstract = {Context: Modern Information Systems require the use of contemporary software systems such as Cyber-Physical Systems, Embedded Systems, and Smart Cities-based Systems, eventually built under the paradigm of the Internet of Things. These Contemporary Software Systems (CSS) add new challenges for their construction, maintainability, and evolution, including the involvement of many actors with the software project and the necessary management of dependencies among hardware/things, software systems, and people. Problem: These technological challenges jeopardize the final quality of modern information systems due to the lack of adequate mechanisms supporting the engineering of CSS. Solution: Continuous Experimentation (CE) deserves some investigation regarding its suitability to mitigate and reduce engineering CSS risks. IS Theory: This research is under the General Systems Theory and is consistent with the Systems Information challenges regarding building smart cities-based systems. Method: To undertake a Structured Literature Review (StLR) supported with snowballing to reveal CE’s empirical studies. Results: The StLR identified seven primary studies on CE adoption to support CSS building. Many studies are in the domain of embedded systems and CPS. Besides, the findings allowed us to conjecture a set of challenges and opportunities regarding using CE in CSS engineering. Conclusion: There are emergent technologies to support CE’s execution in the context of web-based systems. However, several challenges and gaps surround CE’s use for engineering CSS. Furthermore, the lack of software technologies, blueprints, or concrete guidance to promote CE in these software systems can motivate further investigations into its use in engineering the important parts of modern information systems. © 2023 Copyright held by the owner/author(s).},
	publisher = {Association for Computing Machinery},
	author = {de Souza, B.P. and dos Santos, P.S.M. and Travassos, G.H.},
	year = {2023},
	keywords = {Embedded systems, Information systems, Information use, Literature reviews, Computer software, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Industry 4.0, Project management, Cybe-physical systems, Cyber-physical systems, Software project, Blueprints, Embedded-system, Smart city, Software systems risks, Technological challenges, Evidence-Based Software Engineering, Continuous Experimentation, Literature Review},
	annote = {Export Date: 23 June 2024},
}

@article{mars_survey_2023-1,
	title = {A survey on automation approaches of smart contract generation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153378608&doi=10.1007%2fs11227-023-05262-8&partnerID=40&md5=77add21c036dff71f65ebea7c8b82f5f},
	abstract = {In the blockchain environment, smart contracts are computer programs that run on the blockchain platform. However, the development of smart contracts is a major challenge for developers, since blockchain platforms are still evolving. Owing to the inherited nature of blockchain, developing smart contracts without introducing vulnerabilities is not an easy task, as the deployed code is immutable and can be invoked by anyone with access to the network. Smart contracts have proved to be error-prone in practice due to the complexity of programming. Additionally, non-functional requirements, such as service cost, security, performance, authorization, and authentication, should be well implemented and defined in computer systems. In this paper, we aim to present a systematic literature review to outline in detail different approaches of smart contracts generation. Furthermore, we present a comparison of the existing approaches based on a classification according to automation paradigm and a set of defined criteria. Finally, we discuss the gaps in the literature, as well as identify a set of potential challenges which can significantly strengthen the existing work. The study shows that the examined works focused only on a limited number of specific features, such as authorization, asset control, and security. Additionally, formal verification of smart contracts and data privacy are poorly addressed. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Mars, R. and Cheikhrouhou, S. and Kallel, S. and Hadj Kacem, A.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Systematic literature review, Automation, Data privacy, Network security, Non-functional requirements, Block-chain, Blockchain, Smart contract, Authentication, Authorization, Error prones, Security authentication, Security performance, Service authentication, Service costs, Service security, Smart contract generation},
	annote = {Export Date: 23 June 2024},
}

@article{sun_design_2023-1,
	title = {A {Design} of {Automatic} {Magnetic} {Properties} {Measurement} {System} for {Under}/{Postgraduate} {Open} {Experimental} {Project}: {Effective}, {Low} {Cost}, and {Scalable}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135754845&doi=10.1109%2fTE.2022.3194356&partnerID=40&md5=a88d8dccf12434e4ff710534ab824b0e},
	abstract = {Contribution: This research designs an experimental education project of an automatic material magnetism properties measurement system. It explores how the do-it-yourself (DIY), hands-on establishment, and hardware-software integration experiment system could be leveraged to enhance the understanding of the electromagnetism theory and nonelectrical quantity measurement application. Background: Designing and establishing a prototype of an automatic measurement system are essential, inspiring and beneficial to comprehending and consolidating the theoretical principles and simulations in electromagnetism and nonelectrical quantity measurement technology courses. Furthermore, in the vision of batch deployment and flexible group teaching, a cost-efficiency, hard/soft integration system for the electromagnetism property measurement definitely fits the aim of innovation laboratory education. Intended Outcomes: This project is intended to guide the students to finish a comprehensive research-oriented project loop for multiple skills training. It is also intended to encourage student motivation and confidence through theory investigation, hands-on group collaboration, and tunning for performance calibration. Application Design: The design of the proposed project includes the literature review, hardware selection and assembly, system integration, software programming, interface visualization, and system performance calibration. The participants are assessed with both objective exam questions and subjective open survey for training objectives to reveal their increased knowledge, interests, skill confidence, and creative thinking. Findings: The proposed project has been used in 4-year laboratory open experiments with students at different levels from freshmen undergraduates to 1st-year postgraduates. A use case and survey assessment demonstrate tremendous knowledge, skills, and confidence improvement among students in electromagnetism and measurement technology courses.  © 1963-2012 IEEE.},
	author = {Sun, X. and Liu, H. and Liang, P.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Application programs, E-learning, Students, Personnel training, Calibration, Curricula, Epstein frame, Epstein frames, Experiential learning, Hard/soft integration system, Integration systems, Magnetic properties, Magnetic property measurements, Magnetism, Material magnetic property measurement, Measurement technologies, Property measurement, Surveys, Teaching, Virtual instrument, Epstein frame (EPF), experiential learning, hard/soft integration system, material magnetic property measurement, virtual instrument},
	annote = {Export Date: 23 June 2024},
}

@article{trieflinger_discovery_2023-1,
	title = {The discovery effort worthiness index: {How} much product discovery should you do and how can this be integrated into delivery?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150361731&doi=10.1016%2fj.infsof.2023.107167&partnerID=40&md5=d7394c3ca1383f10bc646afa2ddc0576},
	abstract = {Context: In a world of high dynamics and uncertainties, it is almost impossible to have a long-term prediction of which products, services, or features will satisfy the needs of the customer. To counter this situation, the conduction of Continuous Improvement or Design Thinking for product discovery are common approaches. A major constraint in conducting product discovery activities is the high effort to discover and validate features and requirements. In addition, companies struggle to integrate product discovery activities into their agile processes and iterations. Objective: This paper aims at suggests a supportive tool, the “Discovery Effort Worthiness (DEW) Index”, for product owners and agile teams to determine a suitable amount of effort that should be spent on Design Thinking activities. To operationalize DEW, proposals for practitioners are presented that can be used to integrate product discovery into product development and delivery. Method: A case study was conducted for the development of the DEW index. In addition, we conducted an expert workshop to develop proposals for the integration of product discovery activities into the product development and delivery process. Results: First, we present the "Discovery Effort Worthiness Index" in form of a formula. Second, we identified requirements that must be fulfilled for systematic integration of product discovery activities into product development and delivery. Third, we derived from the requirements proposals for the integration of product discovery activities with a company's product development and delivery. Conclusion: The developed "Discovery Effort Worthiness Index" provides a tool for companies and their product owners to determine how much effort they should spend on Design Thinking methods to discover and validate requirements. Integrating product discovery with product development and delivery should ensure that the results of product discovery are incorporated into product development. This aims to systematically analyze product risks to increase the chance of product success. © 2023 Elsevier B.V.},
	author = {Trieflinger, S. and Lang, D. and Spies, S. and Münch, J.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Product management, Integration, Uncertainty, Agile development, Design thinking, Scra, Customer satisfaction, High dynamic, Long-term prediction, Product design, Product development, Product discovery, Product roadmaps, Product service, Product roadmap, Scrum},
	annote = {Export Date: 23 June 2024},
}

@article{kotti_machine_2023-1,
	title = {Machine {Learning} for {Software} {Engineering}: {A} {Tertiary} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152600375&doi=10.1145%2f3572905&partnerID=40&md5=1e148a3bcb1e10b59bf60db05eeba5f0},
	abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.  © 2023 Association for Computing Machinery.},
	author = {Kotti, Z. and Galanopoulou, R. and Spinellis, D.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Systematic literature review, Software testing, Life cycle, Tertiary study, Machine learning, Machine-learning, Industrial research, Software testings, Computer software selection and evaluation, Additional key word and phrasestertiary study, Key words, Life cycle activities, Machine learning techniques, Software engineering life-cycle, Software Quality, machine learning, systematic literature review, Additional Key Words and PhrasesTertiary study, software engineering},
	annote = {Export Date: 23 June 2024},
}

@article{sadi_webapik_2023-1,
	title = {{WEBAPIK}: a body of structured knowledge on designing web {APIs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149938330&doi=10.1007%2fs00766-023-00401-2&partnerID=40&md5=31e57dd7c1ca274b4ed4f36deb2f8c7f},
	abstract = {With the rise in initiatives such as software ecosystems and Internet of Things (IoT), developing robust web Application Programming Interfaces (web APIs) has become an increasingly important practice. One main concern in developing web APIs is that they expose back-end systems and data toward clients. This exposure threatens critical non-functional requirements, such as the security of back-end systems, the performance of provided services, and the privacy of communications with clients. Although dealing with non-functional requirements during software design has been long studied, there is still little guide on addressing these requirements in web APIs. In this paper, we present WEBAPIK, a body of structured knowledge on addressing non-functional requirements in the design of web APIs. WEBAPIK is comprised of 27 distinct non-functional requirements, 37 distinct design techniques to address some of the identified requirements, and the trade-offs of 22 design techniques, presented in two forms of natural language and knowledge graphs. The design knowledge compiled in WEBAPIK is systematically extracted and aggregated from 80 heterogeneous online literature resources, including 7 books, 15 weblogs and tutorial, 5 vendor white papers, 6 design standards, and 47 research papers. These resources are systematically retrieved from two search engines of Google and Google Scholar and five research databases of Web of Science, IEEE Xplore, ACM Digital Library, SpringerLink, and ScienceDirect in two periods of March to August 2018 and August 2022. WEBAPIK gathers and structures expert and scholarly discussions to provide insight about addressing non-functional requirements in the design of web APIs. The structure brought to the design knowledge makes it amenable towards extension and creates the potential for employing it in the database of knowledge-based systems that aid software developers in design decision-making. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	author = {Sadi, M.H. and Yu, E.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic Review, Decision making, Software design, Application programs, Computer software reusability, Economic and social effects, Search engines, WEB application, Web applications, Knowledge based systems, Non-functional requirements, Software architecture, Quality attributes, Internet of things, Digital libraries, Application programming interfaces (API), Applications programming interfaces, Commerce, Design Patterns, Knowledge reuse, Reviews, Trade off, Web API, Systematic review, Design patterns, Trade-offs, Web APIs},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{de_campos_usability_2023-1,
	title = {Usability and {User} {Experience} {Evaluation} of {Touchable} {Holographic} {Solutions}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184291519&doi=10.1145%2f3638067.3638071&partnerID=40&md5=6667873e7cd3469430b5d782f20258a7},
	abstract = {Interacting with holograms using the hands is on track to reach a broad audience in the coming years. Therefore, a current challenge is understanding how to evaluate this new interaction scenario concerning Usability and User eXperience (UX). In this use context, the user interacts with virtual objects in your real environment. Some already known evaluation technologies have been applied to this type of solution, although they were not created considering aspects such as immersion and presence, typical in this interactive environment. Thus, this paper presents a Systematic Mapping Study (SMS) that sought to identify usability and UX evaluation technologies applied to touchable holographic solutions in augmented reality or mixed reality environments. Furthermore, the SMS sought to answer questions about evaluation technologies and holographic solutions. The SMS examined 3551 publications and selected 40 that presented 106 usability or UX evaluation technologies in a touchable holographic solution. The results shed light on methods and aspects little addressed and showed patterns and preferences for combinations of devices, gestures, and feedback types. This work contributes to HCI researchers by better understanding the state of the art of usability and UX evaluation technologies applied to touchable holographic solutions, classifying them, and discussing the main gaps and opportunities.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {De Campos, T.P. and Damasceno, E.F. and Valentim, N.M.C.},
	year = {2023},
	keywords = {Mapping, Systematic mapping studies, Users' experiences, 'current, Augmented reality, Mixed reality, Holograms, Interactive Environments, Real environments, Usability, Usability and user experience evaluation, Usability engineering, Use context, Virtual objects, systematic mapping study, augmented reality, holograms, mixed reality, usability, user experience},
	annote = {Export Date: 23 June 2024},
}

@article{molleri_backsourcing_2023-1,
	title = {Backsourcing of {IT} with focus on software development—{A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164225673&doi=10.1016%2fj.jss.2023.111771&partnerID=40&md5=b4c1364eb0462d21b5de6514e26d9187},
	abstract = {Context: Backsourcing is the process of insourcing previously outsourced activities. Backsourcing can be a viable alternative when companies experience environmental or strategic changes, or challenges with outsourcing. While outsourcing and related processes have been extensively studied, few studies report experiences with backsourcing. Objectives: We summarize the results of the research literature on backsourcing of IT, with a focus on software development. By identifying practically relevant experience, we present findings that may help companies considering backsourcing. In addition, we identify gaps in the current research literature and point out areas for future work. Method: Our systematic literature review (SLR) started with a search for empirical studies on the backsourcing of IT. From each study, we identified the context in which backsourcing occurred, the factors leading to the decision, the backsourcing process, and the outcomes of backsourcing. We employed inductive coding to extract textual data from the papers and qualitative cross-case analysis to synthesize the evidence. Results: We identified 17 papers that reported 26 cases of backsourcing, six of which were related to software development. The cases came from a variety of contexts. The most common reasons for backsourcing were improving quality, reducing costs, and regaining control of outsourced activities. We model the backsourcing process as containing five sub-processes: change management, vendor relationship management, competence building, organizational build-up, and transfer of ownership. We identified 14 positive outcomes and nine negative outcomes of backsourcing. We also aggregated the evidence and detailed three relationships of potential use to companies considering backsourcing. Finally, we have highlighted the knowledge areas of software engineering associated with the backsourcing of software development. Conclusion: The backsourcing of IT is a complex process; its implementation depends on the prior outsourcing relationship and other contextual factors. Our systematic literature review contributes to a better understanding of this process by identifying its components and their relationships based on the peer-reviewed literature. Our results can serve as a motivation and baseline for further research on backsourcing and provide guidelines and process fragments from which practitioners can benefit when they engage in backsourcing. © 2023 Elsevier Inc.},
	author = {Molléri, J.S. and Lassenius, C. and Jørgensen, M.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Systematic literature review, Software design, Outsourcing, 'current, Quality control, Engineering research, Backsourcing, Empirical studies, Environmental change, Insourcing, Software engineering management, Strategic challenges, Strategic changes, Textual data, Software development, Information technology},
	annote = {Export Date: 23 June 2024},
}

@article{omerkhel_exploring_2023-1,
	title = {{EXPLORING} {STRATEGIES} {FOR} {OVERCOMING} {ISSUES} {OF} {USER} {INVOLVEMENT} {IN} {AGILE} {SOFTWARE} {DEVELOPMENT}: {A} {SYSTEMATIC} {LITERATURE} {REVIEW}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175462629&partnerID=40&md5=c6798a876dfb842582a5174b0b26b09f},
	abstract = {The present systematic literature review (SLR) explores the challenges and strategies associated with managing users during requirement elicitation within agile software development. Drawing insights from an analysis of 24 relevant studies, this study comprehensively examines the issues that arise and the effective approaches to overcome them. The findings reveal five prominent challenges of user involvement during requirement elicitation. The most dominant issues identified are the lack of user involvement, insufficient user knowledge, and a deficit in the expertise of the Product Owner. These challenges can hinder the effective integration of user perspectives and needs into the development process. To address these challenges, the study identifies seven strategies that Product Owners can adopt to facilitate effective user involvement. These strategies include Mind Maps, User Interface Mockups, Workshops, Hybridism (combining agile and non-agile techniques), Face-to-Face Meetings, Continuous Delivery, and Training and Learning initiatives. The application of these strategies empowers Product Owners and software practitioners to enhance user involvement, improve communication, and streamline the requirement elicitation process in agile software development. The outcomes of this SLR provide valuable insights for both researchers and software practitioners, exploring the complex dynamics of user involvement in agile contexts. By recognizing these challenges and deploying effective strategies, software development teams can ensure more successful requirement elicitation processes, leading to the creation of software products that better align with user needs and expectations. This review contributes to a deeper understanding of user involvement challenges and offers actionable guidance for optimizing the requirement elicitation within agile software development paradigm. © 2023 Little Lion Scientific.},
	author = {Omerkhel, Q. and Yusop, O.M. and Ismail, S.A. and Azmi, A.},
	year = {2023},
	note = {Publisher: Little Lion Scientific},
	keywords = {Systematic Literature Review, Agile Software Development, Requirement Elicitation, User Involvement},
	annote = {Export Date: 23 June 2024},
}

@article{wang_systematic_2023-1,
	title = {A {Systematic} {Literature} {Review} of {Software} {Traceability} {Links} {Automation} {Techniques}},
	shorttitle = {软件跟踪链自动化技术研究综述},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173597434&doi=10.11897%2fSP.J.1016.2023.01919&partnerID=40&md5=f05d3c01adf3d021d5a13587264108ce},
	abstract = {As an important software capability, software traceability aims to capture, link and trace each crucial software artifact via constructing the traceability links between them. The stud-y of software traceability covers many aspects, including traceability modelling, traceability assessment and traceability implementation. Traceability links interconnect software artifacts with each other and use the resulting associative networks to resolve issues with software products and their development processes. Traceability links provide critical support for many software engineering activities, including impact analysis, software verification, test case selection, compliance verification, system security assurance and defect detection. Traceability links refer to a specific relationship between a pair of software artifacts, one of which is the source artifact and the other is the target artifact. They records various relationships between software artifacts such as dependencies, influences, and causal relationships. The direction of traceability links can be oneway or two-way. Various traceability links can help software developers to understand, develop and manage systems both efficiently and effectively. At the same time, traceability links can help people involved in all phases of software development activities to accomplish their development tasks. Requirements traceability links, as the most widely used traceability links, enable the construction and maintenance of traceability links between requirements and other software artifacts. Moreover, traceability links also include the establishment of links between code and tests, design and code, models and code, defects and code, and so on. In recent years, the creation, maintenance and validation of traceability links with information retrieval, natural language processing, machine learning, and deep learning can reduce the manual handling cost of traceability links by developers, and therefore have received extensive attentions from academia and industry. There arc also some works reviewing software traceability links approaches and techniques. In this paper, we focus on the automation techniques of the creation, maintenance and validation of traceability links so as to sort out and summarize the research progress in the past ten years. The main content includes the statistics and analysis of approaches and techniques for automated creation, maintenance and validation of traceability links, the application research of traceability links, the state-of-the-art traceability links related evaluation research and tools support, and the key problems of the current traceability links techniques. The problems arc summarized from the technical difficulties around seventh parts: the complexity of the tracing software, the granularity problem, the unsatisfying accuracy, the type limitation, the validation efficiency, the application scale and time, and the incomprehensive evaluation of traceability links. . Besides, several possible solution ideas and future development trends of the problems arc elaborated, including the construction of horizontal traceability links between software artifacts, the scalable and configurable automation techniques of traceability links, the integration of traditional approaches with artificial intelligence techniques, the creation of multiple types of traceability links using intermediary artifacts, the interactive verification of traceability links, the real-time retrieval of traceability links and open sourcing of related source codes. This review also reveals that: (1) The creation of traceability links has received a lot of academic attentions, but the research on the maintenance, verification, and application of traceability links needs more attention; (2) Requirements-to-code links are the most concerned type of researchers, followed by rcquirements-to-design and design-to-code, while other traceability links such as code/data-to-model and screenshot-to-defect are also starting to enter the vision of researchers; (3) With the continuous development of artificial intelligence (AI), Al-bascd techniques such as Naive Baycs, SVM, Bert, Doc2Vcc, RNN have been widely used in the creation, maintenance and verification of traceability links; (4) In the creation of traceability links, it is difficult to achieve good results by relying only on information retrieval and artificial intelligence techniques. Information retrieval, machine learning or deep learning techniques should be combined with traditional heuristics, model-based methods and so on, to make up for the deficiencies in AI technologies and traditional methods to further improve the quality of traceability links; (5) Research on traceability links automation techniques in complex environments, cross-platform and cross-language should be on the agenda in the future. © 2023 Science Press. All rights reserved.},
	author = {Wang, Y. and Hu, K. and Jiang, B. and Xia, X. and Tang, X.-S.},
	year = {2023},
	note = {Publisher: Science Press},
	keywords = {Software design, Software testing, Application programs, Deep learning, Learning algorithms, Learning systems, Natural language processing systems, Requirements engineering, Automation, Natural languages, Machine-learning, Defects, Compliance control, Language processing, Natural language processing, Codes (symbols), Verification, Computer software selection and evaluation, Software artefacts, Automation techniques, Computer software maintenance, Software traceability, Software traceability link, Traceability links, artificial intelligence, machine learning, deep learning, natural language processing, software traceability links},
	annote = {Export Date: 23 June 2024},
}

@article{torre_how_2023-1,
	title = {How consistency is handled in model-driven software engineering and {UML}: an expert opinion survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128744172&doi=10.1007%2fs11219-022-09585-2&partnerID=40&md5=ec55130b1fa92036e10fb41920ba4ebe},
	abstract = {Model-driven software engineering (MDSE) is an established approach for developing complex software systems. The unified modelling language (UML) is one of the most used model languages for applying the MDSE approach. UML has 14 diagram types that describe different perspectives of a software system under development. These diagrams are strongly dependent on each other and must be consistent with one another. The main objectives of this paper are as follows: (1) to understand (i) how aware experts are of model consistency issues and (ii) how relevant these issues are to experts, in order to understand model consistency in the MDSE/UML contexts, and more importantly, (2) to validate a set of 116 UML consistency rules that was systematically collected from the literature, so as to identify the rules that should always be enforced. We conducted a personal opinion survey with 106 experts in SE and MDSE, by means of an online questionnaire. The survey results describe an overview of how the topic of MDSE/UML consistency is handled by experts in the field. In addition, this survey identified a set of 52 UML consistency rules which should always be checked in every UML diagram. The majority of these 52 rules were understood by the majority of respondents and are general-purpose rules that are involved in the Design software development phase. This subset of 52 rules could be considered to be (1) added to the UML standard, (2) used as a reference to researchers who study UML/MDSE, and (3) used as a practical example for teaching purposes. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Torre, D. and Genero, M. and Labiche, Y. and Elaasar, M.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Software design, Computer software, Unified Modeling Language, Surveys, Empirical studies, Expert opinion, Model consistency, Model-driven software engineering consistency, Model-driven software engineerings, Opinion surveys, Personal opinion survey, Software engineering model, Unified modeling language consistency rule, Empirical study, MDSE consistency, Model-driven software engineering, UML consistency rules, Unified modelling language},
	annote = {Export Date: 23 June 2024},
}

@article{bianco_reducing_2023-1,
	title = {Reducing the user labeling effort in effective high recall tasks by fine-tuning active learning},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146621225&doi=10.1007%2fs10844-022-00772-y&partnerID=40&md5=27482213cefa9ee93b873591699a61b7},
	abstract = {High recall Information REtrieval (HIRE) aims at identifying only and (almost) all relevant documents for a given query. HIRE is paramount in applications such as systematic literature review, medicine, legal jurisprudence, among others. To address the HIRE goals, active learning methods have proven valuable in determining informative and non-redundant documents to reduce user effort for manual labeling. We propose a new active learning framework for the HIRE task. REVEAL-HIRE selects a very reduced set of documents to be labeled, significantly mitigating the user’s effort. The proposed approach selects the most representative documents by exploiting a novel, specifically designed active learning strategy for HIRE, called REVEAL (RelEVant rulE-based Active Learning). REVEAL aims at selecting the maximum number of relevant documents for a given query based on discriminative rule-based patterns and a penalization factor. The method is applied to the top-ranked documents to choose the most informative ones to be labeled, a hard task due to data skewness – most documents are irrelevant for a given query. The enhanced active learning process is repeated incrementally until a stopping point is achieved, using REVEAL to identify the point in the process when relevant documents should stop to be sampled. Experimental results in several standard benchmark datasets (e.g. 20-Newsgroups, Trec Total Recall, and CLEF eHealth) demonstrate that REVEAL-HIRE can reduce the user labeling effort up to 3 times (320\% of reduction) in comparison with state-of-the-art baselines while keeping the effectiveness at the highest levels. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Bianco, G.D. and Duarte, D. and Gonçalves, M.A.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Artificial intelligence, Learning systems, Classification (of information), Information retrieval, Labelings, Active Learning, Fine tuning, Hire, Labeling process, Relevant documents, Rule based, SSAR, Supervised classifiers, User labeling, Active learning, Supervised classifier},
	annote = {Export Date: 23 June 2024},
}

@article{yang_seeing_2023-1,
	title = {Seeing the {Whole} {Elephant}: {Systematically} {Understanding} and {Uncovering} {Evaluation} {Biases} in {Automated} {Program} {Repair}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161982187&doi=10.1145%2f3561382&partnerID=40&md5=c5af7c7df149502ee029204053597e14},
	abstract = {Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation.  © 2023 Association for Computing Machinery.},
	author = {Yang, D. and Lei, Y. and Mao, X. and Qi, Y. and Yi, X.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Systematic literature review, Open source software, Repair, Key words, Additional key word and phrasesautomated program repair, Bias studies, Constraint-based, Empirical evaluations, Executables, Petroleum reservoir evaluation, Repair techniques, Systematic exploration, Validation strategies, Additional Key Words and PhrasesAutomated program repair, bias study, empirical evaluation},
	annote = {Export Date: 23 June 2024},
}

@article{alonso_systematic_2023-1,
	title = {A systematic mapping study and practitioner insights on the use of software engineering practices to develop {MVPs}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145972508&doi=10.1016%2fj.infsof.2022.107144&partnerID=40&md5=27902f1fdcb0c646416221c72975d06a},
	abstract = {Background: Many startup environments and even traditional software companies have embraced the use of MVPs (Minimum Viable Products) to allow quickly experimenting solution options. The MVP concept has influenced the way in which development teams apply Software Engineering (SE) practices. However, the overall understanding of this influence of MVPs on SE practices is still poor. Objective: Our goal is to characterize the publication landscape on practices that have been used in the context of software MVPs and to gather practitioner insights on the identified practices. Method: We conducted a systematic mapping study using a hybrid search strategy that consists of a database search and parallel forward and backward snowballing. Thereafter, we discussed the mapping study results in two focus groups sessions involving twelve industry practitioners that extensively use MVPs in their projects to capture their perceptions on the findings of the mapping study. Results: We identified 33 papers published between 2013 and 2020. We observed some trends related to MVP ideation (or MVP conception) and evaluation practices. For instance, regarding ideation, we found six different approaches (e.g., Design Thinking, Lean Inception) and mainly informal end-user involvement practices (e.g., workshops, interviews). Regarding evaluation, there is an emphasis on end-user validations based on practices such as usability tests, A/B testing, and usage data analysis. However, there is still limited research related to MVP technical feasibility assessment and effort estimation. Practitioners of the focus group sessions reinforced the confidence in our results regarding ideation and evaluation practices, being aware of most of the identified practices. They also reported how they deal with the technical feasibility assessments (involving developers during the ideation and conducting informal experiments) and effort estimation in practice (based on expert opinion and using practices common to agile methodologies, such as Planning Poker). Conclusion: Our analysis suggests that there are opportunities for solution proposals and evaluation studies to address literature gaps concerning technical feasibility assessment and effort estimation. Overall, more effort needs to be invested into empirically evaluating the existing MVP-related practices. © 2022 Elsevier B.V.},
	author = {Alonso, S. and Kalinowski, M. and Ferreira, B. and Barbosa, S.D.J. and Lopes, H.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software engineering, Software company, Mapping, Systematic mapping studies, Search engines, Systematic mapping, Mapping studies, Effort Estimation, Feasibility assessment, Focus groups, Minimum viable product, Software engineering practices, Focus group, MVP},
	annote = {Export Date: 23 June 2024},
}

@article{reis_automated_2023-1,
	title = {Automated guided vehicles position control: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122319071&doi=10.1007%2fs10845-021-01893-x&partnerID=40&md5=d765d8c0523ba75a0782a045796b8994},
	abstract = {Automated Guided Vehicles (AGVs) are essential elements of manufacturing intralogistics and material handling. Improving the position accuracy along the AGV trajectory allows the vehicle to work on narrower aisles with lower error tolerance. Despite the increasing number of papers in AGVs and mobile robots’ position control research area, there is a lack of curatorial work presenting and analyzing the control strategies applied in the problem domain. Therefore, the main objective is to analyze the published researches of the past seven years on the position control of AGVs to recognize research patterns, gaps, and tendencies, outlining the research field. The paper proposes a systematic literature review to investigate the research field from the controller design perspective. Its protocol and procedures are presented in detail. Four main research topics were addressed: the control strategies used in the AGV position control problem, how the literature presents the AGV operating requirement of position accuracy, how the literature validate the proposed controller and present their results regarding the system’s position accuracy, and the technological tendencies the proposed solutions reveals. Besides, within the main topics, other points were investigated, such as the AGV application area, the considered mathematical model, the sensors and guidance system used, and the maximum payload of the vehicle and operation under different load conditions. The data synthesis shows the predominant control strategies applied to the problem and the interaction among distinct control theory areas, indicating a notable interaction of Intelligent Control techniques to the other strategies. The paper’s contributions are using a systematic literature review method over the AGV position control publications, presenting an overview of the research area, analyzing the research question topics from selected articles, and proposing a research agenda. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Reis, W.P.N. and Couto, G.E. and Junior, O.M.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Systematic literature review, Research areas, Automated guided vehicles, Automatic guided vehicles, Control strategies, Control system synthesis, Controllers, Essential elements, Intelligent control, Material handling, Materials handling, Mobile robots, Path tracking, Position accuracy, Position control, Research fields, Vehicle position, Automated guided vehicle, Control systems, Mobile robot},
	annote = {Export Date: 23 June 2024},
}

@article{borstler_investigating_2023-1,
	title = {Investigating acceptance behavior in software engineering—{Theoretical} perspectives},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146227386&doi=10.1016%2fj.jss.2022.111592&partnerID=40&md5=4df944097102b7c6c0fb2ecbb7397c86},
	abstract = {Background: Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology. Objective: To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development. Method: We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking). Results: We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior. Conclusions: Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories. © 2022 The Author(s)},
	author = {Börstler, J. and Ali, N.B. and Svensson, M. and Petersen, K.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Software design, Behavioral research, Software engineering research, Acceptance behavior, TAM, TPB, Dual-process theories, Oriented process, Software development practices, Technology acceptance, Theory, UTAUT, Dual process theory},
	annote = {Export Date: 23 June 2024},
}

@article{kotti_impact_2023-1,
	title = {Impact of {Software} {Engineering} {Research} in {Practice}: {A} {Patent} and {Author} {Survey} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139448523&doi=10.1109%2fTSE.2022.3208210&partnerID=40&md5=d72b5940144a9dec58e6b088e8d70a4e},
	abstract = {Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers published in four leading SE venues between 1975-2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26\% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic.  © 1976-2012 IEEE.},
	author = {Kotti, Z. and Gousios, G. and Spinellis, D.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Software engineering, Economic and social effects, Industrial research, Software engineering research, Engineering research, Software, Cost benefit analysis, Surveys, Empirical studies, Collaboration, Interview, Patent, Patent citation, Patents and inventions, Practical impact, Quantitative and qualitative analysis, Survey analysis, empirical study, patent citations, practical impact, survey},
	annote = {Export Date: 23 June 2024},
}

@article{al-ahmad_overview_2023-1,
	title = {Overview on {Case} {Study} {Penetration} {Testing} {Models} {Evaluation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160556929&doi=10.28991%2fESJ-2023-07-03-025&partnerID=40&md5=9da6f31eb866e14860834a06ff6466c6},
	abstract = {Model evaluation is a cornerstone of scientific research as it represents the findings' accuracy and model performance. A case study is commonly used in evaluating software engineering models. Due to criticism in terms of generalization from a single case study and testers, deciding on the number of case studies used for evaluation and the number of testers has been one of the researchers’ challenges. Multiple case studies with multiple testers can be difficult in some domains, such as penetration testing, due to the complexity and time needed to prepare test cases. This study aims to review the literature and examine the evaluation methods used pertaining to the number of case studies and testers involved. This study is beneficial for researchers, students, and penetration testers as it provides case study design steps that are useful to determine the appropriate number of test cases and testers required. The paper's findings and novelty highlight that a single case study with a single tester is enough to evaluate a model. It also strikes a balance between what is enough for the evaluation and the need to reduce criticisms of a single case study by using two case studies with a single tester. © 2023 by the authors. Licensee ESJ, Italy.},
	author = {Al-Ahmad, A.S. and Kahtan, H. and Alzoubi, Y.I.},
	year = {2023},
	note = {Publisher: Ital Publication},
	keywords = {Case Study, Model Evaluation, Penetration Testing, Software Engineering, Tester},
	annote = {Export Date: 23 June 2024},
}

@article{miloudi_systematic_2023-1,
	title = {Systematic {Review} of {Machine} {Learning}-{Based} {Open}-{Source} {Software} {Maintenance} {Effort} {Estimation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143977206&doi=10.2174%2f2666255816666220609110712&partnerID=40&md5=4d4bca6f178d40ff4531940a93a67f66},
	abstract = {Background: Software maintenance is known as a laborious activity in the software lifecycle and is often considered more expensive than other activities. Open-Source Software (OSS) has gained considerable acceptance in the industry recently, and the Maintenance Effort Estimation (MEE) of such software has emerged as an important research topic. In this context, researchers have conducted a number of open-source software maintenance effort estimation (O-MEE) studies based on statistical as well as machine learning techniques for better estimation. Objective: The objective of this study is to perform a systematic literature review (SLR) to analyze and summarize the empirical evidence of O-MEE ML techniques in current research through a set of five Research Questions (RQs) related to several criteria (e.g. data pre-processing tasks, data mining tasks, tuning parameter methods, accuracy criteria and statistical tests, as well as ML techniques reported in the literature that outperformed). Methods: We performed a systematic literature review of 36 primary empirical studies published from 2000 to June 2020, selected based on an automated search of six digital databases. Results: The findings show that Bayesian networks, decision tree, support vector machines and instance-based reasoning were the ML techniques most used; few studies opted for ensemble or hybrid techniques. Researchers have paid less attention to O-MEE data pre-processing in terms of feature selection, methods that handle missing values and imbalanced datasets, and tuning parameters of ML techniques. Classification data mining is the task most addressed using different accuracy criteria such as Precision, Recall, and Accuracy, as well as Wilcoxon and Mann-Whitney statistical tests. Conclusion: This SLR identifies a number of gaps in the current research and suggests areas for further investigation. For instance, since OSS includes different data source formats, researchers should pay more attention to data pre-processing and develop new models using ensemble techniques since they have proved to perform better. © 2023 Bentham Science Publishers.},
	author = {Miloudi, C. and Cheikhi, L. and Abran, A.},
	year = {2023},
	note = {Publisher: Bentham Science Publishers},
	keywords = {Systematic literature review, Life cycle, Learning algorithms, Learning systems, Open source software, 'current, Decision trees, Effort Estimation, Support vector machines, Open systems, Open-source softwares, Data mining, Bayesian networks, Machine learning techniques, Empirical studies, Computer software maintenance, Data preprocessing, Ensemble techniques, Maintenance effort estimation, Maintenance efforts, Statistical tests, open-source software, data pre-processing, empirical studies, ensemble techniques, maintenance effort estimation},
	annote = {Export Date: 23 June 2024},
}

@article{anuar_multivocal_2023-1,
	title = {A multivocal literature review on record management potential components in {CRUD} operation for web application development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131085987&doi=10.1142%2fS1793962323410192&partnerID=40&md5=c69d2412dddcbff1a771c169b69a7201},
	abstract = {In recent years, web application frameworks have been widely practised by many developers to increase programming productivity as the frameworks are more flexible, rapidly built using CRUD operation, MVC-based, secure and most of them are published under an open-source license which will reduce the final cost of development. Although the CRUD automation in the web application framework boosts the development process, there are many important aspects of a web application absent from the CRUD output. Therefore, this multivocal literature review investigates the record management aspects that are required in modern WA and the perceived benefit of integrating the record management aspect into CRUD operation. The study extracted 284 publications from respectable scientific resources and the grey resources literature created by WA development practitioners outside academic mediums. After a detailed review process, only 14 scientific primary studies and 13 gray studies were considered for this review based on defined inclusion and exclusion criteria. The review shows that the most important aspects required in WA are search, role-based access control, retention, appraisal, search, audit trail, digital archiving, sharing, reporting, inactive files management and several other features. These important aspects have been analyzed and characterized according to its function and features. The method and procedure for integrating the specified aspect into CRUD operation are identified and discussed. Integrating and implementing the specified record management features into CRUD operation will boost the WA development productivity by producing more features as a standard output with integrated record management functions.  © 2023 World Scientific Publishing Company.},
	author = {Anuar, A.W. and Kama, N. and Azmi, A. and Rusli, H.M.},
	year = {2023},
	note = {Publisher: World Scientific},
	keywords = {Literature reviews, Open source software, Multivocal literature review, Access control, CRUD, Electronic records managements, Final costs, Open source license, Productivity, Record management, Web application development, Web application frameworks, Web frameworks, electronic record management, web application development, web framework},
	annote = {Export Date: 23 June 2024},
}

@article{findrik_drivers_2023-1,
	title = {Drivers and barriers for consumers purchasing bioplastics – {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153893687&doi=10.1016%2fj.jclepro.2023.137311&partnerID=40&md5=63fcad7bc912638f6c3e6e550db8c7b5},
	abstract = {Plastic pollution has adverse effects on the ecosystem and on the human body. Bioplastics might provide an alternative to plastic for environmentally friendly consumers. A systematic literature review was conducted to analyze and summarize the state of the art regarding consumers' response to bioplastics using a hierarchy of effects model. The model holistically represents the relevant steps in understanding consumers' journey from stimuli to the behavioral stage. The review was based on 67 scientific journal articles on consumer studies related to bioplastics published in English language (all peer-reviewed). Most studies researched bioplastic packaging applications e.g., food and beverage packaging using quantitative consumer surveys. Many studies focused on consumer preferences and willingness to pay for bioplastics, while awareness, knowledge, and post-purchase behavior—usage and disposal—was the least researched. Many of the studies applied text or oral and rarely real product stimuli. The results of the synthesis pointed out some purchasing barriers e.g. consumers' low knowledge about the environmental impact, characteristics such as material source or end-of-life character of bioplastics; or consumers' uncertainty about bioplastic recognition versus conventional plastics. Drivers of consumers' purchasing bioplastics are also identified, for instance consumers' positive attitude, available product information or consumers' green value. Bioplastic products meeting consumers' preferences such as low price, biogenic resource base, and local origin also act as purchasing drivers. Studies also found that bioplastic related information of a product influences consumers’ willingness to pay. The review revealed research gaps, highlighting in particular the need for cross-cultural studies, non-hypothetical research designs and the analysis of labelling systems related to bioplastic products. © 2023 The Authors},
	author = {Findrik, E. and Meixner, O.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Systematic literature review, Systematic Review, State of the art, Adverse effect, Bio-plastics, Consumer, Consumer behavior, Consumers' preferences, Environmental impact, Human bodies, Labeling, Plastic pollutions, Purchasing, Reinforced plastics, Sales, Willingness to pay, Systematic review, Bioplastics},
	annote = {Export Date: 23 June 2024},
}

@article{pizard_assessing_2023-1,
	title = {Assessing attitudes towards evidence-based software engineering in a government agency},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141479463&doi=10.1016%2fj.infsof.2022.107101&partnerID=40&md5=958be07a4396062f45df6bce9c1d9292},
	abstract = {Context: Evidence-based practice (EBP) has allowed several disciplines to become more mature by emphasizing the use of evidence from well-designed and well-conducted research in decision-making. Its application in SE, Evidence-based software engineering (EBSE) can help to bridge the gap between academia and industry by bringing together academic rigor and research of practical relevance. To achieve this, it seems necessary to improve its adoption. Objective: We sought both to study the attitudes towards EBSE of stakeholders working in a government agency (GA) and to assess whether knowledge of EBSE would impact their working practices. Method: We conducted a multi-stage field investigation in an Uruguayan national GA that is responsible for digital policies. First, we organized an EBSE awareness lecture and we collected and analyzed participants’ perceptions of the value and limitations of EBSE. Sixteen months later, in a second stage, we contacted the agency and asked participants whether they had made use of the information about EBSE we presented to them. Results: Initially, participants reported that EBSE seemed useful for tackling challenging problems and, in particular, considered its use appropriate given the agency's responsibilities. Perceived barriers to EBSE adoption were the need for institutional support, the lack of government practice reports, inadequate skills or motivation, the cost of conducting systematic reviews, and the lack of evidence about emerging issues. In the follow-up survey, although the participants were not undertaking systematic reviews themselves, many reported improvements in how they searched for and evaluated information to support their work. Conclusion: Our study presents some insights to better understand EBSE adoption. With the exception of GA-specific issues, perceived value and barriers to adoption were consistent with those reported in software engineering and other disciplines. Our follow-up study confirms the potential value of evidence in the context of IT regulatory and government bodies. © 2022 Elsevier B.V.},
	author = {Pizard, S. and Acerenza, F. and Vallespir, D. and Kitchenham, B.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systematic Review, Decision making, Decisions makings, Application programs, Search engines, Evidence Based Software Engineering, Surveys, Focus groups, Bridges, Evidence-based practices, Field investigation, Follow-up Studies, Government agencies, ITS applications, Working practices, Focus group, Evidence-based practice, Evidence-based software engineering, Follow-up study, Government agency},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{junior_towards_2023-1,
	title = {Towards {Federated} {Ontology}-{Driven} {Data} {Integration} in {Continuous} {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174524159&doi=10.1145%2f3613372.3613380&partnerID=40&md5=3e4b6d35c3cbb7db05fb9d3cadd95ac4},
	abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Júnior, P.S.S. and Almeida, J.P.A. and Barcellos, M.},
	year = {2023},
	keywords = {Decision making, Decisions makings, Software design, Computer software reusability, Continuous software engineerings, Ontology, Ontology's, Project management, Quality assessment, Software engineering practices, Assessment tool, Daily activity, Data integration, Data items, Integrated data, Project management tools, Continuous Software Engineering, Data Integration},
	annote = {Export Date: 23 June 2024},
}

@article{russo_exploring_2023-1,
	title = {Exploring a {Multidisciplinary} {Assessment} of {Organisational} {Maturity} in {Business} {Continuity}: {A} {Perspective} and {Future} {Research} {Outlook}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192352194&doi=10.3390%2fapp132111846&partnerID=40&md5=0304e1dbc51d2f583b2eab9b338ef4fc},
	abstract = {In a competitive business landscape heavily reliant on information and communication technology, organisations must be prepared to address disruptions in their business operations. Business continuity management involves effective planning for the swift reestablishment of business processes in the short term. However, there are still obstacles to implementing business continuity plans, which can be justified by various factors. The purpose of this study is to present the perspectives and future research paths based on a systematic literature review from the peer-reviewed literature published from 1 January 2000 to 31 December 2021. This systematic literature review adheres to the guidelines established by evidence-based software engineering and leverages the Parsifal online tool. The primary research results identify and establish connections between the common components and activities of business continuity management as defined in international standards and frameworks to identify gaps in the existing knowledge. These findings will contribute to the development of a framework that provides a practical approach applicable to organisations of all sizes, taking into account each aspect of business continuity management, with a particular emphasis on information and communication technology systems. This paper’s contribution lies in offering insights from a systematic literature review regarding the strategic principles for designing and implementing a business continuity plan, along with a comprehensive overview of related research. Furthermore, it presents a path forward to guide future research efforts aimed at addressing the gaps in the literature concerning continuity planning. © 2023 by the authors.},
	author = {Russo, N. and Mamede, H.S. and Reis, L. and Martins, J. and Branco, F.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {information and communication technology, business continuity plan, framework, guidelines, literature review, organisational resilience, strategy},
	annote = {Export Date: 23 June 2024},
}

@article{zhao_systematic_2023-1,
	title = {A {Systematic} {Survey} of {Just}-in-{Time} {Software} {Defect} {Prediction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147853196&doi=10.1145%2f3567550&partnerID=40&md5=f90ec760e71ec41329ce66005b60aa30},
	abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP.  © 2023 Association for Computing Machinery.},
	author = {Zhao, Y. and Damevski, K. and Chen, H.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Machine learning, Machine-learning, Forecasting, Software defect prediction, Computer software selection and evaluation, Change defect density, Change-level software defect prediction, Defect density, Defects density, Just in time production, Just-in-time, Just-in-time software defect prediction, Quality assurance, Release software defect prediction, Searching-based algorithm, Software change, Software change metric, machine learning, change defect density, change-level software defect prediction, just-in-time software defect prediction, release software defect prediction, searching-based algorithms, software change metrics},
	annote = {Export Date: 23 June 2024},
}

@article{khan_analysis_2023-1,
	title = {Analysis of {Cursive} {Text} {Recognition} {Systems}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167725839&doi=10.1145%2f3592600&partnerID=40&md5=bd2a595a4a3f8ecc36b5dc5846efdd8a},
	abstract = {Regional and cultural diversities around the world have given birth to a large number of writing systems and scripts, which consist of varying character sets. Developing an optimal character recognition for such a varying and large character set is a challenging task. Unlimited variations in handwritten text due to mood swings, varying writing styles, changes in medium of writing, and many more puzzle the research community. To overcome this problem, researchers have proposed various techniques for the automatic recognition of cursive languages like Urdu, Pashto, and Arabic. With the passage of time, the field of text recognition matured, and the number of publications exponentially increased in the targeted field. It is very difficult to find all the techniques developed, calculate the time and resource consumptions, and understand the cost-benefit tradeoffs among these techniques. These tradeoffs resist making this technology able for practical use. To address these tradeoffs, this article systematic analysis to identify gaps in the literature and suggest new enhanced solution accordingly. A total of 153 of the most relevant articles from 2008 to 2022 are analyzed in this systematic literature review (SLR) work. This systematic review process shows (1) the list of techniques suggested for cursive text recognition purposes and its capabilities, (2) set of feature extraction techniques proposed, and (3) implementation tools used to design and simulate the empirical studies in this specialized field. We have also discussed the emerging trends and described their implications for the research community in this specialized domain. This systematic assessment will ultimately help researchers to perform an overview of the existing character/text recognition approaches, recognition capabilities, and time consumption and subsequently identify the areas that requires a significant attention in the near future. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	author = {Khan, S. and Nazir, S. and Khan, H.U.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Systematic literature review, Research communities, Cost benefit analysis, Key words, Commerce, Additional key word and phrasescursive language, Character recognition, Character sets, Cultural diversity, Feature technique, Recognition algorithm, Recognition systems, Text recognition, Time consumption, systematic literature review, Additional Key Words and PhrasesCursive languages, feature techniques, recognition algorithms},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{bernardes_understanding_2023-1,
	title = {On the {Understanding} of the {Role} of {Continuous} {Experimentation} in {Technology}-{Based} {Startup}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174536882&doi=10.1145%2f3613372.3613414&partnerID=40&md5=1bce7cfc7bf056d361b28ebbb75f6671},
	abstract = {Technology startups are constantly emerging, trying to create innovative solutions in environments of uncertainty, and because they face numerous challenges, they have high failure rates. The scarcity of resources and the product's lack of adherence to market needs are among the challenges. In an attempt to alleviate these challenges, initiatives such as Continuous Experimentation arise. This approach supports systematical tests of hypotheses, helping teams prioritize deliveries that increase perceived value by the users. Our interview-based study aimed to identify how Continuous Experimentation is being adopted and how it underlies software engineering activities throughout the product development cycle of technology-based startups. We found that data-driven decisions and reduced development effort are benefits of adopting such an approach while the low competence and education in experimentation is among the main challenges, suggesting that there is room for qualifying professionals in the matter.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Bernardes, M. and Marczak, S.},
	year = {2023},
	keywords = {Software engineering, Systematic literature review, Engineering education, Uncertainty, Continuous experimentation, Entrepreneurship, Failure analysis, Failure rate, Innovative solutions, Lean startup, Market needs, Technology start-up, Technology-based, Systematic Literature Review, Lean Startup, Continuous Experimentation, Software Engineering},
	annote = {Export Date: 23 June 2024},
}

@article{kabir_meta-synthesis_2023-1,
	title = {A {Meta}-{Synthesis} of the {Barriers} and {Facilitators} for {Personal} {Informatics} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173236525&doi=10.1145%2f3610893&partnerID=40&md5=55ea134f22e9de4c81f1c9058f760ae0},
	abstract = {Personal informatics (PI) systems are designed for diverse users in the real world. Even when these systems are usable, people encounter barriers while engaging with them in ways designers cannot anticipate, which impacts the system's effectiveness. Although PI literature extensively reports such barriers, the volume of this information can be overwhelming. Researchers and practitioners often find themselves repeatedly addressing the same challenges since sifting through this enormous volume of knowledge looking for relevant insights is often infeasible. We contribute to alleviating this issue by conducting a meta-synthesis of the PI literature and categorizing people's barriers and facilitators to engagement with PI systems into eight themes. Based on the synthesized knowledge, we discuss specific generalizable barriers and paths for further investigations. This synthesis can serve as an index to identify barriers pertinent to each application domain and possibly to identify barriers from one domain that might apply to a different domain. Finally, to ensure the sustainability of the syntheses, we propose a Design Statements (DS) block for research articles. © 2023 Owner/Author.},
	author = {Kabir, K.S. and Wiese, J.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Different domains, Real-world, Applications domains, Barrier and facilitator, Informatics systems, Meta-synthesis, Personal informatics, Self-tracking, Synthesised, System effectiveness, barriers and facilitators, meta-synthesis, self-tracking},
	annote = {Export Date: 23 June 2024},
}

@article{bertolino_devopret_2023-1,
	title = {{DevOpRET}: {Continuous} reliability testing in {DevOps}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088248050&doi=10.1002%2fsmr.2298&partnerID=40&md5=7e6aa3c834bb767e1422efdaf28f9791},
	abstract = {To enter the production stage, in DevOps practices candidate software releases have to pass quality gates, where they are assessed to meet established target values for key indicators of interest. We believe software reliability should be an important such indicator, as it greatly contributes to the end-user satisfaction. We propose DevOpRET, an approach for reliability testing as part of the acceptance testing stage in DevOps. DevOpRET relies on operational-profile–based testing, a common reliability assessment technique. DevOpRET leverages usage and failure data monitored in operations to continuously refine its estimate. We evaluate accuracy and efficiency of DevOpRET through controlled experiments with a real-world open source platform and with a microservice architectures benchmark. The results show that DevOpRET provides accurate and efficient estimates of the true reliability over subsequent DevOps cycles. © 2020 John Wiley \& Sons, Ltd.},
	author = {Bertolino, A. and Angelis, G.D. and Guerriero, A. and Miranda, B. and Pietrantuono, R. and Russo, S.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Software testing, Open source software, Acceptance tests, End-user satisfactions, Key indicator, Operational profile, Production stage, Quality gates, Reliability testing, Software release, Software reliability, Software reliability testing, Software-Reliability, Target values, DevOps, acceptance test, operational profile, quality gate, software reliability testing},
	annote = {Export Date: 23 June 2024},
}

@article{ahmad_requirements_2023-2,
	title = {Requirements engineering framework for human-centered artificial intelligence software systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163544926&doi=10.1016%2fj.asoc.2023.110455&partnerID=40&md5=de91d4822852a3733b100d5ff3c485e6},
	abstract = {Context: Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Objective: Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. Method: In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. Results: The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. Conclusion: We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software. © 2023 The Author(s)},
	author = {Ahmad, K. and Abdelrazek, M. and Arora, C. and Agrahari Baniya, A. and Bano, M. and Grundy, J.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Software engineering, Artificial intelligence, Requirement engineering, Requirements engineering, Intelligence software, Software-systems, Machine learning, Machine-learning, Engineering education, E-learning, Conceptual model, Empirical Software Engineering, Engineering frameworks, Human-centered, In-buildings, Software solution, Virtual reality, Conceptual modeling, Empirical software engineering},
	annote = {Export Date: 23 June 2024},
}

@article{belle_bolstering_2023-1,
	title = {Bolstering the {Persistence} of {Black} {Students} in {Undergraduate} {Computer} {Science} {Programs}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181846534&doi=10.1145%2f3617896&partnerID=40&md5=1a67a519327ea45a06612612bb99fadb},
	abstract = {Background: People who are racialized, gendered, or otherwise minoritized are underrepresented in computing professions in North America. This is reflected in undergraduate computer science (CS) programs, in which students from marginalized backgrounds continue to experience inequities that do not typically affect White cis-men. This is especially true for Black students in general, and Black women in particular, whose experience of systemic, anti-Black racism compromises their ability to persist and thrive in CS education contexts. Objectives: This systematic mapping study endeavours to (1) determine the quantity of existing non-deficit-based studies concerned with the persistence of Black students in undergraduate CS; (2) summarize the findings and recommendations in those studies; and (3) identify areas in which additional studies may be required. We aim to accomplish these objectives by way of two research questions: (RQ1) What factors are associated with Black students’ persistence in undergraduate CS programs?; and (RQ2) What recommendations have been made to further bolster Black students’ persistence in undergraduate CS education programs? Methods: This systematic mapping study was conducted in accordance with PRISMA 2020 and SEGRESS guidelines. Studies were identified by conducting keyword searches in seven databases. Inclusion and exclusion criteria were designed to capture studies illuminating persistence factors for Black students in undergraduate CS programs. To ensure the completeness of our search results, we engaged in snowballing and an expert-based search to identify additional studies of interest. Finally, data were collected from each study to address the research questions outlined above. Results: Using the methods outlined above, we identified 16 empirical studies, including qualitative, quantitative, and mixed-methods studies informed by a range of theoretical frameworks. Based on data collected from the primary studies in our sample, we identified 13 persistence factors across four categories: (I) social capital, networking, \& support; (II) career \& professional development; (III) pedagogical \& programmatic interventions; and (IV) exposure \& access. This data-collection process also yielded 26 recommendations across six stakeholder groups: (i) researchers; (ii) colleges and universities; (iii) the computing industry; (iv) K-12 systems and schools; (v) governments; and (vi) parents. Conclusion: This systematic mapping study resulted in the identification of numerous persistence factors for Black students in CS. Crucially, however, these persistence factors allow Black students to persist, but not thrive, in CS. Accordingly, we contend that more needs to be done to address the systemic inequities faced by Black people in general, and Black women in particular, in computing programs and professions. As evidenced by the relatively small number of primary studies captured by this systematic mapping study, there exists an urgent need for additional, asset-based empirical studies involving Black students in CS. In addition to foregrounding the intersectional experiences of Black women in CS, future studies should attend to the currently understudied experiences of Black men. © 2023 Copyright held by the owner/author(s).},
	author = {Belle, A.B. and Sutherland, C. and Adesina, O.O. and Kpodjedo, S. and Ojong, N. and Cole, L.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Mapping, Systematic mapping studies, Economic and social effects, Engineering education, Research questions, Students, Empirical studies, Anti-black racism in computer science, Black student in computer science, Computer Science Education, Computer science programs, Education computing, Employment, Equity diversity inclusion, Equity in computer science education, Student persistences, anti-Black racism in computer science, Black students in computer science, Equity Diversity Inclusion, equity in computer science education},
	annote = {Export Date: 23 June 2024},
}

@article{beke_what_2023-1,
	title = {What managers can learn from knowledge intensive technology startups? {Exploring} the skillset for developing adaptive organizational learning capabilities of a successful start-up enterprise in management education},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163816643&doi=10.1556%2f204.2022.00027&partnerID=40&md5=ce39554a7e9a704186ca5f51032ae19b},
	abstract = {The study shows what management students could learn from technology startups from an organizational learning (learning organization) perspective; and whether or on what level this entrepreneurial mindset is built into management education. First, the organizational learning patterns and adaptive entrepreneurial skillset of startups are identified, based on a review of the recent literature focusing on knowledge-intensive technology startups’ organizational learning patterns. Then, qualitative interviews and document analysis are applied to find out whether or on what level the improvement of these skills for developing an adaptive and successful startup are present as ‘learning organizations’ are integrated in top Central-European higher management education curricula. Based on the literature review, the theoretical framework is introduced, consisting of five pillars of ‘start-up learning’: ambidextrous entrepreneurial learning, business model development, failure and experiential learning, benchmarking and learning from others, and agile product development. The empirical research looks for these pillars in management MSc programs of a top Central-European business school. The most important findings reveal that the analyzed management education programs strongly prepare students with benchmarking skills. However, the study also showed that the culture and experience of failure and the capability of learning from failure are missing from these education programs. © 2022 The Author(s).},
	author = {Beke, D.D. and Sólyom, A. and Klér, A.J.},
	year = {2023},
	note = {Publisher: Akademiai Kiado ZRt.},
	keywords = {entrepreneurial skills, entrepreneurship education, higher education, management education, organizational learning, startup},
	annote = {Export Date: 23 June 2024},
}

@article{dowlut_forecasting_2023-1,
	title = {Forecasting resort hotel tourism demand using deep learning techniques – {A} systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165006073&doi=10.1016%2fj.heliyon.2023.e18385&partnerID=40&md5=cbaba4fc27e8cfc275704c2f67103ae5},
	abstract = {In the hospitality industry, revenue management is vital for the sustainability of the business. Powering this strategic concept is the occupancy rate (OR) forecast. Predicting occupancy of the hotel is essential for managers’ decision-making process as it gives an estimate of the future business performance. However, the fast-changing marketing demands in the tourism sector, boosted by the advent of online booking, generating accurate forecast figures is nowadays a tough task - needing personnel with advance technical skills and expensive software. The aim of the Systematic Literature review is to provide an insight of the use of Deep Learning techniques for OR prediction. The latest trends in this field over five years (from 2017 to 2022) are highlighted. Through this SRL, three research questions are answered. The questions are related to the variables, deep learning algorithms for prediction and the evaluation metrics used for evaluating the models developed. The Snowballing methodology was used to carry out the SLR. 50 papers were selected for the final analysis. Five categories of variables were identified. LSTM was found to be the most popular deep learning algorithm used to build prediction models. Seven performance metrics were found and among them MAPE was the most popular. To conclude it was found that the hybrid model, CNN-LSTM, to increase accuracy and required more investigation. © 2023 The Authors},
	author = {Dowlut, N. and Gobin-Rahimbux, B.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Deep learning, Hospitality industry, LSTM, Occupacy rate},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{gerosa_systematic_2023-1,
	title = {A {Systematic} {Literature} {Review} on {Physical} and {Action} {Based} {Activities} in {Computing} {Education} for {Early} {Years} and {Primary}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175996894&doi=10.1145%2f3605468.3605500&partnerID=40&md5=8f2258fc74e302b7466acdc655af879e},
	abstract = {Educational systems worldwide are including computer science (CS) education in compulsory curriculums from a very young age. Many activities have been proposed to teach young children CS which include different approaches such as unplugged, physical computing, or completely virtual programming interfaces. Despite this, more research is needed to understand which pedagogical approaches capitalise on young children's cognitive and affective capacities throughout their development to promote learning outcomes. Grounded cognition (GC) proposes that our perception and thought are highly influenced by our bodily experiences and that dynamic actions such as movement affect our understanding of the world around us. For young children, experiences integrating cognitive and sensory-motor aspects are often used. These activities could be conceptualised as grounded activities, as they incorporate concrete representation and action. However, the extent to which these activities impact children's learning outcomes has, to our knowledge, not been explored thus far. Moreover, the theoretical background informing these activities and how these map onto the grounded cognition background is often an under-reported aspect in the literature. This study aims to bridge this gap by conducting a systematic literature review. We identified empirical research reporting CS learning activities with a grounded cognition approach and analysed its activities, CS concepts targeted, how their theoretical background informed their pedagogical design and their outcomes. This paper has important implications for computer science education. Firstly, it presents the empirical evidence using this theoretical background with an emphasis on activity design, which will be useful for academics or practitioners looking to incorporate grounded cognition theory into their instruction. Secondly, it identifies significant gaps in the current practices, specifically in the links between theory and practice and thus is a stepping stone for further research in this interdisciplinary area.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Gerosa, A. and Kallia, M. and Cutts, Q.},
	year = {2023},
	keywords = {Systematic literature review, Engineering education, Teaching strategy, Computer Science Education, Education computing, Computation theory, Computing, Computing education, Early childhood educations, Educational systems, Learning outcome, Primary, Young children, computing, early childhood education, primary, teaching strategies},
	annote = {Export Date: 23 June 2024},
}

@article{mei_deriving_2023-1,
	title = {Deriving {Thresholds} of {Object}-{Oriented} {Metrics} to {Predict} {Defect}-{Proneness} of {Classes}: {A} {Large}-{Scale} {Meta}-{Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153954593&doi=10.1142%2fS0218194023500110&partnerID=40&md5=61e07db7b45db195ab4a1982d2cc5279},
	abstract = {Many studies have explored the methods of deriving thresholds of object-oriented (i.e. OO) metrics. Unsupervised methods are mainly based on the distributions of metric values, while supervised methods principally rest on the relationships between metric values and defect-proneness of classes. The objective of this study is to empirically examine whether there are effective threshold values of OO metrics by analyzing existing threshold derivation methods with a large-scale meta-analysis. Based on five representative threshold derivation methods (i.e. VARL, ROC, BPP, MFM, and MGM) and 3268 releases from 65 Java projects, we first employ statistical meta-analysis and sensitivity analysis techniques to derive thresholds for 62 OO metrics on the training data. Then, we investigate the predictive performance of five candidate thresholds for each metric on the validation data to explore which of these candidate thresholds can be served as the threshold. Finally, we evaluate their predictive performance on the test data. The experimental results show that 26 of 62 metrics have the threshold effect and the derived thresholds by meta-analysis achieve promising results of GM values and significantly outperform almost all five representative (baseline) thresholds. \#c World Scientific Publishing Company.},
	author = {Mei, Y. and Rong, Y. and Liu, S. and Guo, Z. and Yang, Y. and Lu, H. and Tang, Y. and Zhou, Y.},
	year = {2023},
	note = {Publisher: World Scientific},
	keywords = {meta-analysis, Defects, Class A, Defect proneness, Large-scales, Meta-analysis, Metric values, Object oriented, Object oriented metrics, OO metrics, Predictive performance, Sensitivity analysis, Threshold, defect-proneness, Object-oriented metric, threshold},
	annote = {Export Date: 23 June 2024},
}

@article{barcellos_flatsat_2023-1,
	title = {{FlatSat} {Platforms} for {Small} {Satellites}: {A} {Systematic} {Mapping} and {Classification}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177718452&doi=10.1109%2fJMASS.2023.3249044&partnerID=40&md5=21cad600af368cb7aa793fc91cd172cd},
	abstract = {Recent trends indicate an increase in the number of small satellite missions, which can be developed more quickly and at a lower cost than traditional satellites. This has led to a growing interest in university-based satellite development, despite a lack of expertise in the space field, which has resulted in a high failure rate for such missions. To address this issue, the implementation of robust and reliable verification and validation (V\&V) methods has become essential, and it has been demonstrated that the use of a FlatSat during the V\&V campaign increases reliability. Despite the significance of FlatSat, there is a dearth of information on the platforms used to implement it, as well as a classification scheme for locating them. This article contributes to bridging this gap by conducting a systematic mapping of 65 works that were selected based on specific criteria and subsequently analyzed. The primary characteristics of the platforms are enumerated, and a new classification for FlatSat platforms into Raw, Bridge, Dock, and Modular is proposed. In order to provide a comprehensive understanding of the topic, the principal tests conducted on these platforms were also covered.  © 2019 IEEE.},
	author = {Barcellos, J.C.E. and Spengler, A.W. and Seman, L.O. and Silva, R.D.C.E. and Roldan, H.P. and Bezerra, E.A.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Embedded systems, Mapping, Systematic mapping, Classification (of information), Embedded-system, Failure analysis, Cubesats, Flatsats, Low-costs, Recent trends, Small satellite mission, Small satellites, Small-satellite, Verification and validation, Verification-and-validation, CubeSats, embedded systems, FlatSats, small satellites, verification and validation (V\&V)},
	annote = {Export Date: 23 June 2024},
}

@article{boaye_belle_evidence-based_2023-1,
	title = {Evidence-based decision-making: {On} the use of systematicity cases to check the compliance of reviews with reporting guidelines such as {PRISMA} 2020},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257221&doi=10.1016%2fj.eswa.2023.119569&partnerID=40&md5=826373e77ccc2d151c6b2bc7bfbe1bb6},
	abstract = {Background and context: Systematic reviews aim to provide high-quality evidence-based syntheses for efficacy under real-world conditions and allow understanding the correlations between exposures and outcomes. They are increasingly popular and have several stakeholders (e.g., healthcare providers, researchers, educators, students, journal editors, policy makers, managers) to whom they help make informed recommendations for practice or policy. Problem: Systematic reviews usually exhibit low methodological and reporting quality. To tackle this, reporting guidelines have been developed to support systematic reviews reporting and assessment. Following such guidelines is crucial to ensure that a review is transparent, complete, trustworthy, reproducible, and unbiased. However, systematic reviewers usually fail to adhere to existing reporting guidelines, which may significantly decrease the quality of the reviews they report and may result in systematic reviews that lack methodological rigor, yield low-credible findings and may mislead decision-makers. Methods: To assure that a review complies with reporting guidelines, we rely on assurance cases that are an emerging way of arguing and relaying various safety–critical systems’ requirements in an extensive manner, as well as checking the compliance of such systems with standards to support their certification. Since the nature of assurance cases makes them applicable to various domains and requirements/properties, we therefore propose a new type of assurance cases called systematicity cases. Systematicity cases focus on the systematicity property and allow arguing that a review is systematic i.e., that it sufficiently complies with the targeted reporting guideline. The most widespread reporting guidelines include PRISMA (Preferred Reporting Items for Systematic reviews and meta-Analyses). We measure the confidence in a systematicity case representing a review as a means to quantify the systematicity of that review i.e., the extent to which that review is systematic. We rely on rule-based Artificial Intelligence to create a knowledge-based system that automatically supports the inference mechanism that a given systematicity case embodies and that allows making a decision regarding the systematicity of a given review. Results: An empirical evaluation performed on 25 reviews (self-identifying as systematic) showed that these reviews exhibit a suboptimal systematicity. More specifically, the systematicity of the analyzed reviews varies between 32.96\% and 66.49\% and its average is 54.42\%. More efforts are therefore needed to report systematic reviews of higher quality. More experiments are also needed to further explore the factors hindering and/or assuring the systematicity of reviews. Audience: The main beneficiaries of our work are journal reviewers, journal editors, managers, policymakers, researchers, organizations developing reporting guidelines, peer reviewers, students, insurers, evidence users, as well as reporting guidelines developers. © 2023 The Author(s)},
	author = {Boaye Belle, A. and Zhao, Y.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Artificial intelligence, Regulatory compliance, Systematic Review, Decision making, Compliance control, Knowledge-based systems, Meta-analysis, Assurance case, Assurance case (systematicity case), Guideline adherence, Knowledge representation, Knowledge representation and reasoning, Preferred reporting item for systematic review and meta-analyze statement, Reporting guideline adherence, Systematicity, Assurance cases (systematicity cases), PRISMA (Preferred Reporting Items for Systematic reviews and meta-Analyses) statement},
	annote = {Export Date: 23 June 2024},
}

@article{bermejo_arvr_2023-1,
	title = {{AR}/{VR} {Teaching}-{Learning} {Experiences} in {Higher} {Education} {Institutions} ({HEI}): {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163713996&doi=10.3390%2finformatics10020045&partnerID=40&md5=32e5bb794c27f1d7f8eb7f209e9158b4},
	abstract = {During the last few years, learning techniques have changed, both in basic education and in higher education. This change has been accompanied by new technologies such as Augmented Reality (AR) and Virtual Reality (AR). The combination of these technologies in education has allowed a greater immersion, positively affecting the learning and teaching processes. In addition, since the COVID-19 pandemic, this trend has been growing due to the diversity of the different fields of application of these technologies, such as heterogeneity in their combination and their different experiences. It is necessary to review the state of the art to determine the effectiveness of the application of these technologies in the field of university higher education. In the present paper, this aim is achieved by performing a systematic literature review from 2012 to 2022. A total of 129 papers were analyzed. Studies in our review concluded that the application of AR/VR improves learning immersion, especially in hospitality, medicine, and science studies. However, there are also negative effects of using these technologies, such as visual exhaustion and mental fatigue. © 2023 by the authors.},
	author = {Bermejo, B. and Juiz, C. and Cortes, D. and Oskam, J. and Moilanen, T. and Loijas, J. and Govender, P. and Hussey, J. and Schmidt, A.L. and Burbach, R. and King, D. and O’Connor, C. and Dunlea, D.},
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {systematic literature review, higher education, AR/VR, teaching-learning process},
	annote = {Export Date: 23 June 2024},
}

@article{fernandes_digital_2023-1,
	title = {Digital {Alternative} {Communication} for {Individuals} with {Amyotrophic} {Lateral} {Sclerosis}: {What} {We} {Have}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169140808&doi=10.3390%2fjcm12165235&partnerID=40&md5=e23f9bba2b47e8dc0e6ea6345f35fddd},
	abstract = {Amyotrophic Lateral Sclerosis is a disease that compromises the motor system and the functional abilities of the person in an irreversible way, causing the progressive loss of the ability to communicate. Tools based on Augmentative and Alternative Communication are essential for promoting autonomy and improving communication, life quality, and survival. This Systematic Literature Review aimed to provide evidence on eye-image-based Human–Computer Interaction approaches for the Augmentative and Alternative Communication of people with Amyotrophic Lateral Sclerosis. The Systematic Literature Review was conducted and guided following a protocol consisting of search questions, inclusion and exclusion criteria, and quality assessment, to select primary studies published between 2010 and 2021 in six repositories: Science Direct, Web of Science, Springer, IEEE Xplore, ACM Digital Library, and PubMed. After the screening, 25 primary studies were evaluated. These studies showcased four low-cost, non-invasive Human–Computer Interaction strategies employed for Augmentative and Alternative Communication in people with Amyotrophic Lateral Sclerosis. The strategies included Eye-Gaze, which featured in 36\% of the studies; Eye-Blink and Eye-Tracking, each accounting for 28\% of the approaches; and the Hybrid strategy, employed in 8\% of the studies. For these approaches, several computational techniques were identified. For a better understanding, a workflow containing the development phases and the respective methods used by each strategy was generated. The results indicate the possibility and feasibility of developing Human–Computer Interaction resources based on eye images for Augmentative and Alternative Communication in a control group. The absence of experimental testing in people with Amyotrophic Lateral Sclerosis reiterates the challenges related to the scalability, efficiency, and usability of these technologies for people with the disease. Although challenges still exist, the findings represent important advances in the fields of health sciences and technology, promoting a promising future with possibilities for better life quality. © 2023 by the authors.},
	author = {Fernandes, F. and Barbalho, I. and Bispo Júnior, A. and Alves, L. and Nagem, D. and Lins, H. and Arrais Júnior, E. and Coutinho, K.D. and Morais, A.H.F. and Santos, J.P.Q. and Machado, G.M. and Henriques, J. and Teixeira, C. and Dourado Júnior, M.E.T. and Lindquist, A.R.R. and Valentim, R.A.M.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {human, systematic review, amyotrophic lateral sclerosis, augmentative and alternative communication, clinical article, eye tracking, eyelid reflex, human computer interaction, Review, machine learning, chronic neurological conditions, communication assistance, computer vision, image processing, neurodegenerative diseases},
	annote = {Export Date: 23 June 2024},
}

@article{guimaraes_responsible_2023-1,
	title = {Responsible innovation assessment tools: a systematic review and research agenda},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171182021&doi=10.1108%2fTECHS-11-2022-0043&partnerID=40&md5=85afe10c7e7cdc6cf361c0cf9ba06032},
	abstract = {Purpose: Responsible innovation assessment tools (RIATs) are key instruments that can help organizations, associations and individuals measure responsible innovation. Accordingly, this study aims to review the current status of research on responsible innovation and, in particular, of studies that either present the relevance of RIATs or provide empirical evidence of their adoption. Design/methodology/approach: A systematic literature review is conducted to identify and review how RIATs are being addressed in academic research and the applications that are proposed. A systematic process is implemented using the Web of Science and Scopus bibliographic databases, aiming not only to summarize existing studies, but also to include a perspective on gaps and future research. Findings: A total of 119 publications were identified and included in the review process. The study identifies that RIATs have attracted growing interest from the scientific community, with a greater predominance of studies involving qualitative and mixed methods. A well-balanced mix of conceptual and exploratory studies is also registered, with a greater predominance of analysis of RIATs application domains in the past years, with greater incidence in the finance, water, energy, construction, manufacturing and health sectors. Originality/value: This study is pioneering in identifying 16 dimensions and 60 sub-dimensions for measuring responsible innovation. It also suggests the need to include multidimensional perspectives and individuals with interdisciplinary competencies in this process. © 2022, Emerald Publishing Limited.},
	author = {Guimarães, C. and Amorim, V. and Almeida, F.},
	year = {2023},
	note = {Publisher: Emerald Publishing},
	keywords = {Systematic literature review, Metrics, Measurement, Responsible innovation, Sustainable innovation},
	annote = {Export Date: 23 June 2024},
}

@article{daun_context_2023-1,
	title = {Context modeling for cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128518651&doi=10.1002%2fsmr.2451&partnerID=40&md5=ae0f37165027c923e06274ec9f617fd9},
	abstract = {When developing cyber-physical systems (CPS), the context is of vital importance. CPS interact with the world not only through sensing the environment and acting upon it (like embedded systems) but also by communicating with other CPS (like systems in the Internet of Things [IoT]). This means that the context interactions CPS must deal with are much greater than regular embedded or IoT systems: On the one hand, external systems and human users constrain the specific interaction among them. On the other hand, properties of these external systems, human users, and laws, regulations, or standards constrain the way the CPS must be developed. In this paper, we propose a comprehensive, ontologically grounded context modeling framework to systematically explore the problem space in which a CPS under development will operate. This allows for the systematic elicitation of requirements for the CPS, early validation and verification of its properties, and safety assessment of its context interactions at runtime. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Daun, M. and Tenbergen, B.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Embedded systems, Requirement engineering, Requirements engineering, Verification, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Internet of things, Collaborative system network, Collaborative systems, Context, Context analysis, Context models, Dynamic contexts, Laws and legislation, Model dynamics, Model-based engineering, Systems networks, Validation, requirements engineering, cyber-physical systems, collaborative system networks, context, Context analyse, context modeling, dynamic context, context analysis, context modeling, dynamic context, model-based engineering, validation, verification},
	annote = {Export Date: 23 June 2024},
}

@article{saleemi_ubiquitous_2023-1,
	title = {Ubiquitous healthcare: a systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091477767&doi=10.1007%2fs12652-020-02513-x&partnerID=40&md5=3506b055c1c76d4dcdb81cf6a121df29},
	abstract = {Ubiquitous healthcare is an emerging area that employs ubiquitous technologies to enable technology oriented environment for healthcare professionals for provision of efficient and effective healthcare services. In past years, research community has proposed various technological solutions in different healthcare areas such as chronic disease monitoring, gait analysis, mood and fall detection, neuropathic monitoring, physiological and vital signs monitoring, pulmonogical monitoring, etc. However, in-depth analysis of these proposed solutions is required to analyze the form of proposed ubiquitous healthcare solutions; the extent ubiquitous technologies are integrated in these solutions; the type of real problem addressed; and how far these solutions are evaluated in real world settings? The addressal of these questions is critical to understand and evaluate the progress made in the area of ubiquitous healthcare and identify the challenges that are hindering the progress in this area. Therefore, in this research, a systematic research technique in the form of mapping study (also known as scoping study) is employed for in-depth analysis of evidences available on ubiquitous healthcare. The mapping study adopts a systematic approach to construct chain of evidences related to a particular topic and is a well-defined research technique in evidence based software engineering. This study identified a total of 103 primary studies, published between 2007 and 2018, for analysis of area under investigation. The study findings reveal that research trend in ubiquitous healthcare is horizontally spread by involving broad range of healthcare areas. The proposed solutions largely fall under the category of validation studies where experiments are conducted in laboratory settings rather real world environment. Another interesting finding is the lack of involvement of relevant healthcare community in proposed solution design. The challenges such as context awareness, data ownership, privacy and security, usability and trust are limiting the adoption of proposed solutions. Therefore, more extensive studies are required to first evaluate the applicability of proposed solutions in their respective environment, second, engagement and ownership of relevant community in solution design need to be considered. Third, the broad coverage of healthcare areas does not provide significant clusters of similar research in any particular area therefore future research should focus on strengthening these areas by conducting evaluation based longitudinal studies. In this way, the effects of proposed solutions can only be measured objectively and can be added to the body of knowledge. Finally, this research provides a thorough insight into the research on ubiquitous healthcare and offers an opportunity to conduct further research in this area. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
	author = {Saleemi, M. and Anjum, M. and Rehman, M.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Software engineering, Systematic literature review, Mapping, Mapping studies, Ambient assisted living, Assisted living, Bodyarea networks (BAN), Digital health, E-services, Ehealth, Emerging healthcare technology, Fall detection, Healthcare technology, Medical computing, mHealth, Patient monitoring, Pattern recognition, Telecare, Telehealth, Telemedicine, Telemedicine and wellness, Ubiquitous health care, Mapping study, Body area networks, EHealth, Emerging healthcare technologies, EServices, MHealth, Mobile health, Ubiquitous healthcare},
	annote = {Export Date: 23 June 2024},
}

@article{alfayez_what_2023-1,
	title = {What is asked about technical debt ({TD}) on {Stack} {Exchange} question-and-answer ({Q}\&{A}) websites? {An} observational study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146921032&doi=10.1007%2fs10664-022-10269-5&partnerID=40&md5=148ce9655aa6c1affadbd9c4f0406f3b},
	abstract = {Technical debt (TD) is a term coined by agile software pioneer Ward Cunningham to account for the added software system effort or cost resulting from taking early software project shortcuts. Previous research on TD has extensively outlined and discussed the various consequences derived from accumulating TD and the difficulty in managing it. A review of the software engineering literature revealed that Stack Exchange question-and-answer (Q\&A) websites can provide valuable, real world perspectives on a number of software engineering topics. Therefore, this study aims to observe how the TD term is utilized on Stack Exchange Q\&A websites. Specifically, this study utilizes a dataset derived from three Stack Exchange Q\&A websites, which are Stack Overflow (SO), Software Engineering (SE), and Project Management (PM), to retrieve and analyze 578 TD-related questions. The results unveiled that TD-related questions can be categorized into 14 different categories, a total of 636 unique tags are utilized in the acquired set of TD-related questions, and a few TD-related categories both lack accepted answers and have a longer median time to receive an accepted answer than other categories. This study’s findings highlight the TD-related challenges that are addressed by Stack Exchange Q\&A website users, which may prove beneficial in steering future TD-related efforts. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	author = {Alfayez, R. and Ding, Y. and Winn, R. and Alfayez, G. and Harman, C. and Boehm, B.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Software engineering, Computer software, Software-systems, Technical debts, Project management, Technical debt management, Software project, Real-world, Agile softwares, Observational study, Stackoverflow, Websites, Technical debt, Question-and-answer  website, Question-and-answer (Q\&A) websites},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{koana_ownership_2023-1,
	title = {Ownership in the {Hands} of {Accountability} at {Brightsquid}: {A} {Case} {Study} and a {Developer} {Survey}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180546160&doi=10.1145%2f3611643.3613890&partnerID=40&md5=ee4f1bde53f308b0d3a0a4497d826c01},
	abstract = {The COVID-19 pandemic has accelerated the adoption of digital health solutions. This has presented significant challenges for software development teams to swiftly adjust to the market needs and demand. To address these challenges, product management teams have had to adapt their approach to software development, reshaping their processes to meet the demands of the pandemic. Brighsquid implemented a new task assignment process aimed at enhancing developer accountability toward the customer. To assess the impact of this change on code ownership, we conducted a code change analysis. Additionally, we surveyed 67 developers to investigate the relationship between accountability and ownership more broadly. The findings of our case study indicate that the revised assignment model not only increased the perceived sense of accountability within the production team but also improved code resilience against ownership changes. Moreover, the survey results revealed that a majority of the participating developers (67.5\%) associated perceived accountability with artifact ownership. © 2023 ACM.},
	publisher = {Association for Computing Machinery, Inc},
	author = {Koana, U.A. and Chew, F. and Carlson, C. and Nayebi, M.},
	year = {2023},
	keywords = {Product management, Software design, COVID-19, Human resource management, Case-studies, Software development teams, Computer software selection and evaluation, Software Quality, Market needs, Accountability, Management team, Market demand, Ownership, Tasks assignments, Software Engineering},
	annote = {Export Date: 23 June 2024},
}

@article{thajeel_machine_2023-1,
	title = {Machine and {Deep} {Learning}-based {XSS} {Detection} {Approaches}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163933483&doi=10.1016%2fj.jksuci.2023.101628&partnerID=40&md5=7e7780a4abc9d6b6e38e915244463792},
	abstract = {Web applications are paramount tools for facilitating services providing in the modern world. Unfortunately, the tremendous growth in the web application usage has resulted in a rise in cyberattacks. Cross-site scripting (XSS) is one of the most frequent cyber security attack vectors that threaten the end user as well as the service provider with the same degree of severity. Recently, an obvious increase of the Machine learning and deep learning ML/DL techniques adoption in XSS attack detection. The goal of this review is to come with a special attention and highlight of Machine learning and deep learning approaches. Thus, in this paper, we present a review of recent advances applied in ML/DL for XSS attack detection and classification. The existing proposed ML/DL approaches for XSS attack detection are analyzed and taxonomized comprehensively in terms of domain areas, data preprocessing, feature extraction, feature selection, dimensionality reduction, Data imbalance, performance metrics, datasets, and data types. Our analysis reveals that the way of how the XSS data is preprocessed considerably impacts the performance and the attack detection models. Proposing a full preprocessing cycle reveals how various ML/DL approaches for XSS attacks detection take advantage of different input data preprocessing techniques. The most used ML/DL and preprocessing stages have also been identified. The limitations of existing ML/DL-based XSS attack detection mechanisms are highlighted to identify the potential gaps and future trends. © 2023 The Author(s)},
	author = {Thajeel, I.K. and Samsudin, K. and Hashim, S.J. and Hashim, F.},
	year = {2023},
	note = {Publisher: King Saud bin Abdulaziz University},
	keywords = {Deep learning, Machine learning, Cybersecurity, Cross-site scripting (XSS) attacks, Web application security},
	annote = {Export Date: 23 June 2024},
}

@article{ferreira_towards_2023-1,
	title = {Towards an understanding of reliability of software-intensive systems-of-systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172749&doi=10.1016%2fj.infsof.2023.107186&partnerID=40&md5=0d674782eca671dd406f7e52554660b5},
	abstract = {Context: Large-scale software-intensive Systems-of-Systems (SoS) have become present in several critical domains and have sometimes depended on diverse trending technologies, such as cloud computing and machine learning. At the same time, the SoS dynamic architecture makes it difficult to assure SoS reliability leading to diverse studies with specific solutions, while the need for a shared view of what precisely SoS reliability refers to still exists. Objective: The main contribution of this article is to go towards an understanding of SoS reliability. We present a conceptual model whose concepts as well as their definitions and relationships were defined by systematically examining the literature of the field. Methods: We surveyed 36 practitioners and researchers regarding ambiguity, explanatory power, parsimony, generality, and utility of our model. Next, we adjusted our model according to their contribution. Results: We reach a conceptual model containing 29 concepts and their relationships that help to comprehend SoS reliability. In addition, we provided a glossary with a definition of each concept of our conceptual model. We also proposed a SoS reliability definition grounded on the literature. Conclusions: By organizing the knowledge of SoS reliability, this conceptual model makes it possible to expand the body of knowledge in the area and opens several opportunities for further investigations; in particular, this model serves as a basis for novel solutions aiming to assure SoS reliability. © 2023 Elsevier B.V.},
	author = {Ferreira, F.H.C. and Nakagawa, E.Y. and Santos, R.P.D.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Machine-learning, Reliability, Empirical studies, Software reliability, Conceptual model, Large-scales, Cloud-computing, Critical domain, Distributed computer systems, Software intensive systems, System of systems, System reliability, System-of-systems, Empirical study, SoS},
	annote = {Export Date: 23 June 2024},
}

@article{khan_sql_2023-1,
	title = {{SQL} and {NoSQL} {Database} {Software} {Architecture} {Performance} {Analysis} and {Assessments}—{A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163681824&doi=10.3390%2fbdcc7020097&partnerID=40&md5=ca9df47cac308d21d9cec88cac5c7444},
	abstract = {The competent software architecture plays a crucial role in the difficult task of big data processing for SQL and NoSQL databases. SQL databases were created to organize data and allow for horizontal expansion. NoSQL databases, on the other hand, support horizontal scalability and can efficiently process large amounts of unstructured data. Organizational needs determine which paradigm is appropriate, yet selecting the best option is not always easy. Differences in database design are what set SQL and NoSQL databases apart. Each NoSQL database type also consistently employs a mixed-model approach. Therefore, it is challenging for cloud users to transfer their data among different cloud storage services (CSPs). There are several different paradigms being monitored by the various cloud platforms (IaaS, PaaS, SaaS, and DBaaS). The purpose of this SLR is to examine the articles that address cloud data portability and interoperability, as well as the software architectures of SQL and NoSQL databases. Numerous studies comparing the capabilities of SQL and NoSQL of databases, particularly Oracle RDBMS and NoSQL Document Database (MongoDB), in terms of scale, performance, availability, consistency, and sharding, were presented as part of the state of the art. Research indicates that NoSQL databases, with their specifically tailored structures, may be the best option for big data analytics, while SQL databases are best suited for online transaction processing (OLTP) purposes. © 2023 by the authors.},
	author = {Khan, W. and Kumar, T. and Zhang, C. and Raj, K. and Roy, A.M. and Luo, B.},
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {Systematic literature review, Data handling, Digital storage, Software architecture, BASE, Cloud analytics, Data Analytics, Database software, Database systems, DBaaS, Horizontal expansion, Map-reduce, MapReduce, Performance assessment, Performances analysis, SQL and NoSQL database, SQL database, ACID, aggregation, big data, SQL and NoSQL databases},
	annote = {Export Date: 23 June 2024},
}

@article{bovo_digital_2023-1,
	title = {Digital twins for the rapid startup of manufacturing processes: a case study in {PVC} tube extrusion},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164837924&doi=10.1007%2fs00170-023-11906-z&partnerID=40&md5=20ab08169248f2a738ac6318cfd26345},
	abstract = {In this work, a soft sensor–based digital twin (DT) was developed to reduce the startup time in manufacturing plastic tubes and enable real-time product quality monitoring, i.e., the weight per unit length and the inner and outer diameters of the tube. An experimental campaign was conducted on a real tube extrusion line using three polyvinyl chloride (PVC) compounds and different process conditions, and machine learning regression algorithms were trained and tested to create the models of the extruder and the extrusion die the DT is based on. The characterization of the considered material, whose properties were given as input to the digital models, was carried out according to a procedure based only on the data collected by the production line. The DT was tested for the startup of the production of a single-layer tube and allowed to achieve the specified customer requirements (thickness and weight) in a few minutes. The proposed solution thus proved to be a valuable tool for reducing the setup time, thus increasing the efficiency of the process. © 2023, The Author(s).},
	author = {Bovo, E. and Sorgato, M. and Lucchetta, G.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Machine learning, Case-studies, Manufacturing process, Chlorine compounds, Data-driven model, Digital manufacturing, Extrusion, Plastic tube, Polyvinyl chlorides, Rapid startups, Real- time, Soft sensors, Startup time, Tube extrusions, Tubes (components), Data-driven modeling, Digital twin, Tube extrusion},
	annote = {Export Date: 23 June 2024},
}

@article{machado_literature_2023-1,
	title = {Literature review of digital twin in healthcare},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170409582&doi=10.1016%2fj.heliyon.2023.e19390&partnerID=40&md5=3b0a9a7a5fd6da02279c410ec7d5ed26},
	abstract = {This article aims to make a bibliometric literature review using systematic scientific mapping and content analysis of digital twins in healthcare to know the evolution, domain, keywords, content type, and kind and purpose of digital twin's implementation in healthcare, so a consolidation and future improvement of existing knowledge can be made and gaps for new studies can be identified. The increase in publications of digital twins in healthcare is quite recent and it is still concentrated in the domain of technology sources. The subject is majorly concentrated in patient's digital twin group and in precision medicine and aspects, issues and/or policies subgroups, although the publications keywords mirror it only at the group side. Digital twins in healthcare are probably stepping out of the infancy phase. On the other hand, digital twins in hospital group and the device and facilities management subgroups are more mature with all knowledge gathered from the manufacturing sector. There is an absence of some publication's types in general, device and care subgroup and no whole body or hospital digital twin was reported. Based on the presented arguments, guidelines for future research were presented: advance in the creation of general frameworks, in subgroups not as much explored, and in groups and subgroups already explored, but that need more advancement to achieve the main goals of a whole human or hospital digital twin with the main issues resolved. © 2023 The Authors},
	author = {Machado, T.M. and Berssaneti, F.T.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Review, Bibliometric, Digital twin, Healthcare, Simulation},
	annote = {Export Date: 23 June 2024},
}

@article{oliveira_systematic_2023-1,
	title = {A systematic literature review on the impact of formatting elements on code legibility},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163388981&doi=10.1016%2fj.jss.2023.111728&partnerID=40&md5=1ac99e82c24196970128ea680e222e6b},
	abstract = {Context: Software programs can be written in different but functionally equivalent ways. Even though previous research has compared specific formatting elements to find out which alternatives affect code legibility, seeing the bigger picture of what makes code more or less legible is challenging. Goal: We aim to find which formatting elements have been investigated in empirical studies and which alternatives were found to be more legible for human subjects. Method: We conducted a systematic literature review and identified 15 papers containing human-centric studies that directly compared alternative formatting elements. We analyzed and organized these formatting elements using a card-sorting method. Results: We identified 13 formatting elements (e.g., indentation) and 33 levels of formatting elements (e.g., two-space indentation), which are about formatting styles, spacing, block delimiters, long or complex code lines, and word boundary styles. While some levels were found to be statistically better than other equivalent ones in terms of code legibility, e.g., appropriate use of indentation with blocks, others were not, e.g., formatting layout. For identifier style, we found divergent results, where one study found a significant difference in favor of camel case, while another study found a positive result in favor of snake case. Conclusion: The number of identified papers, some of which are outdated, and the many null and contradictory results emphasize the relative lack of work in this area and underline the importance of more research. There is much to be understood about how formatting elements influence code legibility before the creation of guidelines and automated aids to help developers make their code more legible. © 2023},
	author = {Oliveira, D. and Santos, R. and Madeiral, F. and Masuhara, H. and Castor, F.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Systematic literature review, Codes (symbols), Software project, Empirical studies, Card-sorting, Code legibility, Formatting element, Human subjects, Human-centric, Indentation, Program understandability, Understandability, Formatting elements},
	annote = {Export Date: 23 June 2024},
}

@article{broekhuizen_ai_2023-1,
	title = {{AI} for managing open innovation: {Opportunities}, challenges, and a research agenda},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166189921&doi=10.1016%2fj.jbusres.2023.114196&partnerID=40&md5=5f455a87a517d661bcaaa134c340eeaa},
	abstract = {Artificial intelligence (AI) provides ample opportunities for enabling effective knowledge sharing among organizations seeking to foster open innovation. Past research often investigates the capability of AI to perform ‘human’ tasks in structured application fields. Yet, there is a lack of research that systematically analyzes when and how AI can be used for the more complex and unstructured tasks of open innovation (OI). We present a framework for leveraging AI-enabled applications to foster productive OI collaborations. Specifically, we create a 3x3 matrix by aligning the three OI stages (initiation, development, realization) with the three management functions of AI (mapping, coordinating, controlling). This matrix assists in identifying how various AI applications may augment or automate human intelligence, thereby helping to resolve prevailing OI challenges. It provides guidance on how organizations can use AI to establish, execute and govern exchanges across the OI stages. Finally, we lay out an agenda for future research. © 2023 The Authors},
	author = {Broekhuizen, T. and Dekker, H. and de Faria, P. and Firk, S. and Nguyen, D.K. and Sofka, W.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Artificial intelligence, Collaborative innovation, Knowledge development, Open innovation},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{imamura_towards_2023-1,
	title = {Towards a {Catalog} of {Heuristics} for the {Design} of {Systems}-of-{Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165978127&doi=10.1145%2f3592813.3592897&partnerID=40&md5=1c68db0b4c5b8832e2973121156381a6},
	abstract = {Context: Systems-of-Systems (SoS) are arrangements of independent systems that are increasingly present in everyday life and can be observed in domains such as healthcare, transport, and Industry 4.0, to mention a few. Problem: A significant concern regarding SoS refers to the constituent systems’ (CS) independence. CS are managed by different organizations that control them independently of SoS. Hence, the design of SoS is challenging as it involves careful investigation, allocation, and integration of CS to ensure proper operation. Solution: This paper provides a catalog of good practices and recommendations, herein referred to as “heuristics”, which can be applied to the SoS design. The main purpose of the catalog is to provide directions on what practitioners should consider during the design phase to ensure the proper operation of the SoS. IS theory: This research is based on the General Systems Theory that allows understanding SoS as a complex system constructed with independent systems. Method: We conducted a systematic mapping study (SMS) to identify which heuristics have been applied to SoS design. The results were discussed in a focus group with professionals to organize the heuristics. Summary of Results: After reaching a consensus on the focus group, we organized a catalog of fifteen heuristics into five categories: initiation, CS, interoperability, emergent behavior, and monitoring. Contributions and Impact in the IS area: The heuristics catalog, which is grounded in the literature, would support researchers and professionals in identifying critical issues during the SoS design phase. © 2023 Copyright held by the owner/author(s).},
	publisher = {Association for Computing Machinery},
	author = {Imamura, M. and Ferreira, F. and Fernandes, J. and Neto, V.V.G. and dos Santos, R.P.},
	year = {2023},
	keywords = {Systematic mapping studies, Focus groups, System of systems, System-of-systems, Design phase, Electric utilities, Emergent behaviours, General systems theory, Good practices, Heuristic methods, Independent systems, Systems interoperability, Systems-of-systems design},
	annote = {Export Date: 23 June 2024},
}

@article{silva_energy_2023-1,
	title = {Energy awareness and energy efficiency in internet of things middleware: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144045635&doi=10.1007%2fs12243-022-00936-5&partnerID=40&md5=33938e423af15f297f51ad01617f424d},
	abstract = {The Internet of Things (IoT) is characterized by a myriad of physical deices, together with high heterogeneity in both software and hardware. Middleware platforms have been proposed in order to alleviate such heterogeneity, providing relevant services and easing application development. In IoT systems, energy consumption is a key concern due to the proliferation of devices and their limited battery capacity. IoT middleware platforms can play an important role in providing applications with strategies, and support, for energy awareness and energy efficiency. Although there is a significant existing body of work related to IoT middleware, there is, as yet, no complementary overview of the state of the art on how these platforms can contribute to energy efficiency and energy awareness in IoT systems. This paper provides such an overview in the form of a systematic literature review (SLR). The SLR was carried out by following a systematic, rigorous procedure to search, select, and analyze primary studies available in the literature. Our corpus, as presented in this paper, is made up of twenty-two such studies, each presenting strategies and solutions on middleware support for energy efficiency and energy awareness in IoT systems. These strategies mainly focus on network adaptation, task offloading, and concrete implementations. However, most of these studies do not consider energy-aware/efficiency abstractions, and focus on solutions working at the end-user application side. In conclusion, this paper also raises relevant challenges and potential directions for further research resulting from the main SLR findings. © 2022, Institut Mines-Télécom and Springer Nature Switzerland AG.},
	author = {Silva, P.V.B.C. and Taconet, C. and Chabridon, S. and Conan, D. and Cavalcante, E. and Batista, T.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Systematic literature review, State of the art, Internet of things, Application development, Battery capacity, Energy efficiency, Energy utilization, Energy-awareness, Green computing, High heterogeneity, Middleware, Middleware platforms, Power management, Software and hardwares, System energy consumption, Work-related, Internet of Things, Energy awareness},
	annote = {Export Date: 23 June 2024},
}

@article{abdulazeem_human_2023-1,
	title = {Human {Factors} {Considerations} for {Quantifiable} {Human} {States} in {Physical} {Human}–{Robot} {Interaction}: {A} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170345069&doi=10.3390%2fs23177381&partnerID=40&md5=25dd2b033ee1d3ee530c5a69317e90de},
	abstract = {As the global population rapidly ages with longer life expectancy and declining birth rates, the need for healthcare services and caregivers for older adults is increasing. Current research envisions addressing this shortage by introducing domestic service robots to assist with daily activities. The successful integration of robots as domestic service providers in our lives requires them to possess efficient manipulation capabilities, provide effective physical assistance, and have adaptive control frameworks that enable them to develop social understanding during human–robot interaction. In this context, human factors, especially quantifiable ones, represent a necessary component. The objective of this paper is to conduct an unbiased review encompassing the studies on human factors studied in research involving physical interactions and strong manipulation capabilities. We identified the prevalent human factors in physical human–robot interaction (pHRI), noted the factors typically addressed together, and determined the frequently utilized assessment approaches. Additionally, we gathered and categorized proposed quantification approaches based on the measurable data for each human factor. We also formed a map of the common contexts and applications addressed in pHRI for a comprehensive understanding and easier navigation of the field. We found out that most of the studies in direct pHRI (when there is direct physical contact) focus on social behaviors with belief being the most commonly addressed human factor type. Task collaboration is moderately investigated, while physical assistance is rarely studied. In contrast, indirect pHRI studies (when the physical contact is mediated via a third item) often involve industrial settings, with physical ergonomics being the most frequently investigated human factor. More research is needed on the human factors in direct and indirect physical assistance applications, including studies that combine physical social behaviors with physical assistance tasks. We also found that while the predominant approach in most studies involves the use of questionnaires as the main method of quantification, there is a recent trend that seeks to address the quantification approaches based on measurable data. © 2023 by the authors.},
	author = {Abdulazeem, N. and Hu, Y.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {human, Humans, Literature reviews, aged, Aged, Birth rates, Domestic services, ergonomics, Ergonomics, Global population, Healthcare services, Human robot interaction, industry, Industry, Life expectancies, Long life, Man machine systems, Manipulators, Physical humanrobot interaction (phri), Population statistics, Robot applications, robotics, Robotics, Robots manipulators, social behavior, Social behavior, Social Behavior, Social behaviour, human factors, physical human–robot interaction, robot manipulators},
	annote = {Export Date: 23 June 2024},
}

@article{ferreira_lessons_2023-1,
	title = {Lessons learned to improve the {UX} practices in agile projects involving data science and process automation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145568139&doi=10.1016%2fj.infsof.2022.107106&partnerID=40&md5=e1f34b13a26ed1d8c5db7eb024963f24},
	abstract = {Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams’ perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users’ routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions. © 2022 Elsevier B.V.},
	author = {Ferreira, B. and Marques, S. and Kalinowski, M. and Lopes, H. and Barbosa, S.D.J.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {End-users, Automation, Users' experiences, Agile, Agile Methodologies, Agile teams, Data Science, Decision power, Lean inception, Process automation, Process control, User centered design, User involvement, User participation, User experience, Data science, User-centered design},
	annote = {Export Date: 23 June 2024},
}

@article{kitchenham_segress_2023-1,
	title = {{SEGRESS}: {Software} {Engineering} {Guidelines} for {REporting} {Secondary} {Studies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132534569&doi=10.1109%2fTSE.2022.3174092&partnerID=40&md5=a3ded056d25379cd7fc5bad8fa40273c},
	abstract = {Context: Several tertiary studies have criticized the reporting of software engineering secondary studies. Objective: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). Method: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. Results: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, ad hoc recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. Conclusions: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs.  © 2022 IEEE.},
	author = {Kitchenham, B. and Madeyski, L. and Budgen, D.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Software engineering, Systematic Review, Mapping, Risk assessment, Evidence Based Software Engineering, Mapping studies, Systematic, Quality assessment, Guideline, Software, Mixed method, Mixed-method review, PRISMA 2020, Quality reviews, Reporting guideline, Risk of bias, Threat to validity, Evidence-based software engineering, mapping studies, mixed-methods reviews, quality assessment, quality reviews, reporting guidelines, risk of bias, systematic reviews, threats to validity},
	annote = {Export Date: 23 June 2024},
}

@article{dakkak_continuous_2023-1,
	title = {Continuous deployment in software-intensive system-of-systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150259304&doi=10.1016%2fj.infsof.2023.107200&partnerID=40&md5=fcf9069aa453b5d80bc44a98117f42ad},
	abstract = {Context: While continuous deployment is popular among web-based software development organizations, adopting continuous deployment in software-intensive system-of-systems is more challenging. On top of the challenges arising from deploying software to a single software-intensive embedded system, software-intensive system-of-systems (SiSoS) add a layer of complexity as new software undergoes an extensive field validation applied to individual components of the SiSoS, as well as the overall SiSoS, to ensure that both legacy and new functionalities are working as desired. Objectives: This paper aims to study how SiSoS transitions to continuous deployment by exploring how continuous deployment impacts field testing and validation activities, how continuous deployment can be practiced in SiSoS, and to identify the success factors that companies need to consider when transitioning to continuous deployment. Method: We conducted a case study at Ericsson AB focusing on the embedded software of the Third Generation Radio Access Network (3G RAN). The 3G RAN consists of two large-scale software-intensive embedded systems, representing a simple SiSoS composed of two systems. 3G RAN software was the first to transition to continuous deployment and is used as a reference case for other products within Ericsson AB. Results: Software deployment, in addition to field testing and validation, have transitioned from being a discrete activity performed at the end of software development to a continuous process performed in parallel to software development. Further, our study reveals an orchestrating approach for software deployment, which allows pre/post validation of legacy behavior and new features in a shorter release and deployment cadence. Furthermore, we identified the essential success factors that organizations should consider when transitioning to continuous deployment. Conclusion: Transition to continuous deployment, in addition to field testing and validation, shall be considered and planned carefully. In this paper, we provide a set of success factors and orchestration technique that helps organization when transitioning to continuous deployment in the software-intensive embedded system-of-systems context. © 2023 Elsevier B.V.},
	author = {Dakkak, A. and Bosch, J. and Olsson, H.H. and Issa Mattos, D.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Embedded systems, Software design, Software testing, Success factors, Agile software development, Legacy systems, Continuous software engineerings, Software intensive systems, System of systems, System-of-systems, Continuous deployment, Ericsson, Field testing, Field validation, Software-intensive system-of-system, Continuous software engineering, Software-intensive system-of-systems},
	annote = {Export Date: 23 June 2024},
}

@article{toledo_algorithmic_2023-1,
	title = {Algorithmic {Thinking} and {Extension} of {Its} {Definition} for {Trainee} {Software} {Developers}: {A} {Systematic} {Literature} {Mapping}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666630&doi=10.1109%2fRITA.2023.3323784&partnerID=40&md5=193d79d0896f7dc849cea2f0ec49c32d},
	abstract = {This paper exhibits a systematic literature mapping of the considerations required to develop algorithmic thinking in a first course in computer programming (CS1) in university academic programs in computing. In the methodological process of this study, 5 stages were proposed: research questions, search, selection, quality assessment and synthesis extraction. In this way, 5 guiding questions were drawn, 136 articles generated by the search stage were analyzed and the synthesis of 55 documents that met the criteria of this research was concluded, thus compiling the different practices used for the development of algorithmic thinking. In addition, as a result of the systematic literature mapping, a definition of Algorithmic Thinking oriented Software Engineering and didactics is proposed. © 2023 IEEE.},
	author = {Toledo, J.A.J. and Collazos, C.A. and Ortega, M. and Ramos, D.X.},
	year = {2023},
	note = {Publisher: Education Society of IEEE (Spanish Chapter)},
	keywords = {Software engineering, Mapping, Research questions, Quality assessment, Computation theory, Computing, Academic program, Algorithmic thinking, Computational logic, Computational thinkings, Logic programming, Software developer, computing, Algorithms, computational logic, computational thinking, computer programming},
	annote = {Export Date: 23 June 2024},
}

@article{borstler_double-counting_2023-1,
	title = {Double-counting in software engineering tertiary studies — {An} overlooked threat to validity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150795598&doi=10.1016%2fj.infsof.2023.107174&partnerID=40&md5=62bd250321bb30d74a7637c85f9bf590},
	abstract = {Context: Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in software engineering. Objective: We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue. Method: We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them. Results: In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies. Conclusions: Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies. © 2023 The Author(s)},
	author = {Börstler, J. and bin Ali, N. and Petersen, K.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software engineering, Tertiary study, Bias, Research method, Tertiary review, Guideline, Population statistics, Double counting, Empirical, Meta-review, Overview of review, Recommendation, Review of review, Umbrella review, Double-counting, Guidelines, Overview of reviews, Recommendations, Research methods, Review of reviews},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{rokani_fault_2023-1,
	title = {Fault diagnosis of induction motors using artificial intelligence techniques: {A} systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159667695&doi=10.1063%2f5.0129239&partnerID=40&md5=f5dc44315df865856408a2a1d79eebbd},
	abstract = {This manuscript proposes a systematic review of the latest publications, between 2010-2021, of research articles that investigate Induction Motor Fault Diagnosis (IMFD), using Artificial Intelligence techniques. Artificial Intelligence (AI) is an innovative branch of science and engineering. AI techniques constitute the most cutting-edge method in IMFD. Induction motors are regarded as more extensively used than other electric machines. Therefore, preserving their health is critical. It is vital to prevent incipient faults by monitoring the condition of the (IMs) and using diagnostic techniques. An incipient failure in an IM should be detected as early as possible to interrupt the evolution of the fault and reduce the financial losses and the repair period. This paper aspires to: a) Condense the existing surveys concerning the fault diagnosis in induction motors using AI techniques by searching the benefits and limitations of those surveys, b) Determine the gaps in existing research to recommend ideas for further investigation, c) Implement a background in this realm of AI for novel research projects. Moreover, it is followed a particular review protocol that defines the research questions and the methods applied to conduct the systematic review. © 2023 Author(s).},
	publisher = {American Institute of Physics Inc.},
	author = {Rokani, V. and Karaisas, P. and Kaminaris, S.D.},
	year = {2023},
	keywords = {Systematic Review, Artificial Intelligence, Condition Monitoring, Fault Detection, Fault Diagnosis, Motors},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{ockiya_review_2023-1,
	title = {A {Review} of {Human} {Factors} in {Remote} {Software} {Project} {Management}: {A} {Progressive} {Look} at {Human} {Based} {Issues} in {Remote} {Software} {Development} {Environments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184518421&doi=10.1145%2f3634848.3634858&partnerID=40&md5=ed5ffbcf2eb102cece0c656199f7585b},
	abstract = {There is a documented high rate of project failure within the software engineering community. Many researchers have discussed what the issues are and how to solve them using novel processes, technology, and tools, but the statistics remain mostly unchanged. We explored these issues from another perspective focusing on human based factors and how it affects remote software teams. Using a systematic literature review approach with selected criteria we explored the issues under several clusters. We found there exists a relationship between human based factors in remote software teams and success/failure, which, if better understood and managed could be used to reduce the challenges experienced by remote teams. We also identified a limitation in the availability of empirically tested data and suggest further research in understanding human based factors in remote teams as a precursor to reducing the rate of failure. © 2023 Owner/Author.},
	publisher = {Association for Computing Machinery},
	author = {Ockiya, T.F. and Lock, R.},
	year = {2023},
	keywords = {Software design, Project management, Human engineering, Failure analysis, Agile Methodologies, Engineering community, High rate, Novel process, Process Technologies, Project failures, Remote software development, Software project management, Software teams, Software-development environments, human factors, agile methodology},
	annote = {Export Date: 23 June 2024},
}

@article{liu_citizen_2023-1,
	title = {Citizen involvement in digital transformation: a systematic review and a framework},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165768993&doi=10.1108%2fOIR-04-2022-0237&partnerID=40&md5=11a1d93c18925a8a4b0e076a792744ef},
	abstract = {Purpose: The purpose of this paper is to improve the understanding of the factors influencing the success of digital transformation (DT) and problems/challenges in DT as well as the communication methods used to involve citizens, based on a systematic literature review of research articles about citizen involvement in DT published between January 2010 and May 2021. Design/methodology/approach: After establishing inclusion and exclusion criteria, a systematic review of relevant studies was conducted. Out of a total of 547 articles, 33 met the paper selection criteria. Findings: The analysis of the included 33 empirical studies reveals that the factors influencing the success of DT can be described as the opposite side from challenges and problems in DT. These factors and challenges/problems all influence DT and they can be grouped into organisational values, management capabilities, organisational infrastructure, and workforce capabilities. The communication methods for citizen involvement in DT include: (1) communication mediated by human, (2) communication mediated by computers, and (3) mixed communication methods. Originality/value: The study identified specific factors that influence DT supported by citizen involvement, at a more fine-grained level. The findings concerning communication methods extend related studies for citizen involvement by adding town hall meetings and communication methods mediated by computers. Furthermore, this study links the research findings to develop a framework for citizen involvement in DT, assisting in better selecting communication methods to involve citizens for addressing problem areas in DT. Peer review: The peer review history for this article is available at: https://publons.com/publon/10.1108/OIR-04-2022-0237 © 2022, Emerald Publishing Limited.},
	author = {Liu, C. and Zowghi, D.},
	year = {2023},
	note = {Publisher: Emerald Publishing},
	keywords = {Digital transformation, Systematic literature review, Systematic Review, Citizen involvement, Communication method, Design/methodology/approach, Inclusion and exclusions, Paper selections, Peer review, Selection criteria, Systematic review, Communication methods},
	annote = {Export Date: 23 June 2024},
}

@article{szabo_user-centered_2023-1,
	title = {User-centered approaches in software development processes: {Qualitative} research into the practice of {Hungarian} companies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137081694&doi=10.1002%2fsmr.2501&partnerID=40&md5=e408c37e01d89f1dc78aa027550e4269},
	abstract = {Integrating user-centered approaches into development processes is one of the main challenges nowadays that derives from different objectives of software engineering (SE) and human-computer interaction (HCI) fields. For SE experts, the main goal is quality code creation, whereas for HCI professionals, it is the continuous product interaction with the users. The major question is what tools and timings can be used together to achieve these goals effectively. Therefore, this article provides comparative, exploratory, and qualitative research about possible solutions on how practitioners transfer HCI values and practices to SE processes. The current practice of software companies was studied by conducting interviews on a sample of 13 Hungarian Information Technology companies to explore the SE processes in respect of several dimensions (applied development models, the integrity of user-centered methods, and the user experience [UX] maturity). According to preliminary expectations, the development processes of the various companies proceed in different steps; nevertheless, they can be well grouped together based on the UX methods applied. The results representing the various user-centered processes can be considered useful for future decision makers of software companies worldwide. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Szabó, B. and Hercegfi, K.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Development process, Decision making, Software design, Computer software, Human computer interaction, Users' experiences, Software process, Surveys, Interview, Hungarians, Qualitative research, User experience (UX), User-centered approach, UX maturity, UX method, interviews, qualitative research, software processes, user experience (UX), UX methods},
	annote = {Export Date: 23 June 2024},
}

@article{binamungu_behaviour_2023-1,
	title = {Behaviour driven development: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160605559&doi=10.1016%2fj.jss.2023.111749&partnerID=40&md5=0d6c623a12e99c3edeb2a8a0624478da},
	abstract = {Context: Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps. Objective: To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021. Method: By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research. Results: The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects. Conclusion: The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research. © 2023 Elsevier Inc.},
	author = {Binamungu, L.P. and Maro, S.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Specifications, Software design, Mapping, Systematic mapping studies, Semi-structured, Software requirements, Natural languages, 'current, Scientific literature, Petroleum reservoir evaluation, Behavior driven development, Boolean functions, Natural language specifications, Philosophical aspects, Systematic mapping study in software engineering, Systematic mapping study, Behaviour Driven Development, Systematic mapping studies in software engineering},
	annote = {Export Date: 23 June 2024},
}

@article{alanazi_software_2023-1,
	title = {Software {Engineering} {Techniques} for {Building} {Sustainable} {Cities} with {Electric} {Vehicles}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167892803&doi=10.3390%2fapp13158741&partnerID=40&md5=2974ae54e3ef1b855a7e8fd9235640e7},
	abstract = {As the process of urbanization continues to accelerate, the demand for sustainable cities has become more critical than ever before. The incorporation of electric vehicles (EVs) is a key component in creating sustainable cities. However, the development of smart cities for EVs entails more than just the installation of charging stations. Software engineering plays a crucial role in realizing smart cities for electric vehicles. This paper examines the role of software engineering in the creation of smart cities for electric vehicles, the techniques utilized in electric vehicle charging infrastructure, the obstacles faced by software engineers, and the future of software engineering in sustainable cities. Specifically, the paper explores the significance of software engineering in integrating EVs into the transportation system, including the design of smart charging and energy management systems, and the establishment of intelligent transportation systems. Additionally, the paper offers case studies to demonstrate successful software engineering implementations for smart cities. Finally, the paper concludes with a discussion of the challenges that software engineers encounter in implementing intelligent transportation systems for EVs and provides future directions for software engineering in sustainable cities. © 2023 by the authors.},
	author = {Alanazi, F. and Alenezi, M.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {software engineering, electric vehicles, intelligent transportation systems, smart cities},
	annote = {Export Date: 23 June 2024},
}

@article{vianna_grey_2023-1,
	title = {A {Grey} {Literature} {Review} on {Data} {Stream} {Processing} applications testing},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160024077&doi=10.1016%2fj.jss.2023.111744&partnerID=40&md5=279db1183abd50513795efc85821d5f3},
	abstract = {Context: The Data Stream Processing (DSP) approach focuses on real-time data processing by applying specific techniques for capturing and processing relevant data for on-the-fly results, i.e. without necessarily requiring prior storage. Like in any other software, testing plays a vital role in the quality assurance of DSP applications. However, testing such kind of software is not a simple task. In this context, some factors that make challenging testing are message temporality, parallelism, data volume, complex infrastructure, variability, and speed of messages. Objective: This work aims to map and synthesize industry knowledge and experience regarding DSP application testing. Specifically, we want to know about challenges, test purposes, test approaches, test data sources, and adopted tools. Method: To achieve the objective, we performed a Grey Literature Review (e.g., blog posts, white papers, discussion lists, lecture themes at technical events, professional social networks, software repositories, and other web-published) on testing DSP applications. We searched the grey literature using Google's regular search engine in addition to specific searches on technical software development content websites. The selected studies were analyzed using qualitative and quantitative techniques. Results: Results are based on evidence from 154 selected sources. The challenges for testing DSP applications are the complexity of DSP applications, test infrastructure complexity, timing, and data acquisition issues. The main test objectives identified are functional suitability, performance efficiency, reliability, and maintainability. The main test approaches reported: Performance Testing, Regression Testing, Property-Based Testing, Chaos Testing, and Contract/Schema Testing. The strategies adopted by practitioners to obtain test data: Historical Data, Production Data Mirroring, Semi-Synthetic Data, and Synthetic Data. We also report 50 tools used in various testing activities, which are used for: automating infrastructure, generating test data, test utilities, dealing with timing issues, load generation, simulation, and others. Furthermore, we identified gaps and opportunities for future scientific work. Conclusion: This work selected and summarized content produced by practitioners regarding DSP application testing. We identified that knowledge, techniques, and tools intrinsic to the practice were not present in the formal literature, so this study helps reduce the gap between industry and academia on this topic. The document has delivered benefits to industry practitioners and academic researchers. © 2023 Elsevier Inc.},
	author = {Vianna, A. and Kamei, F.K. and Gama, K. and Zimmerle, C. and Neto, J.A.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Software design, Software testing, Application programs, Social networking (online), Literature reviews, Search engines, Data handling, Software testings, Digital storage, Social sciences computing, Application testing, Complex networks, Data acquisition, Data stream, Data streams processing, Grey literature, Processing applications, Synthetic data, Test data, Testing data, Data streams},
	annote = {Export Date: 23 June 2024},
}

@article{ahmad_requirements_2023-3,
	title = {Requirements engineering for artificial intelligence systems: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149281892&doi=10.1016%2fj.infsof.2023.107176&partnerID=40&md5=cf81845e526ed8ccc8d8ddea0bf657f1},
	abstract = {Context: In traditional software systems, Requirements Engineering (RE) activities are well-established and researched. However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE. Existing literature has focused on using AI to manage RE activities, with limited research on RE for AI (RE4AI). Objective: This paper investigates current approaches for specifying requirements for AI systems, identifies available frameworks, methodologies, tools, and techniques used to model requirements, and finds existing challenges and limitations. Method: We performed a systematic mapping study to find papers on current RE4AI approaches. We identified 43 primary studies and analyzed the existing methodologies, models, tools, and techniques used to specify and model requirements in real-world scenarios. Results: We found several challenges and limitations of existing RE4AI practices. The findings highlighted that current RE applications were not adequately adaptable for building AI systems and emphasized the need to provide new techniques and tools to support RE4AI. Conclusion: Our results showed that most of the empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data requirements, and areas such as ethics, trust, and explainability need further research. © 2023 Elsevier B.V.},
	author = {Ahmad, K. and Abdelrazek, M. and Arora, C. and Bano, M. and Grundy, J.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software engineering, Artificial intelligence, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Software-systems, Engineering activities, Machine learning, Machine-learning, 'current, Engineering education, Artificial intelligence systems, Model requirements, System requirements, Tools and techniques, Systematic mapping study},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{fischer_becoming_2023-1,
	title = {Becoming a {Data}-{Driven} {Organization}: {A} {Comparative} {Case} {Study} on {Digital} {Transformation} {Strategies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192539148&partnerID=40&md5=1db34add5835badee7f963d868850262},
	abstract = {In today’s data-centric era, organizations increasingly aim to operate more data-driven and therefore engage in digital transformations toward becoming a data-driven organization (DDO). To govern such transformations, top managers develop digital transformation strategies (DTS) characterized by different organizational ambidexterity approaches. This study analyzes how such DTS influence the process and (intermediate) outcomes of organizations’ digital transformations toward becoming a DDO by studying two organizations undertaking such DDO transformations using the concept of organizational ambidexterity as a theoretical lens. On this empirical basis, we find that DTS characterized by different organizational ambidexterity approaches lead to different transformation processes and (intermediate) outcomes. Thereby, this study contributes to existing academic literature in the field of DDOs and DTS, as such transformation journeys toward becoming a DDO have not been studied in its entirety yet. Furthermore, our paper offers practical guidance for top managers to develop and implement a DTS suitable for their organization. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	publisher = {Association for Information Systems},
	author = {Fischer, H.},
	year = {2023},
	keywords = {Digital transformation, Metadata, Information systems, Information use, Case-studies, Data driven, Organisational, Data centric, Data-driven organization, Digital transformation strategy, Organizational ambidexterity, Top managers, Data-driven organization (DDO), digital transformation, digital transformation strategy (DTS), organizational ambidexterity, top managers},
	annote = {Export Date: 23 June 2024},
}

@article{khan_software_2023-1,
	title = {Software architecture for quantum computing systems — {A} systematic review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151881159&doi=10.1016%2fj.jss.2023.111682&partnerID=40&md5=949a3516e7bfda8a68429eeb94166d2d},
	abstract = {Quantum computing systems rely on the principles of quantum mechanics to perform a multitude of computationally challenging tasks more efficiently than their classical counterparts. The architecture of software-intensive systems can empower architects who can leverage architecture-centric processes, practices, description languages to model, develop, and evolve quantum computing software (quantum software for short) at higher abstraction levels. We conducted a Systematic Literature Review (SLR) to investigate (i) architectural process, (ii) modelling notations, (iii) architecture design patterns, (iv) tool support, and (iv) challenging factors for quantum software architecture. Results of the SLR indicate that quantum software represents a new genre of software-intensive systems; however, existing processes and notations can be tailored to derive the architecting activities and develop modelling languages for quantum software. Quantum bits (Qubits) mapped to Quantum gates (Qugates) can be represented as architectural components and connectors that implement quantum software. Tool-chains can incorporate reusable knowledge and human roles (e.g., quantum domain engineers, quantum code developers) to automate and customise the architectural process. Results of this SLR can facilitate researchers and practitioners to develop new hypotheses to be tested, derive reference architectures, and leverage architecture-centric principles and practices to engineer emerging and next generations of quantum software. © 2023 The Authors},
	author = {Khan, A.A. and Ahmad, A. and Waseem, M. and Liang, P. and Fahmideh, M. and Mikkonen, T. and Abrahamsson, P.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Systematic literature review, Systematic Review, Software testing, Software architecture, Modeling languages, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Software intensive systems, Architectural process, Architecture-centric, Classical counterpart, Qubits, Quantum computing},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{mayr_unified_2023-1,
	title = {Unified {Theory} of {Acceptance} and {Use} of {Technology} ({UTAUT}) for {Intelligent} {Process} {Automation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192543634&partnerID=40&md5=c1660d81fb09bfe882968ce8d157b22d},
	abstract = {Intelligent process automation is a technological innovation that combines symbolic automation tools with machine learning. Intelligent process automation can automate complex tasks that otherwise have to be performed by humans when symbolic automation is not powerful enough. Regardless of the high economic potential for companies, the adoption rate in practice is comparatively low. This could be due to the adoption behavior of the employees. In our work, we iteratively develop a Unified Theory of Acceptance and use of Technology (UTAUT) model for the adoption of intelligent process automation and evaluate it with an empirical study. With our research we want to empower designers to adapt the corresponding tools in the future to increase adoption. The study shows that, in addition to established factors for technology adoption, trust, transparency, and attitude towards technology are primary decision factors. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	publisher = {Association for Information Systems},
	author = {Mayr, A. and Stahmann, P. and Nebel, M. and Janiesch, C.},
	year = {2023},
	keywords = {Information systems, Information use, Technological innovation, Automation, Machine-learning, Technology adoption, Empirical studies, UTAUT, Process control, Adoption behavior, Automation tools, Complex task, Economic potentials, Intelligent process automation, The unified theory of acceptance and use of technology(UTAUT), Intelligent Process Automation, Technology Adoption},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{garcia_advances_2023-1,
	title = {Advances in {Web} {API} testing: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192841057&doi=10.1109%2fENC60556.2023.10508648&partnerID=40&md5=2ce84324710ed395808ffc09f45b2a83},
	abstract = {Web APIs serving as an intermediary for communication between distributed systems has increased recently. It has become critical to test these APIs to ensure their functionality and quality thoroughly. This study aims to systematically map the literature to analyze the techniques, methods, artifacts, and strategies employed during the testing phase of web APIs. Utilizing a systematic mapping study approach (SMS), we identified 42 studies that outlined various tests applicable to these APIs. Further, our analysis uncovered numerous methods, techniques, and strategies. Types of test artifacts such as the API specification, test cases, or test matrices were also found. Finally, testing activities were identified through approaches presented by each study's authors, in which test results were applied and analyzed. The findings will establish the basis for the development of a testing guide, which in turn will support professionals who need to test APIs and who lack specific knowledge on how to carry out this activity due to the lack of standards. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Garcia, J.C. and Hernández, J.O.O. and Arriaga, J.C.P. and Riaño, H.J.L.},
	year = {2023},
	keywords = {Systematic Review, Software testing, systematic review, Mapping, Systematic mapping studies, Software testings, Testing process, Test case, Web API, Quality assurance, API specifications, Distributed systems, Specification test, Testing phase, systematic mapping study, Web APIs, quality assurance, software testing, testing process},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{horne_ten_2023-1,
	title = {Ten regulatory principles to scaffold the design, manufacture, and use of trustworthy autonomous systems, illustrated in a maritime context},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167979891&doi=10.1145%2f3597512.3599701&partnerID=40&md5=696a9887e71dfc7d264d1bc2b56d5fc8},
	abstract = {Autonomous systems are increasingly prevalent around the world, with the benefits related to safety, efficiency, and sustainability attractive in addition to the opportunity to establish entirely new capabilities. In order to operationalise autonomous systems technology it is critical to have in place the legal, regulatory and ethical infrastructure necessary to enable safe and trusted operation. While there is a growing body of literature regarding ethical Artificial Intelligence (AI), there is a need for more academic exploration of legal and regulatory best practice for autonomous systems used in commercial and defence contexts. This paper addresses that literature gap by considering the role of regulation and its relationship with trust in a multi-disciplinary context, before proposing 10 principles to base regulatory development and implementation on. These principles, Trust-centred; Collaborative; Risk-based; Evidence-led; Facilitate experimentation; Systems-focussed; Usable; Consistent; Adaptable and Reviewable, collectively provide a domain and technology agnostic basis for a regulatory framework development and implementation approach that supports the design, manufacture and operation of safe and trusted autonomous systems. The paper concludes by recommending next steps towards the regulation of safe and trusted autonomous systems, including a focus on collaboration and experimentation.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Horne, R. and Law-Walsh, C. and Assaad, Z. and Joiner, K.},
	year = {2023},
	keywords = {Best practices, Laws and legislation, Autonomous system, Ethical technology, Law 3.0, Regulation, Regulation of autonomy, Regulatory frameworks, Regulatory principles, Risk-based, Scaffolds, Trust and regulation, Trustworthy autonomous system, autonomous systems, regulation, trust and regulation, trustworthy autonomous systems},
	annote = {Export Date: 23 June 2024},
}

@article{pereira_junior_systematic_2023-1,
	title = {Systematic {Literature} {Review} on {Virtual} {Electronics} {Laboratories} in {Education}: {Identifying} the {Need} for an {Aeronautical} {Radar} {Simulator}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163803143&doi=10.3390%2felectronics12122573&partnerID=40&md5=a2d0097e4a068fae9af69afceff35003},
	abstract = {The objective of this work is to propose the development of a virtual electronics laboratory with an aeronautical radar simulator using immersive technologies to help students learn. To verify whether this proposal was viable, the systematic literature review (SLR) methodology was used, whose objective was to verify whether immersive technologies were being used effectively in education and, also, what challenges, opportunities, and benefits they bring to Education 4.0. For this, eight Research Questions (RQs) were formulated to be answered by articles based on the highest SLR scores. The results presented by SLR were as follows: there was an increase in the use of immersive technologies in education, but virtual reality (VR) is still more used in education than AR, despite VR being more expensive than AR; the use of these new technologies brings new challenges, opportunities, and benefits for education; there was an increase in the quality of teaching for complex subjects; and there was an increase in students’ interest in the content presented. © 2023 by the authors.},
	author = {Pereira Júnior, E.L. and Moreira, M.Â.L. and Portella, A.G. and de Azevedo Junior, C.M. and de Araújo Costa, I.P. and Fávero, L.P. and Gomes, C.F.S. and dos Santos, M.},
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {augmented reality, Education 4.0, RADAR},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{nikiforova_identification_2023-1,
	title = {Identification of {High}-{Value} {Dataset} determinants: is there a silver bullet for efficient sustainability-oriented data-driven development?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167867852&doi=10.1145%2f3598469.3598556&partnerID=40&md5=617d9c410c006582371208a8f10e79d1},
	abstract = {Open Government Data (OGD) are seen as one of the trends that has the potential to benefit the economy, improve the quality, efficiency, and transparency of public administration, and change the lives of citizens, and the society as a whole facilitating efficient sustainability-oriented data-driven services. However, the quick achievement of these benefits is closely related to the "value"of the OGD, i.e., how useful, and reusable the data provided by public agencies are for creating value for the above stakeholder. This is where the notion of "high-value datasets"(HVD), defined by the European Commission in Open Data Directive, comes, referring to data that can create the most value for society, the economy, and the environment. This is even more so, considering the proliferation of Artificial Intelligence (AI) and machine learning (ML) applications in various domains. While there are some efforts in that direction, there is still no available framework for identifying country-specific high-value datasets (and their determinants). The objective of the workshop is to raise awareness and build a network of key stakeholders around the HVD issue, to allow each participant to think about how and whether the determination of HVD is taking place in their country, how this can be improved with the help of portal owners, data publishers, data owners, businesses and citizens, what are and can be determinants to be used for identifying HVDs, whether they are SMART. Our main motivation is that, as members of the dg.o community, we can collaboratively answer the above questions, and those raised during the previous two editions of this workshop at ICEGOV2022 and ICOD2022, forming an initial knowledge base, as well as assessing currently used indicators. In this 3rd edition of the workshop, previously obtained results, which make up a list of the most promising indicators, will be discussed, validated and possibly refined through live discussions with the workshop participants following the DELPHI method. © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Nikiforova, A. and Alexopoulos, C. and Rizun, N. and Ciesielska, M.},
	year = {2023},
	keywords = {Decision making, Economic and social effects, Data driven, High-value dataset, OGD, Open Data, Open datum, Open government data, Sustainable development, Knowledge based systems, Artificial intelligence learning, Determinant, European Commission, Machine learning applications, Public administration, Public agencies, Silver, open government data, determinant, high-value datasets, indicator, sustainability},
	annote = {Export Date: 23 June 2024},
}

@article{kitchenham_how_2023-1,
	title = {How {Should} {Software} {Engineering} {Secondary} {Studies} {Include} {Grey} {Material}?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128295733&doi=10.1109%2fTSE.2022.3165938&partnerID=40&md5=c83dc9fa989c40ab6aa2d2b6f0c71aa3},
	abstract = {Context: Recent papers have proposed the use of grey literature (GL) and multivocal reviews. These papers have raised issues about the practices used for systematic reviews (SRs) in software engineering (SE) and suggested that there should be changes to the current SR guidelines. Objective: To investigate whether current SR guidelines need to be changed to support GL and multivocal reviews. Method: We discuss the definitions of GL and the importance of GL and of industry-based field studies in SE SRs. We identify properties of SRs that constrain the material used in SRs: a) the nature of primary studies; b) the requirements of SRs to be auditable, traceable, and reproducible; and explain why these requirements restrict the use of blogs in SRs. Results: SR guidelines have always considered GL as a possible source of primary studies and have never supported exclusion of field studies that incorporate the practitioners' viewpoint. However, the concept of GL, which was meant to refer to documents that were not formally published, is now being extended to information from sources such as blogs/tweets/Q\&A posts. Thus, it might seem that SRs do not make full use of GL because they do not include such information. However, the unit of analysis for an SR is the primary study. Thus, it is not the source but the type of information that is important. Any report describing a rigorous empirical evaluation is a candidate primary study. Whether it is actually included in an SR depends on the SR eligibility criteria. However, any study that cannot be guaranteed to be publicly available in the long term should not be used as a primary study in an SR. This does not prevent such information from being aggregated in surveys of social media and used in the context of evidence-based software engineering (EBSE). Conclusions: Current guidelines for SRs do not require extensions, but their scope needs to be better defined. SE researchers require guidelines for analysing social media posts (e.g., blogs, tweets, vlogs), but these should be based on qualitative primary (not secondary) study guidelines. SE researchers can use mixed-methods SRs and/or the fourth step of EBSE to incorporate findings from social media surveys with those from SRs and to develop industry-relevant recommendations.  © 1976-2012 IEEE.},
	author = {Kitchenham, B. and Madeyski, L. and Budgen, D.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Software engineering, Systematic Review, Systematic mapping studies, Social networking (online), Evidence Based Software Engineering, Social sciences computing, Systematic, Guideline, Surveys, Mixed method, Mixed-method review, Grey literature, Blogs, Government, Multivocal review, Evidence-based software engineering, mixed-methods reviews, systematic reviews, grey literature, multivocal reviews, systematic mapping studies},
	annote = {Export Date: 23 June 2024},
}

@article{da_silva_relationship_2023-1,
	title = {Relationship between ecosystem innovation and performance measurement models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134164589&doi=10.1108%2fIJPPM-06-2021-0349&partnerID=40&md5=ad93977f490e4d1e1c8aba9e3c9501c5},
	abstract = {Purpose: This study examines the relationship between the innovation ecosystem and performance measurement models. Although the innovation ecosystem and measurement models are widely recognized, the existing literature lacks a comprehensive understanding of the relationship between the proposed themes. Furthermore, it does not reveal how studies can be grouped to propose a thematic typology of the relationship. Design/methodology/approach: The authors present a systematic literature review conducted in the Web of Science and Scopus databases, from a textual corpus that aided the proposition of the typology that aims to provide answers regarding the addressed themes. Findings: The results of this review are based on a total of sixty peer-reviewed articles from the innovation ecosystem literature and performance measurement models between 1995 and 2020. The results make several contributions to the literature. First, by integrating evidence from empirical studies, the authors identified a typology formed by three classes: (1) ecosystem agents (2) analytical focus and (3) structured measurement tools. Second, the authors verified the relationship between the themes and discovered the existence of gaps to be filled, with the proposition of three drivers. Third, the authors presented a comprehensive mapping of field studies with a descriptive analysis of the textual corpus. Originality/value: The results of the research provide important implications for researchers, managers and policy makers. Furthermore, the authors suggest directions for future research, including the need to examine the performance of the entire innovation ecosystem, integrating the different agents that exist for performance measurement. © 2022, Emerald Publishing Limited.},
	author = {da Silva, D.J.C. and Lopes, L.F.D. and Santos Costa Vieira da Silva, L. and da Silva, W.V. and Teixeira, C.S. and Veiga, C.},
	year = {2023},
	note = {Publisher: Emerald Publishing},
	keywords = {Innovation ecosystems, Performance measurement, Performance measurement tools, Systematic literature review},
	annote = {Export Date: 23 June 2024},
}

@article{sworna_nlp_2023-1,
	title = {{NLP} methods in host-based intrusion detection systems: {A} systematic review and future directions},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174894190&doi=10.1016%2fj.jnca.2023.103761&partnerID=40&md5=006065bd017f3587ee6b0c34803dfda5},
	abstract = {Host-based Intrusion Detection System (HIDS) is an effective last line of defense for defending against cyber security attacks after perimeter defenses (e.g., Network-based Intrusion Detection System and Firewall) have failed or been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among the top two most used security tools by Security Operation Centers (SOC) of organizations. Although effective and efficient HIDS is highly desirable for industrial organizations, the evolution of increasingly complex attack patterns causes several challenges resulting in performance degradation of HIDS (e.g., high false alert rate creating alert fatigue for SOC staff). Since Natural Language Processing (NLP) methods are better suited for identifying complex attack patterns, an increasing number of HIDS are leveraging the advances in NLP that have shown effective and efficient performance in precisely detecting low footprint, zero-day attacks and predicting an attacker's next steps. This active research trend of using NLP in HIDS demands a synthesized and comprehensive body of knowledge of NLP-based HIDS. Despite the drastically growing adoption of NLP in HIDS development, there has been relatively little effort allocated to systematically analyze and synthesize the available peer review literature to understand how NLP is used in HIDS development. The lack of a synthesized and comprehensive body of knowledge on such an important topic motivated us to conduct a Systematic Literature Review (SLR) of the papers on the end-to-end pipeline of the use of NLP in HIDS development. For the end-to-end NLP-based HIDS development pipeline, we identify, taxonomically categorize and systematically compare the state-of-the-art of NLP methods usage in HIDS, attacks detected by these NLP methods, datasets and evaluation metrics which are used to evaluate the NLP-based HIDS. We highlight the relevant prevalent practices, considerations, advantages and limitations to support the HIDS developers. We also outline the future research directions for the NLP-based HIDS development. © 2023 The Authors},
	author = {Sworna, Z.T. and Mousavi, Z. and Babar, M.A.},
	year = {2023},
	note = {Publisher: Academic Press},
	keywords = {Natural language processing systems, Natural languages, Language processing, Natural language processing, System development, Complex networks, Anomaly detection, Computer crime, Cyber security, Cybersecurity, Host-based intrusion detection, Host-based intrusion detection system, Intrusion detection, Pipeline processing systems, Pipelines, Processing method, Security operation center, Zero-day attack},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{fortuna_surveying_2023-1,
	title = {Surveying the {Relevance} of the {Critical} {Success} {Factors} of {Agile} {Transformation} {Initiatives} from a {Project} {Management} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180157252&doi=10.1145%2f3629479.3629515&partnerID=40&md5=ed9d9b9181f0ac254ac6c07e2875a10a},
	abstract = {Background: Agile methods and practices have been consistently adopted in recent years as alternatives to traditional software development processes to address the ever-changing needs of IT organizations. In a previous systematic mapping study, we identified twelve critical success factors of agile transformations from a project management perspective. Objective: In this paper, we investigate how practitioners perceive the relevance of these factors and whether other factors should be considered. Method: We conducted a survey research involving project managers from several organizations undergoing agile transformations. Results: The participants' perceptions provided valuable insights into the relevance of the critical success factors. Additionally, we identified five new critical success factors: organizational ambidexterity, use of tools and automation, breaking down organizational silos, team commitment, and alignment of organizational goals and expectations. These newly identified factors contribute to a more comprehensive understanding of organizations' challenges during an agile transformation. Based on the results and the literature, we formulated three propositions representing recommendations that can foster agile transformation. Conclusions: The evidence gathered in this study indicates that the factors investigated previously are highly relevant. Moreover, organizations should consider them to enhance the chances of success of agile transformation initiatives.  © 2023 ACM.},
	publisher = {Association for Computing Machinery},
	author = {Fortuna, A. and Mattos, C.S. and Andrade, Á.J.D.C. and Ramos, L.F. and Dutra, E. and Santos, R.P.D. and Santos, G.},
	year = {2023},
	keywords = {Software design, Success factors, Agile software development, Organisational, Project management, Agile methods, Agile practices, Agile transformations, Changing needs, Critical success factor, IT organizations, Software development process, Agile Software Development, Agile Transformation, Critical Success Factors, Project Management},
	annote = {Export Date: 23 June 2024},
}

@article{erthal_characterization_2023-2,
	title = {Characterization of continuous experimentation in software engineering: {Expressions}, models, and strategies},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759787&doi=10.1016%2fj.scico.2023.102961&partnerID=40&md5=fe2272f51ad17194de939829974123e2},
	abstract = {Context: Continuous Experimentation (CE) has become increasingly popular across industry and academic communities. Major software organizations use CE to increase their revenue by adding value to end-users, and researchers are investigating the CE adoption process and usage to expand its success. Given this rapid evolution, observing a shared understanding of CE definitions, processes, and experiment strategies is difficult, potentially jeopardizing new implementations and focused research efforts. Objective: To characterize CE from the perspective of its definitions, processes, and strategies for experimentation available in the technical literature and to evolve the understanding perspectives for “continuous experimentation” and “data-driven development” definitions. Method: To select and analyze sources of information in the technical literature dealing with different aspects of continuous experimentation through a Literature Study using an ad hoc search improved with snowballing (backward and forward). Organize the findings into new perspectives for CE definitions, processes, and experiment strategies. Results: It was possible to identify many different definitions, processes, and experimental strategies used to describe CE in the 72 analyzed empirical papers, making it difficult to decide on their combination to be applied in a real software development project. Therefore, it has been proposed to evolve the CE understanding perspective, to categorize its experiment strategies, and to offer a combined development process for CE combining parts of other processes. Besides, conjectural requirements have been identified, which can contribute to better differentiating requirements and hypotheses in the CE context. Conclusion: Likely, a better understanding of CE is still missing. It can contribute towards organizing a common taxonomy to facilitate the possible choices for the experiment strategies. Therefore, there is space for more investigations on its applicability and value in different categories of software systems, despite all the advancements of CE and its promotion in developing modern software systems. © 2023 Elsevier B.V.},
	author = {Erthal, V.M. and de Souza, B.P. and dos Santos, P.S.M. and Travassos, G.H.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software design, Software testing, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Data driven, Data-driven development, A/b testing, Controlled experiment, Engineering expression, Expression modeling, Technical literature, Evidence-based software engineering, Controlled experiments},
	annote = {Export Date: 23 June 2024},
}

@article{santos_distributed_2023-1,
	title = {Distributed {Scrum}: {A} {Case} {Meta}-analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180152360&doi=10.1145%2f3626519&partnerID=40&md5=8d7c6e0cd850d803596d4150c118f884},
	abstract = {Distributed Scrum adapts the Scrum project management framework for geographically distributed software teams. Experimentally evaluating the effectiveness of Distributed Scrum is impractical, but many case studies and experience reports describe teams and projects that used Distributed Scrum. This article synthesizes the results of these cases using case meta-analysis, a technique for quantitatively analyzing qualitative case reports. On balance, the evidence suggests that Distributed Scrum has no impact, positive or negative, on overall project success. Consequently, claims by agile consultants who present Distributed Scrum as a recipe for project success should be treated with great caution, while researchers should investigate more varied perspectives to identify the real drivers of success in distributed and global software development.  © 2023 held by the owner/author(s).},
	author = {Santos, R.D.S. and Ralph, P. and Arshad, A. and Stol, K.-J.},
	year = {2023},
	note = {Publisher: Association for Computing Machinery},
	keywords = {Software design, Human resource management, Project management, Global software engineering, Scra, Meta-analysis, Software teams, Case meta-analyse, Distributed scrums, Distributed software, Distributed software development, Project management frameworks, Project success, Scrum, case meta-analysis, Distributed Scrum, distributed software development, global software engineering},
	annote = {Export Date: 23 June 2024},
}

@article{dobaj_towards_2023-1,
	title = {Towards {DevOps} for {Cyber}-{Physical} {Systems} ({CPSs}): {Resilient} {Self}-{Adaptive} {Software} for {Sustainable} {Human}-{Centric} {Smart} {CPS} {Facilitated} by {Digital} {Twins}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175031906&doi=10.3390%2fmachines11100973&partnerID=40&md5=5e0d1126be87b6fd21b7b77f667e9194},
	abstract = {The Industrial Revolution drives the digitization of society and industry, entailing Cyber-Physical Systems (CPSs) that form ecosystems where system owners and third parties share responsibilities within and across industry domains. Such ecosystems demand smart CPSs that continuously align their architecture and governance to the concerns of various stakeholders, including developers, operators, and users. In order to satisfy short- and long-term stakeholder concerns in a continuously evolving operational context, this work proposes self-adaptive software models that promote DevOps for smart CPS. Our architectural approach extends to the embedded system layer and utilizes embedded and interconnected Digital Twins to manage change effectively. Experiments conducted on industrial embedded control units demonstrate the approach’s effectiveness in achieving sub-millisecond real-time closed-loop control of CPS assets and the simultaneous high-fidelity twinning (i.e., monitoring) of asset states. In addition, the experiments show practical support for the adaptation and evolution of CPS through the dynamic reconfiguring and updating of real-time control services and communication links without downtime. The evaluation results conclude that, in particular, the embedded Digital Twins can enhance CPS smartness by providing service-oriented access to CPS data, monitoring, adaptation, and control capabilities. Furthermore, the embedded Digital Twins can facilitate the seamless integration of these capabilities into current and future industrial service ecosystems. At the same time, these capabilities contribute to implementing emerging industrial services such as remote asset monitoring, commissioning, and maintenance. © 2023 by the authors.},
	author = {Dobaj, J. and Riel, A. and Macher, G. and Egretzberger, M.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)},
	keywords = {DevOps, embedded systems, CPS, Digital Twin, distributed control system, IIoT, industrial product service system, industry 4.0, industry 5.0, self-adaptive systems},
	annote = {Export Date: 23 June 2024},
}

@incollection{bhat_engineering_2023-1,
	title = {Engineering {Challenges} in the {Development} of {Artificial} {Intelligence} and {Machine} {Learning} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179277659&doi=10.1201%2f9781032624983-7&partnerID=40&md5=8d4c838a9eb34b83f7f86e0841ed7615},
	abstract = {In this chapter, we first introduce artificial intelligence and machine learning (AI/ML) as state-of-the-art in engineering software and then outline the major differences between AI/ML and traditional software development. In particular, we categorize AI/ML engineering challenges in different phases. Eventually, different challenges are generalized and categorized. Finally, we observe that software testing, quality assurance, and management of the data are the most challenging issues that engineers/developers are currently facing. © 2024 Taylor \& Francis Group, LLC.},
	publisher = {CRC Press},
	author = {Bhat, M.I. and Yaqoob, S.I. and Imran, M.},
	year = {2023},
	annote = {Export Date: 23 June 2024},
}

@article{giray_use_2023-1,
	title = {On the use of deep learning in software defect prediction},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140965251&doi=10.1016%2fj.jss.2022.111537&partnerID=40&md5=abc6530954079e153d281e71264be476},
	abstract = {Context: Automated software defect prediction (SDP) methods are increasingly applied, often with the use of machine learning (ML) techniques. Yet, the existing ML-based approaches require manually extracted features, which are cumbersome, time consuming and hardly capture the semantic information reported in bug reporting tools. Deep learning (DL) techniques provide practitioners with the opportunities to automatically extract and learn from more complex and high-dimensional data. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of the utilization of DL algorithms for SDP in the literature. Method: We systematically selected a pool of 102 peer-reviewed studies and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: Main highlights include: (1) most studies applied supervised DL; (2) two third of the studies used metrics as an input to DL algorithms; (3) Convolutional Neural Network is the most frequently used DL algorithm. Conclusion: Based on our findings, we propose to (1) develop more comprehensive DL approaches that automatically capture the needed features; (2) use diverse software artifacts other than source code; (3) adopt data augmentation techniques to tackle the class imbalance problem; (4) publish replication packages. © 2022 The Authors},
	author = {Giray, G. and Bennin, K.E. and Köksal, Ö. and Babur, Ö. and Tekinerdogan, B.},
	year = {2023},
	note = {Publisher: Elsevier Inc.},
	keywords = {Systematic literature review, Deep learning, Machine-learning, Defects, Forecasting, Software defect prediction, Convolutional neural networks, Data mining, Machine learning techniques, Quality assurance, Bug reporting, Clustering algorithms, Defect prediction methods, Learning-based approach, Reporting tools, Semantics, Semantics Information},
	annote = {Export Date: 23 June 2024},
}

@article{fischer_data-driven_2023-1,
	title = {Data-{Driven} {Organizations}: {Review}, {Conceptual} {Framework}, and {Empirical} {Illustration}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180313770&doi=10.3127%2fAJIS.V27I0.4425&partnerID=40&md5=e02e898eb6866062fdfeaaa30c52d11d},
	abstract = {With companies and other organizations increasingly striving to become (more) data-driven, there has been growing research interest in the notion of a data-driven organization (DDO). In existing literature, however, different understandings of such an organization emerged. The study at hand sets forth to synthesize the fragmented body of research through a review of existing DDO definitions and implicit understandings of this concept in the information systems and related literatures. Based on the review results and drawing on the established concept of the “knowing organization,” our study identifies five core dimensions of a DDO—namely, data sourcing \& sensemaking, data capabilities, data-driven culture, data-driven decision-making, and data-driven value creation—which we integrate into a conceptual DDO framework. Most notably, the proposed framework suggests that—like its predecessor, the knowing organization—a DDO may draw on an outside-in view; however, it may also draw on an inside-out view, or even combine the two views, thereby setting itself apart from the knowing organization. To illustrate our conceptual DDO framework and demonstrate its usefulness, we apply this framework to three empirical examples. Theoretical and practical contributions as well as directions for future research are discussed. © (2023), (Australasian Association for Information Systems). All Rights Reserved.},
	author = {Fischer, H. and Wiener, M. and Strahringer, S. and Kotlarsky, J. and Bley, K.},
	year = {2023},
	note = {Publisher: Australasian Association for Information Systems},
	keywords = {Literature review, Data-driven organization (DDO), Conceptual DDO framework, DDO dimensions, DDO understandings, Empirical examples, Knowing organization},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{cordeiro_towards_2023-1,
	title = {Towards a {Framework} {Based} on {Open} {Science} {Practices} for {Promoting} {Reproducibility} of {Software} {Engineering} {Controlled} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166021485&partnerID=40&md5=145a60cab90f6f1a29901242e43e78d6},
	abstract = {Experimentation in Software Engineering has increased in the last decades as a way to provide evidence on theories and technologies. In a controlled experiment life cycle, several artifacts are used/reused and even produced. Such artifacts are mostly in the form of data, which should favor the reproducibility of such experiments. In this context, reproducibility can be defined as the ability to reproduce a study. Different benefits, such as methodology and data reuse, can be achieved from this ability. Despite the recognized benefits, several challenges have been faced by researchers regarding the experiments’ reproducibility capability. To overcome them, we understand that Open Science practices, related to provenance, preservation, and curation, might aid in improving such a capability. Therefore, in this paper, we present the proposal for an open science-based Framework to deal with controlled experiment research artifacts towards making such experiments de facto reproducible. To do so, different models associated with open science practices are planned to be integrated into the Framework. © 2023 CIbSE 2023 - XXVI Ibero-American Conference on Software Engineering. All rights reserved.},
	publisher = {Ibero-American Conference on Software Engineering},
	author = {Cordeiro, A.F.R.},
	year = {2023},
	keywords = {Software engineering, Life cycle, Open science, Controlled experiment, Curation, Data reuse, Experiment research, Reproducibilities, Research artefacts},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{ris_systemic_2023-1,
	title = {A {Systemic} {Mapping} of {Methods} and {Tools} for {Performance} {Analysis} of {Data} {Streaming} with {Containerized} {Microservices} {Architecture}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169796889&doi=10.23919%2fCISTI58278.2023.10211834&partnerID=40&md5=d6a237cf4005729c3547af7a0673b592},
	abstract = {With the Internet of Things (IoT) growth and customer expectations, the importance of data streaming and streaming processing has increased. Data Streaming refers to the concept where data is processed and transmitted continuously and in real-time without necessarily being stored in a physical location. Personal health monitors and home security systems are examples of data streaming sources. This paper presents a systematic mapping study of the performance analysis of Data Streaming systems in the context of Containerization and Microservices. The research aimed to identify the main methods, tools, and techniques used in the last five years for the execution of this type of study. The results show that there are still few performance evaluation studies for this system niche, and there are gaps that must be filled, such as the lack of analytical modeling and the disregard for communication protocols' influence.  © 2023 ITMA.},
	publisher = {IEEE Computer Society},
	author = {Ris, S. and Araujo, J. and Beserra, D.},
	year = {2023},
	keywords = {Mapping, Performance, Containers, Microservice, Internet of things, Performances analysis, Real- time, Analysis of data, Customer expectation, Data reduction, Data streaming, Data transfer, Physical locations, Realibility, Streaming processing, availability, container, data streaming, microservice, performance, realibility},
	annote = {Export Date: 23 June 2024},
}

@article{silva_digital_2023-1,
	title = {The {Digital} {Twin} {Paradigm} {Applied} to {Soil} {Quality} {Assessment}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146668292&doi=10.3390%2fs23021007&partnerID=40&md5=14555c99ca9482a8958de1c8b705d8ab},
	abstract = {This article presents the results regarding a systematic literature review procedure on digital twins applied to precision agriculture. In particular, research and development activities aimed at the use of digital twins, in the context of predictive control, with the purpose of improving soil quality. This study was carried out through an exhaustive search of scientific literature on five different databases. A total of 158 articles were extracted as a result of this search. After a first screening process, only 11 articles were considered to be aligned with the current topic. Subsequently, these articles were categorised to extract all relevant information, using the preferred reporting items for systematic reviews and meta-analyses methods. Based on the obtained results, there are two main conclusions to draw: First, when compared with industrial processes, there is only a very slight rising trend regarding the use of digital twins in agriculture. Second, within the time frame in which this work was carried out, it was not possible to find any published paper on the use of digital twins for soil quality improvement within a model predictive control context. © 2023 by the authors.},
	author = {Silva, L. and Rodríguez-Sedano, F. and Baptista, P. and Coelho, J.P.},
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {Systematic literature review, Systematic Review, systematic review, Scientific literature, Development activity, Model predictive control, Precision agriculture, Precision Agriculture, Predictive control, Research activities, Research and development, soil, Soil, Soil quality assessments, Soils, Soils qualities, digital twins, precision agriculture, soil quality},
	annote = {Export Date: 23 June 2024},
}

@article{schipor_gearwheels_2023-1,
	title = {{GearWheels}: {A} {Software} {Tool} to {Support} {User} {Experiments} on {Gesture} {Input} with {Wearable} {Devices}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134623880&doi=10.1080%2f10447318.2022.2098907&partnerID=40&md5=5be729e9568048f5de88c0aea18ab9e7},
	abstract = {We introduce GearWheels, a software tool for studies about gesture input with wearables, including smartwatches, rings, and glasses. GearWheels features an event-based asynchronous software architecture design implemented exclusively with web standards, communications protocols, and data formats, which makes it flexible to support many wearables via HTTP and WebSocket communications. GearWheels differentiates from prior software tools for gesture acquisition, elicitation, recognition, and analysis with its web-based, wearable-oriented, experiment-centered architecture design. We demonstrate GearWheels with a device affixed to the index finger, wrist, and the temple of a pair of glasses to illustrate touch stroke-gesture and motion-gesture input acquisition. We also perform a technical evaluation of GearWheels in the form of a simulation experiment, and report the request-response time performance of the software components of GearWheels with off-the-shelf wearables. We release GearWheels as open source software to assist researchers and practitioners in implementing studies about gesture input with wearables. © 2022 Taylor \& Francis Group, LLC.},
	author = {Schipor, O.-A. and Vatavu, R.-D.},
	year = {2023},
	note = {Publisher: Taylor and Francis Ltd.},
	keywords = {Software design, Open source software, Open systems, Communications data, Communications protocols, Computer aided software engineering, Event-based, Gesture input, Glass, HTTP, Network architecture, Software architecture design, Software-tools, User experiments, Wearable devices, Wearable technology, Web standards, Websocket},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{zapata_systematic_2023-1,
	title = {Systematic {Mapping} of the {Literature} on the {Conceptual} {Modeling} of {Industry} 4.0},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173573848&doi=10.1007%2f978-3-031-34147-2_15&partnerID=40&md5=1b3b1b38bf9c300501453861ddb7a8fa},
	abstract = {The Industry 4.0 concept refers to a new way of producing through the adoption of 4.0 technologies based on solutions focused on interconnectivity, automation, and real-time data. Given the importance of conceptualizing the problem domain and its solution, this paper presents the results of a systematic mapping to identify the state of the art and discover the existing contributions to the conceptual modeling of industry 4.0. A search was carried out in the Scopus, IEEE Xplore, and ACM DL digital libraries from January 2017 to May 2022. It was found that no article describes the model through a language known for this purpose, except for two articles that use Domain Specific Modeling Languages (DSML) and Unified Modeling Language (UML). Of the total number of primary studies, 63.33\% propose a model-based solution, while 13.34\% propose the use of tools, methods, and processes. Finally, 23.33\% present the state of the art. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Zapata, A. and Fransoy, M. and Soto, S. and Di Felice, M. and Panizzi, M.},
	year = {2023},
	keywords = {Embedded systems, State of the art, Mapping, Unified Modeling Language, Systematic mapping, Industry 4.0, Digital libraries, Technology-based, Conceptual model, Domain specific modeling languages, Domain-Specific Modelling Languages, Interconnectivity, Problem domain, Real-time data, Specification languages, Systematic mapping of the literature, Conceptual modeling, industry 4.0, systematic mapping of the literature},
	annote = {Export Date: 23 June 2024},
}

@incollection{bhambri_software_2023,
	title = {Software {Effort} {Estimation} with {Machine} {Learning} – {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152307515&doi=10.1002%2f9781119896838.ch15&partnerID=40&md5=8e9a36b192b41d691f4c4960256acc6b},
	abstract = {In 1959 the concept of machine learning techniques and algorithm was introduced by Artur Samuel, an IBmer from the United States who made a name for himself in the fields of computer gaming and artificial intelligence. The influence of literature reviews which is done systematically (SLRs), which are the preferred techniques and methods for aggregating effort, is examined in this study. We conducted a systematic literature review using the conventional procedure, which included a manual search of nine periodicals and a few conference proceedings. Eight of the twenty studies that were relevant focused on latest trends in research instead of technique evaluation. Seven LRs dealt with the estimation of effort. The SLR’s quality was best suited with only those in which fields are qualitatively checked not quantitatively. SLRs currently cover a large number of topics, but not all of them. Systematic literature reviews appear to be the most popular among researchers from Asia and Europe, particularly those at the Simula Laboratory. © 2023 Scrivener Publishing LLC.},
	publisher = {wiley},
	author = {Bhambri, P.},
	year = {2023},
	keywords = {machine learning, software development, software engineering, software development life cycle (SDLC), Software effort estimation, software process},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{de_castro_understanding_2023-1,
	title = {Understanding {Sustainable} {Knowledge}-{Sharing} in {Agile} {Projects}: {Utilizing} {Follow}-the-{Sun} {Technique} ({FTS}) in {Virtual} {Teams}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183565184&doi=10.1016%2fj.procs.2023.10.023&partnerID=40&md5=e8a2113a7c5e0ce18a6d80b3fec2ca8e},
	abstract = {In Agile IT projects, promoting effective knowledge sharing is essential not only for achieving success but also for supporting Sustainable Development Goals (SDGs). However, Companies using virtual teams may face challenges in coordinating work, particularly when teams are distributed across different time zones, ultimately hindering their ability to consistently share knowledge. This can lead to delays and inefficiencies, ultimately impacting the project outcomes and the organization's profitability. To ensure sustainable knowledge sharing, a comprehensive framework is necessary that addresses the environmental, social, economic, and political aspects of the project. This paper proposes a framework that combines the Follow-the-Sun (FTS) technique and the Sustainable Knowledge Sharing Model, enabling 24-hour knowledge sharing in virtual teams and benefiting IT agile projects. © 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)},
	publisher = {Elsevier B.V.},
	author = {de Castro, R.O. and Sanin, C. and Levula, A. and Szczerbicki, E.},
	year = {2023},
	keywords = {Knowledge management, Sustainable development, Project management, 24-hour knowledge-sharing cycle, Agile IT project, Economic responsibility, Environmental responsibility, IT project, Knowledge-sharing, Political responsibility, Social responsibilities, Sustainable knowledge sharing, Sustainable knowledge sharing model, Virtual team, Agile IT projects, Follow-the-sun  technique, Follow-the-Sun (FTS) technique, Social responsibility, Sustainable Knowledge Sharing Model, Virtual teams},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{vasylieva_how_2023-1,
	title = {How {Agile} {Are} you? {Discussing} {Maturity} {Levels} of {Agile} {Maturity} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320257&doi=10.1109%2fSEAA60479.2023.00049&partnerID=40&md5=328e1188c0b79baca242d53e65b4626e},
	abstract = {With the emergence of agile software development methods, new approaches for determining agile maturity have become necessary. Other than for traditional maturity and capability models like CMMI and ISO/IEC 15504, the field of agile maturity models is not yet settled. Even worse, a common understanding regarding agility in general and the levels of agility in particular is missing. The paper at hand aims to shed light on the field of agile maturity models with a particular focus on maturity levels, their definition, and their evaluation and computation. We conducted a systematic literature review to extract maturity levels and provide an initial harmonization of the levels found. Our findings from analyzing 19 agile maturity models show that there is yet no agreement with regard to the maturity levels. In total, 69 maturity levels have been analyzed for harmonization opportunities. Two major dimensions of maturity levels of agile maturity models could be identified: (1) team-related and (2) general maturity, which is comparable to standard approaches. However, the procedures to assess organizations and processes, if at all present, are to a large extent focused on persons and their personal opinion, which paves the way for future research, e.g., in terms of developing measurement systems for assessing agile maturity.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Vasylieva, K. and Kuhrmann, M. and Xavier, M.K. and Klunder, J.},
	year = {2023},
	keywords = {Software design, Agile software development, Software process, Agile maturity model, Capability model, Harmonisation, Maturity levels, Maturity model, New approaches, Software development methods, Software development, software process, agile maturity models, capability and maturity models},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{ntinda_aligning_2023-1,
	title = {Aligning {Academic} {Efforts} with {Key} {Industries}: {A} {Case} of {Computing} at the {University} of {Namibia}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182994686&doi=10.1109%2fFIE58773.2023.10343344&partnerID=40&md5=f95405a625e2b7d7d2cb3afce77a7778},
	abstract = {Preparing future graduates for the workplace should involve linkage with industry. Ultimately, students gain valuable insight into real-life projects, preparing them for future careers. However, universities in some developing countries lag in their initiatives to promote industry-academia collaboration. In this paper, we explore how to strengthen academic efforts at the University of Namibia (UNAM) with the assistance of industry in Namibia. In the study, we analyse: 1) current practices of industry collaboration worldwide published in ACM and IEEE digital library through a scoping review, and 2) students' capstone projects conducted in the final year of the Bachelor of Science (Honors) in the computing discipline in 2020 - 2022 at UNAM. The analysis from the scoping review found six (6) different University-Industry Collaboration initiatives employed in universities worldwide. Additionally, the review of current students' theses indicates that they are not aligned with all four key industries in Namibia: Mining, Tourism, Fisheries, and Agriculture. Hence, we contextualised the analysis by reflecting upon the economic drivers and demands of the country. The preliminary outcomes of this study allowed us to propose the incorporation of the Conceive, Design, Implement, and Operate model in the computing degree programme that UNAM can adopt in developing an effective curriculum that aligns with the demands of the key relevant industries in Namibia. The aim is to support the development of new talent that will promote the country's economic growth. Reflecting on this process can also benefit other universities in developing countries by assisting them in contextualising their curricula and addressing their local and national requirements. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ntinda, M.N. and Sedano, C.I. and Apiola, M. and Sutinen, E.},
	year = {2023},
	keywords = {Students, Digital libraries, Curricula, Computing, Bachelor of science, Capstone projects, CDIO model, Computing disciplines, Current practices, Developing countries, Economic analysis, Industry collaboration, Mining, Namibia, Scoping review, University industries, University-Industry},
	annote = {Export Date: 23 June 2024},
}

@article{jagstedt_dependencies_2023-1,
	title = {Dependencies as a barrier for continuous innovation in cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174302916&doi=10.1504%2fIJTM.2023.133915&partnerID=40&md5=09819f40631a7253d848d4f119ce6628},
	abstract = {In the automotive domain, as an example of cyber-physical systems, continuous software deployment is actively explored to deliver increasingly capable features to existing fleets of vehicles. The distributed nature of software coupled with tight hardware integration and potentially tremendous variability between vehicles make ensuring compatibility of updated software a significant challenge – both technically and managerially. While the automotive industry commonly forms larger multi-brand organisations to utilise economies of scale, processes for continuous deployment contradictory assumes a single organisation with full control. This paper sets out to shed light on challenges of adopting continuous deployment in the context of such a multi-brand cyber-physical systems organisation. Following a case study, the paper describes a tension between the managerial perspective concerned with platform strategies, and the engineering perspective responsible for developing products from those platforms. The paper highlights software dependencies as a barrier to continuous innovation of cyber-physical systems in multi-brand organisations. Copyright © 2023 Inderscience Enterprises Ltd.},
	author = {Jagstedt, S. and Mellegård, N. and Lind, K.},
	year = {2023},
	note = {Publisher: Inderscience Publishers},
	keywords = {Embedded systems, Software design, Agile software development, Continuous integrations, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Continuous deployment, Continuous innovation, Dependency, Economics, Multi-brand organization, Product architecture, Product platforms, automotive, cyber-physical system, agile software development, continuous deployment, continuous innovation, continuous integration, dependencies, multi-brand organisations, product architecture, product platform},
	annote = {Export Date: 23 June 2024},
}

@article{iannone_secret_2023-1,
	title = {The {Secret} {Life} of {Software} {Vulnerabilities}: {A} {Large}-{Scale} {Empirical} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895799&doi=10.1109%2fTSE.2022.3140868&partnerID=40&md5=8645cb8f157905454c978f4ae194b36e},
	abstract = {Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database-pertaining to 1,096 open-source software projects on GitHub-and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues.  © 1976-2012 IEEE.},
	author = {Iannone, E. and Guadagni, R. and Ferrucci, F. and De Lucia, A. and Palomba, F.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Information management, Software design, Life cycle, Software vulnerabilities, Software-systems, Open source software, Codes (symbols), Source codes, Open systems, Data mining, Software, Empirical Software Engineering, Code, Mining software, Mining software repository, Software development management, Software repositories, empirical software engineering, mining software repositories},
	annote = {Export Date: 23 June 2024},
}

@article{mubarkoot_software_2023-1,
	title = {Software {Compliance} {Requirements}, {Factors}, and {Policies}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141927596&doi=10.1016%2fj.cose.2022.102985&partnerID=40&md5=972aacda02a7c5ba18e90461a46647d6},
	abstract = {Background: Recent statistics reveal that 56\% of software attacks are caused by insider negligence and 26\% are caused by malicious insiders. They also show that 67\% of organizations experience at least 21 incidents per year. Most of these incidents require significant time and effort to contain them. In this regard, ensuring compliance with corporate policies, regulations, and industry best practices is paramount. Purpose: This study investigates software compliance requirements, factors, and policies together with the challenges they address. By taking a wider perspective, this study aims at bringing an understanding of existing research foci, evolving issues, and research directions. Method: The study uses a systematic literature review and keyword analysis, to identify relevant studies that address the derived research questions. Considering scholarly articles published in the last decade, 4,772 results were retrieved and checked through an initial screening. A thorough screening is then conducted to further reduce the results to 77 primary articles. Findings: The requirement on security of end users is gaining more attention. There is an emphasis on the gap between domain and compliance experts on the one side and software engineers on the other side. The review also identified 55 factors (and their underlying theories) that impact behavioral compliance with a majority of them focusing on individuals. Our results also list nineteen policies and compliance challenges they address. No distinction is found between open-source and proprietary software among the reviewed studies. The most mentioned policies are security education, training, and awareness (SETA), compliance automation, and organizational climate. The evolving topics in the field are: theory of workarounds, compliance and privacy by design, policy as code, security stress, and home-office users. Implications: The review provides 9 recommendations, comprising practical implications for decision makers, theoretical implications for future research, and potential enhancement of the underlying theories. © 2022 The Author(s)},
	author = {Mubarkoot, M. and Altmann, J. and Rasti-Barzoki, M. and Egger, B. and Lee, H.},
	year = {2023},
	note = {Publisher: Elsevier Ltd},
	keywords = {Systematic literature review, Decision making, Open source software, Impact, Factor, Best practices, Open systems, Corporate policies, Decision theory, Malicious insiders, Policy regulations, Requirement, Software attacks, Software compliance, Factors, Policies, Requirements, Software Compliance, Systematic literature Review},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{gardey_ux-painter_2023-1,
	title = {{UX}-{Painter}: {Fostering} {UX} {Improvement} in an {Agile} {Setting}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149840802&doi=10.1007%2f978-3-031-25648-6_4&partnerID=40&md5=50a565c23e05f522a0b2f5833cbc014b},
	abstract = {It is generally difficult in agile teams, specially those geographically distributed, to keep up with the user experience (UX) issues that emerge on each product increment. UX designers need the help of developers to set up user testing environments and to code improvements to the user interface, while developers are too busy with functionality issues. This paper describes a tool called UX-Painter and shows through a case study, how it may help in the above setting to synchronize UX practices and allow for continuous UX improvement during an agile development. UX-Painter allows designers to set up A/B testing environments, exploring interface design alternatives without the need of programming skills, through predefined transformations called client-side web refactorings. Once a design alternative is selected to be implemented in the application’s codebase, UX-Painter may also facilitate this step, exporting the applied refactorings to different frontend frameworks. Thus, we foster a method where UX backlog items can be systematically tackled and resolved in an agile setting. © 2023, Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Gardey, J.C. and Grigera, J. and Rossi, G. and Garrido, A.},
	year = {2023},
	keywords = {User interfaces, Users' experiences, Refactorings, User testing, Agile methods, Agile teams, Code improvement, Design alternatives, Show through, Testing environment, Web engineering, User experience},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{nunez-agurto_traffic_2023-1,
	title = {Traffic {Classification} in {Software}-{Defined} {Networking} by {Employing} {Deep} {Learning} {Techniques}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175975883&doi=10.1007%2f978-3-031-45682-4_6&partnerID=40&md5=9a95f5b968d737eb60ef3bdb967e4d27},
	abstract = {Software-Defined Networking provides a global vision of the network, centralized controller, dynamic routing, dynamic update of the flow table, and traffic analysis. The features of Software-Defined Networking and the integration of Deep Learning techniques allow the introduction of intelligence to optimize, manage and maintain them better. In this context, this work aims to provide a Systematic Literature Review on traffic classification in Software-Defined Networking with Deep Learning techniques. Furthermore, we analyze and synthesize the selected studies based on the categorization of traffic classes and the employed Deep Learning techniques to draw meaningful research conclusions. Finally, we identify new challenges and future research directions on this topic. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Nuñez-Agurto, D. and Fuertes, W. and Marrone, L. and Benavides-Astudillo, E. and Vásquez-Bermúdez, M.},
	year = {2023},
	keywords = {Systematic literature review, Deep learning, Learning algorithms, Learning systems, Centralized controllers, Controller dynamics, Dynamic routing, Global vision, Learning techniques, Routing dynamics, Software defined networking, Software-defined networkings, Traffic classification, Software-defined networking},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{volden_wayfinding_2023-1,
	title = {Wayfinding and {Navigation} in the {Outdoors}: {Quantitative} and {Data} {Driven} {Development} of {Personas} and {Requirements} for {Wayfinding} in {Nature}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173041138&doi=10.1007%2f978-3-031-35129-7_14&partnerID=40&md5=77edf2f35a0d4ce956149ab12f497864},
	abstract = {Persona development in human-centered design processes is mostly done in a qualitative process involving procedures like interviews, focus-groups and workshops. These are methods that are criticized for being prone to biases, as not being based on rigorous empirical data and for using small and possibly non-representative populations. Quantitative approaches are an alternative or supplement to qualitative methods. Through a survey (n = 693) we have investigated how people navigate and find their way in the nature. The questionnaire contains questions on demographics, activities, and wayfinding behaviors when out in the nature. The study’s aim was twofold: First we wanted to investigate the use of a quantitative approach for exploring user behaviors and attitudes when having access to a sufficiently large data material. Secondly, we wanted to provide for persona-development for way-finding systems used in the nature. The methods applied in this study is a combination of Principal Component Analyses (PCA) and Cluster Analyses (CA). Based on these methods three factors where identified, which again lead to three clusters of respondents. The study concludes that when having access to quantitative data as we managed to have in this study, the combination of PCA and CA is an efficient and precise way to describe requirements and develop Personas. Results also indicate significant effects of demographic variables like age and gender for technology preferences. as well as for confidence in abilities when navigating and finding the way in nature. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Volden, F. and Wattne, O.E.},
	year = {2023},
	keywords = {Behavioral research, Data driven, Design-process, Population statistics, Cluster analysis, Cluster analyze, Human-centred designs, Principal component analysis, Principal-component analysis, Qualitative process, Quantitative approach, Quantitative persona development, Way finding, Wayfinding and navigations, Cluster analyses, Wayfinding},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{souza_santos_lgbtqia_2023-1,
	title = {{LGBTQIA}+ ({In}) {Visibility} in {Computer} {Science} and {Software} {Engineering} {Education}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165153862&doi=10.1109%2fCHASE58964.2023.00026&partnerID=40&md5=91575c7fa526b69d70f2e73d63079186},
	abstract = {Modern society is diverse, multicultural, and multifaceted. Because of these characteristics, we are currently observing an increase in the debates about equity, diversity, and inclusion in different areas, especially because several groups of individuals are underrepresented in many environments. In computer science and software engineering, it seems counterintuitive that these areas, which are responsible for creating technological solutions and systems for billions of users around the world, do not reflect the diversity of the society to which it serves. In trying to solve this diversity crisis in the software industry, researchers started to investigate strategies that can be applied to increase diversity and improve inclusion in academia and the software industry. However, the lack of diversity in computer science and related courses, including software engineering, is still a problem, in particular when some specific groups are considered. LGBTQIA+ students, for instance, face several challenges to fit into technology courses, even though most students in universities right now belong to Generation Z, which is described as open-minded to aspects of gender and sexuality. In this study, we aimed to discuss the state-of-art of publications about the inclusion of LGBTQIA+ students in computer science education. Using a mapping study, we identified eight studies published in the past six years that focused on this public. We present strategies developed to adapt curricula and lectures to be more inclusive to LGBTQIA+ students and discuss challenges and opportunities for future research. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Souza Santos, R.D. and Stuart-Verner, B. and De Magalhaes, C.V.C.},
	year = {2023},
	keywords = {Computer software, Engineering education, Students, Software industry, Computer Science Education, Education computing, Computer science and software engineerings, Computer Science course, Computer-related course, Diversity, Inclusions, LGBTQIA+, Software engineering education, Technological solution, Technological system, education, software engineering, diversity, EDI, inclusion},
	annote = {Export Date: 23 June 2024},
}

@article{pantoja_yepez_training_2023-1,
	title = {Training software architects suiting software industry needs: {A} literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174223101&doi=10.1007%2fs10639-023-12149-x&partnerID=40&md5=857c26b2b7109dcd5a963c38e629e347},
	abstract = {The ability to define, evaluate, and implement software architectures is a fundamental skill for software engineers. However, teaching software architecture can be challenging as it requires students to be involved in real-context projects with high degrees of complexity. This involves making trade-off decisions among several quality attributes. Furthermore, the academic perception of software architecture differs from the industrial viewpoint. To address this issue, a study was conducted to identify and analyze the strategies, challenges, and course experiences used for teaching software architectures. The study analyzed 56 articles reporting on teaching experiences focused specifically on software architectures or focused on software engineering in general but discussing software architecture. The main contributions of this work include identifying strategies used in educating software architecture students aligned with the needs of the software industry. These strategies include short design projects, large development projects, and projects with actual clients. Additionally, the study compared curriculum contents in software development and architecture courses and identified recurring topics such as architecture patterns, quality attributes, and architectural views. This study also recognizes the set of skills that students of software architecture should develop during training, such as leadership and negotiation. The challenges in software architecture training were discussed, such as instructors’ lack of experience in actual projects, the abstract and fuzzy nature of software architectures, and the difficulty of involving clients and industry experts. Evaluation methods commonly used in training software architects, such as surveys, pre-test/post-test, and quality metrics on architectural artifacts, were identified and described. Overall, this study guides researchers and educators in improving their software architecture courses by incorporating strategies reported by the literature review. These strategies can bring architecture courses closer to the needs and conditions of the software industry. © 2023, The Author(s).},
	author = {Pantoja Yépez, W.L. and Hurtado Alegría, J.A. and Bandi, A. and Kiwelekar, A.W.},
	year = {2023},
	note = {Publisher: Springer},
	keywords = {Systematic mapping, Software architecture, Industry, Training},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{heyn_automotive_2023-1,
	title = {Automotive {Perception} {Software} {Development}: {An} {Empirical} {Investigation} into {Data}, {Annotation}, and {Ecosystem} {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165140236&doi=10.1109%2fCAIN58948.2023.00011&partnerID=40&md5=73c99440d36cf7a3c3c1d5b7444837f6},
	abstract = {Software that contains machine learning algorithms is an integral part of automotive perception, for example, in driving automation systems. The development of such software, specifically the training and validation of the machine learning components, requires large annotated datasets. An industry of data and annotation services has emerged to serve the development of such data-intensive automotive software components. Wide-spread difficulties to specify data and annotation needs challenge collaborations between OEMs (Original Equipment Manufacturers) and their suppliers of software components, data, and annotations.This paper investigates the reasons for these difficulties for practitioners in the Swedish automotive industry to arrive at clear specifications for data and annotations. The results from an interview study show that a lack of effective metrics for data quality aspects, ambiguities in the way of working, unclear definitions of annotation quality, and deficits in the business ecosystems are causes for the difficulty in deriving the specifications. We provide a list of recommendations that can mitigate challenges when deriving specifications and we propose future research opportunities to overcome these challenges. Our work contributes towards the on-going research on accountability of machine learning as applied to complex software systems, especially for high-stake applications such as automated driving. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Heyn, H.-M. and Habibullah, K.M. and Knauss, E. and Horkoff, J. and Borg, M. and Knauss, A. and Li, P.J.},
	year = {2023},
	keywords = {Data, Ecosystems, Specifications, Software design, Application programs, Learning algorithms, Automation, Machine learning, Machine-learning, Machine learning algorithms, Software-component, Automotive industry, Automotives, Accountability, Annotation, Data annotation, Empirical investigation, Large dataset, Machine components, Requirements specifications, machine learning, accountability, annotations, data, ecosystems, requirements specification},
	annote = {Export Date: 23 June 2024},
}

@article{neves_data_2023-1,
	title = {Data privacy in the {Internet} of {Things} based on anonymization: {A} review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161049485&doi=10.3233%2fJCS-210089&partnerID=40&md5=31fb26c942cd9ccd8d3943c25a607885},
	abstract = {The Internet of Things (IoT) has shown rapid growth in recent years. However, it presents challenges related to the lack of standardization of communication produced by different types of devices. Another problem area is the security and privacy of data generated by IoT devices. Thus, with the focus on grouping, analyzing, and classifying existing data security and privacy methods in IoT, based on data anonymization, we have conducted a Systematic Literature Review (SLR). We have therefore reviewed the history of works developing solutions for security and privacy in the IoT, particularly data anonymization and the leading technologies used by researchers in their work. We also discussed the challenges and future directions for research. The objective of the work is to give order to the main approaches that promise to provide or facilitate data privacy using anonymization in the IoT area. The study's results can help us understand the best anonymization techniques to provide data security and privacy in IoT environments. In addition, the findings can also help us understand the limitations of existing approaches and identify areas for improvement. The results found in most of the studies analyzed indicate a lack of consensus in the following areas: (i) with regard to a solution with a standardized methodology to be applied in all scenarios that encompass IoT; (ii) the use of different techniques to anonymize the data; and (iii), the resolution of privacy issues. On the other hand, results made available by the k-anonymity technique proved efficient in combination with other techniques. In this context, data privacy presents one of the main challenges for broadening secure domains in applying privacy with anonymity. © 2023 - IOS Press. All rights reserved.},
	author = {Neves, F. and Souza, R. and Sousa, J. and Bonfim, M. and Garcia, V.},
	year = {2023},
	note = {Publisher: IOS Press BV},
	keywords = {Systematic literature review, Data privacy, Privacy, Internet of things, Anonymization, Data anonymization, Data security and privacy, Dataflow, K-Anonymity, Problem areas, Rapid growth, Security and privacy, Internet of Things, privacy, data anonymization, data flow, k-anonymity},
	annote = {Export Date: 23 June 2024},
}

@article{denecke_developing_2023-1,
	title = {Developing a {Technical}-{Oriented} {Taxonomy} to {Define} {Archetypes} of {Conversational} {Agents} in {Health} {Care}: {Literature} {Review} and {Cluster} {Analysis}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147143364&doi=10.2196%2f41583&partnerID=40&md5=acb31e1475488e8316c9d3c5a54d84e0},
	abstract = {Background: The evolution of artificial intelligence and natural language processing generates new opportunities for conversational agents (CAs) that communicate and interact with individuals. In the health domain, CAs became popular as they allow for simulating the real-life experience in a health care setting, which is the conversation with a physician. However, it is still unclear which technical archetypes of health CAs can be distinguished. Such technical archetypes are required, among other things, for harmonizing evaluation metrics or describing the landscape of health CAs. Objective: The objective of this work was to develop a technical-oriented taxonomy for health CAs and characterize archetypes of health CAs based on their technical characteristics. Methods: We developed a taxonomy of technical characteristics for health CAs based on scientific literature and empirical data and by applying a taxonomy development framework. To demonstrate the applicability of the taxonomy, we analyzed the landscape of health CAs of the last years based on a literature review. To form technical design archetypes of health CAs, we applied a k-means clustering method. Results: Our taxonomy comprises 18 unique dimensions corresponding to 4 perspectives of technical characteristics (setting, data processing, interaction, and agent appearance). Each dimension consists of 2 to 5 characteristics. The taxonomy was validated based on 173 unique health CAs that were identified out of 1671 initially retrieved publications. The 173 CAs were clustered into 4 distinctive archetypes: a text-based ad hoc supporter; a multilingual, hybrid ad hoc supporter; a hybrid, single-language temporary advisor; and, finally, an embodied temporary advisor, rule based with hybrid input and output options. Conclusions: From the cluster analysis, we learned that the time dimension is important from a technical perspective to distinguish health CA archetypes. Moreover, we were able to identify additional distinctive, dominant characteristics that are relevant when evaluating health-related CAs (eg, input and output options or the complexity of the CA personality). Our archetypes reflect the current landscape of health CAs, which is characterized by rule based, simple systems in terms of CA personality and interaction. With an increase in research interest in this field, we expect that more complex systems will arise. The archetype-building process should be repeated after some time to check whether new design archetypes emerge. © 2023 Journal of Medical Internet Research. All rights reserved.},
	author = {Denecke, K. and May, R.},
	year = {2023},
	note = {Publisher: JMIR Publications Inc.},
	keywords = {human, Humans, human computer interaction, Review, artificial intelligence, Artificial Intelligence, cluster analysis, Cluster Analysis, Communication, controlled study, data privacy, data processing, Delivery of Health Care, health care, health care delivery, intelligence, internet access, interpersonal communication, k means clustering, language, Language, machine learning, personality, sentiment analysis, taxonomy, communication, delivery of health care, delivery of health care and methods, mobile phone, telemedicine, trends, user-computer interface},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{tsui_detect_2023-1,
	title = {Detect and {Interpret}: {Towards} {Operationalization} of {Automated} {User} {Experience} {Evaluation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169436114&doi=10.1007%2f978-3-031-35702-2_6&partnerID=40&md5=71662b96b384aa4acc41fbb5146fa2f3},
	abstract = {The evaluation of user experience (UX) with software products is widely recognized as a critical aspect of supporting a product lifecycle. However, existing UX evaluation methods tend to require high levels of human involvement in data collection and analysis. This makes the ongoing UX monitoring particularly challenging, especially given the increasing number of products, growing user base and associated data. Thus, there is a strong demand in developing UX evaluation systems that are able to automatically track UX and provide insights on required design improvements. The few existing frameworks for such automated systems can help identify user-centric metrics for UX evaluation, but mostly focus on providing recommendations on best practices of determining metrics and tend to reflect only parts of the UX. Moreover, these frameworks predominantly rely on high-level UX concepts, but do not necessarily allow measurements to reveal the underlying causes of UX challenges. In this paper, we demonstrate how the above-mentioned challenges can be addressed through a combination of data gathering and analysis paths employed by the traditional UX evaluation methods. Our paper contributes to the field by providing a review of existing automated UX evaluation approaches and common UX evaluation data collection methods, and offering a two-tier measurement approach for developing automated UX evaluation system, which augments the reflective power of traditional UX evaluation methods. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Tsui, A.S.M. and Kuzminykh, A.},
	year = {2023},
	keywords = {Life cycle, Automation, Users' experiences, Evaluation methods, Data acquisition, Automatic UX evaluation, Data collection, Evaluation method and technique, Evaluation of users, Method and technique, Product life cycles, Software products, User experience evaluations, Automatic UX Evaluation, Evaluation Methods and Techniques, User Experience, UX},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{heikkinen_continual_2023-1,
	title = {Continual {Service} {Improvement}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172415273&doi=10.1007%2f978-3-031-43703-8_3&partnerID=40&md5=68142092a8a39278b60b3241c351fb51},
	abstract = {Continual Service Improvement (CSI) is an ongoing activity to identify and improve organization practices and services to align them with changing needs. CSI is one of the core elements of IT Service Management (ITSM) frameworks. However, as a research topic it is still an emerging research area of service science. This study explores implementation of CSI and its seven-step improvement process in the context of ITSM. The goal of this paper is to present results of systematic literature review increasing understanding about the CSI and seven-step improvement process, and provide topics for future research. A Systematic Literature Review (SLR) was carried out to analyse CSI-related academic articles. Our main finding is that CSI-related terminology needs clarification and consistency both in academia and in practice to guide the future CSI research for example clarify roles and internal practices of CSI; provide a staged approach for continual improvement; and identify models that support improving and automating the seven-step improvement process. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Heikkinen, S. and Jäntti, M. and Tukiainen, M.},
	year = {2023},
	keywords = {Systematic literature review, Continual service improvement, IT service management, IT services, ITIL, Service improvement, Service management, Seven-step improvement process, Systematic Literature Review, CSI, ITSM, seven-step improvement process},
	annote = {Export Date: 23 June 2024},
}

@article{ouhaichi_research_2023-1,
	title = {Research trends in multimodal learning analytics: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151456109&doi=10.1016%2fj.caeai.2023.100136&partnerID=40&md5=4c1ed5486737fbf780207ba5b51fa068},
	abstract = {Understanding and improving education are critical goals of learning analytics. However, learning is not always mediated or aided by a digital system that can capture digital traces. Learning in such environments can be studied by recording, processing, and analyzing different signals, including video and audio, so that traces of actors’ actions and interactions are captured. Multimodal Learning Analytics refers to analyzing these signals through the use and integration of these multiple modes. However, a need exists to evaluate how research is conducted in the emerging field of multimodal learning analytics to aid and evaluate how these systems work. With the growth of multimodal learning analytics, research trends and technologies are needed to support its development. We conducted a systematic mapping study based on established systematic literature practices to identify multimodal learning analytics research types, methodologies, and trending research themes. Most mapped papers presented different solutions and used evaluation-based research methods to demonstrate an increasing interest in multimodal learning analytics technologies. In addition, we identified 14 topics under four themes––learning context, learning process, systems and modality, and technologies––that can contribute to the growth of multimodal learning analytics. © 2023 The Authors},
	author = {Ouhaichi, H. and Spikol, D. and Vogel, B.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Artificial intelligence, Mapping study, Learning technologies, Multimodal learning analytics},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{baron_evidence_2023-1,
	title = {Evidence {Profiles} for {Validity} {Threats} in {Program} {Comprehension} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171794365&doi=10.1109%2fICSE48619.2023.00162&partnerID=40&md5=eb9f911cd27ddf564d8b8e6fbcac370a},
	abstract = {Searching for clues, gathering evidence, and reviewing case files are all techniques used by criminal investigators to draw sound conclusions and avoid wrongful convictions. Medicine, too, has a long tradition of evidence-based practice, in which administering a treatment without evidence of its efficacy is considered malpractice. Similarly, in software engineering (SE) research, we can develop sound methodologies and mitigate threats to validity by basing study design decisions on evidence. Echoing a recent call for the empirical evaluation of design decisions in program comprehension experiments, we conducted a 2-phases study consisting of systematic literature searches, snowballing, and thematic synthesis. We found out (1) which validity threat categories are most often discussed in primary studies of code comprehension, and we collected evidence to build (2) the evidence profiles for the three most commonly reported threats to validity. We discovered that few mentions of validity threats in primary studies (31 of 409) included a reference to supporting evidence. For the three most commonly mentioned threats, namely the influence of programming experience, program length, and the selected comprehension measures, almost all cited studies (17 of 18) did not meet our criteria for evidence. We show that for many threats to validity that are currently assumed to be influential across all studies, their actual impact may depend on the design and context of each specific study. Researchers should discuss threats to validity within the context of their particular study and support their discussions with evidence. The present paper can be one resource for evidence, and we call for more meta-studies of this type to be conducted, which will then inform design decisions in primary studies. Further, although we have applied our methodology in the context of program comprehension, our approach can also be used in other SE research areas to enable evidence-based experiment design decisions and meaningful discussions of threats to validity. © 2023 IEEE.},
	publisher = {IEEE Computer Society},
	author = {Baron, M.M. and Wyrich, M. and Graziotin, D. and Wagner, S.},
	year = {2023},
	keywords = {Software engineering, Design, Study design, Software engineering research, Empirical evaluations, Evidence-based practices, Empirical Software Engineering, Threat to validity, Case files, Design decisions, Evaluation of designs, Program comprehension, threats to validity, empirical software engineering, program comprehension},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{dallegrave_action_2023-1,
	title = {Action {Research} for {Industry} {Academia} {Collaboration} : {A} replication {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169786536&doi=10.23919%2fCISTI58278.2023.10211674&partnerID=40&md5=947725c1c9cf30739a4b22bde3653ba7},
	abstract = {Collaboration between industry and academic communities requires considerable work but has the power to foster innovation. This relationship with joint trust promotes knowledge exchange that helps develop more qualified researchers and professionals. The Action Research (AR) method combines theory and practice, and studies involving industry-academia collaboration (IAC) have shown encouraging results. Nevertheless, further investigation is required to verify the effects of applying this method. This research investigates the perceptions of academic master's and doctoral program students and professionals involved in projects that applied the AR method as a strategy to foster IAC. This article replicates a case study with different projects that conducted an AR in software companies. This study indicated high satisfaction among students (83\%) when using action research in the course. All students considered the practical knowledge very relevant and would like to use the method again in other opportunities throughout their academic and professional life. This investigation showed that conducting IAC projects using the AR method within the industry in an educational context was challenging. That occurred due to the lack of experience in using empirical methods. Also, the professional's unavailability delayed the results and, consequently, the activities in the project that already had a very tight schedule.  © 2023 ITMA.},
	publisher = {IEEE Computer Society},
	author = {Dallegrave, T. and Santos, W.B.},
	year = {2023},
	keywords = {Industry-academia collaboration, Knowledge management, Students, Power, Research method, Replication study, Replication, Academic community, Action research, Collaborative practice research, Collaborative practices, Practice researches, Action Research, Collaborative Practice Research, Industry-Academia Collaboration},
	annote = {Export Date: 23 June 2024},
}

@article{sadeghiani_sayings_2023-1,
	title = {Sayings and doings become ‘practice’ through ‘practice thirdness’: pivot in recipes for practice},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162251678&doi=10.1080%2f08985626.2023.2225044&partnerID=40&md5=057becbb87581d7172bf60a6275f5339},
	abstract = {The abductive logic behind the practice lens allows practice researchers to contextualize theorizing and emphasize non-generalizability of their findings. However, scholars are critical of this non-generalizability flaw. In this conceptual paper, we aim to go beyond such criticisms and constructively discuss how this flaw might be resolved. In doing so, we theorize ‘practice thirdness’ as the shared understanding of knowing how to do practice, at local and universal levels, and provide a framework for discussing the generalizability of practice. We take ‘pivot’, at the heart of the Lean Startup as our case, and based on different interpretations of this practice, we argue what entrepreneurs have said and what scholars have interpreted of what entrepreneurs have said do not show what they have actually done. Therefore, despite the formation of practice local thirdness, i.e. practice thirdness in a particular context, in the case of pivot, still, we need academic conversation to reach practice universal thirdness, i.e. practice thirdness across different contexts. We suggest that practice researchers take a neopragmatic lens for studying practice patterns across different contexts. Also, we argue why practice researchers should be open to other methods besides the commonly recommended (non)participant observation. Moreover, we propose a model for communicating and generalizing practice based on Peirce’s triadic model of semiosis and Nonaka and Takeuchi’s model of knowledge management. © 2023 Informa UK Limited, trading as Taylor \& Francis Group.},
	author = {Sadeghiani, A. and Anderson, A. and Ahmadi, S. and Shokouhyar, S. and Hajipour, B.},
	year = {2023},
	note = {Publisher: Routledge},
	keywords = {knowledge, modeling, research work, theoretical study, entrepreneurship-as-practice, Nonaka and Takeuchi’s model of knowledge conversion, Peirce’s triadic model of semiosis, pivot, practice local thirdness, practice thirdness, practice universal thirdness, text as practice},
	annote = {Export Date: 23 June 2024},
}

@article{saarikallio_quality_2023-1,
	title = {Quality culture boosts agile transformation—{Action} research in a business-to-business software business},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137236655&doi=10.1002%2fsmr.2504&partnerID=40&md5=f87d77293e772685a8659d46dc1dbc29},
	abstract = {Agile methodologies are sometimes adopted, with the assumption that benefits can be attained by only using a set of best practices, which can sometimes work to a degree. In this paper, a case is discussed where a software-producing organization of seven teams achieved significant improvements. The goal of the research was to answer two questions: how an already agile organization could improve its performance further and what is the impact of promoting quality aspects? The questions were answered by implementing interventions based on prior literature and data emerging from semi-structured interviews. The context was an established business with a complex revenue stream structure, meaning the mix of various project/service/product based work rendered the adoption of agile methods a challenge. Action research comprising three rounds of interventions was conducted to improve the organization and its quality culture while enforcing code review practices. Interventions resulted in a significant improvement in quality, as measured by reported defects. Therefore, it is suggested that agile methods are not sufficient on their own to take software business forward unless a quality-focused culture is simultaneously achieved through a mindset change and organizational structures to enforce quality practices. The paper contributes to research on the managerial practices of software business and agile transformation by providing empirical support to introducing formal quality improvement to the agile mix as a method for practitioners to improve organizations with complex business models and multiple teams. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	author = {Saarikallio, M. and Tyrväinen, P.},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd},
	keywords = {Computer software, Human resource management, Empirical, Agile adoptions, B2B, Business models, Development method, Hybrid development method, Increment planning event, Mixed business model, Quality, Revenue streams, Scaled agile, Team coordination, agile adoption, empirical, hybrid development methods, increment planning event, mixed business model, quality, revenue stream, scaled agile, team coordination},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{nikiforova_towards_2023-1,
	title = {Towards {High}-{Value} {Datasets} {Determination} for {Data}-{Driven} {Development}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172028920&doi=10.1007%2f978-3-031-41138-0_14&partnerID=40&md5=568da301604faa0be07050ee6fd28679},
	abstract = {Open government data (OGD) is seen as a political and socio-economic phenomenon that promises to promote civic engagement and stimulate public sector innovations in various areas of public life. To bring the expected benefits, data must be reused and transformed into value-added products or services. This, in turn, sets another precondition for data that are expected to not only be available and comply with open data principles, but also be of value, i.e., of interest for reuse by the end-user. This refers to the notion of “high-value dataset” (HVD), recognized by the European Data Portal as a key trend in the OGD area in 2022. While there is a progress in this direction, e.g., the Open Data Directive, incl. identifying 6 key categories, a list of HVDs and arrangements for their publication and re-use, they can be seen as “core”/“base” datasets aimed at increasing interoperability of public sector data with a high priority, contributing to the development of a more mature OGD initiative. Depending on the specifics of a region and country - geographical location, social, environmental, economic issues, cultural characteristics, (under)developed sectors and market specificities, more datasets can be recognized as of high value for a particular country. However, there is no standardized approach to assist chief data officers in this, and there is a clear lack of conceptualizations for the determination of HVD and systematic oversight. In this paper, we present a systematic review of existing literature on the HVD determination, which is expected to form an initial knowledge base for this process, including used approaches and indicators to determine them, data, stakeholders. © 2023, IFIP International Federation for Information Processing.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Nikiforova, A. and Rizun, N. and Ciesielska, M. and Alexopoulos, C. and Miletić, A.},
	year = {2023},
	keywords = {Data driven, Open Data, Open data ecosystem, Open datum, Open government data, Public values, Knowledge based systems, Public administration, High-value data, Public sector, Stakeholder, Value data, High-value Data, Open Data Ecosystem, Open Government Data, Public Administration, Public Value},
	annote = {Export Date: 23 June 2024},
}

@incollection{gabriele_human-car_2023-1,
	title = {Human-{Car} {Interface}: {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138724816&doi=10.1007%2f978-3-031-12547-8_50&partnerID=40&md5=3637be87138725a152cd4fa146cc5323},
	abstract = {A systematic literature review, or SLR, seeks to structure the review carried out in the defined areas in a replicable and auditable method, in order to facilitate and objectify both the search for answers to research questions and their accessibility by peers. In this study, we present an SLR carried out in November 2021 by the PRISMA method, on interaction and interface design focused on the automotive User Experience, having these three research questions: (RQ1) What are the objects of study of the articles? (RQ2) Which methods are used to analyze the object of study? (RQ3) What are the samples size of the surveys carried out? At the end of the Screening, 20 articles were selected to answer the research questions, and some data deserve attention, such as the 60\% that didn't identify the use of UX assessment questionnaires or the 35\% that had incomplete demographic data. We also saw that the objects of study are concentrated in 3 major areas and that the methodology used is, for the most part, similar in structure. The lack of studies carried out in South America prompted us to develop a research project focused on the Brazilian User. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Gabriele, F. and Martins, L.},
	year = {2023},
	keywords = {SLR, User experience, UX, Human–Computer interaction, Interaction design},
	annote = {Export Date: 23 June 2024},
}

@incollection{asdecker_dirty_2023-1,
	title = {A {Dirty} {Little} {Secret}? {Conducting} a {Systematic} {Literature} {Review} {Regarding} {Overstocks}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165980153&doi=10.1007%2f978-3-031-38145-4_13&partnerID=40&md5=9d35ba41edd77488927f63ed1bf14f32},
	abstract = {Due to numerous media reports, overstocks in supply chains have recently attracted attention alongside the public sustainability debate. The goal of this paper is to aggregate the current body of knowledge and develop a better understanding regarding (1) the quantification of overstocks (what?), (2) the management approaches used (how?), and (3) the motives of managing overstocks (why?). The review synthesizes 48 relevant publications that were systematically gathered from three of the leading scientific databases. Based on the results of the review, a research agenda is derived that identifies ten particularly promising avenues for future investigations. Furthermore, the review shows that the existing knowledge about overstocks and the way they are managed is not only limited, but also very fragmented. A holistic perspective is missing, which motivates this paper to call for a conceptualization in the sense of an “overstock management” function. To initiate this process, a definition of the term is proposed. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media B.V.},
	author = {Asdecker, B. and Tscherner, M. and Kurringer, N. and Felch, V.},
	year = {2023},
	keywords = {Inventory Management, Overstock Management, Overstocks, Waste Management},
	annote = {Export Date: 23 June 2024},
}

@article{marques_gamification_2023-1,
	title = {Gamification for agile: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161850209&doi=10.1504%2fIJASM.2023.130838&partnerID=40&md5=5bd2dad7d6ee9d8c774d0e1ed8fe714e},
	abstract = {Gamification has been used in software engineering to motivate practitioners to adopt agile. This study assesses the state of the art regarding the use of gamification in agile projects. A systematic literature review was followed by searching for peer-reviewed papers and dissertations on the topic and assessing their quality. Overall, 225 studies were found, but only 12 selected. Most studies focused on the Scrum framework, and the completion of stories/tasks was the practice subject to gamification more times. While the impact of gamification initiatives was positive, these studies lacked a proper empirical validation of the proposed gamification solutions. Despite the novelty of this field, there seems to be potential in the use of gamification to improve agile projects, but future studies should address the gaps identified in this analysis and provide more detail when reporting their results, namely regarding the discussion of the impact, benefits, and challenges of gamification. Copyright © 2023 Inderscience Enterprises Ltd.},
	author = {Marques, R. and da Silva, M.M. and Gonçalves, D.},
	year = {2023},
	note = {Publisher: Inderscience Publishers},
	keywords = {SLR, gamification, systematic literature review, agile, scrum, software engineering, agile management, agile practices, agile projects, motivation, software engineers},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{abdullah_controlling_2023-1,
	title = {Controlling {Automatic} {Experiment}-{Driven} {Systems} {Using} {Statistics} and {Machine} {Learning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186763994&doi=10.1007%2f978-3-031-36889-9_9&partnerID=40&md5=9e42d9434fbe1e6f36858bb2983e5e75},
	abstract = {Experiments are used in many modern systems to optimize their operation. Such experiment-driven systems are used in various fields, such as web-based systems, smart-* systems, and various selfadaptive systems. There is a class of these systems that derive their data from running simulations or another type of computation, such as in digital twins, online planning using probabilistic model-checking, or performance benchmarking. To obtain statistically significant results, these systems must repeat the experiments multiple times. As a result, they consume extensive computation resources. The GraalVM benchmarking project detects performance changes in the GraalVM compiler. However, the benchmarking project has an extensive usage of computational resources and time. The doctoral research project proposed in this paper focuses on controlling the experiments with the goal of reducing computation costs. The plan is to use statistical and machine learning approaches to predict the outcomes of experiments and select the experiments yielding more useful information. As an evaluation, we are applying these methods to the GraalVM benchmarking project; the initial results confirm that these methods have the potential to significantly reduce computation costs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Abdullah, M.},
	year = {2023},
	keywords = {Model checking, Machine learning, Machine-learning, Online systems, Statistics, Cost reduction, Benchmarking, Computation costs, Driven system, Experiment-driven system, On-line planning, Running simulations, Self-adaptive system, Smart System, Statistics learning, Web-based system, Experiment-Driven Systems, Machine Learning},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{southier_systematic_2023-1,
	title = {Systematic {Mapping} {Review} on {Log} {Preparation} for {Process} {Mining}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164696308&doi=10.1109%2fCSCWD57460.2023.10152649&partnerID=40&md5=fff1faec33e5ffca044fa44095cb3bfa},
	abstract = {Process Mining (PM) is a research discipline that helps organizations track and optimize processes to support their business. Further, it focuses on providing process analysis techniques and tools, and several of its applications have been described in the literature. The start point for PM is using event logs generated by information systems to analyze processes. These event logs need to be extracted from databases and prepared for use because the quality of the event logs used as input is critical to the success of any PM effort. In this article, we present a systematic mapping review to provide the reader with highlights of the state-of-the-art techniques for event log preparation. Based on the retrieved studies, we identified six main categories of log preparation techniques: extraction, cleaning, repair, non-adequate granularity, quality evaluation, and privacy. The results are explored quantitatively and qualitatively. All results are made available through spreadsheets and charts. We believe this paper is a starting point for researchers to identify the studies that would help them prepare event logs for PM.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Southier, L.F.P. and De Freitas, S.C. and Pizzini, A. and Santos, E.A.P. and Scalabrin, E.E.},
	year = {2023},
	keywords = {Information systems, Information use, Mapping, It focus, Systematic mapping, Quality control, Data mining, Business Process, Business process management, Enterprise resource management, Event logs, Log preparation, Log preprocessing, Process analysis tool, Process management, Process mining, Business Process Management, Log Preparation, Log Preprocessing, Process Mining},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{jena_systematic_2023-1,
	title = {Systematic {Literature} {Review} on {Object} {Oriented} {Software} {Testing} {Techniques}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159096103&doi=10.1109%2fICIDCA56705.2023.10100236&partnerID=40&md5=bec544b3c28fab8be6514087f8c26751},
	abstract = {The software industry is quickly adopting the object-oriented paradigm. OO is now widely recognized as the ideal paradigm for designing complex systems. Since flaws might be added over the course of the software development process, the finished result requires to be tested. Many software testing approaches available to test object-oriented software are thoroughly surveyed in this study. Because of principles like inheritance, polymorphism, and others, software built using object-oriented technology presents testing issues. It is crucial to trace where each object is formed and where that definition is referenced to monitor how each object behaves over its lifetime. This means that the capacity of test cases to uncover flaws is crucial to testing. Although the creation of test cases is essential for the testing process therefore it is the primary focus for study in the analysis of software testing. The entire process of thoroughly analyzing and categorizing primary studies took four months. The test cases' efficacy guarantees the system's quality while lowering the risk of system failure. Although there are several methods to approach this problem, it is obvious that a full study of the issue and a step-by-step resolution are needed. For various strategies utilized in the OO system, authors have examined and reviewed several articles. A complete study of the various OO-based Testing Techniques has been examined and reviewed by this methodical review of the literature. The recommendations made in the review can be used by future scholars to close the research gaps. It was discovered from this review that using the right software testing methods can lower the likelihood of system failure.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Jena, D. and Kumari, A. and Titoria, J. and Rathee, N. and Kumar, B.},
	year = {2023},
	keywords = {Systematic literature review, Software design, Software testing, Software testings, Object oriented programming, Test case, Software industry, Biomimetics, Class testing, Nature inspired technique, Object-Oriented Software Testing, Software testing techniques, System failures, Testing technique, Systematic Literature Review, Class Testing, Nature inspired techniques, Object oriented software testing, Testing techniques},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{iftikhar_catalog_2023-1,
	title = {A {Catalog} of {Source} {Code} {Metrics} – {A} {Tertiary} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161231906&doi=10.1007%2f978-3-031-31488-9_5&partnerID=40&md5=e8e0766c41b52cfff333dbcc80900e58},
	abstract = {Context: A large number of source code metrics are reported in the literature. It is necessary to systematically collect, describe and classify source code metrics to support research and practice. Objective: We aim to utilize existing secondary studies to develop a catalog of source code metrics together with their descriptions. The catalog will also provide information about which units of code (e.g., operators, operands, lines of code, variables, parameters, code blocks, or functions) are used to measure the internal quality attributes and the scope on which they are collected. Method: We conducted a tertiary study to identify secondary studies reporting source code metrics. We have classified the source code metrics according to the measured internal quality attributes, the units of code used in the measures, and the scope at which the source code metrics are collected. Results: From 711 secondary studies, we identified 52 relevant secondary studies. We reported 423 source code metrics together with their descriptions and the internal quality attributes they measure. Source code metrics predominantly incorporate function as a unit of code to measure internal quality attributes. In contrast, several source code metrics use more than one unit of code when measuring internal quality attributes. Nearly 51\% of the source code metrics are collected at the class scope, while almost 12\% and 15\% of source code metrics are collected at module and application levels, respectively. Conclusions: Researchers and practitioners can use the extensive catalog to assess which source code metrics meet their individual needs based on the description and classification scheme presented. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Iftikhar, U. and Ali, N.B. and Börstler, J. and Usman, M.},
	year = {2023},
	keywords = {Tertiary study, Codes (symbols), Code quality, Computer programming languages, Internal quality, Quality attributes, Source code metrics, Code measurement, Internal quality attribute, Line of codes, Number of sources, Variable-parameters, Internal quality attributes},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{adinegoro_comparison_2023-1,
	title = {Comparison of {UI}/{UX} {Development} {Using} {Design} {Thinking} vs {Lean} {UX} : {A} {Comparative} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181532115&doi=10.1109%2fICE3IS59323.2023.10335225&partnerID=40&md5=4108ff0d37fc6ba7cb414bfaa977fe1c},
	abstract = {The User Experience (UX) development method plays an important role in ensuring the successful design of the user experience of an application that is effective, satisfying and meets user needs. This study compares two popular methods that are often used in developing UX, namely Design Thinking and Lean UX. The purpose of this study is to evaluate the advantages, disadvantages and focus of each method in developing UX designs. The results of this study indicate that the Design Thinking method focuses more on understanding the user or is more user-centered, while Lean UX focuses more on flexibility and fast iteration as well as continuous experimentation or testing. Based on this, it is interesting to further investigate how a team determines which method to use in their UX design project, considering that both methods have their own strengths and weaknesses, and there are various factors that influence the UX design development stage, such as resources, time, project scale, and others. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Adinegoro, R. and Suakanto, S. and Fakhrurroja, H. and Hardiyanti, M.},
	year = {2023},
	keywords = {Design, User interfaces, Users' experiences, Iterative methods, Design thinking, Development method, Comparatives studies, Design development, Design programs, Development stages, Lean UX, User experience, User need, User-centred, Well testing, User Experience, Design Thinking, User Interface},
	annote = {Export Date: 23 June 2024},
}

@article{li_reproducible_2023-1,
	title = {Reproducible {Searches} in {Systematic} {Reviews}: {An} {Evaluation} and {Guidelines}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165868236&doi=10.1109%2fACCESS.2023.3299211&partnerID=40&md5=4c299fd1736b7dade073a0f83544d7d7},
	abstract = {[Context:] The Systematic Review is promoted as a more reliable way of producing a high-quality review of prior research. But there are a range of threats that can undermine the reliability and quality of such reviews. One threat is the reproducibility of automated searches. [Objectives:] To evaluate the state-of-practice of reproducible searches in secondary studies, and to consider ways to improve the reproducibility of searches. [Method:] We re-run the searches of 621 secondary studies and analyse the outcomes of those (attempted) re-runs. We use the outcomes, and our experience of re-running the searches, to propose ways to improve the reproducibility of automated searches. [Results:] With the 621 studies, more than 50\% of the literal search strings (ignoring other settings) are not reusable; about 87\% of the searches (e.g., with settings) cannot be repeated; and around 94\% of the searches (including all elements of the search) are irreproducible. We propose guidelines for automated search, directing particular attention at the formulation of search strings. [Conclusion:] While some aspects of automated search are beyond the direct control of researchers (e.g., variations in features, constraints and performance of search engines), many aspects can be effectively managed through more careful formulation and execution of the search strings themselves, and of the search settings. While the results of our evaluation are disappointing there are many simple, concrete steps that researchers can make to improve the reproducibility of their searches.  © 2013 IEEE.},
	author = {Li, Z. and Rainer, A.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Systematic Review, systematic review, Search engines, Evidence Based Software Engineering, Automation, Systematic, Quality control, Guideline, search engine, Software reliability, Reproducibilities, Automated searches, Reproducibility of result, Search problem, Secondary study, Automated search, evidence based software engineering, reproducibility, secondary study},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{silva_systematic_2023-1,
	title = {Systematic {Literature} {Review} of the {Use} of {Virtual} {Reality} in the {Inclusion} of {Children} with {Autism} {Spectrum} {Disorders} ({ASD})},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172211707&doi=10.1007%2f978-3-031-40113-8_49&partnerID=40&md5=25bb4d5c9139fb13f06871578bff30cf},
	abstract = {Virtual reality (VR) technologies have been evolving in recent decades, allowing simulating real-life situations in controlled and safe virtual environments, where they reveal increasingly realistic details. There is an increase in the number of publications on virtual reality interventions in different areas, especially in Education, particularly in interventions with children diagnosed with Autism Spectrum Disorders (ASD). The lack of social skills prevents these children diagnosed with ASD to respond appropriately and adapt to the most diverse daily social situations. On this basis, VR has revealed a set of evidences that present promising results and show great acceptance among the diversified population with ASD. In order to understand how VR may contribute to the improvement of skills, allowing their inclusion, we conducted a systematic review of the literature. We present considerations on the selected studies, identifying the main gaps and pointing out possible directions for future research. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Silva, R.M. and Carvalho, D. and Martins, P. and Rocha, T.},
	year = {2023},
	keywords = {Systematic literature review, Systematic Review, E-learning, Diseases, Virtual reality, Autism spectrum disorders, Children with autisms, Main Gap, Social skills, Virtual reality technology, Autism Spectrum Disorder, Education, Virtual Environment, Virtual Reality},
	annote = {Export Date: 23 June 2024},
}

@article{liu_information_2023-1,
	title = {Information quality of conversational agents in healthcare},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159121148&doi=10.1177%2f02666669231172434&partnerID=40&md5=0f4aa9701b5b8b1e355e46fa0ce52f4a},
	abstract = {Artificial Intelligence has found applications in a wide range of fields, including conversational agents designed for healthcare services. The quality of healthcare services greatly depends on the quality of the information provided by the agents. Achieving quality-assured information from conversational agents to support effective decision-making remains as a significant challenge in healthcare. Although prior review studies have shown an interest in investigating the information quality (IQ) of conversational agents in healthcare, no systematic review has been performed to present IQ definitions, factors influencing IQ, and IQ impacts. We conducted a systematic review of 45 articles published up to 2021 to investigate IQ definitions, factors influencing IQ, and IQ impacts in the context of conversational agents applied in healthcare. The findings of this review are integrated into a conceptual framework for the IQ research program in the context of conversational agents in healthcare, which has not been received attention in the literature, guiding future research directions. The present study also discusses implications for both researchers and practitioners to enhance the agents’ IQ and improve the quality of health-related services. © The Author(s) 2023.},
	author = {Liu, C. and Zowghi, D. and Peng, G. and Kong, S.},
	year = {2023},
	note = {Publisher: SAGE Publications Ltd},
	keywords = {systematic review, artificial intelligence, conversational agents, healthcare, information quality},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{de_souza_using_2023-1,
	title = {Using {Experimentation} to {Evaluate} {Security} {Requirements} in {IoT} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184815056&doi=10.1109%2fSIoT60039.2023.10390013&partnerID=40&md5=470cbe05f85e03a3fc7e6a659e834c21},
	abstract = {Security requirements are critical success factors for Internet of Things (IoT) software systems due to how they can mitigate vulnerabilities, for instance, prevent unauthorized access to system and device data by third parties, assuring the final quality of the software system. Then, problems related to security requirements and vulnerabilities must be addressed in the early stage of IoT development projects. In this way, Continuous Experimentation (CE) is a promising software construction practice to observe alternative security and vulnerability solutions. Thus, this paper evaluates security requirements based on the vulnerabilities of IoT software systems using such CE. First, we identified an evidence-based set of IoT vulnerability issues to be assessed. Thus, this work reports an exploratory study using CE to mitigate some vulnerabilities in IoT software systems, indicating that not all security requirements can be worked out with CE. Therefore, further studies are necessary to categorize the IoT software systems vulnerabilities that can be mitigated using continuous experimentation.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {De Souza, B.P. and De Paiva, B.D. and Travassos, G.H.},
	year = {2023},
	keywords = {Computer software, Software-systems, Internet of things, Empirical Software Engineering, Cryptography, Development programmes, Device data, IS critical success factors, Security requirements, Security vulnerabilities, Third parties, Unauthorized access, Vulnerability, empirical software engineering, internet of things, security requirements, vulnerabilities},
	annote = {Export Date: 23 June 2024},
}

@article{buchgeher_using_2023-1,
	title = {Using {Architecture} {Decision} {Records} in {Open} {Source} {Projects} - {An} {MSR} {Study} on {GitHub}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162914500&doi=10.1109%2fACCESS.2023.3287654&partnerID=40&md5=04585b93cdd9a9d4124142acf3977fb7},
	abstract = {Architecture decision records (ADRs) have been proposed as a resource-efficient means for capturing architectural design decisions (ADDs), and have received attention not only from researchers but also from practitioners. We conducted a mining software repositories (MSR) study, in which we analyzed the use of ADRs in open source repositories at GitHub. Our results show that the adoption of ADRs is still low, although the number of repositories using ADRs is increasing every year. About 50\% of all repositories with ADRs contain just one to five ADRs suggesting that the concept has been tried but not yet definitively adopted. In repositories that use ADRs more systematically, we observed that recording decisions is a team activity conducted by two or more users over a longer period of time. In most repositories the template proposed by Michael Nygrad is used. We, finally, provide an interpretation of the obtained results and discuss open future research challenges by elaborating on implications of the study's findings as well as on recommendations on how to further increase the adoption of ADRs.  © 2013 IEEE.},
	author = {Buchgeher, G. and Schoberl, S. and Geist, V. and Dorninger, B. and Haindl, P. and Weinreich, R.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Decision making, Software design, Open source software, Knowledge management, Software architecture, Open systems, Open-source softwares, Mining software, Mining software repository, Software development management, Software repositories, Secondary study, Architecture decision record, Architecture decisions, Github, Open source projects, Software architecture knowledge managements, mining software repositories, secondary study, Architecture decision records, GitHub, open source projects, software architecture, software architecture knowledge management},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{alshahwan_software_2023-1,
	title = {Software {Testing} {Research} {Challenges}: {An} {Industrial} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161898000&doi=10.1109%2fICST57152.2023.00008&partnerID=40&md5=1d3e066d49cf0d1adbddebd59727ba72},
	abstract = {There have been rapid recent developments in automated software test design, repair and program improvement. Advances in artificial intelligence also have great potential impact to tackle software testing research problems. In this paper we highlight open research problems and challenges from an industrial perspective. This perspective draws on our experience at Meta Platforms, which has been actively involved in software testing research and development for approximately a decade. As we set out here, there are many exciting opportunities for software testing research to achieve the widest and deepest impact on software practice. With this overview of the research landscape from an industrial perspective, we aim to stimulate further interest in the deployment of software testing research. We hope to be able to collaborate with the scientific community on some of these research challenges.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Alshahwan, N. and Harman, M. and Marginean, A.},
	year = {2023},
	keywords = {Artificial intelligence, Software testing, Automation, Repair, Industrial research, Software testings, Artificial Intelligence, Automated program repair, Automated remediation, Automated software engineering, Genetic improvements, Research challenges, Research problems, Test designs, Test projects, Test repair, Software Testing, Automated Program Repair, Automated Remediation, Automated Software Engineering, Genetic Improvement},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{karras_divide_2023-1,
	title = {Divide and {Conquer} the {EmpiRE}: {A} {Community}-{Maintainable} {Knowledge} {Graph} of {Empirical} {Research} in {Requirements} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666312&doi=10.1109%2fESEM56168.2023.10304795&partnerID=40&md5=89b0f483a3169d34b96c76308bbd4fe1},
	abstract = {[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews.  © 2023 IEEE.},
	publisher = {IEEE Computer Society},
	author = {Karras, O. and Wernlein, F. and Klunder, J. and Auer, S.},
	year = {2023},
	keywords = {Literature reviews, Requirement engineering, Requirements engineering, 'current, Empirical research, Research communities, Divide-and-conquer, Infrastructure, Knowledge graph, Knowledge graphs, Long-term goals, Technical infrastructure, requirements engineering, literature review, sustainability, empirical research, infrastructure},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{olsson_all_2023-1,
	title = {All data is equal or is some data more equal? {On} strategic data collection and use in the embedded systems domain},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326861&doi=10.1109%2fSEAA60479.2023.00056&partnerID=40&md5=0daed7f6ec9ae9aa3afd9e4c555cb505},
	abstract = {Effective collection and use of data is key for companies across domains and it is only increasing in importance. For companies in the embedded systems domain, data constitutes the basis not only for quality assurance and diagnostics of their systems but also for new service development and innovation. For these companies, data is an enabler for continuous delivery of customer value and hence, a key asset for entirely new and recurring revenue streams. However, effective use of data requires careful collection of different kinds of data depending on the purpose and context for which it is intended to be used. In this paper, we identify the challenges that companies experience in their contemporary data practices and we outline the kinds of data that companies need to collect as they evolve through different maturity stages. In addition, we provide concrete guidance on the specific data to collect during each maturity stage.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Olsson, H.H. and Bosch, J.},
	year = {2023},
	keywords = {Embedded systems, Embedded-system, Quality assurance, Data acquisition, Data collection, Data collectio, Data exploitatio, Data practices, Maturity stages, New service development, Service innovation, Strategic data, System domain, data collectio, data exploitatio, data practice, maturity stage},
	annote = {Export Date: 23 June 2024},
}

@article{gaol_continuous_2023,
	title = {Continuous {Software} {Engineering} for {Augmented} {Reality}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799327&doi=10.14569%2fIJACSA.2023.0140719&partnerID=40&md5=783a913267516c101f5c106d990de433},
	abstract = {Continuous software engineering is a new trend that has attracted increasing attention from the research community in recent years. In software engineering there are “continuous” stages that are used depending on the number of artifact repositories such as databases, meta data, virtual machines, networks and servers, various logs, and reports. Augmented Reality (AR) technology is currently growing rapidly. We can find this technology in various fields of life, but unfortunately sustainable software engineering for Augmented Reality is not found. The method shown in previous research is a general method in software engineering so that a theory is needed for sustainable software engineering for AR considering that AR is not just an ordinary application but there are 3D elements and specific components that must be met so that it can be called AR. The main idea behind this research is to find a continuous pattern from the stages of the existing method so far. For example, in general the stages of system development are planning, analysis, design, implementation and maintenance. Then after the application has been built, does it finish there? As we know software always grows and develops according to human needs. Therefore, there are continuous stages that must be patterned so that the life cycle process can be maintained. In this paper we present our initial findings about the continuous stages of continuous software engineering namely continuous planning, continuous analysis, continuous design, continuous programming, continuous integration, and continuous maintenance. © 2023, Science and Information Organization. All Rights Reserved.},
	author = {Gaol, F.L. and Oktavia, T.},
	year = {2023},
	note = {Publisher: Science and Information Organization},
	keywords = {Life cycle, Application programs, Augmented reality, Continuous integrations, Continuous software engineerings, Research communities, Computer programming, Continuous analysis, Continuous design, Continuous maintenance, Continuous planning, Continuous programming, Maintenance, Method in software engineering, Sustainable softwares, Continuous software engineering, augmented reality, continuous integration, continuous analysis, continuous design, continuous maintenance, continuous planning, continuous programming, method in software engineering},
	annote = {Export Date: 23 June 2024},
}

@article{stradowski_exploring_2023-1,
	title = {Exploring the challenges in software testing of the {5G} system at {Nokia}: {A} survey},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138479890&doi=10.1016%2fj.infsof.2022.107067&partnerID=40&md5=0095b98ee47f2ca332b69f85235a5d54},
	abstract = {Context: The ever-growing size and complexity of industrial software products pose significant quality assurance challenges to engineering researchers and practitioners, despite the constant effort to increase knowledge and improve the processes. 5G technology developed by Nokia is one example of such a grand and highly complex system with improvement potential. Objective: The following paper provides an overview of the current quality assurance processes used by Nokia to develop the 5G technology and provides insight into the most prominent challenges by an evaluation of perceived importance, urgency, and difficulty to understand the future opportunities. Method: Nokia mode of operation, briefly introduced in this paper, has been subjected to extensive analysis by a selected group of experienced test-oriented professionals to define the most critical areas of concern. Secondly, the identified problems were evaluated by Nokia gNB system-level test professionals in a dedicated survey. Results: The questionnaire was completed by 312 out of 2935 (10.63\%) possible respondents. The challenges are seen as the most important and urgent: customer scenario testing, performance testing, and competence ramp-up. Challenges seen as the most difficult to solve are low occurrence failures, hidden feature dependencies, and hardware configuration-specific problems. Conclusions: Our research identified several improvement areas in the quality assurance processes used to develop the 5G technology by determining the most important and urgent problems that at the same time have a low perceived difficulty. Such initiatives are attractive from a business perspective. On the other hand, challenges seen as the most impactful yet difficult may be of interest to the academic research community. © 2022 The Author(s)},
	author = {Stradowski, S. and Madeyski, L.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Software testing, Computer software selection and evaluation, Quality control, 5G mobile communication systems, Surveys, Quality assurance, \% reductions, 5g technology, Efficiency improvement, Engineering challenges, Quality assurance process, Software engineering challenge, Software quality assurance, System level testing, Test effort reduction, Test efforts, 5G technology, Software engineering challenges},
	annote = {Export Date: 23 June 2024},
}

@book{scutari_pragmatic_2023-1,
	title = {The {Pragmatic} {Programmer} for {Machine} {Learning}: {Engineering} {Analytics} and {Data} {Science} {Solutions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183218511&doi=10.1201%2f9780429292835&partnerID=40&md5=8d4692d72420d4b30ed26d6a7d295edb},
	abstract = {Machine learning has redefined the way we work with data and is increasingly becoming an indispensable part of everyday life. The Pragmatic Programmer for Machine Learning: Engineering Analytics and Data Science Solutions discusses how modern software engineering practices are part of this revolution both conceptually and in practical applictions. Comprising a broad overview of how to design machine learning pipelines as well as the state-of-the-art tools we use to make them, this book provides a multi-disciplinary view of how traditional software engineering can be adapted to and integrated with the workflows of domain experts and probabilistic models. From choosing the right hardware to designing effective pipelines architectures and adopting software development best practices, this guide will appeal to machine learning and data science specialists, whilst also laying out key high-level principlesin a way that is approachable for students of computer science and aspiring programmers. © 2023 Marco Scutari and Mauro Malvestio.},
	publisher = {CRC Press},
	author = {Scutari, M. and Malvestio, M.},
	year = {2023},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{denecke_investigating_2023-1,
	title = {Investigating conversational agents in healthcare: {Application} of a technical-oriented taxonomy},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164261105&doi=10.1016%2fj.procs.2023.01.413&partnerID=40&md5=1ec35be7bce132042a4312cfb84fbe07},
	abstract = {Conversational agents (CA) are increasingly applied to realize health applications that collect patient data, provide information or even deliver health interventions. We developed a taxonomy focusing on technical characteristics of health CA with the purpose of creating a reporting guideline towards health CA and of building technical-oriented archetypes. The taxonomy comprises 18 dimensions which can be grouped into four perspectives. In this work, we wanted to find out whether the taxonomy is complete and can be applied appropriately by researcher to describe the technical characteristics of their health CA. Through a literature review, we identified 103 unique health CA for which publications have been published in 2021 and 2022. We contacted the corresponding or first authors of those papers asking for providing the information along our taxonomy for the CA described in their paper. For this purpose, our taxonomy was transformed into a questionnaire. To study applicability and understandability of the taxonomy, we also extracted the requested information from the papers using the taxonomy and compared the results to those of the participants. 95 E-Mails could be delivered. 26 persons out of 95 replied to our request resulting in a return rate of 27.3\%. Results show that the majority of CA is simple in terms of CA personality; visualized as avatar or without embodiment. Systems are mainly rule-based, domain-specific and support one language. We recognized several differences between replies given by the participants and what has been extracted from the publications on the CA by us. We conclude that in order to apply the taxonomy as reporting guideline clear definitions must be given for the single characteristics. Some additional characteristics have to be added. © 2023 Elsevier B.V.. All rights reserved.},
	publisher = {Elsevier B.V.},
	author = {Denecke, K. and May, R.},
	year = {2023},
	keywords = {Literature reviews, Taxonomies, Chatbots, Understandability, taxonomy, Conversational agents, Health care application, Health chatbot, Health interventions, Hospital data processing, Patient data, Patient treatment, Simple++, Technology, conversational agents, health chatbots, technology},
	annote = {Export Date: 23 June 2024},
}

@article{lagos_electric_2023-1,
	title = {Electric {Vehicles} and the {Use} of {Demand} {Projection} {Models}: {A} {Systematic} {Mapping} of {Studies}},
	shorttitle = {Vehículos eléctricos y el uso de modelos de proyección de demanda: un mapeo sistemático de estudios},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145909076&doi=10.15446%2fing.investig.99251&partnerID=40&md5=185c4ab25d57790fba1e7cc50593cd3d},
	abstract = {In today’s world, electric vehicles have become a real solution to the problem of pollution caused by petrol and diesel-powered vehicles. However, incorporating them successfully into the global vehicle park poses new challenges. Some of these challenges have to do with meeting the electricity demand, providing the physical installations for charging, and the size and capacity of the electric grid required to deliver the necessary supply. Solving these new problems requires determining or projecting the electrical and/or physical requirements involved, but there is no single model or methodology to do this, nor any single document which summarizes the existing information. To address this situation, this work presents the result of a systematic mapping study that seeks to provide organized information about the (mathematical) models for the demand arising from electric vehicles, as well as to answer a series of questions posed for this research. The results obtained show that there is a wide variety of models used to determine demand requirements –of either physical or electrical elements– in which mathematical modelling and operations research tools are normally used. Other results indicate that demand models are mainly focused on the electrical requirements rather than on physical ones, and that, in most cases, the type of vehicle for which the demand is studied is not mentioned. © Universidad Nacional de Colombia.},
	author = {Lagos, D. and Mancilla, R. and Reinecke, C. and Leal, P.},
	year = {2023},
	note = {Publisher: Universidad Nacional de Colombia},
	keywords = {electric vehicles, demand, models, systematic mapping},
	annote = {Export Date: 23 June 2024},
}

@article{ciesla_analysis_2023-1,
	title = {{AN} {ANALYSIS} {OF} {THE} {IMPLEMENTATION} {OF} {ACCESSIBILITY} {TOOLS} {ON} {WEBSITES}},
	shorttitle = {{ANALIZA} {IMPLEMENTACJI} {NARZĘDZI} {DOSTĘPNOŚCI} {NA} {STRONACH} {WWW}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180501621&doi=10.35784%2fiapgos.4459&partnerID=40&md5=a147574e2d2512c8186ffed285bd14dc},
	abstract = {The websites of higher education institutions, due to the fact that they are addressed to multiple stakeholder groups, not only need to have an appropriately designed information structure but must also be useful. Additionally, in the case of public universities, their services are expected to be accessible to the widest possible audience, especially for people with disabilities. The accessibility tools used on websites should be quickly located, easily identifiable and user-friendly. So far, no standards have been developed regarding these issues, and therefore, there are various solutions on the web. The objective of this study is to analyze various implementations of accessibility tools on university websites in terms of their location, form of presentation and ways that enable access to them. A study was conducted in which web interfaces were evaluated with the participation of users. The experiment consisted of two parts: the first one used the eye tracking technique, whereas in the second one, a survey was conducted. The research material was prototypes of websites from four different universities. Each website had two versions differing in implementation of accessibility tools. In the study, 35 participants were divided into two groups of people. Each group was shown one of the two sets of website prototypes and the users were tasked with finding and activating a specific accessibility tool. After exploring the websites, each participant completed a questionnaire that pertained to their opinions regarding aspects such as appearance, placement and a way to access tools dedicated to people with disabilities. The obtained data, processed to the form of heatmaps and fixation maps, were subjected to a qualitative analysis. The survey results and eye tracking data were analyzed quantitatively. On the basis of performed analyzes it can be concluded that the following factors have an impact on the reduction in efficiency and productivity of users: placement of accessibility tools on university websites in a place other than the upper right corner, an indirect access to t hese tools or their non-standard appearance. © 2023, Politechnika Lubelska. All rights reserved.},
	author = {Cieśla, M. and Dzieńkowski, M.},
	year = {2023},
	note = {Publisher: Politechnika Lubelska},
	keywords = {eye tracking, A/B tests, web accessibility, web usability},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{ansyah_usability_2023-1,
	title = {Usability {Testing} of {User} {Experience} and {User} {Interface} {Design} on {Mobile} {Map} {Applications}: {A} {Comparative} {Study} of {User} {Perception} and {Interaction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180361940&doi=10.1109%2fICTS58770.2023.10330882&partnerID=40&md5=76aa72c4a0d6b18a15a12a75a0cdd186},
	abstract = {Map applications play a crucial role in everyday life, assisting users in finding locations and optimizing travel routes. This study aims to evaluate the usability of map applications, focusing on UX testing using GOMS and SUS techniques and UI testing using the A/B Testing method. The results show the effectiveness of the three applications (GMaps, Petal, and Waze) in completing the given tasks. GMaps demonstrated the fastest task completion time and the highest SUS score, followed by Petal and Waze. GMaps' advantage may be due to most respondents being accustomed to using Android and the native applications of this operating system. Furthermore, A/B Testing of UI elements revealed a nearly balanced preference between GMaps and Petal, indicating Petal's potential despite not being an Android-native application. Based on these findings, the development of map applications with good usability can be achieved by combining the strengths of each tested application.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ansyah, A.S.S. and Masruri, M.Z. and Rochimah, S.},
	year = {2023},
	keywords = {User interfaces, Users' experiences, Usability engineering, Comparatives studies, Android (operating system), GOMS, Mobile-map application, SUS, Travel routes, Usability testing, User interaction, User interface designs, User perceptions, User Experience, Maps, Usability Testing},
	annote = {Export Date: 23 June 2024},
}

@article{kantsepolsky_exploring_2023-1,
	title = {Exploring {Quantum} {Sensing} {Potential} for {Systems} {Applications}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151562593&doi=10.1109%2fACCESS.2023.3262506&partnerID=40&md5=65f96769350f4d726502220c429403ef},
	abstract = {The current rise of quantum technology is compelled by quantum sensing research. Thousands of research labs are developing and testing a broad range of sensor prototypes. However, there is a lack of knowledge about specific applications and real-world use cases where the benefits of these sensors will be most pronounced. This study presents a comprehensive review of quantum sensing state-of-practice. It also provides a detailed analysis of how quantum sensing overcomes the existing limitations of sensor-driven systems' precision and performance. Based on the review of over 500 quantum sensor prototype reports, we determined four groups of quantum sensors and discussed their readiness for commercial usage. We concluded that quantum magnetometry and quantum optics are the most advanced sensing technologies with empirically proven results. In turn, quantum timing and kinetics are still in the early stages of practical validation. In addition, we defined four systems domains in which quantum sensors offer a solution for existing limitations of conventional sensing technologies. These domains are 1) GPS-free positioning and navigating services, 2) time-based operations, 3) topological visibility, and 4) environment detection, prediction, and modeling. Finally, we discussed the current constraints of quantum sensing technologies and offered directions for future research. © 2013 IEEE.},
	author = {Kantsepolsky, B. and Aviv, I. and Weitzfeld, R. and Bordo, E.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Quantum optics, Current rise, Extraterrestrial measurements, Magnetometers, Magnetometry, Quantum information science, Quantum sensing, Quantum sensing technology, Quantum sensors, Quantum system, Sensing technology, Sensitivity, Superconducting magnets, System applications, Temperature measurement, quantum computing, quantum information science, quantum sensors, quantum systems},
	annote = {Export Date: 23 June 2024},
}

@incollection{danilovaite_acoustic_2023-1,
	title = {Acoustic {Analysis} for {Vocal} {Fold} {Assessment}—{Challenges}, {Trends}, and {Opportunities}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149948719&doi=10.1007%2f978-3-031-24453-7_8&partnerID=40&md5=f4509bb5138c74c510919fefff4baa37},
	abstract = {The goal of this study was a review of trends in non-invasive vocal fold assessment to identify the significance of acoustic analysis within the scope of proposed methods. A review protocol for selected relevant studies was developed using systematic review guidelines. A classification scheme was applied to process the selected relevant study set, data were extracted and mapped in a systematic map. A systematic map was used to synthesize data for a quantitative summary of the main research question. A tabulated summary was created to summarize supporting topics. Results show that non-invasive vocal fold assessment is influenced by general computer science trends. Machine learning techniques dominate studies and publications, i.e., 51\% of the set used at least one method to detect and classify vocal fold pathologies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Danilovaitė, M. and Tamulevičius, G.},
	year = {2023},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{pauzi_descriptive_2023-1,
	title = {From {Descriptive} to {Predictive}: {Forecasting} {Emerging} {Research} {Areas} in {Software} {Traceability} {Using} {NLP} from {Systematic} {Studies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160566966&doi=10.5220%2f0011964100003464&partnerID=40&md5=f46987b89f92b2b7189014ceb75f3cad},
	abstract = {Systematic literature reviews (SLRs) and systematic mapping studies (SMSs) are common studies in any discipline to describe and classify past works, and to inform a research field of potential new areas of investigation. This last task is typically achieved by observing gaps in past works, and hinting at the possibility of future research in those gaps. Using an NLP-driven methodology, this paper proposes a meta-analysis to extend current systematic methodologies of literature reviews and mapping studies. Our work leverages a Word2Vec model, pre-trained in the software engineering domain, and is combined with a time series analysis. Our aim is to forecast future trajectories of research outlined in systematic studies, rather than just describing them. Using the same dataset from our own previous mapping study, we were able to go beyond descriptively analysing the data that we gathered, or to barely 'guess' future directions. In this paper, we show how recent advancements in the field of our SMS, and the use of time series, enabled us to forecast future trends in the same field. Our proposed methodology sets a precedent for exploring the potential of language models coupled with time series in the context of systematically reviewing the literature. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	publisher = {Science and Technology Publications, Lda},
	author = {Pauzi, Z. and Capiluppi, A.},
	year = {2023},
	keywords = {Software engineering, Systematic Review, Mapping, Systematic mapping studies, Natural language processing systems, Natural languages, Forecasting, Mapping studies, Language processing, Natural language processing, Research areas, Engineering research, Software traceability, Systematic study, Time series analysis, Times series, Natural Language Processing, Software Traceability, Time Series},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{chadbourne_applications_2023-1,
	title = {Applications of {Causality} and {Causal} {Inference} in {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770994&doi=10.1109%2fSERA57763.2023.10197835&partnerID=40&md5=4585f442f1486a0ef77c40af2d43d737},
	abstract = {Causal inference is a study of causal relationships between events and the statistical study of inferring these relationships through interventions and other statistical techniques. Causal reasoning is any line of work toward determining causal relationships, including causal inference. This paper explores the relationship between causal reasoning and various fields of software engineering. This paper aims to uncover which software engineering fields are currently benefiting from the study of causal inference and causal reasoning, as well as which aspects of various problems are best addressed using this methodology. With this information, this paper also aims to find future subjects and fields that would benefit from this form of reasoning and to provide that information to future researchers. This paper follows a systematic literature review, including; the formulation of a search query, inclusion and exclusion criteria of the search results, clarifying questions answered by the found literature, and synthesizing the results from the literature review. Through close examination of the 45 found papers relevant to the research questions, it was revealed that the majority of causal reasoning as related to software engineering is related to testing through root cause localization. Furthermore, most causal reasoning is done informally through an exploratory process of forming a Causality Graph as opposed to strict statistical analysis or introduction of interventions. Finally, causal reasoning is also used as a justification for many tools intended to make the software more human-readable by providing additional causal information to logging processes or modeling languages.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Chadbourne, P. and Eisty, N.U.},
	year = {2023},
	keywords = {Systematic literature review, Software testing, Application programs, Modeling languages, Inclusion and exclusions, Causal inferences, Causal reasoning, Causal relationships, Causality graph, Engineering fields, Professional aspects, Search queries, Statistical study, Statistical techniques, systematic literature review, software engineering, causal inference, causal reasoning, causality graph},
	annote = {Export Date: 23 June 2024},
}

@article{pinciroli_modeling_2023-1,
	title = {Modeling more software performance antipatterns in cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180204421&doi=10.1007%2fs10270-023-01137-x&partnerID=40&md5=4d0a00e3145240ff506af59818085186},
	abstract = {The design of cyber-physical systems (CPS) is challenging due to the heterogeneity of software and hardware components that operate in uncertain environments (e.g., fluctuating workloads), hence they are prone to performance issues. Software performance antipatterns could be a key means to tackle this challenge since they recognize design problems that may lead to unacceptable system performance. This manuscript focuses on modeling and analyzing a variegate set of software performance antipatterns with the goal of quantifying their performance impact on CPS. Starting from the specification of eight software performance antipatterns, we build a baseline queuing network performance model that is properly extended to account for the corresponding bad practices. The approach is applied to a CPS consisting of a network of sensors and experimental results show that performance degradation can be traced back to software performance antipatterns. Sensitivity analysis investigates the peculiar characteristics of antipatterns, such as the frequency of checking the status of resources, that provides quantitative information to software designers to help them identify potential performance problems and their root causes. Quantifying the performance impact of antipatterns on CPS paves the way for future work enabling the automated refactoring of systems to remove these bad practices. © 2023, The Author(s).},
	author = {Pinciroli, R. and Smith, C.U. and Trubiani, C.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Embedded systems, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Sensitivity analysis, Performances analysis, Anti-patterns, Model-based OPC, Model-based performance analyse, Performance impact, Software modeling, Software performance, Software performance antipattern, Model-based performance analysis, Software performance antipatterns},
	annote = {Export Date: 23 June 2024},
}

@article{zervogianni_user-based_2023-1,
	title = {A user-based information rating scale to evaluate the design of technology-based supports for autism},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159588062&doi=10.1007%2fs10209-023-00995-y&partnerID=40&md5=0737802f2d1107ce78721c9b82cf6935},
	abstract = {The present study aimed to merge expertise from evidence-based practice and user-centered design to develop a rating scale for considering user input and other sources of information about end-users in studies reporting on the design of technology-based support for autism. We conducted a systematic review of the relevant literature to test the reliability and validity of the scale. The scale demonstrated acceptable reliability and validity based on a randomized sample of 211 studies extracted from the output of the systematic review. The scale can help provide a more complete assessment of the quality of the design process of technology-based supports for autism and be beneficial to autistic people, their families, and related professionals in making informed decisions regarding such supports. © 2023, The Author(s).},
	author = {Zervogianni, V. and Fletcher-Watson, S. and Herrera, G. and Goodwin, M.S. and Triquell, E. and Pérez-Fuster, P. and Brosnan, M. and Grynszpan, O.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH},
	keywords = {Software engineering, Systematic Review, Evidence Based Software Engineering, Diseases, Evidence-based practices, Technology-based, User centered design, Autism, Digital technologies, Rating scale, Reliability and validity, Sources of informations, User input, Evidence-based practice, Evidence-based software engineering, User-centered design, Digital technology},
	annote = {Export Date: 23 June 2024},
}

@article{hanna_web_2023-1,
	title = {Web applications testing techniques: a systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153523073&doi=10.1504%2fIJWET.2022.129250&partnerID=40&md5=a26478fab291606ae9d407737eef2aaa},
	abstract = {Due to the importance of web application testing techniques for detecting faults and assessing quality attributes, many research papers were published in this field. For this reason, it became essential to analyse, classify and summarise the research in the field. To achieve this goal, this research conducted a systematic mapping study on 98 research papers in the field of web applications testing published between 2008 and 2021. The results showed that the most commonly used web applications testing techniques in literature are model-based testing and security testing. Besides, the most commonly used models in model-based testing are finite-state machines. The most targeted vulnerability in security testing is SQL injection. Test automation is the most targeted testing goal in both model-based and security testing. For other web applications testing techniques, the main goals of testing were test automation, test coverage, and assessing security quality attributes. Copyright © 2022 Inderscience Enterprises Ltd.},
	author = {Hanna, S. and Ahmad, A.A.-S.},
	year = {2023},
	note = {Publisher: Inderscience Publishers},
	keywords = {Model checking, Software testing, Mapping, Systematic mapping studies, Automation, Model based testing, Test-coverage, Testing technique, Security testing, SMS, Test Automation, Testing purpose, Web application testing, Web application testing technique, systematic mapping study, model-based testing, security testing, test automation, test coverage, testing purposes, web applications testing techniques},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{sukmandhani_recent_2023-1,
	title = {Recent {Trends} for {Text} {Summarization} in {Scientific} {Documents}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186658985&doi=10.1109%2fICCED60214.2023.10425025&partnerID=40&md5=ee6395ea43c82ca2bb7a21bae1fc9482},
	abstract = {Text summarization is a natural language processing (NLP) technique in artificial intelligence that has been studied in recent years. Every document containing text is tested to get a good summary result. In producing a good summary, proper accuracy is required; therefore, various techniques and methods are used, and another challenge is the use of language in the documents being tested. Various types of documents have been used as research for text summarization, including scientific documents, so researchers are interested in studying them more intensely to get the state of the art in research. This research was conducted to review previous research using literature studies from various publications that published research on text summarization in scientific documents. The researcher collected data from several publishers who had published this type of research from 1988 to 2023 quarter 1 and obtained three hundred and eleven articles, then the researcher analyzed the data using the Kitchenham method to get the right literature to answer the research question. Thirty-seven articles were selected for further in-depth analysis. In addition, the researcher found some interesting findings beyond the research question, including other techniques used to solve the text summarization problem and other types of output that are not commonly produced so that it can increase knowledge in researching text summarization. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Sukmandhani, A.A. and Arifin, Y. and Zarlis, M. and Budiharto, W.},
	year = {2023},
	keywords = {State of the art, Natural language processing systems, Natural languages, Literature studies, Research questions, Recent trends, In-depth analysis, Kitchenham, Language processing techniques, Scientific documents, Text processing, Text Summarisation, Scientific-Document, Text-summarization},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{malik_chess_2023-1,
	title = {{CHESS}: {A} {Framework} for {Evaluation} of {Self}-{Adaptive} {Systems} {Based} on {Chaos} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166300952&doi=10.1109%2fSEAMS59076.2023.00033&partnerID=40&md5=320addb57c1eb68c0bc46ef60c86add5},
	abstract = {There is an increasing need to assess the correct behavior of self-adaptive and self-healing systems due to their adoption in critical and highly dynamic environments. However, there is a lack of systematic evaluation methods for self-adaptive and self-healing systems. We proposed CHESS, a novel approach to address this gap by evaluating self-adaptive and self-healing systems through fault injection based on chaos engineering (CE).The artifact presented in this paper provides an extensive overview of the use of CHESS through two microservice-based case studies: a smart office case study and an existing demo application called Yelb. It comes with a managing system service, a self-monitoring service, as well as five fault injection scenarios covering infrastructure faults and functional faults. Each of these components can be easily extended or replaced to adopt the CHESS approach to a new case study, help explore its promises and limitations, and identify directions for future research.  © 2023 IEEE.},
	publisher = {IEEE Computer Society},
	author = {Malik, S. and Naqvi, M.A. and Moonen, L.},
	year = {2023},
	keywords = {Software testing, Case-studies, Adaptive systems, Self-adaptive system, Artifact, Chaos engineerings, Dynamic environments, Evaluation, Fault injection, Resilience, Self-healing, Self-healing materials, Self-healing systems, artifact, chaos engineering, evaluation, resilience, self-healing},
	annote = {Export Date: 23 June 2024},
}

@article{nunes_public_2023-1,
	title = {Public {Policies} for {Renewable} {Energy}: {A} {Review} of the {Perspectives} for a {Circular} {Economy}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145769847&doi=10.3390%2fen16010485&partnerID=40&md5=69befd59d2f418f67bae9cf3954f42a8},
	abstract = {The development and implementation of public policies towards renewable energies are crucial in order to address the contemporary challenges faced by humanity. The 3Rs (reduce, reuse, and recycle), as a circular economic practice, are often cited as one of the best solutions for sustainable development. Therefore, this study analyzed public policies for renewable energy from the perspective of the circular economy. Accordingly, a systematic review of the literature was carried out with respect to the beneficiaries and convergences of circularities, with a focus on public policies for renewable energies. The sample had public policies classified into three types (distributive, redistributive, and regulatory policies). The results showed that the first studies began in 1999, with a significant increase in publications during the 2010s, in which Germany was the country with the greatest contribution. The analyses associated with space showed the countries committed to the use of renewable energies and the 3Rs of the circular economy to reduce greenhouse gas emissions. The economic analyses revealed that the circular economy for the generation of renewable energy has a positive economic return in terms of social well-being and the mitigation of environmental degradation. There is a barrier to the circular economy’s development posed by the cost of its implementation in the private sector and the resistance to raising awareness in society, requiring strong public sector engagement in decision making and the constant evaluation of public policies. It is concluded that the circular economy facilitates more efficient, productive structures and public policies, promoting alternatives for energy security and sustainability for the world energy matrix. © 2023 by the authors.},
	author = {Nunes, A.M.M. and Coelho Junior, L.M. and Abrahão, R. and Santos Júnior, E.P. and Simioni, F.J. and Rotella Junior, P. and Rocha, L.C.S.},
	year = {2023},
	note = {Publisher: MDPI},
	keywords = {Systematic Review, Decision making, Reuse, Sustainable development, bibliometrics, Economic analysis, Bibliometric, Bio economy, Circular economy, Circularity, Classifieds, Clean energy, Energy policy, Energy security, Energy transitions, Gas emissions, Greenhouse gases, Renewable energies, bio economy, circularity, clean energy, energy transition, sustainable development},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{garcia-mireles_profile_2023-1,
	title = {A {Profile} of {Practices} for {Reporting} {Systematic} {Reviews}: {A} {Conference} {Case}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142725070&doi=10.1007%2f978-3-031-20322-0_3&partnerID=40&md5=cf245e893be2c48982af13f33f268194},
	abstract = {Several criticisms about the quality of reporting systematic reviews (SRs) have been published and new guidelines propose to use standardized instruments to report them. To identify practices for reporting SRs, I reviewed 32 SRs published in the International Conference on Software Process Improvement (CIMPS). Well reported practices are related to the execution of the automatic database search process and the identification of selection criteria. However, issues arise related to the completeness of the search process, procedures for dealing with inconsistencies during selection, extraction, and classification of data. Besides, validity threats only are addressed by a third of SRs. As a conclusion, the identification of reporting practices can help SR authors to identify both strengths and opportunities areas for conducting and reporting SRs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {García-Mireles, G.A.},
	year = {2023},
	keywords = {Systematic literature review, Systematic mapping study, Practices for reporting systematic reviews},
	annote = {Export Date: 23 June 2024},
}

@article{fathullah_methodological_2023-1,
	title = {Methodological {Investigation}: {Traditional} and {Systematic} {Reviews} as {Preliminary} {Findings} for {Delphi} {Technique}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166663606&doi=10.1177%2f16094069231190747&partnerID=40&md5=d2ca2c58a0c1b475d56ad690f25c431c},
	abstract = {The Delphi method has been used as a way to reach consensus among experts established in the 1950s. The method was originally conceived to be used as a forecasting instrument for business in a mixed-method study. The Delphi method is usually conducted with elements of anonymity, iteration, controlled feedback and statistical group response. Delphi method had traditionally used expert opinions through methods such as interviews and brainstorming for the initial point of studies conducted. However, with the expanding volumes of research papers and articles in this age, can techniques such as Traditional Literature Review (TLR) and Systematic Literature Review (SLR) also be viable technique to be an initial point of studies for a Delphi study. As such this study aims 1) to examine and discover whether TLR and SLR are appropriate techniques to be used in a Delphi study and 2) construct a Delphi study process overview. This study adopts a methodology in which seven articles that have used TLR and SLR in their Delphi study will be analyzed. The results shows that TLR and SLR are appropriate techniques to be used in a Delphi study and that there are two types of processes to incorporate them into the study. We have also constructed a Delphi process overview through our analysis. We hope that the results of this study will be able help researchers who are interested in doing a Delphi study to know on what are the benefits of TLR and SLR in these studies along with how to incorporate them. © The Author(s) 2023.},
	author = {Fathullah, M.A. and Subbarao, A. and Muthaiyah, S.},
	year = {2023},
	note = {Publisher: SAGE Publications Inc.},
	keywords = {systematic literature review, delphi method, delphi technique, traditional literature review},
	annote = {Export Date: 23 June 2024},
}

@article{ramos_gutierrez_when_2023-1,
	title = {When business processes meet complex events in logistics: {A} systematic mapping study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139865511&doi=10.1016%2fj.compind.2022.103788&partnerID=40&md5=2a8944612f473febd716772e7f8b2474},
	abstract = {Logistics processes are attracting growing attention because of the globalisation of the market. Its growing complexity and the need for reducing costs have provoked the seek of new solutions based on the processing of the complex events that the business processes produce. Event-Driven Business Process Management (EDBPM) is a discipline that studies the integration of business processes and complex events. The analysis of the maturity level of the approaches and gaps to point out future lines of research could help not only logistics organisations, but also academia. Logistics organisation could benefit from producing more environmentally friendly and optimal solutions in transport, and academia could benefit from revealing open problems. Thus, this study aims to identify current approaches, frameworks, and tools that integrate business processes and complex events in the logistics domain. To do so, we follow a systematic approach to do a mapping study that captures and synthesises the approaches, frameworks, and tools that integrate these two fields. As a result, 10,978 articles were gathered and 169 of them were selected for extraction. We have classified the selected studies according to several criteria, including the business process life cycle in which they are being applied, the business process modelling language, and the event process modelling language, among others. Our synthesis reveals the open challenges and the most relevant frameworks and tools. However, there is no mature enough framework or tool ready to be used in companies, and a promising research must provide solutions that cover all phases in the process life cycle. © 2022 The Author(s)},
	author = {Ramos Gutiérrez, B. and Reina Quintero, A.M. and Parody, L. and Gómez López, M.T.},
	year = {2023},
	note = {Publisher: Elsevier B.V.},
	keywords = {Systems engineering, Systematic literature review, Life cycle, Mapping, Systematic mapping studies, Modeling languages, Business Process, Enterprise resource management, Administrative data processing, Complex event processing, Complex events, Event Processing, Event-driven, Event-driven business process, Process engineering, Process life cycles, Systematic mapping study, Business process, Logistics},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{gomes_engagement_2023-1,
	title = {Engagement, {Participation}, and {Liveness}: {Understanding} {Audience} {Interaction} in {Technology}-{Based} {Events}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160820251&doi=10.5220%2f0011848600003467&partnerID=40&md5=c623e829f3cc5063272a4783ea66b092},
	abstract = {Technologies have been changing how the audience participates in different events. This participation is distinct in each type of event. For example, in educational settings, polls with clickers and word clouds are usually used to involve the audience. For music festivals and other musical performances, organizers opt out of providing led sticks, necklaces and wristbands. Different uses for the smartphones, such as using them as lanterns aiming at obtaining crowd effect, are other ordinary and spontaneous ways of interaction. Recently, more research has been published in journals and scientific conferences discussing the use of these technologies, with techniques for fostering interaction and collaboration. Therefore, we conducted a literature review using forward and backward snowballing, looking for articles about how researchers use new technologies to increase audience experience in different contexts of events and what concepts are raised from that perspective. As a result, we propose a taxonomy of those concepts related to audience experience through three lenses: engagement, participation, and liveness. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	publisher = {Science and Technology Publications, Lda},
	author = {Gomes, G. and Conte, T. and Castro, T. and Gadelha, B.},
	year = {2023},
	keywords = {Engagement, Technology-based, Audience interaction, Audience participation, Educational settings, Event, Liveness, Music, Musical performance, Smart phones, Word clouds, Audience Participation, Events},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{lopez-tenorio_comparing_2023-1,
	title = {Comparing {Machine} {Learning} for {SQL} {Injection} {Detection} in {Web} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188434335&doi=10.1109%2fISCMI59957.2023.10458664&partnerID=40&md5=568c3adc519c4cd0bac1461be4d6ecc9},
	abstract = {This work analyzes the machine learning techniques most used in SQL injection (SQLi) detection in order to make a comparison in terms of precision, as well as characterize the data with which the models for SQLi detection are generated. For the analysis, a systematic literature review is developed to extract the data reported from the state-of-the-art. A total of 31 primary studies are selected, of which 22 address the analysis and exploring ML techniques for SQLi detection; 20 conduct experiments to test the models in terms of performance and accuracy; and 14 explore the characteristics of the data with which ML models are prepared. In 22 of the 31 papers, 5 ML algorithms for classification problems stand out: Decision Tree, K-Nearest Neighbors, Naive Bayes, Random Forest, and Support Vector Machine. Decision Tree is the most used algorithm for detecting SQLi, appearing in 18 of 31 papers. The t-student test is applied for samples of unequal variances. The results demonstrate a marginal difference between techniques, although Random Forest is one of the techniques with the greatest consistency in accuracy.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lopez-Tenorio, B. and Dominguez-Isidro, S. and Cortes-Verdin, M.K. and Perez-Arriaga, J.C.},
	year = {2023},
	keywords = {Systematic literature review, State of the art, Performance, Learning systems, Machine-learning, Decision trees, Support vector machines, Machine learning techniques, Nearest neighbor search, Nearest-neighbour, Random forests, SQL injection, Web system, Work analysis, systematic literature review, Machine Learning, quantitative analysis},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{martinez_fault_2023-1,
	title = {Fault {Tree} {Analysis} and {Failure} {Modes} and {Effects} {Analysis} for {Systems} with {Artificial} {Intelligence}: {A} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183473600&doi=10.1109%2fICSRS59833.2023.10381456&partnerID=40&md5=090510ddac7703ca68c93042075ad314},
	abstract = {Reliability engineering has well-established analysis techniques to design critical systems that will be safe to operate. Two already field-proven techniques are the FTA (Fault Tree Analysis) and the FMECA (Failure Modes, Effects, and Criticality Analysis). These techniques, recommended or required by several supervisory authorities or independent assessments, will continue to be the main assets for the analysis of potential failures and faults. This mapping study revisits FMECA and FTA from the perspective of how they are used when dealing with systems with Artificial Intelligence (AI) components. After the literature database search and selection, 24 primary sources were leveraged to map them regarding their context, scope, considerations, and maturity. The diversity of safety-critical application domains and functions, and the need of more evidences from the evaluations of the proposed approaches, suggest that this field requires a pressing attention. The extracted considerations can be relevant elements of industrial guidelines.  © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Martinez, J. and Eguia, A. and Urretavizcaya, I. and Amparan, E. and Negro López, P.},
	year = {2023},
	keywords = {Artificial intelligence, Mapping, Search engines, Mapping studies, Analysis techniques, Critical systems, Failure (mechanical), Failure mode and effects analysis, Failure modes, Failure modes effects and criticality analysis, Fault tree analyses (FTA), Fault tree analysis, FMEA, Independent assessment, Reliability (engineering), Reliability analysis, Safety factor, Supervisory authority, Safety, FMECA, FTA},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{trieflinger_elevating_2023-1,
	title = {Elevating {Software} {Quality} {Through} {Product} {Discovery} {Techniques}: {Key} {Findings} from a {Grey} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172412219&doi=10.1007%2f978-3-031-43703-8_4&partnerID=40&md5=90c881cce119c4f33fbb22896447dde3},
	abstract = {In the era of digital transformation, the notion of software quality transcends its traditional boundaries, necessitating an expansion to encompass the realms of value creation for customers and the business. Merely optimizing technical aspects of software quality can result in diminishing returns. Product discovery techniques can be seen as a powerful mechanism for crafting products that align with an expanded concept of quality—one that incorporates value creation. Previous research has shown that companies struggle to determine appropriate product discovery techniques for generating, validating, and prioritizing ideas for new products or features to ensure they meet the needs and desires of the customers and the business. For this reason, we conducted a grey literature review to identify various techniques for product discovery. First, the article provides an overview of different techniques and assesses how frequently they are mentioned in the literature review. Second, we mapped these techniques to an existing product discovery process from previous research to provide concrete guidelines for establishing product discovery in their organizations. The analysis shows, among other things, the increasing importance of techniques to structure the problem exploration process and the product strategy process. The results are interpreted regarding the importance of the techniques to practical applications and recognizable trends. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	publisher = {Springer Science and Business Media Deutschland GmbH},
	author = {Trieflinger, S. and Weiss, L. and Münch, J.},
	year = {2023},
	keywords = {Digital transformation, Product management, Literature reviews, Users' experiences, Computer software selection and evaluation, Product discovery, Software Quality, Grey literature, Technical aspects, Traditional boundaries, Value creation, user experience, digital transformation, product discovery, product management, software quality},
	annote = {Export Date: 23 June 2024},
}

@article{usman_quality_2023-1,
	title = {A {Quality} {Assessment} {Instrument} for {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152967598&doi=10.37190%2fe-Inf230105&partnerID=40&md5=66143efbcfdadca0a7c250438942e4e3},
	abstract = {Background: Systematic literature reviews (SLRs) have become a standard practice as part of software engineering (SE) research, although their quality varies. To build on the reviews, both for future research and industry practice, they need to be of high quality. Aim: To assess the quality of SLRs in SE, we put forward an appraisal instrument for SLRs. Method: A well-established appraisal instrument from research in healthcare was used as a starting point to develop the instrument. It is adapted to SE using guidelines, checklists, and experiences from SE. The first version was reviewed by four external experts on SLRs in SE and updated based on their feedback. To demonstrate its use, the updated version was also used by the authors to assess a sample of six selected systematic literature studies. Results: The outcome of the research is an appraisal instrument for quality assessment of SLRs in SE. The instrument includes 15 items with different options to capture the quality. The instrument also supports consolidating the items into groups, which are then used to assess the overall quality of an SLR. Conclusion: The presented instrument may be helpful support for an appraiser in assessing the quality of SLRs in SE. © 2023 The Authors. Published by Wrocław University of Science and Technology Publishing House.},
	author = {Usman, M. and Ali, N.B. and Wohlin, C.},
	year = {2023},
	note = {Publisher: Wroclaw University of Science and Technology},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Tertiary study, Industrial research, Software engineering research, Quality assessment, AMSTAR 2, Assessment instruments, Critical appraisal, Industry practices, Standard practices, systematic literature review, tertiary study, quality assessment, critical appraisal, Systematic reviews},
	annote = {Export Date: 23 June 2024},
}

@inproceedings{silva_are_2023-1,
	title = {Are safety-critical systems really survivable to attacks?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161903777&doi=10.1109%2fSysCon53073.2023.10131114&partnerID=40&md5=f78a5de334951fc6c0a773986807fa84},
	abstract = {Safety-Critical Systems (SCS) stand for those systems designed to tackle events that could potentially cause human injury or loss of life, significant property damage, financial loss, or damage to the environment, among others. Modern SCS are in transport, infrastructure, medicine, nuclear engineering, recreation, and many other fields. Nevertheless, SCS are prone to various attacks aiming to explore their inherent complexity and broad attack surface to jeopardize essential services and assets. Survivability is key to protecting SCS through integrating preventive, reactive, and tolerant defenses. This paper stands out by analyzing the survivability aspects of SCS under attack through a systematic literature review of recently published articles. Based on the review, we devise a classification to separate the studies that focus on survivability for SCS from those that deal with related aspects, such as resistance, recognition, or tolerance to attacks. Further, we expose literature limitations indicating why there is still no guaranteed survivability of SCS in the presence of attacks. © 2023 IEEE.},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Silva, H. and Vieira, M. and Neto, A.},
	year = {2023},
	keywords = {Safety engineering, Systematic literature review, Safety critical systems, Security systems, Attack, Financial loss, Human injury, Loss of life, Losses, Mission critical systems, Property damage, Survivability, Transport infrastructure, systematic literature review, attacks, mission-critical systems, safety-critical systems, survivability},
	annote = {Export Date: 23 June 2024},
}

@article{silva_extended_2023-1,
	title = {Extended {Remote} {Laboratories}: {A} {Systematic} {Review} of the {Literature} {From} 2000 to 2022},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159716536&doi=10.1109%2fACCESS.2023.3271524&partnerID=40&md5=13a741dc197bddbc5b16f6dd86a3df80},
	abstract = {Remote Laboratories (RLs) break barriers in education since they provide real experimentation anytime and anywhere. However, their greatest drawback is the lack of immersion. However, Extended Reality (XR) can overcome this shortcoming through the integration of AR, or VR techniques, into remote experimentation, thus developing Extended Reality Remote Laboratories (XRLs), which can be Augmented Reality Remote Laboratories (ARLs), or Virtual Reality Remote Laboratories (VRLs). Our study consists in a systematic review concerning the state-of-the-art of XRLs during the period 2000-2022. Findings from the systematic review generated two main results. First, a thorough analysis that reports: a timeline of publications, studies per country, most influential universities, most popular journals and conferences, most cited publications, description of ARLs, description of VRLs, and most notable publications. Secondly, a classification of the XRLs encountered during research, and a proposed architecture for creating XRLs, in order to guide developers wishing to integrate XR into their experiments.  © 2013 IEEE.},
	author = {Silva, I.N.D. and Garcia-Zubia, J. and Hernandez-Jayo, U. and Alves, J.B.D.M.},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.},
	keywords = {Systematic Review, Augmented reality, Virtual reality, Extended reality, Laboratories, Metaverses, Pandemic, Remote experimentation, Remote laboratories, STEM, X reality, extended reality, remote laboratories, virtual reality},
	annote = {Export Date: 23 June 2024},
}

@article{pelaez_practice_2024-2,
	title = {A practice for specifying user stories in multimedia system design: {An} approach to reduce ambiguity},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195370340&doi=10.55612%2fs-5002-060-009&partnerID=40&md5=9a590e19db567e98bcfa6de5b1317058},
	doi = {10.55612/s-5002-060-009},
	abstract = {Various agile approaches have guided the development of solutions based on a minimum viable product for the use of user stories (USs). These approaches provide a generic process for specifying USs for a wide variety of solutions. This study presents an original proposal for a practical approach to the specification of USs for multimedia systems, expressed using the Essence graphical notation language. The practice includes a set of activities, techniques, and tools that can be used by professionals to define the USs of a multimedia system, and which can help to mitigate potential ambiguity problems such as vagueness and insufficiency in formulation. To explore its application, a case study was carried out with the participation of two groups of professionals: an experimental group, and a control group. The results for the analysis factors are promising, and show that by carrying out the activities that make up the practice, a work team can achieve the specification of USs at three levels of concreteness, which contribute to the reduction of problems of vagueness and insufficiency in the USs of a multimedia system. Using this approach, the USs can be guaranteed to meet the value proposition of the multimedia system that will be implemented. © (2024), (ASLERD). All Rights Reserved.},
	number = {60},
	journal = {Interaction Design and Architecture(s)},
	author = {Peláez, Carlos Alberto and Solano, Andrés},
	year = {2024},
	note = {Publisher: ASLERD
Type: Article},
	pages = {214 -- 236},
	annote = {Cited by: 0},
}

@article{olsson_strategic_2024-2,
	title = {Strategic {Digital} {Product} {Management} in the {Age} of {AI}},
	volume = {500 LNBIP},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188683557&doi=10.1007%2f978-3-031-53227-6_24&partnerID=40&md5=ff21f9d7826ae4b6d04a0c34aee99441},
	doi = {10.1007/978-3-031-53227-6_24},
	abstract = {The role of software product management is key for building, implementing and managing software products. However, although there is prominent research on software product management (SPM) there are few studies that explore how this role is rapidly changing due to digitalization and digital transformation of the software-intensive industry. In this paper, we study how key trends such as DevOps, data and artificial intelligence (AI), and the emergence of digital ecosystems are rapidly changing current SPM practices. Whereas earlier, product management was concerned with predicting the outcome of development efforts and prioritizing requirements based on these predictions, digital technologies require a shift towards experimental ways-of-working and hypotheses to be tested. To support this change, and to provide guidelines for future SPM practices, we first identify the key challenges that software-intensive embedded systems companies experience with regards to current SPM practices. Second, we present an empirically derived framework for strategic digital product management (SPM4AI) in which we outline what we believe are key practices for SPM in the age of AI. © The Author(s) 2024.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Olsson, Helena Holmström and Bosch, Jan},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Artificial intelligence, Data, Digital ecosystem, Digital products, Digital transformation, Digitalization, Ecosystems, Embedded systems, Information management, Management IS, Management practises, Metadata, Product management, Software product management, Strategic digital product management},
	pages = {344 -- 359},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{costa_multicriteria_2024-2,
	title = {Multicriteria {Decision}-{Making} in {Public} {Security}: {A} {Systematic} {Review}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195897347&doi=10.3390%2fmath12111754&partnerID=40&md5=4022e1fcd0721b69f61fec3f77e49eda},
	doi = {10.3390/math12111754},
	abstract = {The Multiple Criteria Decision-Making/Analysis (MCDM/A) methods have been widely used in several management contexts. In public security, their use enhances managerial decision-making by considering the decision-maker’s preference structure and providing a multidimensional view of problems. However, methodological support for their applications in this field lacks clarity, including selecting appropriate methods, addressing pertinent problematics, and identifying alternatives and criteria. To address this gap, this article conducts a Systematic Literature Review (SLR) to diagnose the state of the art and identify the main directions of the research in multicriteria models applied to public security management. The research methodology involves five main research questions, and the extraction and analysis of data from 51 articles selected through a structured filtering process. The analysis includes identifying the number of publications and citations, as well as listing the MCDM/A approaches and issues employed. Furthermore, the criteria used and the number of criteria considered are discussed, as well as the method employed. Finally, the identification of the main research directions in MCDM/A models applied to public security is presented. The findings suggest that prioritization and classification are common problematics, social criteria are frequently considered, and the AHP method is widely used, often employing fuzzy sets and hybrid models. © 2024 by the authors.},
	number = {11},
	journal = {Mathematics},
	author = {Costa, Jefferson and Silva, Maisa},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{kanwal_systematic_2024-2,
	title = {Systematic review on contract-based safety assurance and guidance for future research},
	volume = {146},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178994069&doi=10.1016%2fj.sysarc.2023.103036&partnerID=40&md5=42770677fc83293a0709b4a44140a836},
	doi = {10.1016/j.sysarc.2023.103036},
	abstract = {The safety requirements are often described via specifications called contracts. To verify that the system fulfils certain safety requirements, for instance, in the assume-guarantee contract specification, the key safety indicators are organized, so that if certain assumptions hold then the respective behaviour is guaranteed. Safety contracts provide a means of exposing potential incompatibilities early in the development process, selecting components to reuse, certifying systems, and identifying uncertainty sources during the operational phase. There exist several studies on contract-based safety assurance, however, there is not any systematic study in this field. For this, a first Systematic Literature Review (SLR) is carried out to obtain an overview of the various contract-based safety assurance concepts, problems, proposed solutions, and their usefulness. In our study, the identification and selection of the primary studies were based on a well-planned search strategy. The search process identified a total of 2881 studies published between 1969 and 2021, out of which 66 studies were selected through a multi-stage process according to our predefined SLR protocol. This SLR aims to highlight the state-of-the-art of contract-based safety assurance and identify potential gaps for future research. Based on research topics in selected studies, we identified the following main categories: contract type, analysis techniques for system safety, compliance with standards, development stage, domain, level of automation, type of study and evaluation, and tool support. The findings of the systematic review not only highlight that the contracts are even more important for advanced safety-critical systems but also strategies to exploit their full potential should be considered in future studies. The suggestions revealed for future research include the usage of contracts for adapting new behaviour, defining system boundaries, interacting with other systems, managing risk during operation, dynamic/runtime safety assurance, and integration of safety with security. © 2023 The Authors},
	journal = {Journal of Systems Architecture},
	author = {Kanwal, Samina and Muram, Faiz Ul and Javed, Muhammad Atif},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Assume-guarantee reasoning, Contract specifications, Contract-based assurance, Development process, Key safety indicator, Model checking, Regulatory compliance, Risk management, Safety assurance, Safety engineering, Safety indicator, Safety requirements, Specifications, Systematic literature review, Systematic Review},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{quin_b_2024-2,
	title = {A/{B} testing: {A} systematic literature review},
	volume = {211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186599305&doi=10.1016%2fj.jss.2024.112011&partnerID=40&md5=eebe13c6f2c48ed90e79c2ef28c186f3},
	doi = {10.1016/j.jss.2024.112011},
	abstract = {A/B testing, also referred to as online controlled experimentation or continuous experimentation, is a form of hypothesis testing where two variants of a piece of software are compared in the field from an end user's point of view. A/B testing is widely used in practice to enable data-driven decision making for software development. While a few studies have explored different facets of research on A/B testing, no comprehensive study has been conducted on the state-of-the-art in A/B testing. Such a study is crucial to provide a systematic overview of the field of A/B testing driving future research forward. To address this gap and provide an overview of the state-of-the-art in A/B testing, this paper reports the results of a systematic literature review that analyzed primary studies. The research questions focused on the subject of A/B testing, how A/B tests are designed and executed, what roles stakeholders have in this process, and the open challenges in the area. Analysis of the extracted data shows that the main targets of A/B testing are algorithms, visual elements, and workflow and processes. Single classic A/B tests are the dominating type of tests, primarily based in hypothesis tests. Stakeholders have three main roles in the design of A/B tests: concept designer, experiment architect, and setup technician. The primary types of data collected during the execution of A/B tests are product/system data, user-centric data, and spatio-temporal data. The dominating use of the test results are feature selection, feature rollout, continued feature development, and subsequent A/B test design. Stakeholders have two main roles during A/B test execution: experiment coordinator and experiment assessor. The main reported open problems are related to the enhancement of proposed approaches and their usability. From our study we derived three interesting lines for future research: strengthen the adoption of statistical methods in A/B testing, improving the process of A/B testing, and enhancing the automation of A/B testing. © 2024 The Author(s)},
	journal = {Journal of Systems and Software},
	author = {Quin, Federico and Weyns, Danny and Galster, Matthias and Silva, Camila Costa},
	year = {2024},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Systematic literature review, A/B test engineering, A/B testing, Controlled experimentation, Data driven decision, Decision making, Decisions makings, End-users, Hypothesis testing, Software design, Software testing, State of the art, Test engineering},
	annote = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
	file = {Quin et al. - 2024 - AB testing A systematic literature review.pdf:/Users/arthursena/Zotero/storage/V6SB2LDZ/Quin et al. - 2024 - AB testing A systematic literature review.pdf:application/pdf},
}

@article{gomes_systematic_2024-2,
	title = {Systematic {Literature} {Review} on {Hybrid} {Robotic} {Vehicles}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188890772&doi=10.3390%2frobotics13030034&partnerID=40&md5=c19609c850e3513650980723f5d1a341},
	doi = {10.3390/robotics13030034},
	abstract = {Autonomous vehicles are a continuously rising technology in several industry sectors. Examples of these technologies lie in the advances in self-driving cars and can be linked to extraterrestrial exploration, such as NASA’s Mars Exploration Rovers. These systems present a leading methodology allowing for increased task performance and capabilities, which are no longer limited to active human support. However, these robotic systems may vary in shape, size, locomotion capabilities, and applications. As such, this report presents a systematic literature review (SLR) regarding hybrid autonomous robotic vehicles focusing on leg–wheel locomotion. During this systematic review of the literature, a considerable number of articles were extracted from four different databases. After the selection process, a filtered sample was reviewed. A brief description of each document can be found throughout this report. © 2024 by the authors.},
	number = {3},
	journal = {Robotics},
	author = {Gomes, Diogo F. and Pinto, Vítor H.},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 0},
}

@article{gutierrez-fernandez_variability_2024-2,
	title = {Variability management and software product line knowledge in software companies},
	volume = {216},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196035889&doi=10.1016%2fj.jss.2024.112114&partnerID=40&md5=01f89174d622cf80d622addaf2be7c35},
	doi = {10.1016/j.jss.2024.112114},
	abstract = {Software product line engineering aims to systematically generate similar products or services within a given domain to reduce cost and time to market while increasing reuse. Various studies recognize the success of product line engineering in different domains. Software variability have increased over the years in many different domains such as mobile applications, cyber–physical systems or car control systems to just mention a few. However, software product line engineering is not as widely adopted as other software development technologies. In this paper, we present an empirical study conducted through a survey distributed to many software development companies. Our goal is to understand their need of software variability management and the level of knowledge the companies have regarding software product line engineering. The survey was answered by 127 participants from more than a hundred of different software development companies. Our study reveals that most of companies manage a catalog of similar products in a way or another (e.g. clone-and-own, common modules that are statically imported,etc.), they mostly document the features of products using text or spreed sheet based documents and more than 66\% of companies identify a base product from which they derive other similar products. We also found a correlation between the lack of Software Product Line (SPL) knowledge and the absence of reuse practices. Notably, this is the first study that explore software variability needs regardless of a company's prior knowledge of SPL. The results encourages further research to understand the reason for the limited knowledge and application of software product line engineering practices, despite the growing demand of variability management. © 2024 The Authors},
	journal = {Journal of Systems and Software},
	author = {Gutiérrez-Fernández, Antonio M. and Chacón-Luna, Ana Eva and Benavides, David and Fuentes, Lidia and Rabiser, Rick},
	year = {2024},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Software design, Application programs, Computer software reusability, Cost engineering, Different domains, Product line engineering, Reduce costs, Reduce time, Reuse, Software company, Software Product Line, Software variabilities, Time to market, Variability management},
	annote = {Cited by: 0},
}

@article{espinosa_predictive_2024-2,
	title = {Predictive models for health outcomes due to {SARS}-{CoV}-2, including the effect of vaccination: a systematic review},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182467732&doi=10.1186%2fs13643-023-02411-1&partnerID=40&md5=4558aa7f912a743649e02aff8975d10a},
	doi = {10.1186/s13643-023-02411-1},
	abstract = {Background: The interaction between modelers and policymakers is becoming more common due to the increase in computing speed seen in recent decades. The recent pandemic caused by the SARS-CoV-2 virus was no exception. Thus, this study aims to identify and assess epidemiological mathematical models of SARS-CoV-2 applied to real-world data, including immunization for coronavirus 2019 (COVID-19). Methodology: PubMed, JSTOR, medRxiv, LILACS, EconLit, and other databases were searched for studies employing epidemiological mathematical models of SARS-CoV-2 applied to real-world data. We summarized the information qualitatively, and each article included was assessed for bias risk using the Joanna Briggs Institute (JBI) and PROBAST checklist tool. The PROSPERO registration number is CRD42022344542. Findings: In total, 5646 articles were retrieved, of which 411 were included. Most of the information was published in 2021. The countries with the highest number of studies were the United States, Canada, China, and the United Kingdom; no studies were found in low-income countries. The SEIR model (susceptible, exposed, infectious, and recovered) was the most frequently used approach, followed by agent-based modeling. Moreover, the most commonly used software were R, Matlab, and Python, with the most recurring health outcomes being death and recovery. According to the JBI assessment, 61.4\% of articles were considered to have a low risk of bias. Interpretation: The utilization of mathematical models increased following the onset of the SARS-CoV-2 pandemic. Stakeholders have begun to incorporate these analytical tools more extensively into public policy, enabling the construction of various scenarios for public health. This contribution adds value to informed decision-making. Therefore, understanding their advancements, strengths, and limitations is essential. © 2024, The Author(s).},
	number = {1},
	journal = {Systematic Reviews},
	author = {Espinosa, Oscar and Mora, Laura and Sanabria, Cristian and Ramos, Antonio and Rincón, Duván and Bejarano, Valeria and Rodríguez, Jhonathan and Barrera, Nicolás and Álvarez-Moreno, Carlos and Cortés, Jorge and Saavedra, Carlos and Robayo, Adriana and Franco, Oscar H.},
	year = {2024},
	pmid = {38229123},
	note = {Publisher: BioMed Central Ltd
Type: Article},
	keywords = {Article, comorbidity, compartment model, coronavirus disease 2019, COVID-19, data base, Health Care, high income country, hospitalization, human, Humans, Joanna Briggs Institute critical appraisal checklist, low income country, mathematical model, outcome assessment, Outcome Assessment, pandemic, Pandemics, predictive model, public health, SARS-CoV-2, SARS-CoV-2 Alpha, SARS-CoV-2 Delta, SARS-CoV-2 Gamma, SARS-CoV-2 Omicron, Severe acute respiratory syndrome coronavirus 2, susceptible exposed infectious recovered model, susceptible infected recovered model, systematic review, United States, vaccination, Vaccination},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{simoes_it_2024-2,
	title = {{IT} {Workforce} {Outsourcing} {Benefits}, {Challenges} and {Success} {Factors} - {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194866459&doi=10.1145%2f3658271.3658305&partnerID=40&md5=cb10e64c7faf72b7af9a9bc6c3436200},
	doi = {10.1145/3658271.3658305},
	abstract = {Context: Organizations are highly dependent on a competent IT workforce. IT workforce outsourcing has become a strategic choice of organizations where IT structures are increasingly needed to make management processes more efficient, cost-effective, innovative, and flexible. Problem: The adoption of IT workforce outsourcing presents benefits, challenges, and success factors that require proper management. Despite their direct or indirect impact on IT workforce management, workforce performance, and the quality of products and services, many organizations may not be fully aware of these factors. Solution: We identified the benefits, challenges, and success factors associated with IT workforce outsourcing in the academic literature. IS Theory: We based the study on the General Systems Theory as our findings highlight intertwined relations between organizations, processes, and people involved in IT workforce outsourcing. Method: We executed a systematic mapping study using Engineering Village and Scopus digital databases and complemented the results with backward and forward snowballing. Summary of Results: Based on 32 studies, we identified 13 benefits, 24 challenges, and 18 success factors. These encompass issues related to aspects such as outsourcing strategy, service capacity, human resources, contractual aspects, and outsourcing planning. Contributions and Impact in the IS Area: We compared the results with a prior field study involving industry practitioners engaged in IT workforce outsourcing. We highlight the importance of hearing from both practitioners and the academic literature. Based on the results, academia can propose related research while practitioners can propose actions to improve the adoption of outsourcing practices, risk reduction, and the relationship between contractors and suppliers, among others. © 2024 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Simões, Carlos Alberto and Santos, Gleison},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Academic literature, Audition, Benefit, Challenge, Cost effectiveness, Information systems, Information use, IT structures, IT Workforce, IT workforce outsourcing, Management process, Mapping, Outsourcing, Strategic choice, Success factors, Systematic mapping studies},
	annote = {Cited by: 0},
}

@article{elder_survey_2024-2,
	title = {A {Survey} on {Software} {Vulnerability} {Exploitability} {Assessment}},
	volume = {56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191395395&doi=10.1145%2f3648610&partnerID=40&md5=4d9c74ed045e6b2d7675c61936db9cb8},
	doi = {10.1145/3648610},
	abstract = {Knowing the exploitability and severity of software vulnerabilities helps practitioners prioritize vulnerability mitigation efforts. Researchers have proposed and evaluated many different exploitability assessment methods. The goal of this research is to assist practitioners and researchers in understanding existing methods for assessing vulnerability exploitability through a survey of exploitability assessment literature. We identify three exploitability assessment approaches: assessments based on original, manual Common Vulnerability Scoring System, automated Deterministic assessments, and automated Probabilistic assessments. Other than the original Common Vulnerability Scoring System, the two most common sub-categories are Deterministic, Program State based, and Probabilistic learning model assessments. © 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	number = {8},
	journal = {ACM Computing Surveys},
	author = {Elder, Sarah and Rahman, Md Rayhanur and Fringer, Gage and Kapoor, Kunal and Williams, Laurie},
	year = {2024},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Assessment approaches, Common vulnerability scoring systems, Deterministic programs, Deterministics, Exploitability, Probabilistic assessments, Program state, Software vulnerabilities, State based, Vulnerability mitigation},
	annote = {Cited by: 1; All Open Access, Bronze Open Access},
}

@article{bekkemoen_explainable_2024-2,
	title = {Explainable reinforcement learning ({XRL}): a systematic literature review and taxonomy},
	volume = {113},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178051880&doi=10.1007%2fs10994-023-06479-7&partnerID=40&md5=8a5a3e37497d50b95590c9e028c76928},
	doi = {10.1007/s10994-023-06479-7},
	abstract = {In recent years, reinforcement learning (RL) systems have shown impressive performance and remarkable achievements. Many achievements can be attributed to combining RL with deep learning. However, those systems lack explainability, which refers to our understanding of the system’s decision-making process. In response to this challenge, the new explainable RL (XRL) field has emerged and grown rapidly to help us understand RL systems. This systematic literature review aims to give a unified view of the field by reviewing ten existing XRL literature reviews and 189 XRL studies from the past five years. Furthermore, we seek to organize these studies into a new taxonomy, discuss each area in detail, and draw connections between methods and stakeholder questions (e.g., “how can I get the agent to do \_?”). Finally, we look at the research trends in XRL, recommend XRL methods, and present some exciting research directions for future research. We hope stakeholders, such as RL researchers and practitioners, will utilize this literature review as a comprehensive resource to overview existing state-of-the-art XRL methods. Additionally, we strive to help find research gaps and quickly identify methods that answer stakeholder questions. © 2023, The Author(s).},
	number = {1},
	journal = {Machine Learning},
	author = {Bekkemoen, Yanzhe},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	keywords = {Systematic literature review, Decision making, Decision-making process, Deep learning, Explainability, Explainable artificial intelligence, Explanation, Interpretability, Literature reviews, Performance, Reinforcement learning, Reinforcement learning systems, Reinforcement learnings, Taxonomies},
	pages = {355 -- 441},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{shuraida_impact_2024-2,
	title = {The {Impact} of {Feature} {Exploitation} and {Exploration} on {Mobile} {Application} {Evolution} and {Success}},
	volume = {25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195242164&doi=10.17705%2f1jais.00844&partnerID=40&md5=5e1d823044298ee989de4f14e0ac2639},
	doi = {10.17705/1jais.00844},
	abstract = {Mobile device applications are the largest segment of IS with an estimated 5 billion users. Yet despite their widespread and growing use, there is little research examining how these mobile applications evolve with each new release update. To ensure market success, developers need to satisfy their user base by incorporating users’ reviews and feedback on the one hand and exploring new features and content that allow them to stay competitive on the other. Drawing on the organizational learning and innovation literature, the findings of the present study suggest that a mix of these two activities of exploitation and exploration in consequent app updates is likely to result in the app’s success. We further contribute to this body of work by examining the influence of users’ online review characteristics on exploitation and exploration activities in app development. The findings suggest that users’ convergence on similar issues (review concurrence) is likely to favor an orientation prioritizing exploitation over exploration activities, while the number of user reviews (review volume) has a curvilinear relationship with it. © 2024 by the Association for Information Systems.},
	number = {3},
	journal = {Journal of the Association for Information Systems},
	author = {Shuraida, Shadi and Gao, Qiang and Safadi, Hani and Jain, Radhika},
	year = {2024},
	note = {Publisher: Association for Information Systems
Type: Article},
	keywords = {Exploitation, Exploitation and explorations, Feature exploitation, Mobile application development, Mobile application evolution, Mobile applications, Mobile computing, Online user review, Online users, Technological innovation, User reviews},
	pages = {648 -- 686},
	annote = {Cited by: 0},
}

@article{dos_santos_automatic_2024-2,
	title = {Automatic user story generation: a comprehensive systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195041962&doi=10.1007%2fs41060-024-00567-0&partnerID=40&md5=7b9c1924d2789134c475860878667716},
	doi = {10.1007/s41060-024-00567-0},
	abstract = {User stories are the lifeblood of agile software development due to their semi-structured format and ease of implementation. However, the variety of data sources and textual formats used to document software requirements bring a challenge for software development teams. They often need to read and comprehend the client’s needs from different sources and convert them into user stories manually. This process demands time, and it is also prone to errors. As an alternative to remedy this issue, there are studies concerning the automatic generation of user stories. We conducted a systematic literature review (SLR) to identify and analyze existing approaches for automatically generating user stories. We investigated which type of corpora were used for training and testing, which Natural Language Processing (NLP) or Machine Learning (ML) techniques are employed to reach this goal, and how researchers are evaluating the quality of the user stories generated. Our SLR followed established guidelines and investigated state-of-the-art research from prominent academic publishers such as ACM, IEEE Xplore, and ScienceDirect. Studies published until April 2024 were included, with a focus on those addressing the research questions proposed. Our findings indicate a critical shortage of publicly available corpora hindering advancements in this field, especially in the current era of ML. The team also found there is a broad variety of techniques being employed on this topic. Finally, the studies need to pay more attention to guidelines for evaluating user stories quality. The automatic user story generation remains in its early stages. We highlight some opportunities for contribution and discuss the direction of future works. © The Author(s), under exclusive licence to Springer Nature Switzerland AG 2024.},
	journal = {International Journal of Data Science and Analytics},
	author = {dos Santos, Carlos Alberto and Bouchard, Kevin and Minetto Napoleão, Bianca},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Review},
	keywords = {Systematic literature review, Software design, Agile software development, Data-source, Learning algorithms, Learning systems, Natural language processing systems, Requirement engineering, Requirements engineering, Semi-structured, Software requirements, Story generations, Text generations, Textual format, User stories},
	annote = {Cited by: 0},
}

@article{liu_design_2024-2,
	title = {Design for dependability — {State} of the art and trends},
	volume = {211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85185411375&doi=10.1016%2fj.jss.2024.111989&partnerID=40&md5=7f5b36b728954f77c7c91606f55da869},
	doi = {10.1016/j.jss.2024.111989},
	abstract = {This paper presents an overview of design for dependability as a process involving three distinct but interrelated activities: risk analysis, risk mitigation, and risk assessment. Although these activities have been the subject of numerous works, few of them address the issue of their integration into rigorous design flows. Moreover, most existing results focus on dependability for small-size safety-critical systems with specific static architectures. They cannot be applied to large systems, such as autonomous systems with dynamic heterogeneous architectures and AI components. The overwhelming complexity and lack of interpretability of AI present challenges to model-based techniques and require empirical approaches. Furthermore, it is impossible to cope with all potential risks at design time; run-time assurance techniques are necessary to cost-effectively achieve the desired degree of dependability. The paper synthesizes the state of the art showing particularly the impact of new trends stemming from the integration of AI components in design flows. It argues that these trends will have a profound impact on design methods and the level of dependability. It advocates the need for a new theoretical basis for dependability engineering that allows the integration of traditional model-based approaches and data-driven techniques in the search for trade-offs between efficiency and dependability. © 2024},
	journal = {Journal of Systems and Software},
	author = {Liu, Hezhen and Huang, Chengqiang and Sun, Ke and Yin, Jiacheng and Wu, Xiaoyu and Wang, Jin and Zhang, Qunli and Zheng, Yang and Nigam, Vivek and Liu, Feng and Sifakis, Joseph},
	year = {2024},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Safety engineering, State of the art, AI systems, Dependable AI system, Design, Design flows, Design for dependability, Economic and social effects, Integration, Risk analysis, Risk assessment, Risk mitigation, Risks assessments, Run-time assurance, Runtimes, Safety critical systems, Search engines},
	annote = {Cited by: 0},
}

@article{travassos_tertiary_2024-2,
	title = {A {Tertiary} {Study} on the {Convergence} of {Human}–{Computer} {Interaction} and {Artificial} {Intelligence}},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191293638&doi=10.1007%2f978-3-031-53957-2_1&partnerID=40&md5=0c7138f0d7563d7b36599adb4b058895},
	doi = {10.1007/978-3-031-53957-2_1},
	abstract = {The emergence of technologies like artificial intelligence (AI) has strengthened computational solutions by using powerful algorithms that can support learning behaviors from available data and replicating such behaviors in software. A possible behavior regards the interaction of humans with computers. In this context, Human–Computer Interaction (HCI) promotes the design and evaluation of intuitive and high-usability interactive systems for human users. However, it is not clear yet the convergence between HCI and AI and how such a convergence can improve the computational solutions to benefit the end users despite the many secondary studies regarding HCI and AI available in the technical literature. Therefore, it is necessary to characterize the convergence between HCI and AI revealed in secondary studies, aiming at a better understanding of the challenges and implications of conveying the convergence of these two areas of interest from the perspective of research by undertaking a tertiary study to acquire knowledge from secondary studies published in the Scopus database until 2022. Thirty-eight secondary studies from 17 countries provided evidence of the possible convergence of HCI and AI. The main purposes of converging these areas are to foster user trust and satisfaction, to improve communication and user experience, to increase learnability and performance, and to enhance environmental observation and services. These findings have been observed in 26 problem domains and 23 system domains, performed with many distinct instruments. The convergence of HCI and AI is of worldwide interest, with a growing use of HCI methods and criteria to support developing and improving AI software systems. However, such convergence does not concern specific areas but concerns AI software systems characteristics, human use, and their interaction. Promoting the convergence of HCI and AI is challenging without a clear vision of the target software system. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	journal = {Learning and Analytics in Intelligent Systems},
	author = {Travassos, Guilherme Horta and Felizardo, Katia Romero and Morandini, Marcelo and Kolski, Christophe},
	year = {2024},
	note = {Publisher: Springer Nature
Type: Book chapter},
	keywords = {Artificial intelligence, Systematic literature review, Computational solutions, Computer software, Contemporary software system engineering, Design and evaluations, Evidence Based Software Engineering, Human computer interaction, Intelligence software, Learning behavior, Software-systems, Support learning, Tertiary study, User interfaces},
	pages = {1 -- 24},
	annote = {Cited by: 0},
}

@article{pedrosa_immersive_2024-2,
	title = {Immersive {Learning} {Environments} for {Self}-regulation of {Learning}: {A} {Literature} {Review}},
	volume = {1904 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85176965303&doi=10.1007%2f978-3-031-47328-9_36&partnerID=40&md5=afd01b0fe3773e626ee444d9ca7af52f},
	doi = {10.1007/978-3-031-47328-9_36},
	abstract = {Self-regulation of learning (SRL) plays a decisive role in learning success but characterizing learning environments that facilitate development of SRL skills constitutes a great challenge. Given the growing interest in Immersive Learning Environments (ILE), we sought to understand how ILE are built with attention to SRL, via a literature review of pedagogical uses, practices and strategies with ILE that have an explicit focus on SRL. From a final corpus of 25 papers, we collected 134 extracts attesting use of ILE for SRL. We classified and mapped them using the Beck, Morgado \& O’Shea framework and its three dimensions of the immersion phenomenon: system, narrative and challenge. There is a predominance of uses of ILE for SRL aligned with Challenge-based immersion: Skill Training, Collaboration, Engagement, and Interactive Manipulation and Exploration. In contrast, uses aligned with System-based immersion (Emphasis, Accessibility, Seeing the Invisible) were not identified. There were few cases of use of Narrative-based immersion. Uses combining the three dimensions of immersive had residual prevalence. We concluded that there is greater tendency in studies of SRL in ILE to enact active roles (aligned with the Challenge dimension of immersion). The low prevalence of Narrative immersion and System immersion evidence gaps in the diversity of pedagogical uses of ILE to develop SRL, which indicate opportunities for research and creation of innovative educational practices. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Pedrosa, Daniela and Morgado, Leonel and Beck, Dennis},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Literature reviews, Computer aided instruction, Deregulation, Educational practice, Educational strategy, Educational use, Immersion, Immersive learning, Learning environments, Pedagogical use, Self regulation, Self-regulated learning},
	pages = {497 -- 511},
	annote = {Cited by: 0},
}

@article{jordanov_containerized_2024-2,
	title = {Containerized {Microservices} for {Mobile} {Applications} {Deployed} on {Cloud} {Systems}},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195214138&doi=10.3991%2fijim.v18i10.45929&partnerID=40&md5=e3396147196473e57712536d54efef0c},
	doi = {10.3991/ijim.v18i10.45929},
	abstract = {This study explores the transformative role of containerized microservices in the sphere of mobile application development, especially within public cloud ecosystems. It focuses on how technologies such as Docker and Kubernetes contribute to improving deployment, scalability, and overall management of mobile applications, with an emphasis on containerizing backend services. We analyze their efficiency in streamlining deployment processes, focusing on how they improve the application’s performance and reliability. Additionally, we examine various alternative deployment strategies, such as blue-green, rolling, and canary releases, to emphasize their effectiveness in minimizing risks and facilitating smooth transitions in dynamic cloud environments. The study takes a comprehensive approach to achieve this goal, which includes a systematic review of existing literature, a thorough examination of relevant use cases, and an assessment of open-source technologies. Our findings reveal not only the practical benefits of these strategies but also their strategic application, offering important insights for software engineers and decision-makers. This study emphasizes the significance of integrating and optimizing containerized microservices in mobile app development to achieve more efficient, scalable, and manageable application lifecycles on cloud-based platforms. © 2024 by the authors of this article.},
	number = {10},
	journal = {International Journal of Interactive Mobile Technologies},
	author = {Jordanov, Jordan and Simeonidis, Dimitrios and Petrov, Pavel},
	year = {2024},
	note = {Publisher: International Federation of Engineering Education Societies (IFEES)
Type: Article},
	keywords = {Decision making, Life cycle, Application programs, Mobile application development, Mobile applications, Mobile computing, Cloud systems, Containerization, Containers, Deployment process, It focus, Microservice, Open source software, Performance and reliabilities, Public clouds, Virtualization, Virtualizations},
	pages = {48 -- 58},
	annote = {Cited by: 0},
}

@article{kompella_innovations_2024-2,
	title = {Innovations, strategic organizational actions, and sailing-ship effect: illustrated with an {IT} product},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182492367&doi=10.1108%2fJSTPM-08-2022-0125&partnerID=40&md5=19fae5223ac33e5337300c31268e3cfb},
	doi = {10.1108/JSTPM-08-2022-0125},
	abstract = {Purpose: In socio-technical transition theory, resistance by existing technology and regime resistance plays a key role. The resistance is in the form of intentional improvements; eventually, the regime destabilizes and adopts the new technology, referred to as the sailing-ship effect. Researchers used a structural view and examined it as a strategic action and its relationship with new technology (competitive/symbiotic) in non-fast-changing sailing systems. This study uses a microlevel view and examines it in a fast-changing where products/services are developed by integrating existing technology with new product innovations; their success depends on addressing technical/market uncertainty. This study examines the sailing-ship effect in a fast-changing system and contributes to the socio-technical transition theory. Design/methodology/approach: The authors need to examine the phenomena of the sailing-ship effect in its setting, and a case-study method is appropriate. The selected case provided diverse analytic and heuristic perspectives to examine the phenomena; therefore, it was a single case study. Findings: In an IT scenario, the strategic actions decide and realize agility and competitive advantage by formulating appropriate goals with required budgets and coevolutionary changes to resources at product, process and organizational levels, addressing technical/market uncertainty. Moreover, the agility displayed by strategic actions determines the relationship with new technology, which is interspersed. Finally, it provided insights into struggle, navigation and negotiations, forming strategic actions to display the sailing-ship effect. Research limitations/implications: The study selected a Banking Financial Services and Insurance product of an IT Services company. As start-ups exhibit inherent (emergent) agility, the authors can examine agility as a combination of emergent and strategic actions by selecting a start-up. Practical implications: The study highlights the strategic actions specific to an IT services company. It developed its product and services by steering clear from IT innovations such as native cloud and continuous deployment. It improved its products/services with necessary organizational changes and achieved the desired agility and competitive advantage. Therefore, organizations devise appropriate strategic actions to combat the sailing-ship effect apart from setting goals and selecting IT innovations. Originality/value: The study expands the socio-technical transition theory by selecting a fast-changing system. It provided insights into the relationship between existing and new technology and the strategic actions necessary to manage technical and market uncertainty and achieve the desired competitive advantage, or the sailing-ship effect. © 2024, Emerald Publishing Limited.},
	journal = {Journal of Science and Technology Policy Management},
	author = {Kompella, Lakshminarayana},
	year = {2024},
	note = {Publisher: Emerald Publishing
Type: Article},
	annote = {Cited by: 0},
}

@article{chen_understanding_2024-2,
	title = {Understanding and evaluating software reuse costs and benefits from industrial cases—{A} systematic literature review},
	volume = {171},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189036914&doi=10.1016%2fj.infsof.2024.107451&partnerID=40&md5=c4671380bf15db91cc961767f9aa77df},
	doi = {10.1016/j.infsof.2024.107451},
	abstract = {Context: Software reuse costs and benefits have been investigated in several primary studies, which have been aggregated in multiple secondary studies as well. However, existing secondary studies on software reuse have not critically appraised the evidence in primary studies. Moreover, there has been relatively less focus on how software reuse costs and benefits were measured in the primary studies, and the aggregated evidence focuses more on software reuse benefits than reuse costs. Objective: This study aims to cover the gaps mentioned in the context above by synthesizing and critically appraising the evidence reported on software reuse costs and benefits from industrial cases. Method: We used a systematic literature review (SLR) to conduct this study. The results of this SLR are based on a final set of 30 primary studies. Results: We identified nine software reuse benefits and six software reuse costs, in which better quality and improved productivity were investigated the most. The primary studies mostly used defect-based and development time-based metrics to measure reuse benefits and costs. Regarding the reuse practices, the results show that software product lines, verbatim reuse, and systematic reuse were the top investigated ones, contributing to more reuse benefits. The quality assessment of the primary studies showed that most of them are either of low (20\%) or moderate (67\%) quality. Conclusion: Based on the number and quality of the studies, we conclude that the strength of evidence for better quality and improved productivity as reuse benefits is high. There is a need to conduct more high quality studies to investigate, not only other reuse costs and benefits, but also how relatively new reuse-related practices, such as InnerSource and microservices architecture, impact software reuse. © 2024 The Author(s)},
	journal = {Information and Software Technology},
	author = {Chen, Xingru and Usman, Muhammad and Badampudi, Deepika},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, Costs, Computer software reusability, Reuse, Cost and benefits, Development time, Evaluating software, Software reuse benefit, Software reuse cost, Software-reuse},
	annote = {Cited by: 0},
}

@article{stojanov_tertiary_2023-2,
	title = {A {Tertiary} {Study} on {Microservices}: {Research} {Trends} and {Recommendations}},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182954403&doi=10.1134%2fS0361768823080200&partnerID=40&md5=aea0b73069a88175c9a499a8ffd106c5},
	doi = {10.1134/S0361768823080200},
	abstract = {Abstract: The development and adoption of microservices, as one of the most promising directions for developing heterogeneous distributed software systems, have been driven by dynamic changes in business and technology. In addition to the development of new applications, a significant aspect of microservices is the migration from legacy monolithic systems to microservice architectures. Such development trends are accompanied by an increase in the number of primary and secondary publications addressing microservices, highlighting the need to systematize research at a higher level. The objective of this study is to comprehensively analyze secondary studies in the field of microservices from the following five aspects: (1) publishing trends, (2) quality trends of secondary studies, (3) research trends, (4) domains of implementation, and (5) future research directions. The study follows the guidelines for conducting a systematic literature review. The findings were derived from 44 secondary studies published in the period from January 2016 to January 2023. These studies were organized and analyzed to address the five proposed research questions pertaining to the study objectives. The findings suggest that the most promising research directions are related to the development, implementation, and validation of new approaches, methods, and tools that encompass all the phases of the life cycle. Additionally, these research directions have applications in a variety of business and human life domains. Recommendations for further literature reviews relate to improvement of quality assessment of selected studies, more detailed review of architecture quality attributes, inquiry of human factor issues, and certain maintenance and operation issues. From the methodological aspect, recommendations relate to using social science qualitative methods for more detailed analysis of selected studies, and inclusion of gray literature that will bring the real experience of experts from industry. © 2023, Pleiades Publishing, Ltd.},
	number = {8},
	journal = {Programming and Computer Software},
	author = {Stojanov, Z. and Hristoski, I. and Stojanov, J. and Stojkov, A.},
	year = {2023},
	note = {Publisher: Pleiades Publishing
Type: Article},
	keywords = {Systematic literature review, Life cycle, Tertiary study, Microservice, Development trends, Distributed software system, Dynamic changes, Future research directions, Legacy systems, Monolithic systems, New applications, Research trends},
	pages = {796 -- 821},
	annote = {Cited by: 0},
}

@book{sharp_humans_2023-2,
	title = {Humans in the loop: {People} at the heart of systems development},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195009972&doi=10.1007%2f978-3-031-45304-5_23&partnerID=40&md5=745be4cd63797d17ef996d3e9070fc1d},
	abstract = {Despite increased automation in the process, people are (still) at the heart of software systems development. This chapter adopts a sociotechnical perspective and explores three areas that characterize the role of humans in software systems development: people as creators, people as users, and people in partnership with systems. Software is created by specialist developers such as software engineers and non-specialists such as "makers." Software developers build communities and operate within several cultures (e.g., professional, company, and national), all of which affect both the development process and the resulting product. Software is used by people. Users also operate within communities and cultures which influence product use, and how systems are used feeds back into future systems development. People and systems are interdependent: they work in partnership to achieve a wide range of goals. However, software both supports what people want to do and shapes what can be done. © The Author(s) 2024. All rights reserved.},
	publisher = {Springer Nature},
	author = {Sharp, Helen},
	year = {2023},
	doi = {10.1007/978-3-031-45304-5_23},
	note = {Publication Title: Introduction to Digital Humanism: A Textbook
Type: Book chapter},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{umar_advances_2024-2,
	title = {Advances in automated support for requirements engineering: a systematic literature review},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183775500&doi=10.1007%2fs00766-023-00411-0&partnerID=40&md5=13e42fd60463aa53bb491243c6804cfd},
	doi = {10.1007/s00766-023-00411-0},
	abstract = {Requirements Engineering (RE) has undergone several transitions over the years, from traditional methods to agile approaches emphasising increased automation. In many software development projects, requirements are expressed in natural language and embedded within large volumes of text documents. At the same time, RE activities aim to define software systems' functionalities and constraints. However, manually executing these tasks is time-consuming and prone to errors. Numerous research efforts have proposed tools and technologies for automating RE activities to address this challenge, which are documented in published works. This review aims to examine empirical evidence on automated RE and analyse its impact on the RE sub-domain and software development. To achieve our goal, we conducted a Systematic Literature Review (SLR) following established guidelines for conducting SLRs. We aimed to identify, aggregate, and analyse papers on automated RE published between 1996 and 2022. We outlined the output of the support tool, the RE phase covered, levels of automation, development approach, and evaluation approaches. We identified 85 papers that discussed automated RE from various perspectives and methodologies. The results of this review demonstrate the significance of automated RE for the software development community, which has the potential to shorten development cycles and reduce associated costs. The support tools primarily assist in generating UML models (44.7\%) and other activities such as omission of steps, consistency checking, and requirement validation. The analysis phase of RE is the most widely automated phase, with 49.53\% of automated tools developed for this purpose. Natural language processing technologies, particularly POS tagging and Parser, are widely employed in developing these support tools. Controlled experimental methods are the most frequently used (48.2\%) for evaluating automated RE tools, while user studies are the least employed evaluation method (8.2\%). This paper contributes to the existing body of knowledge by providing an updated overview of the research literature, enabling a better understanding of trends and state-of-the-art practices in automated RE for researchers and practitioners. It also paves the way for future research directions in automated requirements engineering. © Crown 2024.},
	number = {2},
	journal = {Requirements Engineering},
	author = {Umar, Muhammad Aminu and Lano, Kevin},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Systematic literature review, Software design, Natural language processing systems, Requirement engineering, Requirements engineering, Agile approaches, Automated requirement engineering, Automated support, Automation, Computational linguistics, Engineering activities, Natural languages, Software development projects, Support tool, Unified Modeling Language},
	pages = {177 -- 207},
	annote = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
}

@article{song_when_2024-2,
	title = {When debugging encounters artificial intelligence: state of the art and open challenges},
	volume = {67},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186138673&doi=10.1007%2fs11432-022-3803-9&partnerID=40&md5=9eede2fdcbf17988a79b522100b5aa0c},
	doi = {10.1007/s11432-022-3803-9},
	abstract = {Both software debugging and artificial intelligence techniques are hot topics in the current field of software engineering. Debugging techniques, which comprise fault localization and program repair, are an important part of the software development lifecycle for ensuring the quality of software systems. As the scale and complexity of software systems grow, developers intend to improve the effectiveness and efficiency of software debugging via artificial intelligence (artificial intelligence for software debugging, AI4SD). On the other hand, many artificial intelligence models are being integrated into safety-critical areas such as autonomous driving, image recognition, and audio processing, where software debugging is highly necessary and urgent (software debugging for artificial intelligence, SD4AI). An AI-enhanced debugging technique could assist in debugging AI systems more effectively, and a more robust and reliable AI approach could further guarantee and support debugging techniques. Therefore, it is important to take AI4SD and SD4AI into consideration comprehensively. In this paper, we want to show readers the path, the trend, and the potential that these two directions interact with each other. We select and review a total of 165 papers in AI4SD and SD4AI for answering three research questions, and further analyze opportunities and challenges as well as suggest future directions of this cross-cutting area. © Science China Press 2024.},
	number = {4},
	journal = {Science China Information Sciences},
	author = {Song, Yi and Xie, Xiaoyuan and Xu, Baowen},
	year = {2024},
	note = {Publisher: Science China Press
Type: Review},
	keywords = {Safety engineering, Software design, State of the art, Life cycle, Computer software, Software-systems, Artificial intelligence techniques, Current fields, Fault localization, Hot topics, Image recognition, Machine learning, Machine-learning, Program debugging, Program repair, Repair, Software debugging, Software development life-cycle},
	annote = {Cited by: 0},
}

@article{liu_conversation-based_2024-2,
	title = {Conversation-based hybrid {UI} for the repertory grid technique: {A} lab experiment into automation of qualitative surveys},
	volume = {184},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183466132&doi=10.1016%2fj.ijhcs.2024.103227&partnerID=40&md5=9c4644f20ae1abe675a64043de47f49c},
	doi = {10.1016/j.ijhcs.2024.103227},
	abstract = {A frequent use of conversational user interfaces (CUIs) today is improving the users’ experience with online quantitative surveys. In this paper, we explore the use of CUIs in qualitative surveys. As a concrete use case, we adopt a specific, well-structured, qualitative research method called the repertory grid technique (RGT). We developed a hybrid user interface (HUI) that combines a graphical user interface (GUI) with a CUI to automate the distinct stages in a RGT survey. A pilot study was used to verify the feasibility of the approach and to fine-tune interface aspects of an initial prototype. In this paper, we report the results of a within-subject lab experiment with 24 participants that aimed to establish the performance and UX in a realistic context of a more advanced prototype. We observed a small decrease in UX in some hedonistic aspects, but also confirmed that the HUI performs similarly to a human agent in most pragmatic aspects. These results provide support for our hypothesis that automating qualitative surveys is possible with proper interface design. We hope that our work can inspire other researchers to design additional tools for qualitative survey automation, especially now that generative AI systems, such as ChatGPT, open up interesting new ways for computer systems to interact with users in natural language. © 2024 The Authors},
	journal = {International Journal of Human Computer Studies},
	author = {Liu, Yunxing and Martens, Jean-Bernard},
	year = {2024},
	note = {Publisher: Academic Press
Type: Article},
	keywords = {Automation, Chatbots, Graphical user interfaces, Grid techniques, Hybrid UI, Hybrid User Interfaces, Lab. experiment, Qualitative survey automation, Qualitative surveys, Repertory grid technique, Repertory grids, Users' experiences},
	annote = {Cited by: 1},
}

@article{diaz_how_2024-2,
	title = {How can feature usage be tracked across product variants? {Implicit} {Feedback} in {Software} {Product} {Lines}},
	volume = {211},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186517167&doi=10.1016%2fj.jss.2024.112013&partnerID=40&md5=d2166267177422aa1cf29c0b37bd4de1},
	doi = {10.1016/j.jss.2024.112013},
	abstract = {Implicit feedback involves gathering information about software usage to grasp how and when the software is utilized. This study investigates the integration of implicit feedback mechanisms into Software Product Line Engineering (SPLE). While common in product-based development for identifying bugs, usability issues, and informing requirement prioritization, its adoption in SPLs has been limited due to the unique challenges posed by SPLE, such as the emphasis on Domain Engineering over Application Engineering, and the need for systematic reuse of shared assets. We propose a novel approach to incorporate feedback practices into Domain Engineering, thereby shifting the focus from individual product variants to the SPL platform, and specifically moving from product-based feedback to feature-based feedback. Based on a case study, we suggest that product derivation includes a second step that injects the trackers at the time of derivation, using a Feedback Model that complements the Configuration Model for feedback analysis.To test this approach, we introduce FEACKER, an extension to pure::variants as the variability manager. FEACKER injects trackers when the product variant is derived. The findings are validated through a Technology Acceptance Model (TAM) evaluation and a focus group discussion, providing insights into the feasibility, acceptance, and potential impact of platform-based feedback in SPLE. The results indicate agreement on the benefits of conducting feedback analysis at the platform level and the perception that FEACKER seamlessly extends the capabilities of pure::variants. © 2024 The Authors},
	journal = {Journal of Systems and Software},
	author = {Díaz, Oscar and Medeiros, Raul and Al-Hajjaji, Mustafa},
	year = {2024},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Software design, Product line engineering, Software Product Line, Computer software, Program debugging, Application engineering, Continuous development, Domain engineering, Feedback analysis, Feedback mechanisms, Implicit feedback, Product variants, Requirements prioritization},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{cauz_text_2024-2,
	title = {Text readability in augmented reality: a multivocal literature review},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186594558&doi=10.1007%2fs10055-024-00949-6&partnerID=40&md5=2be8cca4f03f6481cf9e0d914a2d6e02},
	doi = {10.1007/s10055-024-00949-6},
	abstract = {Augmented reality (AR) is making its way into many sectors. Its rapid evolution in recent years has led to the development of prototypes demonstrating its effectiveness. However, to be able to push these prototypes to the scale of fully usable applications, it is important to ensure the readability of the texts they include. To this end, we conducted a multivocal literature review (MLR) to determine the text parameters a designer can tune, as well as the contextual constraints they need to pay attention to, in relation to Optical See-Through (OST) and Video See-Through (VST) displays. We also included guidelines from device manufacturing and game engines sites to compare the current state of research in the academic and industrial worlds. The results show that parameters pertaining more to letter legibility have been extensively studied (e.g., color and size), while those pertaining to the whole text still require further research (e.g., alignment or space between lines). The former group of parameters, and their associated constraints, were assembled in the form of two decision trees to facilitate implementation of AR applications. Finally, we also concluded that there was a lack of alignment between academic and industrial recommendations. © The Author(s) 2024.},
	number = {1},
	journal = {Virtual Reality},
	author = {Cauz, Maxime and Clarinval, Antoine and Dumas, Bruno},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Literature reviews, 'current, Augmented reality, Contextual constraints, Decision trees, Game Engine, Industrial research, Legibility, Mixed reality, Optical see-through, Readability, State of research, Text},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{rico_experiences_2024-2,
	title = {Experiences from conducting rapid reviews in collaboration with practitioners — {Two} industrial cases},
	volume = {167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178453626&doi=10.1016%2fj.infsof.2023.107364&partnerID=40&md5=1973bcd1a7948080b1e54df8a4a4d573},
	doi = {10.1016/j.infsof.2023.107364},
	abstract = {Context: Evidence-based software engineering (EBSE) aims to improve research utilization in practice. It relies on systematic methods to identify, appraise, and synthesize existing research findings to answer questions of interest for practice. However, the lack of practitioners’ involvement in these studies’ design, execution, and reporting indicates a lack of appreciation for the need for knowledge exchange between researchers and practitioners. The resultant systematic literature studies often lack relevance for practice. Objective: This paper explores the use of Rapid Reviews (RRs), in fostering knowledge exchange between academia and industry. Through the lens of two case studies, we delve into the practical application and experience of conducting RRs. Methods: We analyzed the conduct of two rapid reviews by two different groups of researchers and practitioners. We collected data through interviews, and the documents produced during the review (like review protocols, search results, and presentations). The interviews were analyzed using thematic analysis. Results: We report how the two groups of researchers and practitioners performed the rapid reviews. We observed some benefits, like promoting dialogue and paving the way for future collaborations. We also found that practitioners entrusted the researchers to develop and follow a rigorous approach and were more interested in the applicability of the findings in their context. The problems investigated in these two cases were relevant but not the most immediate ones. Therefore, rapidness was not a priority for the practitioners. Conclusion: The study illustrates that rapid reviews can support researcher-practitioner communication and industry-academia collaboration. Furthermore, the recommendations based on the experiences from the two cases complement the detailed guidelines researchers and practitioners may follow to increase interaction and knowledge exchange. © 2023 The Author(s)},
	journal = {Information and Software Technology},
	author = {Rico, Sergio and Ali, Nauman Bin and Engström, Emelie and Höst, Martin},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software engineering, Systematic Review, Literature reviews, Evidence Based Software Engineering, Industrial research, Industry-academia collaboration, Knowledge exchange, Knowledge management, Literature studies, Rapid review, Research relevance, Study design, Systematic method},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{klymenko_integration_2024-2,
	title = {On the {Integration} of {Privacy}-{Enhancing} {Technologies} in the {Process} of {Software} {Engineering}},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193980857&doi=10.5220%2f0012632500003690&partnerID=40&md5=11923a80a832d27738266388d3b58f9d},
	doi = {10.5220/0012632500003690},
	abstract = {The class of technologies known as Privacy-Enhancing Technologies (PETs) has been receiving rising attention in the academic sphere. In practice, however, the adoption of such technologies remains low. Beyond the actual implementation of a PET, it is not clear where along the process of software engineering PETs should be considered, and which activities must take place to facilitate their implementation. In this light, we aim to investigate the placement of PETs in the software engineering process, specifically from the perspective of privacy requirements engineering. To do this, we conduct a systematic literature review and interview 10 privacy experts, exploring the integration of PETs into the software engineering process, as well as identifying associated challenges along with their potential solutions. We systematize our findings in a unified process diagram that illustrates the roles and activities involved in the implementation of PETs in software systems. In addition, we map the identified solution concepts to the diagram, highlighting which stages of the software engineering process are vital in tackling the corresponding challenges and supporting the adoption of PETs. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	booktitle = {International {Conference} on {Enterprise} {Information} {Systems}, {ICEIS} - {Proceedings}},
	publisher = {Science and Technology Publications, Lda},
	author = {Klymenko, Alexandra and Meisenbacher, Stephen and Favaro, Luca and Matthes, Florian},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Software engineering, Systematic literature review, Requirement engineering, Requirements engineering, Software-systems, Data privacy, Engineering education, Privacy engineerings, Privacy enhancing technologies, Privacy requirements, Process diagram, Software engineering process, Solution concepts, Unified process},
	pages = {41 -- 52},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{cacciuttolo_sensor_2024-2,
	title = {Sensor {Technologies} for {Safety} {Monitoring} in {Mine} {Tailings} {Storage} {Facilities}: {Solutions} in the {Industry} 4.0 {Era}},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194092250&doi=10.3390%2fmin14050446&partnerID=40&md5=b1e13d13f95a8659defb3f6d305755f9},
	doi = {10.3390/min14050446},
	abstract = {The recent tailings storage facility (TSF) dam failures recorded around the world have concerned society in general, forcing the mining industry to improve its operating standards, invest greater economic resources, and implement the best available technologies (BATs) to control TSFs for safety purposes and avoid spills, accidents, and collapses. In this context, and as the era of digitalization and Industry 4.0 continues, monitoring technologies based on sensors have become increasingly common in the mining industry. This article studies the state of the art of implementing sensor technologies to monitor structural health and safety management issues in TSFs, highlighting advances and experiences through a review of the scientific literature on the topic. The methodology applied in this article adheres to the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines and utilizes scientific maps for data visualization. To do so, three steps were implemented: (i) a quantitative bibliometric analysis, (ii) a qualitative systematic review of the literature, and (iii) a mixed review to integrate the findings from (i) and (ii). As a result, this article presents the main advances, gaps, and future trends regarding the main characteristics of the sensor technologies applied to monitor TSF structural health and safety management in the era of digitalization. According to the results, the existing research predominantly investigates certain TSF sensor technologies, such as wireless real-time monitoring, remote sensors (RS), unmanned aerial vehicles (UAVs), unmanned survey vessels (USVs), artificial intelligence (AI), cloud computing (CC), and Internet of Things (IoT) approaches, among others. These technologies stand out for their potential to improve the safety management monitoring of mine tailings, which is particularly significant in the context of climate change-related hazards, and to reduce the risk of TSF failures. They are recognized as emerging smart mining solutions with reliable, simple, scalable, secure, and competitive characteristics. © 2024 by the authors.},
	number = {5},
	journal = {Minerals},
	author = {Cacciuttolo, Carlos and Guzmán, Valentina and Catriñir, Patricio and Atencio, Edison},
	year = {2024},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{sumardi_effect_2024-2,
	title = {{THE} {EFFECT} {OF} {ISLAMIC} {ATTRIBUTES} {TO} {CONSUMER} {SATISFACTION}: {A} {META}-{ANALYSIS}},
	volume = {53},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191822486&doi=10.30892%2fgtg.53201-1215&partnerID=40&md5=f4f1dd059b68da375bbab257d3c39e60},
	doi = {10.30892/gtg.53201-1215},
	abstract = {In the growing halal industry, there are differences of opinion among researchers about the effect of Islamic attributes on consumer satisfaction. Therefore, this paper aims to evaluate the effect of Islamic attributes on consumer satisfaction. The Prisma flow diagram indicated 23 papers and consists of 59 studies to analyze with JASP Software. The study identifies significant authors, dominant publishers, methodology, and theories commonly employed in this topic. The result proves that catering to Muslim needs through Islamic attributes can significantly enhance consumer satisfaction and th e presence of other variables as moderators will strengthen tourist satisfaction. © 2024 Editura Universitatii din Oradea. All rights reserved.},
	number = {2},
	journal = {Geojournal of Tourism and Geosites},
	author = {Sumardi, Retno Santi and Mahomed, Anuar Shah Bali and Aziz, Yuhanis Ab},
	year = {2024},
	note = {Publisher: Editura Universitatii din Oradea
Type: Article},
	keywords = {diagram, hotel industry, Islamism, life satisfaction, meta-analysis, software},
	pages = {400 -- 412},
	annote = {Cited by: 0},
}

@article{pretorius_when_2024-2,
	title = {When rationality meets intuition: {A} research agenda for software design decision-making},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189632288&doi=10.1002%2fsmr.2664&partnerID=40&md5=7a7dd1858ceb779f6079fb0c96876131},
	doi = {10.1002/smr.2664},
	abstract = {As society's reliance on software systems escalates over time, so too does the cost of failure of these systems. Meanwhile, the complexity of software systems, as well as of their designs, is also ever-increasing, influenced by the proliferation of new tools and technologies to address intended societal needs. The traditional response to this complexity in software engineering and software architecture has been to apply rationalistic approaches to software design through methods and tools for capturing design rationale and evaluating various design options against a set of criteria. However, research from other fields demonstrates that intuition may also hold benefits for making complex design decisions. All humans, including software designers, use intuition and rationality in varying combinations. The aim of this article is to provide a comprehensive overview of what is known and unknown from existing research regarding the use and performance consequences of using intuition and rationality in software design decision-making. To this end, a systematic literature review has been conducted, with an initial sample of 3909 unique publications and a final sample of 26 primary studies. We present an overview of existing research, based on the literature concerning intuition and rationality use in software design decision-making and propose a research agenda with 14 questions that should encourage researchers to fill identified research gaps. This research agenda emphasizes what should be investigated to be able to develop support for the application of the two cognitive processes in software design decision-making. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	journal = {Journal of Software: Evolution and Process},
	author = {Pretorius, Carianne and Razavian, Maryam and Eling, Katrin and Langerak, Fred},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Application programs, Software-systems, Behavioral research, Design decision-making, Intuition, Rationality, Research agenda, Software design decisions, Software Evolution, Software process},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{hannousse_twenty-two_2024-2,
	title = {Twenty-two years since revealing cross-site scripting attacks: {A} systematic mapping and a comprehensive survey},
	volume = {52},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191018882&doi=10.1016%2fj.cosrev.2024.100634&partnerID=40&md5=f261951f45b0df8972c9f9c044dc6651},
	doi = {10.1016/j.cosrev.2024.100634},
	abstract = {Cross-site scripting (XSS) is one of the major threats menacing the privacy of data and the navigation of trusted web applications. Since its disclosure in late 1999 by Microsoft security engineers, several techniques have been developed with the aim of securing web navigation and protecting web applications against XSS attacks. XSS has been and is still in the top 10 list of web vulnerabilities reported by the Open Web Applications Security Project (OWASP). Consequently, handling XSS attacks has become one of the major concerns of several web security communities. Despite the numerous studies that have been conducted to combat XSS attacks, the attacks continue to rise. This motivates the study of how the interest in XSS attacks has evolved over the years, what has already been achieved to prevent these attacks, and what is missing to restrain their prevalence. In this paper, we conduct a systematic mapping and a comprehensive survey with the aim of answering all these questions. We summarize and categorize existing endeavors that aim to handle XSS attacks and develop XSS-free web applications. The systematic mapping yielded 157 high-quality published studies. By thoroughly analyzing those studies, a comprehensive taxonomy is drawn out outlining various techniques used to prevent, detect, protect, and defend against XSS attacks and vulnerabilities. The study of the literature revealed a remarkable interest bias toward basic (84.71\%) and JavaScript (81.63\%) XSS attacks as well as a dearth of vulnerability repair mechanisms and tools (only 1.48\%). Notably, existing vulnerability detection techniques focus solely on single-page detection, overlooking flaws that may span across multiple pages. Furthermore, the study brought to the forefront the limitations and challenges of existing attack detection and defense techniques concerning machine learning and content-security policies. Consequently, we strongly advocate the development of more suitable detection and defense techniques, along with an increased focus on addressing XSS vulnerabilities through effective detection (hybrid solutions) and repair strategies. Additionally, there is a pressing need for more high-quality studies to overcome the limitations of promising approaches such as machine learning and content-security policies while also addressing diverse XSS attacks in different languages. Hopefully, this study can serve as guidance for both the academic and practitioner communities in the development of XSS-free web applications. © 2024 Elsevier Inc.},
	journal = {Computer Science Review},
	author = {Hannousse, Abdelhakim and Yahiouche, Salima and Nait-Hamoud, Mohamed Cherif},
	year = {2024},
	note = {Publisher: Elsevier Ireland Ltd
Type: Review},
	keywords = {Mapping, Machine learning, Machine-learning, Repair, Data privacy, Cross site scripting, Defense techniques, High quality, Network security, Security systems, Systematic mapping, WEB application, Web applications, WEB security, XSS attack, XSS vulnerability},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{silva_using_2024-2,
	title = {Using {Hypotheses} to {Manage} {Technical} {Uncertainty} and {Architecture} {Evolution} in a {Software} {Start}-up},
	volume = {41},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195806801&doi=10.1109%2fMS.2024.3383628&partnerID=40&md5=e5c7d0c12e832411fc2d35cc91e1d614},
	doi = {10.1109/MS.2024.3383628},
	abstract = {This article presents the case of a start-up applying a technique named ArchHypo that uses hypotheses to express uncertainties related to the software architecture. Ten months after identifying the hypotheses, it was assessed how the usage of this technique impacted their decisions. © 1984-2012 IEEE.},
	number = {4},
	journal = {IEEE Software},
	author = {Silva, Kelson and Melegati, Jorge and Wang, Xiaofeng and Ferreira, Mauricio and Guerra, Eduardo},
	year = {2024},
	note = {Publisher: IEEE Computer Society
Type: Article},
	keywords = {Computer software, Uncertainty},
	pages = {7 -- 13},
	annote = {Cited by: 0},
}

@article{khan_revolutionizing_2024-2,
	title = {Revolutionizing software developmental processes by utilizing continuous software approaches},
	volume = {80},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179315668&doi=10.1007%2fs11227-023-05818-8&partnerID=40&md5=b3d09e613cf9ddb415938502bdd55553},
	doi = {10.1007/s11227-023-05818-8},
	abstract = {The development of smart and innovative software applications in various disciplines has inspired our lives by providing various cutting-edge technologies spanning from online to smart and efficient systems. The proliferation of innovative internet-enabled tools has transformed the nation into a globalized world where individuals can participate on various platforms, collaborate in activities, communicate on issues, and exchange information safely and consistently. Coordination and cooperation are essential in software development. It gathers all software developers in one space, encouraging them to discuss goals and work rationally to accomplish the project goal. In recent years, continuous software development and deployment have become increasingly common in software engineering. Continuous software engineering (CSE) is a method that involves a variety of strategies to increase the regularity of novel and modified software versions. CSE enables a continuous learning and improvement process through rapid software update iteration by combining continuous integration and delivery. Continuous integration is a method that has arisen in order to remove gaps between development and deployment. Software engineers must handle uncertainty and alter stakeholders' requirements, which is possible through continuous software developmental strategies that manage the overall software cycle and produce high-quality software applications. The proposed study is a systematic review related to continuous software development and deployment and focuses on achieving four aims: (1) To explore the impacts of continuous development on software, (2) to pinpoint various tools used to carry out this process, (3) to highlight the challenges faced in adopting continuous approaches for development and (4) to analyze the phases of continuous software engineering. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	number = {7},
	journal = {Journal of Supercomputing},
	author = {Khan, Habib Ullah and Afsar, Waseem and Nazir, Shah and Noor, Asra and Kundi, Mahwish and Maashi, Mashael and Alshahrani, Haya Mesfer},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	keywords = {Software design, Application programs, Computer software reusability, Automation, Automated systems, Continuous integrations, Continuous software, Continuous software engineerings, Cutting edge technology, Iterative methods, Online systems, Reusable softwares, Software applications, Software approach, Software deployment, Software upgradation},
	pages = {9579 -- 9608},
	annote = {Cited by: 0},
}

@inproceedings{unkelos-shpigel_revise_2023-2,
	title = {Revise {That} {Again}: {Are} {You} {Motivated}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180004904&doi=10.1145%2f3617553.3617885&partnerID=40&md5=ff5086d61dda21c9c557267e97ae7636},
	doi = {10.1145/3617553.3617885},
	abstract = {Requirements engineering (RE) presents several challenges stemming from the required collaboration and knowledge transfer between analysists, developers, and customers. Motivation theories have been used occasionally to analyze and encourage motivation and engagement of stakeholders in RE tasks. In recent years, gamification techniques have been used in software engineering tasks, and specifically, in RE tasks in order to promote stakeholder engagement. However, existing research works seldom offer a rigorous method for designing gamification environments for RE tasks. This paper describes a socio-Technical environment, which was built for requirements elicitation and specification. This environment allows researchers and team managers to decide on different mechanisms to gamify the current RE task in practice. The environment was evaluated by experts and was further tested with the participation of students in two proof of concept studies for demonstrating its functionality, yielding some anecdotic results. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Unkelos-Shpigel, Naomi and Berencwaig, Barak and Kas, Sharon},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software engineering, Requirement engineering, Requirements engineering, Knowledge management, Human resource management, Engagement, Engineering tasks, Gamification, Knowledge transfer, Motivation, Motivation and engagements, Motivation theories, Sociotechnical, Stakeholder engagement, Technical environments},
	pages = {6 -- 12},
	annote = {Cited by: 0},
}

@article{ros_theory_2024-2,
	title = {A theory of factors affecting continuous experimentation ({FACE})},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179585225&doi=10.1007%2fs10664-023-10358-z&partnerID=40&md5=c9efd3f16565fe8e41091c8792e51a17},
	doi = {10.1007/s10664-023-10358-z},
	abstract = {Context: Continuous experimentation (CE) is used by many companies with internet-facing products to improve their business models and software solutions based on user data. Some companies deliberately adopt a systematic experiment-driven approach to software development while some companies use CE in a more ad-hoc fashion. Objective: The goal of this study is to identify factors for success in CE that explain the variations in the utility and efficacy of CE between different companies. Method: We conducted a multi-case study of 12 companies involved with CE and performed 27 interviews with practitioners at these companies. Based on that empirical data, we then built a theory of factors at play in CE. Results: We introduce a theory of Factors Affecting Continuous Experimentation (FACE). The theory includes three factors, namely 1) processes and infrastructure for CE, 2) the user problem complexity of the product offering, and 3) incentive structures for CE. The theory explains how these factors affect the effectiveness of CE and its ability to achieve problem-solution and product-market fit. Conclusions: Our theory may inspire practitioners to assess an organisation’s potential for adopting CE and to identify factors that pose challenges in gaining value from CE practices. Our results also provide a basis for defining practitioner guidelines and a starting point for further research on how contextual factors affect CE and how these may be mitigated. © 2023, The Author(s).},
	number = {1},
	journal = {Empirical Software Engineering},
	author = {Ros, Rasmus and Bjarnason, Elizabeth and Runeson, Per},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	keywords = {A/B testing, Business software, Case-studies, Continuous experimentation, Data driven, Data-driven development, Empirical research, Facing products, Multi-case study, Theory building},
	annote = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{cockburn_user_2024,
	title = {User {Interface} {Evaluation} {Through} {Implicit}-{Association} {Tests}},
	volume = {8},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196429470&doi=10.1145%2f3664636&partnerID=40&md5=b62d738ba39bba1a7a3260aaa365e7cd},
	doi = {10.1145/3664636},
	abstract = {The implicit-association test (IAT) is a method for measuring subconscious associations between concepts in memory. It is widely used in social psychology research for assessing associations that people may be unable or unwilling to articulate, including those relating to race, gender, self harm, and risk-taking behaviour. We describe the motivation for adapting the IAT to user interface evaluation, including its potential to support rapid A/B testing that is amenable to online crowd-source dissemination, while also potentially reducing the validity risks caused by biases such as the good subject effect. We present a method (the UI-IAT) for conducting implicit association tests for A/B user interface evaluation, and we present results of two experiments demonstrating that, although the method can successfully discriminate between 'good' and 'bad' interfaces, its sensitivity is low. We discuss implications for practical use of the UI-IAT and for further work. © 2024 Owner/Author.},
	number = {EICS},
	journal = {Proceedings of the ACM on Human-Computer Interaction},
	author = {Cockburn, Andy and Hills, Declan and Chen, Zhe and Gutwin, Carl},
	year = {2024},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Risk management, A/B testing, Risk assessment, User interfaces, Further works, Implicit-association test, Interface evaluation, Practical use, Risk-taking behaviors, Social psychology, Subjective experiences, Subjective testing, User interface evaluation},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{erat_emotion_2024-2,
	title = {Emotion recognition with {EEG}-based brain-computer interfaces: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186412474&doi=10.1007%2fs11042-024-18259-z&partnerID=40&md5=da6a586436c707ff0124ec5644173527},
	doi = {10.1007/s11042-024-18259-z},
	abstract = {Electroencephalography (EEG)-based Brain-Computer Interface (BCI) systems for emotion recognition have the potential to assist the enrichment of human–computer interaction with implicit information since they can enable understanding of the cognitive and emotional activities of humans. Therefore, these systems have become an important research topic today. This study aims to present trends and gaps on this topic by performing a systematic literature review based on the 216 published scientific literature gathered from various databases including ACM, IEEE Xplore, PubMed, Science Direct, and Web of Science from 2016 to 2020. This review gives an overview of all the components of EEG based BCI system from the signal stimulus module which includes the employed device, signal stimuli, and data processing modality, to the signal processing module which includes signal acquisition, pre-processing, feature extraction, feature selection, classification algorithms, and performance evaluation. Thus, this study provides an overview of all components of an EEG-based BCI system for emotion recognition and examines the available evidence in a clear, concise, and systematic way. In addition, the findings are aimed to inform researchers about the issues on what are research trends and the gaps in this field and guide them in their research directions. © The Author(s) 2024.},
	journal = {Multimedia Tools and Applications},
	author = {Erat, Kübra and Şahin, Elif Bilge and Doğan, Furkan and Merdanoğlu, Nur and Akcakaya, Ahmet and Durdu, Pınar Onay},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	keywords = {Systematic literature review, Affective brain-computer interface, Affective Computing, Biomedical signal processing, Brain computer interface, Classification (of information), Cognitive systems, Data handling, Electroencephalography, Electrophysiology, Emotion, Emotion recognition, Emotion Recognition, Feature extraction, Implicit informations, Interface system, Research topics, Scientific literature, Speech recognition, Web of Science},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@book{bratianu_knowledge_2024-2,
	title = {Knowledge translation},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190356698&doi=10.1108%2f9781803828893&partnerID=40&md5=7ecf8b9fc80a0ff97fa83a5e6a870d8d},
	abstract = {Knowledge translation is a topic that originated in the field of health sciences where the need to move research to practice is of critical importance. In parallel, the field of knowledge sciences has developed a research base around knowledge transfer, knowledge sharing, knowledge exchange, knowledge articulation and knowledge absorption. This book brings these two important tracks together and synthesizes the fragmented literatures. It also draws from essential work on human communication and considers how these concepts are affected by the knowledge economy. The book raises awareness of the critical role that knowledge translation plays in every academic field of study, and in everyday life. To demonstrate this role, the book presents a grounding model that readers can use to better see their knowledge translation challenges and opportunities. Drawing on the author teams experience in a range of domains and sectors, the book explores knowledge translation in the fields of manufacturing, infectious diseases, automated call centers, regulatory development and compliance, financial lending, transportation safety, and doctor-patient discourse. All rights reserved.},
	publisher = {Emerald Group Publishing Ltd.},
	author = {Bratianu, Constantin and Garcia-Perez, Alexeis and Dal Mas, Francesca and Bedford, Denise},
	year = {2024},
	doi = {10.1108/9781803828893},
	note = {Publication Title: Knowledge Translation
Type: Book},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{costa_testing_2024-2,
	title = {Testing on {Dynamically} {Adaptive} {Systems}: {Challenges} and {Trends}},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193917885&doi=10.5220%2f0012555900003690&partnerID=40&md5=15527034acfbfd71c1daf5ea77036493},
	doi = {10.5220/0012555900003690},
	abstract = {Dynamically Adaptive Systems (DAS) are systems capable of modifying themselves automatically according to the surrounding environment. Traditional testing approaches are ineffective for these systems due to their dynamic aspects, making fault detection complex. Although various testing approaches have been proposed for DASs, there is no up-to-date overview of the approaches, challenges, and trends. This research therefore presents the results of a systematic literature review to identify the challenges, approaches and trends in testing dynamically adaptable systems. For this objective, 25 articles between 2020 and 2023 were analyzed to answer our research questions. As a result, approaches and their characteristics were identified, such as what type of system they can be applied to, what activity is included in the testing process, and at what level of testing. We also highlighted challenges that are still being faced and trends in testing dynamically adaptive systems. For a more in-depth analysis of the results related to the challenges, grounded theory procedures were applied to organize them and encourage future research that seeks to overcome and mitigate them. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	booktitle = {International {Conference} on {Enterprise} {Information} {Systems}, {ICEIS} - {Proceedings}},
	publisher = {Science and Technology Publications, Lda},
	author = {Costa, Isabely do Nascimento and Santos, Ismayle S. and Andrade, Rossana M.C.},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, Systematic Review, Software testing, Adaptable system, Adaptive systems, Dynamic aspects, Dynamically adaptive systems, Fault detection, Faults detection, Research questions, Software testings, Surrounding environment, Testing process},
	pages = {129 -- 140},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{barcelos_requirements_2024-2,
	title = {Requirements engineering in industry 4.0: {State} of the art and directions to continuous requirements engineering},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186634667&doi=10.1002%2fsys.21753&partnerID=40&md5=f62bda5bb7a47f95a40a286270f546c2},
	doi = {10.1002/sys.21753},
	abstract = {The 4th Industrial Revolution, also known as Industry 4.0, intends to transform manufacturing processes into smart factories with full digitalization and intelligent, decentralized, and flexible production. In this scenario, Industry 4.0 systems (i.e., software-intensive systems that automate smart factories) have required rigorous and continuous development, but smart factory companies often have difficulty dealing with Requirements Engineering (RE) where requirements continuously change and emerge at runtime to support the changeability of complex production processes. Such requirements encompass engineering (e.g., mechanical, electrical, electronic, production/manufacturing) and business areas and involve the vertical and horizontal integration of heterogeneous manufacturing systems. There is also a lack of a panorama of how Industry 4.0 projects have performed with RE activities. The main goal of this paper is to present the state-of-the-art research concerning RE in Industry 4.0 and draw attention to the next most urgent steps. For this, we selected and examined studies that address RE for Industry 4.0, noting that much of this literature is recent but does not fully address the complexity and dynamism of the requirements for Industry 4.0. Grounded on these studies and our academic and industry experience, we highlight the need for Continuous Requirements Engineering (CRE) for Industry 4.0. Significance and Practitioner Points: The main implications of this paper are: (i) For researchers: It offers the state of the art of RE in the context of Industry 4.0 and points out several important open issues that require an urgent investigation through new research topics; and (ii) For practitioners: It provides directions for new or even existing Industry 4.0 projects on how to deal with RE activities aiming to overcome the several challenges to perform them. © 2024 Wiley Periodicals, Inc.},
	journal = {Systems Engineering},
	author = {Barcelos, Leonardo Vieira and Antonino, Pablo Oliveira and Nakagawa, Elisa Yumi},
	year = {2024},
	note = {Publisher: John Wiley and Sons Inc
Type: Review},
	keywords = {State of the art, Requirement engineering, Requirements engineering, Engineering activities, Continuous requirement engineering, Decentralized production, Flexible production, Industrial revolutions, Industry 4.0, Manufacturing process, Smart factory},
	annote = {Cited by: 0},
}

@inproceedings{nayak_gdpr_2024-2,
	title = {{GDPR} {Compliant} {ChatGPT} {Playground}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196073971&doi=10.1109%2fICETCS61022.2024.10543557&partnerID=40&md5=abdead7dc470268c7eadcc3ac9c14abb},
	doi = {10.1109/ICETCS61022.2024.10543557},
	abstract = {ChatGPT is an AI based conversational tool developed by OpenAPI on the design principles of Large Language Model (LLM) and a publicly accessible tool. ChatGPT has increasingly becoming popular tool for enabling applications involving interactive and contextual search across corporate companies, profit/non-profit organizations, educational/medical institutions, and researcher's community to name a few.The corporate companies are abided by GDPR (General Data Protection Regulation) compliance checks and restricted to share confidential, personal or sensitive information's (hereinafter referred as critical data) into public domains. As ChatGPT server is hosted outside corporate boundaries, to fulfil GDPR compliance check, the corporate companies must have necessary systems in place of any data leaving outside of corporate boundaries.We are proposing a novel solution in identifying GDPR noncompliant DPP (Data Privacy and Protection) entities from the prompt query given to ChatGPT. To achieve this, from the corporate documents we first manually tag critical entities from 'named entity tagging tool' in building the corporate specific DPP entities knowledgebase, build custom NER (Named Entity Recognition) model on top of prebuilt corporate specific DPP entities knowledgebase, an ChatGPT playground interface to accept any user's prompt query, before firing the query to ChatGPT we validate the user's prompt query against custom NER model to detect if any corporate specific DPP entities are present, warn the user by highlighting the corporate specific DPP entities if present to facilitate user in negating the same, we enabled feedback loop from the user for the highlighted corporate specific DPP entities to improvise the custom NER model and logging all the input prompt queries fired to ChatGPT to enable corporate auditing process by using techniques from Natural Language Processing (NLP), Information Extraction, Information Retrieval (IR) and Custom NER model. © 2024 IEEE.},
	booktitle = {International {Conference} on {Emerging} {Technologies} in {Computer} {Science} for {Interdisciplinary} {Applications}, {ICETCS} 2024},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Nayak, Shiva Prasad and Pasumarthi, Suresh and Rajagopal, Bharathi and Verma, Ashwani Kumar},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Natural language processing systems, Natural languages, ChatGPT, Compliance control, Custom named entity recognition model, General data protection regulations, Information extraction, Information retrieval, Language model, Language processing, Large language model, Named entity recognition, Natural language processing, Recognition models, Sensitive data},
	annote = {Cited by: 0},
}

@inproceedings{wu_exploratory_2024,
	title = {An exploratory study of v-model in building ml-enabled software: {A} systems engineering perspective},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196516546&doi=10.1145%2f3644815.3644951&partnerID=40&md5=2e7bd8c1abc6da4635717682fdff6fde},
	doi = {10.1145/3644815.3644951},
	abstract = {Machine learning (ML) components are being added to more and more critical and impactful software systems, but the software development process of real-world production systems from prototyped ML models remains challenging with additional complexity and interdisciplinary collaboration challenges. This poses difficulties in using traditional software lifecycle models such as waterfall, spiral, or agile models when building ML-enabled systems. In this research, we apply a Systems Engineering lens to investigate the use of V-Model in addressing the interdisciplinary collaboration challenges when building ML-enabled systems. By interviewing practitioners from software companies, we established a set of 8 propositions for using V-Model to manage interdisciplinary collaborations when building products with ML components. Based on the propositions, we found that despite requiring additional efforts, the characteristics of V-Model align effectively with several collaboration challenges encountered by practitioners when building ML-enabled systems. We recommend future research to investigate new process models that leverage the characteristics of V-Model such as the system decomposition, clear system boundary, and consistency of Validation Verification (VV) for building ML-enabled systems. © 2024 Copyright is held by the owner/author(s). Publication rights licensed to ACM.},
	booktitle = {Proceedings - 2024 {IEEE}/{ACM} 3rd {International} {Conference} on {AI} {Engineering} - {Software} {Engineering} for {AI}, {CAIN} 2024},
	publisher = {Association for Computing Machinery, Inc},
	author = {Wu, Jie J.W.},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Software design, Life cycle, Software-systems, Machine-learning, Real-world, Engineering perspective, In-buildings, Software development process, Machine components, Building machines, Buildings, Exploratory studies, Interdisciplinary collaborations, Software prototyping, V-model},
	pages = {30 -- 40},
	annote = {Cited by: 0; All Open Access, Bronze Open Access},
}

@article{rainer_reporting_2024-2,
	title = {Reporting case studies in systematic literature studies—{An} evidential problem},
	volume = {174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195473840&doi=10.1016%2fj.infsof.2024.107501&partnerID=40&md5=b030b447b2cf73523aec750252a31d2a},
	doi = {10.1016/j.infsof.2024.107501},
	abstract = {Context: The term and label, “case study”, is not used consistently by authors of primary studies in software engineering research. It is not clear whether this problem also occurs for systematic literature studies (SLSs). Objective: To investigate the extent to which SLSs in/correctly use the term and label, “case study”, when classifying primary studies. Methods: We systematically collect two sub-samples (2010–2021 \& 2022) comprising a total of eleven SLSs and 79 primary studies. We examine the designs of these SLSs, and then analyse whether the SLS authors and the primary-study authors correctly label the respective primary study as a “case study”. Results: 76\% of the 79 primary studies are misclassified by SLSs (with the two sub-samples having 60\% and 81\% misclassification, respectively). For 39\% of the 79 studies, the SLSs propagate a mislabelling by the original authors, whilst for 37\%, the SLSs introduce a new mislabel, thus making the problem worse. SLSs rarely present explicit definitions for “case study” and when they do, the definition is not consistent with established definitions. Conclusions: SLSs are both propagating and exacerbating the problem of the mislabelling of primary studies as “case studies”, rather than – as we should expect of SLSs – correcting the labelling of primary studies, and thus improving the body of credible evidence. Propagating and exacerbating mislabelling undermines the credibility of evidence in terms of its quantity, quality and relevance to both practice and research. © 2024 The Author(s)},
	journal = {Information and Software Technology},
	author = {Rainer, Austen and Wohlin, Claes},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Systematic mapping studies, Literature studies, Case-studies, Credible evidence, Labelings, Misclassifications, Software engineering research, Sub-samples},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{waqas_using_2024-2,
	title = {Using {LowCode} and {NoCode} {Tools} in {DevOps}: {A} {Multivocal} {Literature} {Review}},
	volume = {1135},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186469250&doi=10.1007%2f978-3-031-50590-4_5&partnerID=40&md5=c5ae8e1a062bce7898965cd9cbe9abcd},
	doi = {10.1007/978-3-031-50590-4_5},
	abstract = {DevOps and Low-code/No-code tools are two trends that are gaining interest in both academic and industry environments. This study analyzed the usage of low-code and no-code tools in DevOps. It examined the available tools, their applications, and the associated concerns or limitations. To do so, a Multivocal Literature Review (MVLR) was conducted to include academic and grey literature. The results reveal the utilization of a range of tools across multiple aspects of DevOps, including application delivery, workflow automation, process management, continuous integration, deployment, monitoring and logging. Despite their potential benefits, concerns and limitations such as script standardization, security risks, limited customization, vendor lock-in, and scalability issues persist. This review, alongside identified concerns, emphasizes the increasing adoption and benefits of Low-code/No-code tools in DevOps. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2024.},
	journal = {Studies in Computational Intelligence},
	author = {Waqas, Muhammad and Ali, Zohaib and Sánchez-Gordón, Mary and Kristiansen, Monica},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Book chapter},
	pages = {71 -- 87},
	annote = {Cited by: 0},
}

@article{lin_shape_2024-2,
	title = {Shape or size matters? {Towards} standard reporting of tensile testing parameters for human soft tissues: systematic review and finite element analysis},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190110729&doi=10.3389%2ffbioe.2024.1368383&partnerID=40&md5=481782653b6ae903cfe0c77a6615b1b3},
	doi = {10.3389/fbioe.2024.1368383},
	abstract = {Material properties of soft-tissue samples are often derived through uniaxial tensile testing. For engineering materials, testing parameters (e.g., sample geometries and clamping conditions) are described by international standards; for biological tissues, such standards do not exist. To investigate what testing parameters have been reported for tensile testing of human soft-tissue samples, a systematic review of the literature was performed using PRISMA guidelines. Soft tissues are described as anisotropic and/or hyperelastic. Thus, we explored how the retrieved parameters compared against standards for engineering materials of similar characteristics. All research articles published in English, with an Abstract, and before 1 January 2023 were retrieved from databases of PubMed, Web of Science, and BASE. After screening of articles based on search terms and exclusion criteria, a total 1,096 articles were assessed for eligibility, from which 361 studies were retrieved and included in this review. We found that a non-tapered shape is most common (209 of 361), followed by a tapered sample shape (92 of 361). However, clamping conditions varied and were underreported (156 of 361). As a preliminary attempt to explore how the retrieved parameters might influence the stress distribution under tensile loading, a pilot study was performed using finite element analysis (FEA) and constitutive modeling for a clamped sample of little or no fiber dispersion. The preliminary FE simulation results might suggest the hypothesis that different sample geometries could have a profound influence on the stress-distribution under tensile loading. However, no conclusions can be drawn from these simulations, and future studies should involve exploring different sample geometries under different computational models and sample parameters (such as fiber dispersion and clamping effects). Taken together, reporting and choice of testing parameters remain as challenges, and as such, recommendations towards standard reporting of uniaxial tensile testing parameters for human soft tissues are proposed. Copyright © 2024 Lin, Pirrung, Niestrawska, Ondruschka, Pinter, Henyš and Hammer.},
	journal = {Frontiers in Bioengineering and Biotechnology},
	author = {Lin, Alvin C. and Pirrung, Felix and Niestrawska, Justyna A. and Ondruschka, Benjamin and Pinter, Gerald and Henyš, Petr and Hammer, Niels},
	year = {2024},
	note = {Publisher: Frontiers Media SA
Type: Review},
	keywords = {Aspect ratio, Aspect-ratio, Bone, Dogbone, Dumbbell, Finite element method, Geometry, Histology, Human soft tissue, ISO material testing standard, ISO Standards, Load testing, Material testing, Non-tapered, Rectangular, Soft tissue, Stress analysis, Stress concentration, Tapered, Tensile stress, Tensile testing, Testing standards, Tissue engineering},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{cunha_insight_2024-2,
	title = {An insight into the capabilities of professionals and teams in agile software development: {An} update of the systematic literature review},
	volume = {20},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186425034&doi=10.24138%2fjcomss-2023-0172&partnerID=40&md5=0fb0b3eb775f0b92eb3fb70e8c650c20},
	doi = {10.24138/jcomss-2023-0172},
	abstract = {Agile Software Development (ASD) confronts the challenge of effectively measurement and predicting the capabilities of software engineers and teams to improve individual performance, team efficiency, and project success. This study delves into exploring and identifying gaps and research prospects in assessing and predicting human capabilities within ASD. Thus, we conducted a Systematic Literature Review, building upon a prior review from 2001 to 2016 by different authors. To encompass primary studies published after 2016, we extended to 2022. Further, our study extends the scope of the previous SLR with a new research question to identify key attributes in publications focused on agile team formation. Our findings disclosed new attributes for evaluating and predicting the capabilities of professionals engaged in ASD, such as Openness to Creativity and Agile Adaptation. These attributes boost individual performance, contribute to ameliorated team productivity, and facilitate the precise composition of teams. Moreover, this study expands our prior study, providing more details on capability identification and research design, extends the analysis of attributes and prediction models, provides a more granular discussion of discoveries and comparisons with prior review, and more indepth discussion about practical implications and thoroughly examines study validity. We observed that technical metrics were more prevalent than social and innovative aspects. Also, the study identified the prediction of agile capabilities as an emerging research domain necessitating further scrutiny due to the scarcity of existing models. The majority of studies (78\%) supplied detailed metric descriptions, facilitating the evolution of the capabilities repository and supporting future investigations in this domain. Ultimately, these findings can aid agile practitioners in formulating team composition decisions based on individuals’ and teams’ appraised and foreseen abilities. © 2024, Croatian Communications and Information Society. All rights reserved.},
	number = {1},
	journal = {Journal of Communications Software and Systems},
	author = {Cunha, Felipe and Perkusich, Mirko and Guimaraes, Everton and Santos, Ramon and Rique, Thiago and Albuquerque, Danyllo and Perkusich, Angelo and Almeida, Hyggo and Gorgônio, Kyller Costa},
	year = {2024},
	note = {Publisher: Croatian Communications and Information Society
Type: Article},
	pages = {99 -- 112},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{tseng_unlocking_2024-2,
	title = {Unlocking the {Potential} of {Open} {Government} {Data}: {Exploring} the {Strategic}, {Technical}, and {Application} {Perspectives} of {High}-{Value} {Datasets} {Opening} in {Taiwan}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85195261385&doi=10.1145%2f3657054.3657089&partnerID=40&md5=39e4f20be1e5ec31effab47f6affbaed},
	doi = {10.1145/3657054.3657089},
	abstract = {Today, data has an unprecedented value as it forms the basis for data-driven decision-making, including serving as an input for AI models, where the latter is highly dependent on the availability of the data. However, availability of data in an open data format creates a little added value, where the value of these data, i.e., their relevance to the real needs of the end user, is key. This is where the concept of “high-value dataset” (HVD) comes into play, which has become popular in recent years. Defining and opening HVD is a process consisting of a set of interrelated steps, the implementation of which may vary from one country or region to another. Therefore, there has recently been a call to conduct research in a country or region setting considered to be of greatest national value. So far, only a few studies have been conducted at the regional or national level, most of which consider only one step of the process, such as identifying HVD or measuring their impact. With this study, we answer this call and examine the national case of Taiwan by exploring the entire lifecycle of HVD opening. As such, the aim of the paper is to understand and evaluate the life cycle of high-value dataset publishing in one of the world’s leading producers of information and communication technology (ICT) products - Taiwan. To do this, we conduct a qualitative study with exploratory interviews with representatives from government agencies in Taiwan responsible for HVD opening, exploring the entire HVD lifecycle. As such, we examine (1) strategic aspects related to the HVD determination process, (2) technical aspects, and (3) application aspects. © 2024 Copyright held by the owner/author(s).},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Tseng, Hsien-Lee and Nikiforova, Anastasija},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Ecosystems, Decision making, Life cycle, e-government, High-value dataset, High-value dataset”, Impact, OGD, Open Data, Open data ecosystem, Open datum, Open government data, Public data, Public data ecosystem, Public values, SDG, Sustainable development, Sustainable development goal},
	pages = {278 -- 287},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{bourai_deep_2024-2,
	title = {Deep learning-assisted medical image compression challenges and opportunities: systematic review},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191883296&doi=10.1007%2fs00521-024-09660-8&partnerID=40&md5=13b002c543c49b3905dbde4e4d088646},
	doi = {10.1007/s00521-024-09660-8},
	abstract = {Over the preceding decade, there has been a discernible surge in the prominence of artificial intelligence, marked by the development of various methodologies, among which deep learning emerges as a particularly auspicious technique. The captivating attribute of deep learning, characterised by its capacity to glean intricate feature representations from data, has served as a catalyst for pioneering approaches and methodologies spanning a multitude of domains. In the face of the burgeoning exponential growth in digital medical image data, the exigency for adept image compression methodologies has become increasingly pronounced. These methodologies are designed to preserve bandwidth and storage resources, thereby ensuring the seamless and efficient transmission of data within medical applications. The critical nature of medical image compression accentuates the imperative to confront the challenges precipitated by the escalating deluge of medical image data. This review paper undertakes a comprehensive examination of medical image compression, with a predominant focus on sophisticated, research-driven deep learning techniques. It delves into a spectrum of approaches, encompassing the amalgamation of deep learning with conventional compression algorithms and the application of deep learning to enhance compression quality. Additionally, the review endeavours to explicate these fundamental concepts, elucidating their inherent characteristics, merits, and limitations. © The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature 2024.},
	number = {17},
	journal = {Neural Computing and Applications},
	author = {Bourai, Nour El Houda and Merouani, Hayet Farida and Djebbar, Akila},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Review},
	keywords = {Systematic Review, Deep learning, Learning systems, ]+ catalyst, 3d predictor, Digital storage, Exponential growth, Feature representation, Image compression, Lossless compression, Lossy compressions, Medical applications, Medical image compression, Medical images datum, Medical imaging},
	pages = {10067 -- 10108},
	annote = {Cited by: 0},
}

@article{krafft_black-box_2024-2,
	title = {Black-{Box} {Testing} and {Auditing} of {Bias} in {ADM} {Systems}},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194259767&doi=10.1007%2fs11023-024-09666-0&partnerID=40&md5=c0ad5488881bfd7adbca316b7f704c7c},
	doi = {10.1007/s11023-024-09666-0},
	abstract = {For years, the number of opaque algorithmic decision-making systems (ADM systems) with a large impact on society has been increasing: e.g., systems that compute decisions about future recidivism of criminals, credit worthiness, or the many small decision computing systems within social networks that create rankings, provide recommendations, or filter content. Concerns that such a system makes biased decisions can be difficult to investigate: be it by people affected, NGOs, stakeholders, governmental testing and auditing authorities, or other external parties. Scientific testing and auditing literature rarely focuses on the specific needs for such investigations and suffers from ambiguous terminologies. With this paper, we aim to support this investigation process by collecting, explaining, and categorizing methods of testing for bias, which are applicable to black-box systems, given that inputs and respective outputs can be observed. For this purpose, we provide a taxonomy that can be used to select suitable test methods adapted to the respective situation. This taxonomy takes multiple aspects into account, for example the effort to implement a given test method, its technical requirement (such as the need of ground truth) and social constraints of the investigation, e.g., the protection of business secrets. Furthermore, we analyze which test method can be used in the context of which black box audit concept. It turns out that various factors, such as the type of black box audit or the lack of an oracle, may limit the selection of applicable tests. With the help of this paper, people or organizations who want to test an ADM system for bias can identify which test methods and auditing concepts are applicable and what implications they entail. © The Author(s) 2024.},
	number = {2},
	journal = {Minds and Machines},
	author = {Krafft, Tobias D. and Hauer, Marc P. and Zweig, Katharina},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.
Type: Article},
	keywords = {Decision making, Social networking (online), Taxonomies, Machine learning, Machine-learning, Behavioral research, Algorithmics, Auditing, Automated decision making, Bias, Black boxes, Black-box testing, Computing system, Decision-making systems, Small decisions, Social sciences computing, Test method},
	annote = {Cited by: 0},
}

@inproceedings{alshehhi_6dvf_2024-2,
	title = {{6DVF}: {A} {Framework} for the {Development} and {Evaluation} of {Mobile} {Data} {Visualisations}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85193993437&doi=10.5220%2f0012692900003687&partnerID=40&md5=1273a2dbb4880051c8da6756c713353f},
	doi = {10.5220/0012692900003687},
	abstract = {Mobile apps, in particular tracking apps, rely heavily on data visualisations to empower end-users to make decisions about their health, sport, finance, household, and more. This has prompted app designers and developers to invest more effort in delivering quality visualisations. Many frameworks, including the Visualisation and Design Framework and Google Material Design, have been developed to guide the creation of informative and well-designed charts. However, our study reveals the need to incorporate additional aspects in the design process of such data visualisations to address user characterisation and needs, the nature of data to visualise, and the experience on small smart screens. In this paper, we introduce the Six-Dimensions Data Visualization Framework (6DVF), specifically designed for data visualisation on mobile devices. The 6DVF encompasses user characteristics and needs, data attributes, chart styling, interaction, and the mobile environment. We conducted two evaluation studies to measure the effectiveness and practicality of our 6DVF in guiding designers, pinpointing areas for improvement—especially in data completeness and usability for end-users. © 2024 by SCITEPRESS – Science and Technology Publications, Lda.},
	booktitle = {International {Conference} on {Evaluation} of {Novel} {Approaches} to {Software} {Engineering}, {ENASE} - {Proceedings}},
	publisher = {Science and Technology Publications, Lda},
	author = {Alshehhi, Yasmeen Anjeer and Ahmad, Khlood and Abdelrazek, Mohamed and Bonti, Alessio},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {End-users, Design, Data visualization, Evaluation methods, Human-centered computing, Mobile app, Mobile data, Visualization, Visualization design guideline, Visualization designs, Visualization evaluation method, Visualization framework, Visualization technique},
	pages = {555 -- 562},
	annote = {Cited by: 0},
}

@article{melegati_product_2024,
	title = {Product managers in software startups: {A} grounded theory},
	volume = {174},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196167729&doi=10.1016%2fj.infsof.2024.107516&partnerID=40&md5=3e5c805cc1957e62fe7c6b131dd52c4c},
	doi = {10.1016/j.infsof.2024.107516},
	abstract = {Context: Defining and designing a software product is not merely a technical endeavor, but also a socio-technical journey. As such, its success is associated with human-related aspects, such as the value users perceive. To handle this issue, the product manager role has become more evident in software-intensive companies. A unique, challenging context for these professionals is constituted by software startups, emerging companies developing novel solutions looking for sustainable and scalable business models. Objective: This study aims to describe the role of product managers in the context of software startups. Method: We performed a Socio-Technical Grounded Theory study using data from blog posts and interviews. Results: The results describe the product manager as a multidisciplinary, general role, not only guiding the product by developing its vision but also as a connector that emerges in a growing company, enabling communication of software development with other areas, mainly business and user experience. The professional performing this role has a background in one of these areas but a broad knowledge and understanding of key concepts of the other areas is needed. We also describe how differences of this role to other lead roles are perceived in practice. Conclusions: Our findings represent several implications for research, such as better understanding of the role transformation in growing software startups, practice, e.g., identifying the points to which a professional migrating to this role should pay attention, and the education of future software developers, by suggesting the inclusion of related topics in the education and training of future software engineers. © 2024 The Author(s)},
	journal = {Information and Software Technology},
	author = {Melegati, Jorge and Wiese, Igor and Guerra, Eduardo and Chanin, Rafael and Aldaeej, Abdullah and Mikkonen, Tommi and Prikladnicki, Rafael and Wang, Xiaofeng},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Product management, Software design, Agile software development, Sociotechnical, Personnel training, Agile manufacturing systems, Product design, Software products, Business models, Grounded theory, Novel solutions, Product manager, Sociotechnical systems, Software startup},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{ramadhan_combining_2024-2,
	title = {Combining intelligent tutoring systems and gamification: a systematic literature review},
	volume = {29},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167368098&doi=10.1007%2fs10639-023-12092-x&partnerID=40&md5=43507a0e5e0cd0e0cd9b55014c46e64e},
	doi = {10.1007/s10639-023-12092-x},
	abstract = {One of the Information and Communication Technology (ICT) developments used in the learning process is the Intelligent Tutoring System (ITS), and gamification can overcome boredom, lack of interest or motivation, and monotony when using the ITS. In this study, the application of ITS equipped with Gamification is called ITS + G. Currently, several studies have built the ITS + G. However, there has not been a Systematic Literature Review (SLR) that synthesizes the characteristics of the ITS and Gamification combination. Several previous SLRs have been carried out and discussed the ITS only and several other SLRs discussed gamification only. Therefore, this SLR focused on the characteristics of ITS and gamification as a unit. This study succeeded in synthesizing that ITS + G has the potential to be applied to both STEM and non-STEM subjects. Three main game elements are mostly used in ITS + G: levels, points, and progress bars, which are supported for several reasons. Several techniques that have been used to measure the success of ITS + G are synthesized. Several positive impacts of ITS + G are revealed. Some negative impacts that need to be considered and studied in future research are also noticed. The results of this study could be the basis for ITS + G research in the future and increase the repertoire of knowledge related to ITS and Gamification. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2023.},
	number = {6},
	journal = {Education and Information Technologies},
	author = {Ramadhan, Arief and Warnars, Harco Leslie Hendric Spits and Razak, Fariza Hanis Abdul},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	pages = {6753 -- 6789},
	annote = {Cited by: 1},
}

@article{russo_towards_2024-2,
	title = {Towards a {Comprehensive} {Framework} for the {Multidisciplinary} {Evaluation} of {Organizational} {Maturity} on {Business} {Continuity} {Program} {Management}: {A} {Systematic} {Literature} {Review}},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152037415&doi=10.1080%2f19393555.2023.2195577&partnerID=40&md5=d074612c8a1ed3cb1576ba883e6469fd},
	doi = {10.1080/19393555.2023.2195577},
	abstract = {Organizational dependency on Information and Communication Technology (ICT) drives the preparedness challenge to cope with business process disruptions. Business Continuity Management (BCM) encompasses effective planning to enable business functions to resume to an acceptable state of operation within a defined timeframe. This paper presents a systematic literature review that communicates the strategic guidelines to streamline the organizational processes in the BCM program, culminating in the Business Continuity Plan design, according to the organization’s maturity. The systematic literature review methodology follows the Evidence-Based Software Engineering protocol assisted by the Parsifal tool, using the EbscoHost, ScienceDirect, and Scopus databases, ranging from 2000 to February 2021. International Standards and Frameworks guide the BCM program implementation, however, there is a gap in communicating metrics and what needs to be measured in the BCM program. The major paper result is the confirmation of the identified gap, through the analysis of the studies that, according to the BCM components, report strategic guidelines to streamline the BCM program. The analysis quantifies and discusses the contribution of the studies on each BCM component to design a framework supported by metrics, that allows assessing the organization’s preparedness in each BCM component, focusing on Information Systems and ICT strategies. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	number = {1},
	journal = {Information Security Journal},
	author = {Russo, Nelson and Reis, Leonilde and Silveira, Clara and Mamede, Henrique S.},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.
Type: Review},
	keywords = {Software engineering, Information management, Systematic literature review, Information systems, Information use, Business continuity, Business continuity management, Business continuity plans, Information and Communication Technologies, Management components, Management projects, Organisational, Organizational maturity, Program management},
	pages = {54 -- 72},
	annote = {Cited by: 5; All Open Access, Green Open Access},
}

@article{cannon_exploring_2024-2,
	title = {Exploring {Knowledge}-{Based} {Systems} for {Commercial} {Mortgage} {Underwriting}},
	volume = {1898},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181982238&doi=10.1007%2f978-3-031-50385-6_9&partnerID=40&md5=d52fdc852e47217b293e25ba18fad7c1},
	doi = {10.1007/978-3-031-50385-6_9},
	abstract = {While the residential mortgage industry has benefited from automated mortgage applications and underwriting, the commercial mortgage industry still relies heavily on manual underwriting. The paper aims to present the state of research in the domain of knowledge-based systems (KBS) and commercial mortgage underwriting and to identify the challenges in commercial mortgage underwriting by conducting a systematic literature review (SLR). The SLR uses the review process outlined by Kitchenham and includes a hybrid coding approach to analyze the data. The paper finds that KBS and ontologies to automate commercial mortgage underwriting were not studied yet. It also identifies several challenges in mortgage underwriting in data, process, and underwriting categories. The findings of this paper can be used for further research in the field of commercial mortgage underwriting and KBS. KBS appear suitable to address the identified challenges and can be integrated into information system automation or artificial intelligence (AI) implementations for commercial mortgage underwriting. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Cannon, K. Patricia and Preis, Simon J.},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Systematic literature review, State of research, Review process, Commercial mortgage underwriting, Domain of knowledge, Hybrid coding, Knowledge based systems, Knowledge-based systems, Mortgage industry, Ontology, Ontology's, Underwriting challenge},
	pages = {101 -- 113},
	annote = {Cited by: 0},
}

@article{iqbal_exploring_2024-2,
	title = {Exploring issues of story-based effort estimation in {Agile} {Software} {Development} ({ASD})},
	volume = {236},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189756864&doi=10.1016%2fj.scico.2024.103114&partnerID=40&md5=5c6fe43964d8140912f49f9bc15524e9},
	doi = {10.1016/j.scico.2024.103114},
	abstract = {Context: Effort estimation based on user stories plays a pivotal role in agile software development, where accurate predictions of project efforts are vital for success. While various supervised ML tools attempt to estimate effort, the prevalence of estimation errors presents significant challenges, as evidenced by the CHAOS report by the Standish Group, which highlights incorrect estimations contributing to a substantial percentage of failed agile projects. Objectives: This research delves into the domain of user story-based effort estimation in agile software development, aiming to explore the issues arising from inaccurate estimations. The primary goal is to uncover these issues comprehensively and propose potential solutions, thus enhancing the efficacy of the user story-based estimation method. Methods: To achieve the research objectives, a systematic literature review (SLR) is conducted, surveying a wide range of sources to gather insights into issues surrounding user story-based effort estimation. The review encompasses diverse estimation methods, user story attributes, and the array of challenges that can result from inaccurate estimations. Results: The SLR reveals a spectrum of issues undermining the accuracy of user story-based effort estimation. It identifies internal factors like communication, team expertise, and composition as crucial determinants of estimation reliability. Consistency in user stories, technical complexities, and task engineering practices also emerge as significant contributors to estimation inaccuracies. The study underscores the interconnectedness of these issues, emphasizing the need for a standardized protocol to minimize inaccuracies and enhance estimation precision. Conclusion: In light of the findings, it becomes evident that addressing the multi-dimensional factors influencing user story-based effort estimation is imperative for successful agile software development. The study underscores the interplay of various aspects, such as team dynamics, task complexity, and requirement engineering, in achieving accurate estimations. By recognizing these challenges and implementing recommended solutions, software development processes can avoid failures and enhance their prospects of success in the agile paradigm. © 2024 Elsevier B.V.},
	journal = {Science of Computer Programming},
	author = {Iqbal, Muhammad and Ijaz, Muhammad and Mazhar, Tehseen and Shahzad, Tariq and Abbas, Qamar and Ghadi, YazeedYasin and Ahmad, Wasim and Hamam, Habib},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Systematic literature review, Software design, User stories, Agile, Effort Estimation, Issues and challenges, Ml, User story effort estimation},
	annote = {Cited by: 0},
}

@article{verbeke_safeguarding_2024-2,
	title = {Safeguarding {Users} of {Consumer} {Mental} {Health} {Apps} in {Research} and {Product} {Improvement} {Studies}: an {Interview} {Study}},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183317493&doi=10.1007%2fs12152-024-09543-8&partnerID=40&md5=3cfcc42befa8c56721a1a2b252cfaa03},
	doi = {10.1007/s12152-024-09543-8},
	abstract = {Mental health-related data generated by app users during the routine use of Consumer Mental Health Apps (CMHAs) are being increasingly leveraged for research and product improvement studies. However, it remains unclear which ethical safeguards and practices should be implemented by researchers and app developers to protect users during these studies, and concerns have been raised over their current implementation in CMHAs. To better understand which ethical safeguards and practices are implemented, why and how, 17 app developers and researchers were interviewed who had been involved in using CMHA data for studies. Interviewees discussed the impact on stakeholder interests, sufficiency thresholds and procedural alterations of informed consent, data protection, gathering app user perspectives and representing users in app design and study conduct, and ensuring adequate support. Although the reasoning behind how and why these ethical safeguards and practices should be implemented showed considerable variability and several gaps, interviewees converged on various general lines of reasoning. This allowed for the development of a coherent and nuanced account that could prove useful for future CMHA studies and which could stimulate further normative investigation of the role of ethical safeguards and practices in these studies. © 2024, The Author(s), under exclusive licence to Springer Nature B.V.},
	number = {1},
	journal = {Neuroethics},
	author = {Verbeke, Kamiel and Jain, Charu and Shpendi, Ambra and Borry, Pascal},
	year = {2024},
	note = {Publisher: Springer Science and Business Media B.V.
Type: Article},
	keywords = {human, article, consumer, informed consent, interview, mental health, reasoning, research ethics, role playing},
	annote = {Cited by: 0},
}

@inproceedings{vyakaranam_preliminary_2024-2,
	title = {Preliminary {Study}: {Speech} {Emotion} {Recognition} in {Online} {Teaching} {From} the {Perspective} of {Educators} {Especially} {Late} {Deafened}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191480308&doi=10.1109%2fICoSEIT60086.2024.10497503&partnerID=40&md5=a411b6e9dde2de86d370a3b44da0cd85},
	doi = {10.1109/ICoSEIT60086.2024.10497503},
	abstract = {An effective Human Computer Interaction (HCI) for recognizing human emotions automatically is known to as affective computing. Affective computing is achieved through programming, which is meant to detect emotions from facial expressions or verbal utterances. These detected emotions can be further used in many applications. Speech emotion recognition (SER) is one such affective computing application where the detected emotions can be used effectively, particularly in online education. Such an application would allow for the teaching strategies to be tailored for the students, on the basis of their understanding of the ongoing class, which is reflected in their verbal feedback. This valuable application of technology will enhance and support education for all, especially the educators who have issues with hearing verbal feedback from students. The main objective of this study is to find out from educators (with and without hearing impairment) on the necessity of knowing student emotions to enhance teaching outcomes for effective teaching and also the benefits of having an automatic detection system which would display student emotions in an online classroom setting. The outcome of this research clearly indicated how a preliminary user requirement study highlights the need and benefits of implementing a SER system. © 2024 IEEE.},
	booktitle = {2024 2nd {International} {Conference} on {Software} {Engineering} and {Information} {Technology}, {ICoSEIT} 2024},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Vyakaranam, Aparna and Ramayah, Bavani and Maul, Tomas},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Audition, Human computer interaction, Affective Computing, Emotion Recognition, Speech recognition, Computing applications, E-learning, Facial Expressions, Hearing impaired, On-line education, Online teaching, Recognizing Human Emotion, Speech emotion recognition, Student emotions, Students, Teaching strategy},
	pages = {216 -- 221},
	annote = {Cited by: 0},
}

@article{alfayez_technical_2024-2,
	title = {Technical debt ({TD}) through the lens of {Twitter}: {A} survey},
	volume = {36},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150285686&doi=10.1002%2fsmr.2547&partnerID=40&md5=61dbe64fbea9285e8238a45ee0af5365},
	doi = {10.1002/smr.2547},
	abstract = {Technical debt (TD) is a metaphor used to refer to the added software system costs acquired from taking shortcuts. Unfortunately, large amounts of TD can lead to serious consequences, and, thus, the management of TD is essential. Due to TD being a relatively new subject of study, many aspects of TD remain ambiguous. Fortunately, Twitter has been proven to hold a wealth of information on many subjects. As such, this survey study aims to gain a better understanding on how interest in TD has evolved over time and how TD is addressed on Twitter. A total of 128,897 TD-related tweets were scrapped from Twitter and analyzed using a number of proxy measures and Latent Dirichlet Allocation (LDA). The results revealed that interest in TD on Twitter has been generally increasing since the platform's early stages. Furthermore, TD-related tweets were found to revolve around 11 distinct categories. The TD in games category was discovered to be the most popular category, followed by TD communication and TD repayment. The results highlight that TD is a diverse and overarching topic that contains many potential avenues for further exploration. Software engineering researchers, practitioners, and educators can utilize this study to help steer their TD-related future efforts. © 2023 John Wiley \& Sons Ltd.},
	number = {4},
	journal = {Journal of Software: Evolution and Process},
	author = {Alfayez, Reem and Winn, Robert and Ding, Yunyan and Alfayez, Ghaida and Boehm, Barry},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Software engineering, Social networking (online), Topic Modeling, Twitter, Software-systems, Technical debts, Large amounts, Proxy measure, Statistics, Survey study, System costs, Through the lens, Wealth of information},
	annote = {Cited by: 0},
}

@article{gamon-sanz_industries_2024-2,
	title = {Industries, frameworks, and key drivers of lean startup: a systematic literature review; [{Sectores}, marcos de trabajo y factores clave del lean startup: una revisión sistemática de la literatura]},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194199657&doi=10.18845%2fte.v18i2.7137&partnerID=40&md5=7a8ba0c27daa55420998a5bf874847ce},
	doi = {10.18845/te.v18i2.7137},
	abstract = {Lean Startup provides an iterative, hypothesis-driven approach to business creation and product development, promoting data-driven decision-making by involving potential users and customers during the development cycle. Despite increased academic attention, the debate on the benefits of Lean Startup is open. To contribute to the understanding of how organisations incorporate Lean Startup principles, this study conducts a systematic literature review and identifies the industries in which its application has been analysed, the adaptation models followed by organisations, and the key drivers for its adoption in new and established organisations. The study's findings contribute to advancing knowledge on the adoption of Lean Startup principles, tools, and techniques in organisations. The results also help to the academic debate surrounding the usefulness and application of Lean Startup principles across different organisational contexts. © 2024 Business School, Instituto Tecnologico de Costa Rica. All rights reserved.},
	number = {2},
	journal = {Tec Empresarial},
	author = {Gamón-Sanz, Alejandro and Alegre, Joaquín and Chiva, Ricardo},
	year = {2024},
	note = {Publisher: Business School, Instituto Tecnologico de Costa Rica
Type: Article},
	pages = {51 -- 80},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{borstler_acceptance_2024-2,
	title = {Acceptance behavior theories and models in software engineering — {A} mapping study},
	volume = {172},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190986067&doi=10.1016%2fj.infsof.2024.107469&partnerID=40&md5=aeb245400acdd1eab3fe1df63e4db457},
	doi = {10.1016/j.infsof.2024.107469},
	abstract = {Context: The adoption or acceptance of new technologies or ways of working in software development activities is a recurrent topic in the software engineering literature. The topic has, therefore, been empirically investigated extensively. It is, however, unclear which theoretical frames of reference are used in this research to explain acceptance behaviors. Objective: In this study, we explore how major theories and models of acceptance behavior have been used in the software engineering literature to empirically investigate acceptance behavior. Method: We conduct a systematic mapping study of empirical studies using acceptance behavior theories in software engineering. Results: We identified 47 primary studies covering 56 theory uses. The theories were categorized into six groups. Technology acceptance models (TAM and its extensions) were used in 29 of the 47 primary studies, innovation theories in 10, and the theories of planned behavior/ reasoned action (TPB/TRA) in six. All other theories were used in at most two of the primary studies. The usage and operationalization of the theories were, in many cases, inconsistent with the underlying theories. Furthermore, we identified 77 constructs used by these studies of which many lack clear definitions. Conclusions: Our results show that software engineering researchers are aware of some of the leading theories and models of acceptance behavior, which indicates an attempt to have more theoretical foundations. However, we identified issues related to theory usage that make it difficult to aggregate and synthesize results across studies. We propose mitigation actions that encourage the consistent use of theories and emphasize the measurement of key constructs. © 2024 The Author(s)},
	journal = {Information and Software Technology},
	author = {Börstler, Jürgen and Ali, Nauman bin and Petersen, Kai and Engström, Emelie},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Software design, Mapping, Behavioral research, Mapping studies, Acceptance behavior, Fitness, Innovations diffusion, TAM, Technology adoption, Theory and models, Theory use in software engineering, TPB, TRA},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{marques_characterizing_2024,
	title = {Characterizing {UX} {Assessment} in the {Context} of {Immersive} {Experiences}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196371615&doi=10.1080%2f10447318.2024.2351711&partnerID=40&md5=0b306eca36772eea60253a55af96a0da},
	doi = {10.1080/10447318.2024.2351711},
	abstract = {An immersive experience is a multisensory experience across a journey or task that’s contextually relevant, enabled by a combination of interactions that create intuitive and emotional value for the user. As the technological landscape has evolved, immersive experiences have become increasingly integrated into our lives. The rise of immersive experiences comes with a focus on how to evaluate those experiences considering User Experience (UX). UX is a multifaceted construct, and its importance differs according to the type of experience. In this scenario, knowing how to evaluate UX is fundamental to understanding whether immersive experiences are pleasant. Despite some attempts to address the UX, a systematic approach to addressing UX in the immersive context still needs to be developed. This paper presents a Systematic Literature Mapping (SLM) to investigate how UX evaluations have been performed and the main UX dimensions that should be considered in immersive experiences, such as engagement, presence, and immersion. Our main result is a theoretical model that we proposed based on the UX definitions and relations from the literature. Our model can help study the relations between UX dimensions, establishing the primary UX dimensions regarding immersive experiences, and as a base for developing new UX evaluation techniques. © 2024 Taylor \& Francis Group, LLC.},
	journal = {International Journal of Human-Computer Interaction},
	author = {Marques, Leonardo and Barcellos, Monalessa P. and Gadelha, Bruno and Conte, Tayana},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.
Type: Article},
	keywords = {Mapping, Systematic mapping studies, User interfaces, Immersion, Users' experiences, Immersive, Immersive experience, Multisensory, Presence, Theoretical modeling},
	annote = {Cited by: 0},
}

@article{parashar_machine_2024-2,
	title = {Machine {Learning} for {Prediction} of {Cardiovascular} {Disease} and {Respiratory} {Disease}: {A} {Review}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182185184&doi=10.1007%2fs42979-023-02529-y&partnerID=40&md5=690bb2c326e3d41a6ad69c7ffb6e6e89},
	doi = {10.1007/s42979-023-02529-y},
	abstract = {Cardiovascular (CVD) and respiratory diseases (RD) have been in the active domain for machine learning (ML) researchers as these diseases significantly contribute to mortality in humans. Some studies suggest that CVD problems such as cerebrovascular problems, dysrhythmia, inflammatory heart disease, ischemic heart disease (IHD), and RD related problems remain high even after COVID-19 infection clears up. To the best of our knowledge, this is the only study that surveyed these two diseases. This paper’s goal is to explore the existing state of the art in the application of ML in the detection, categorization, and prediction of disorders related to CVD and RD. The review highlights ML algorithms used in prediction of CVD and RD related diseases, datasets used by the articles, technique used for feature selection, features selected for the study, dataset used in the article was unimodal or multimodal, and performance of the algorithm. In CVD category, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used Feature Selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. In case of RDs, it was observed that about 15 studies had their performance metrics range between 91\% and 100\%, 7 studies had between 81\% and 90\% and about 2 studies had their performance between 70\% and 80\%. CNN is the most used feature selection technique. Only three studies were found in our set that worked on the multimodal dataset and others used the unimodal dataset. The intent of this review is to stimulate the interest of scientists in this challenging field and to acquaint them with current advances in the field. To design a system that predict CVD or RD in a patient using uni or multi modal datasets, approaches such as data cleaning, feature selection, region of interest (ROI) identification, and classification are applied. This article provided details related to publicly available datasets, most used classification algorithm with performance metric. © 2024, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	number = {1},
	journal = {SN Computer Science},
	author = {Parashar, Gaurav and Chaudhary, Alka and Pandey, Dilkeshwar},
	year = {2024},
	note = {Publisher: Springer
Type: Review},
	annote = {Cited by: 2},
}

@article{zontou_role_2024,
	title = {On the role of virtual reality in engineering education: a systematic literature review of experimental research (2011–2022)},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196430597&doi=10.1080%2f03043797.2024.2369188&partnerID=40&md5=d04b1d880131b64492239853b550d93f},
	doi = {10.1080/03043797.2024.2369188},
	abstract = {Among the numerous fields permeated by Virtual Reality (VR), this review focuses on education and specifically, engineering education. VR allows engineering students to safely and cost-efficiently experience realistic yet simulated environments otherwise inaccessible, costly and/or dangerous. The feelings of presence and immersion generated by VR help motivate and engage students both in the theoretic and the hands-on laboratory parts of their studies. In this systematic review, 104 journal publications are selected and analyzed to cover research questions along five major axes (VR popularity; VR technology; VR-based intervention features; educational methods and assessment of learning outcomes; open research issues). Results reveal an increasing interest in this field, with equal usage of 2D and 3D VR systems. Active and experiential learning methods dominate on the pedagogical axis and aim to develop cognitive and metacognitive skills. Positive effects of VR on student learning across multiple domains verify the potential of VR to improve engineering education. © 2024 SEFI.},
	journal = {European Journal of Engineering Education},
	author = {Zontou, Evangelia and Kaminaris, Stavros and Rangoussi, Maria},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.
Type: Review},
	keywords = {Systematic literature review, Systematic Review, Cost engineering, Learning systems, Engineering education, E-learning, Students, Virtual reality, Learning outcome, Educational intervention, Educational method, Experimental research, Feeling of presences, Hands-on laboratories, Journal publication, Simulated environment},
	annote = {Cited by: 0},
}

@article{pessoa_journey_2024-2,
	title = {A {Journey} to {Identify} {Users}' {Classification} {Strategies} to {Customize} {Game}-{Based} and {Gamified} {Learning} {Environments}},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173024793&doi=10.1109%2fTLT.2023.3317396&partnerID=40&md5=1f2cf8f37cdfb1825e95025ee65cc6ef},
	doi = {10.1109/TLT.2023.3317396},
	abstract = {Game designers and researchers have sought to create gameful environments that consider user preferences to increase engagement and motivation. In this sense, it is essential to identify the most suitable game elements for users' profiles. Designers and researchers must choose strategies to classify users into predefined profiles and select the most appropriate game elements for each user. This activity may challenge designers, learning designers, and researchers since they must base their choice on personal aspects that require a deep understanding. Therefore, this article aims to assist game designers, learning designers, and researchers in selecting user classification strategies to customize and personalize game-based and gamified learning environments. By conducting systematic literature mapping, we consolidate the most common strategies and explore their applications in games and gamification. Our analysis, based on 25 publications, reveals that we can classify the strategies according to user interaction, user personality, learning style, and motivation for learning. Strategies based on user interactions emerge as the most popular, while questionnaires and log data systems are commonly used instruments for identifying user profiles. The findings of this SLM offer valuable knowledge for game designers and researchers to define the criteria that will be used to evaluate the effect of games and gamified environments in educational contexts. © 2008-2011 IEEE.},
	journal = {IEEE Transactions on Learning Technologies},
	author = {Pessoa, Marcela and Lima, Marcia and Pires, Fernanda and Haydar, Gabriel and Melo, Rafaela and Rodrigues, Luiz and Oliveira, David and Oliveira, Elaine and Galvao, Leandro and Gadelha, Bruno and Isotani, Seiji and Gasparini, Isabela and Conte, Tayana},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Software design, Computer aided instruction, Users' experiences, Gamification, Motivation, Computer games, Distal outcome, Game, Game design, Game elements, Game-based Learning, Job analysis, Learning performance, Psychology, Systematic, Task analysis, User classification, User profile, User type},
	pages = {527 -- 541},
	annote = {Cited by: 2},
}

@article{rahman_systematic_2024-2,
	title = {A {Systematic} {Literature} {Review} on {Software} {Maintenance} {Offshoring} {Decisions}},
	volume = {172},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191657835&doi=10.1016%2fj.infsof.2024.107475&partnerID=40&md5=0ecbbd1c575abec8a390ebb7b093a2cc},
	doi = {10.1016/j.infsof.2024.107475},
	abstract = {Context: Over the last decades, the rapid expansion of the internet has prompted an increasing number of organizations that have taken their work global and have outsourced their information technology (IT) activities to specialized suppliers. The longest part of the software life cycle includes software maintenance, which consumes 60-70\% of the total IT budget. Therefore, organizations have adopted offshoring strategies to reduce maintenance costs and free up resources to focus on their core competencies. Offshore outsourcing decision-making involves technical, social, and other influencing factors; however, there is a limited understanding of the key factors associated with offshoring software maintenance within the global software development context. Objective: This work presents the factors that have influenced the decision-making process of offshoring software maintenance. Further, this research sheds light on decision-making by identifying the models, frameworks, and software tools used within this context. Method: A systematic literature review is conducted, delving into the factors related to the decision-making and analyzing the models, frameworks and tools supporting offshoring software maintenance. Results: This study identifies the top 10 key factors concerning the decision-making process, namely human communication, cost reduction, organizational and employee maturity, project management practices, IT infrastructure support, language constraints, knowledge-based support, changes in requirements, legal issues and cultural diversity. In addition, the models, frameworks, and tools used in the decision-making process of software maintenance are analyzed, and research gaps are identified. Conclusion: The findings reveal that the software industry lacks effective and efficient models tailored explicitly for software offshoring within the global software development landscape. Overall, this study provides valuable insights into the decision-making dynamics of software maintenance offshoring by identifying key factors and research gaps that can pave the way for developing more effective decision support systems. © 2024 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Rahman, Hanif Ur and da Silva, Alberto Rodrigues and Alzayed, Asaad and Raza, Mushtaq},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, Decision making, Decisions makings, Software design, Life cycle, Outsourcing, Decision-making process, Computer software, Knowledge based systems, Budget control, Cost reduction, Decision support systems, Factor, Global software development, Key factors, Modelling framework, Modelling tools, Off-shoring, Offshore oil well production, Project management, Research gaps},
	annote = {Cited by: 0},
}

@article{silva_systematic_2024,
	title = {A {Systematic} {Mapping} {Study} about {Learner} {Experience} {Design} in {Computational} {Systems}},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196317075&doi=10.15388%2finfedu.2024.12&partnerID=40&md5=33399f72297e82ae4dca5d0e4005352f},
	doi = {10.15388/infedu.2024.12},
	abstract = {Contemporary society is characterized by diversity and intricacy, necessitating more meaningful learning experiences. To meet these evolving needs, the incorporation of computational systems into education must acknowledge the distinctive characteristics of learners. Therefore, we conducted a Systematic Mapping Study (SMS) to investigate technologies that support the Learner eXperience (LX) design in computational systems. LX refers to learners’ perceptions, reactions, and achievements while engaging with learning resources, encompassing digital games, simulations, and multimedia. The SMS results uncovered distinct LX design technologies, with a noticeable inclination towards learner-centric strategies. Interestingly, the results highlighted a scarcity of research targeting non-traditional learning environments (e.g., technical visits) and that facilitate interactions among learners beyond their own classmates (e.g., industry experts). In this way, the SMS contributes by revealing LX design technologies, LX design elements, relevant constructs/theories, computational systems, environments, contexts, and other related factors, thereby enhancing the understanding of optimal learning experiences within computational learning systems. © (2024) Vilnius University.},
	number = {2},
	journal = {Informatics in Education},
	author = {SILVA, Deivid E. S. and CONTE, Tayana and VALENTIM, Natasha M. C.},
	year = {2024},
	note = {Publisher: Vilnius University Institute of Data Science and Digital Technologies
Type: Article},
	pages = {439 -- 478},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{larsen_statistical_2024-2,
	title = {Statistical {Challenges} in {Online} {Controlled} {Experiments}: {A} {Review} of {A}/{B} {Testing} {Methodology}},
	volume = {78},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174052217&doi=10.1080%2f00031305.2023.2257237&partnerID=40&md5=6c9c5ad886579d9d80d246f2a4580971},
	doi = {10.1080/00031305.2023.2257237},
	abstract = {The rise of internet-based services and products in the late 1990s brought about an unprecedented opportunity for online businesses to engage in large scale data-driven decision making. Over the past two decades, organizations such as Airbnb, Alibaba, Amazon, Baidu, Booking.com, Alphabet’s Google, LinkedIn, Lyft, Meta’s Facebook, Microsoft, Netflix, Twitter, Uber, and Yandex have invested tremendous resources in online controlled experiments (OCEs) to assess the impact of innovation on their customers and businesses. Running OCEs at scale has presented a host of challenges requiring solutions from many domains. In this article we review challenges that require new statistical methodologies to address them. In particular, we discuss the practice and culture of online experimentation, as well as its statistics literature, placing the current methodologies within their relevant statistical lineages and providing illustrative examples of OCE applications. Our goal is to raise academic statisticians’ awareness of these new research opportunities to increase collaboration between academia and the online industry. © 2023 The Author(s). Published with license by Taylor \& Francis Group, LLC.},
	number = {2},
	journal = {American Statistician},
	author = {Larsen, Nicholas and Stallrich, Jonathan and Sengupta, Srijan and Deng, Alex and Kohavi, Ron and Stevens, Nathaniel T.},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.
Type: Article},
	pages = {135 -- 149},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
	file = {Larsen et al. - 2024 - Statistical Challenges in Online Controlled Experi.pdf:/Users/arthursena/Zotero/storage/WYRN57CI/Larsen et al. - 2024 - Statistical Challenges in Online Controlled Experi.pdf:application/pdf},
}

@inproceedings{dehal_exposing_2024,
	title = {Exposing {Algorithmic} {Discrimination} and {Its} {Consequences} in {Modern} {Society}: {Insights} from a {Scoping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196396178&doi=10.1145%2f3639475.3640098&partnerID=40&md5=f7ae74f92fd081ab01769daa3ec0190a},
	doi = {10.1145/3639475.3640098},
	abstract = {Algorithmic discrimination is a condition that arises when datadriven software unfairly treats users based on attributes like ethnicity, race, gender, sexual orientation, religion, age, disability, or other personal characteristics. Nowadays, as machine learning gains popularity, cases of algorithmic discrimination are increasingly being reported in several contexts. This study delves into various studies published over the years reporting algorithmic discrimination. We aim to support software engineering researchers and practitioners in addressing this issue by discussing key characteristics of the problem. © 2024 Copyright held by the owner/author(s).},
	booktitle = {Proceedings - {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Dehal, Ramandeep Singh and Sharma, Mehak and De Souza Santos, Ronnie},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Software design, Machine learning, Machine-learning, Algorithmics, Algorithmic discrimination, Condition, Key characteristics, Learning gain, Personal characteristics, Scoping, Sexual orientations},
	pages = {69 -- 73},
	annote = {Cited by: 0; All Open Access, Bronze Open Access},
}

@article{zhao_identifying_2024-2,
	title = {Identifying the primary dimensions of {DevSecOps}: {A} multi-vocal literature review},
	volume = {214},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191655409&doi=10.1016%2fj.jss.2024.112063&partnerID=40&md5=950cf6baeee3cf5ed2efac499a8425da},
	doi = {10.1016/j.jss.2024.112063},
	abstract = {Context: Security as a key non-functional requirement of software development is often ignored and devalued in DevOps programs, with security seen as an inhibitor to high velocity required in DevOps implementation. Hence, the DevSecOps approach as a security-orientated expansion to DevOps, has aimed to integrate security into DevOps implementation by promoting collaboration among development, operation and security teams. DevSecOps is a topical concept and rapidly emerging area of practice in both academic and industrial settings. Objective: We reviewed both the white and grey literature to identify recent researches and practical trends of DevSecOps, aiming to: (a) review, document and analyze the current state of DevSecOps in the existing literature; (b) investigate the application of DevSecOps in Global Software Engineering (GSE) contexts. Method: A Multi-vocal Literature Review on DevSecOps and its global application was conducted, by executing a dual-track strategy including white (104 studies) and grey (43 studies) literature from 2012 to 2021. A Thematic Analysis was performed to identify, synthesize and analyze the themes within data for reporting the MLR results. Results: Through the Multi-vocal Literature Review and Thematic Analysis, this paper identifies five major aspects of DevSecOps (Definitions, Challenges, Practices, Tools/Technologies, and Metrics/Measurement); collects related themes of each aspect; and generates a Challenge-Practice-Tool-Metric (CPTM) model by integrating the themes of the latter four aspects within a lifecycle model. Moreover, an unexplored area relating to the global application of DevSecOps has been identified. Conclusion: Based on MLR results, a CPTM (Challenge-Practice-Tool-Metric) model is built to reveal the current status of DevSecOps. The model provides a breakdown and a broad landscape of DevSecOps, from which researchers and practitioners may select an area of focus to improve their knowledge or practice. With DevSecOps spanning the many stages of the lifecycle, we believe the model will enable emphases and absences such as global aspects to be investigated. Editor's note: Open Science material was validated by the Journal of Systems and Software Open Science Board. © 2024 The Author(s)},
	journal = {Journal of Systems and Software},
	author = {Zhao, Xiaofan and Clear, Tony and Lal, Ramesh},
	year = {2024},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Software design, Life cycle, Application programs, Literature reviews, Devsecop, Global applications, Global software engineering, METRIC model, Multivocal literature review, Non-functional requirements, Open science, Security, Thematic analysis},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{gorkovenko_data-enhanced_2023-2,
	title = {Data-{Enhanced} {Design}: {Engaging} {Designers} in {Exploratory} {Sensemaking} with {Multimodal} {Data}},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181706477&doi=10.57698%2fv17i3.01&partnerID=40&md5=d19ee20c1a2b039444e080ad65da687a},
	doi = {10.57698/v17i3.01},
	abstract = {Research in the wild can reveal human behaviors, contexts, and needs around products that are difficult to observe in the lab. Telemetry data from the use of physical products can help facilitate in the wild research, in particular by suggesting hypotheses that can be explored through machine learning models. This paper explores ways for designers without strong data skills to engage with multimodal data to develop a contextual understanding of product use. This study is framed around a lightweight version of a data enhanced design research process where multimodal telemetry data was captured by a GoPro camera attached to a bicycle. This was combined with the video data and conversation with the rider to carry out an exploratory sensemaking process and generate design research questions that could potentially be addressed through data capture, annotation, and machine learning. We identify a range of ways that designers could make use of the data for ideation and developing context through annotating and exploring the data. Participants used data and annotation practices to connect the micro and macro, spot interesting moments, and frame questions around an unfamiliar problem. The work follows the designers’ questions, methods, and explorations, both immediate concerns and speculations about working at larger scales with machine learning models. This points to the possibility of tools that help designers to engage with machine learning, not just for optimization and refinement, but for creative ideation in the early stages of design processes. © 2023 Gorkovenko, Jenkins, Vaniea, \& Murray-Rust.},
	number = {3},
	journal = {International Journal of Design},
	author = {Gorkovenko, Katerina and Jenkins, Adam and Vaniea, Kami and Murray-Rust, Dave},
	year = {2023},
	note = {Publisher: Chinese Institute of Design
Type: Article},
	pages = {1 -- 23},
	annote = {Cited by: 1},
}

@article{birkeland_research_2024-2,
	title = {Research areas and methods of interest in {European} intraday electricity market research—{A} systematic literature review},
	volume = {38},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189751189&doi=10.1016%2fj.segan.2024.101368&partnerID=40&md5=feb6e626b69abee383d73de8104038fa},
	doi = {10.1016/j.segan.2024.101368},
	abstract = {This paper establishes a robust foundation for the expansion of European intraday electricity market research through a systematic literature review. The review encompasses 132 primary studies from various libraries, categorizing them based on research area, methodologies, dataset context, and dataset date. The resulting taxonomy identifies six major research groups: Bidding, Market Modeling, Price Forecasting, Market Design, Forecast Errors, and Market Abuse. The analysis of the review results leads to actionable recommendations for future European intraday electricity market research. These recommendations include the utilization of close-to-live datasets to accurately reflect the impacts of the energy transition, the exploration of market abuse in the energy market industry, and the broadening of national electricity market studies beyond Germany. This systematic review aims to benefit various stakeholders, including academic researchers, industry participants, and European regulators, by providing a structured and objective compilation of existing research and offering insights into the identified gaps within the intraday electricity market research landscape. © 2024 The Author(s)},
	journal = {Sustainable Energy, Grids and Networks},
	author = {Birkeland, Dane and AlSkaif, Tarek},
	year = {2024},
	note = {Publisher: Elsevier Ltd
Type: Review},
	keywords = {Systematic literature review, Forecasting, Electric industry, Intraday electricity market, Market abuse, Market bidding, Market model, Market researches, Power markets, Price forecasting, Research areas, Research groups, Research method},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{ramzan_test-driven_2024-2,
	title = {Test-{Driven} {Development} ({TDD}) in {Small} {Software} {Development} {Teams}: {Advantages} and {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190268315&doi=10.1109%2fICACS60934.2024.10473291&partnerID=40&md5=20eefd4834bd0370b52239da636512c5},
	doi = {10.1109/ICACS60934.2024.10473291},
	abstract = {In the context of small software development teams, this research article gives a thorough investigation of the adoption of test-driven development (TDD) approaches. It aims to highlight the benefits that TDD offers, such as improved code quality through modularization and proactive defect spotting which results in effective debugging and development processes. It also discusses the complex issues that arise when TDD is implemented in smaller teams, such as the learning curve and resource constraints. This study significantly advances the understanding of how TDD can be used to optimize software development techniques in organizations or software houses having small development teams. It also explores the possible advantages and challenges. © 2024 IEEE.},
	booktitle = {2024 5th {International} {Conference} on {Advancements} in {Computational} {Sciences}, {ICACS} 2024},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ramzan, Hafiz Arslan and Ramzan, Sadia and Kalsum, Tehmina},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Development process, Software design, Software testing, Code quality, Debugging process, Development approach, Improved efficiency, Learning curves, Modular construction, Modularizations, Small team, Software development teams, Test driven development},
	annote = {Cited by: 2},
}

@article{ganeshkumar_discovery_2024-2,
	title = {Discovery, development, and deployment of a user-centered point-of-care digital information system to treat and track hypertension and diabetes patients under {India} {Hypertension} {Control} {Initiative} 2019–2022, {India}},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192362882&doi=10.1177%2f20552076241250153&partnerID=40&md5=0f65a3332b2783d660a88b13a123271d},
	doi = {10.1177/20552076241250153},
	abstract = {Background: Hypertension affects 28.5\% of Indians aged 18–69. Real-time registration and follow-up of persons with hypertension are possible with point-of-care digital information systems. We intend to describe herein the experiences of discovering, developing, and deploying a point-of-care digital information system for public health facilities under the India Hypertension Control Initiative. Methods: We have adopted an agile and user-centered approach in each phase in selected states of India since 2017. A multidisciplinary team adopted a hybrid approach with quantitative and qualitative methods, such as contextual inquiries, usability testing, and semi-structured interviews with healthcare workers, to document and monitor utility and usability. Results: During the discovery phase, we adopted a storyboard technique to understand the requirement of a digital information system. The participatory approach in discovery phase co-designed the information system with the nurses and doctors at Punjab state of India. Simple, which is the developed information system, has a front-end Android mobile application for healthcare workers and a backend dashboard for program managers. As of October 2022, over 24,31,962 patients of hypertension and 8,99,829 diabetes were registered in the information system of 10,017 health facilities. The median duration of registering a new patient was 50 seconds, and for recording a follow-up visit was 14 seconds in the app. High satisfaction was reported in 100 app users’ quarterly interviews. Conclusion: Simple was implemented by administering a user-centered approach and agile techniques. It demonstrated high utility and usability among users, highlighting the benefits of a user-centered approach for effective digital health solutions. © The Author(s) 2024.},
	journal = {Digital Health},
	author = {Ganeshkumar, Parasuraman and Bhatnagar, Aarti and Burka, Daniel and Durgad, Kiran and Krishna, Ashish and Das, Bidisha and Chandak, Mahima and Sharma, Meenakshi and Shivasankar, Roopa and Pathni, Anupam Khungar and Kunwar, Abhishek and Kaur, Prabhdeep},
	year = {2024},
	note = {Publisher: SAGE Publications Inc.
Type: Article},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{zhong_domain-driven_2024,
	title = {Domain-{Driven} {Design} for {Microservices}: {An} {Evidence}-{Based} {Investigation}},
	volume = {50},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190171667&doi=10.1109%2fTSE.2024.3385835&partnerID=40&md5=5d494b87292358d0b26456100d040bf1},
	doi = {10.1109/TSE.2024.3385835},
	abstract = {MicroService Architecture (MSA), a predominant architectural style in recent years, still faces the arduous task of identifying the boundaries of microservices. Domain-Driven Design (DDD) is regarded as one of the major design methods for addressing this task in practice, which aims to iteratively build domain models using a series of patterns, principles, and practices. The adoption of DDD for MSA (DDD4M in short) can, however, present considerable challenges in terms of a sufficient understanding of the methodological requirements and the application domains. It is imperative to establish a systematic understanding about the various aspects of employing DDD4M and provide effective guidance. This study reports an empirical inquiry that integrates a systematic literature review and a confirmatory survey. By reviewing 34 scientific studies and consulting 63 practitioners, this study reveals several distinctive findings with regard to the state and challenges of as well as the possible solutions for DDD4M applications, from the 5W1H perspectives: When, Where, Why, Who, What, and How. The analysis and synthesis of evidence show a wide variation in understanding of domain modeling artifacts. The status quo indicates the need for further methodological support in terms of application process, domain model design and implementation, and domain knowledge acquisition and management. To advance the state-of-the-practice, our findings were organized into a preliminary checklist that intends to assist practitioners by illuminating a DDD4M application process and the specific key considerations along the way. © 1976-2012 IEEE.},
	number = {6},
	journal = {IEEE Transactions on Software Engineering},
	author = {Zhong, Chenxing and Li, Shanshan and Huang, Huang and Liu, Xiaodong and Chen, Zhikun and Zhang, Yi and Zhang, He},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Systematic literature review, Design, Iterative methods, Architecture, Empirical studies, Application process, Architectural style, Design method, Domain Knowledge, Domain model, Domain-driven designs, Evidence-based, Industrial surveys, Microservice architecture},
	pages = {1425 -- 1449},
	annote = {Cited by: 0},
}

@article{pranajaya_examining_2024-2,
	title = {Examining the influence of financial inclusion on investment decision: {A} bibliometric review},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183975319&doi=10.1016%2fj.heliyon.2024.e25779&partnerID=40&md5=84031b0918cf4b2a700171cfb937cf8c},
	doi = {10.1016/j.heliyon.2024.e25779},
	abstract = {This study delves into the contemporary landscape of potential financial inclusion in investment decision-making, leveraging bibliometric research methods. Analyzing 161 publications from the Scopus database (2006–2023), the authors employ performance analysis and scientific mapping tools, including VOSviewer and Biblioshiny R studio. Through co-citation analysis, bibliographic coupling, co-occurrence of keywords analysis, thematic mapping, and thematic evolution analysis, the study uncovers the essential characteristics of the research field. The Result underscores that Innovative financial technologies are positioned as enablers of financial inclusion, with fintech's potential to drive positive social impact. The findings underscore that fostering financial literacy, addressing challenges in fintech adoption, and supporting entrepreneurship are crucial for maximizing the benefits of financial technologies. Overall, the study advocates for a comprehensive approach that combines financial inclusion, individual attitudes, and expertise, and fintech innovation to enhance access to financial services and expand investment opportunities for a more inclusive and prosperous economic landscape. However, the study acknowledges limitations, such as reliance on a single database and exclusion of specific keywords, urging a more inclusive approach to ensure a comprehensive understanding of relevant literature in this dynamic field. © 2024 The Authors},
	number = {3},
	journal = {Heliyon},
	author = {Pranajaya, Eko and Alexandri, Mohammad Benny and Chan, Arianis and Hermanto, Bambang},
	year = {2024},
	note = {Publisher: Elsevier Ltd
Type: Review},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{moguel-sanchez_bots_2023-2,
	title = {Bots in {Software} {Development}: {A} {Systematic} {Literature} {Review} and {Thematic} {Analysis}},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182955496&doi=10.1134%2fS0361768823080145&partnerID=40&md5=ce1e41e228545c5e52caa01baa3d1388},
	doi = {10.1134/S0361768823080145},
	abstract = {Abstract: Modern Software Engineering thrives with innovative tools that aid developers in creating better software grounded on quality standards. Software bots are an emerging and exciting trend in this regard, supporting numerous software development activities. As an emerging trend, few studies describe and analyze different bots in software development. This research presents a systematic literature review covering the state of the art of applied and proposed bots for software development. Our study spans literature from 2003 to 2022, with 82 different bots applied in software development activities, covering 83 primary studies. We found four bot archetypes: chatbots which focus on direct communication with developers to aid them, analysis bots that display helpful information in different tasks, repair bots for resolving software defects, and development bots that combine aspects of other bot technologies to provide a service to the developer. The primary benefits of using bots are increasing software quality, providing useful information to developers, and saving time through the partial or total automation of development activities. However, drawbacks are reported, including limited effectiveness in task completion, high coupling to third-party technologies, and some prejudice from developers toward bots and their contributions. We discovered that including Bots in software development is a promising field of research in software engineering that has yet to be fully explored. © 2023, Pleiades Publishing, Ltd.},
	number = {8},
	journal = {Programming and Computer Software},
	author = {Moguel-Sánchez, R. and Martínez-Palacios, C. S. Sergio and Ocharán-Hernández, J.O. and Limón, X. and Sánchez-García, A.J.},
	year = {2023},
	note = {Publisher: Pleiades Publishing
Type: Article},
	keywords = {Systematic literature review, Software design, State of the art, Chatbots, Thematic analysis, Bot, Botnet, Computer software selection and evaluation, Development activity, Development bot, Emerging trends, Quality standard, Thematic synthesis},
	pages = {712 -- 734},
	annote = {Cited by: 0},
}

@article{dominguez_role_2024-2,
	title = {The role of ontologies in smart contracts: {A} systematic literature review},
	volume = {40},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192928073&doi=10.1016%2fj.jii.2024.100630&partnerID=40&md5=25e13e4895435b8b01c9f92b71783473},
	doi = {10.1016/j.jii.2024.100630},
	abstract = {The aim of this systematic literature review is to provide a comprehensive understanding of how ontologies address current Smart Contract challenges, identify application scenarios, and present tools and technologies associated with their use. This systematic literature review (SLR), following Kitchenham's methodology, analyses peer-reviewed articles from 2015 to August 2022 from databases such as Scopus, IEEE, Science Direct, Springer Link and ACM. Of the 501 publications identified, 21 are selected for in-depth review based on inclusion, exclusion and quality assessment criteria. The results of this SLR show that ontologies provide solutions to the challenges faced by Smart Contracts mainly at the creation stage. They allow the terms of the contract and the roles of the parties to be defined. Ontologies also enable the development of Smart Contract templates. This facilitates their use by people without technical programming expertise. Despite these potential solutions to the challenges that Smart Contracts face throughout their lifecycle, they lack verification. This increases the vulnerabilities to which Smart Contracts are exposed. Developing validation and verification tools could facilitate using ontologies to create Smart Contracts for different real-world cases. © 2024 Elsevier Inc.},
	journal = {Journal of Industrial Information Integration},
	author = {Dominguez, Johnny Alvarado and Gonnet, Silvio and Vegetti, Marcela},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, Life cycle, 'current, Ontology, Ontology's, Application scenario, Assessment criteria, Block-chain, Blockchain, Inclusion-exclusion, Quality assessment, Smart contract, Tools and technologies, Validation and verification},
	annote = {Cited by: 0},
}

@article{khoshnevis_search-based_2024-2,
	title = {Search-based approaches to optimizing software product line architectures: {A} systematic literature review},
	volume = {170},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188512328&doi=10.1016%2fj.infsof.2024.107446&partnerID=40&md5=4dd032426e87fee8e0653cb59b8215a1},
	doi = {10.1016/j.infsof.2024.107446},
	abstract = {Context: Software product line architecture (PLA) plays an important role in developing software product lines (SPLs) and other configurable systems. Search-based (SB) approaches can optimize the design of PLAs according to a given set of metrics as fitness functions. Although this area has been explored by researchers, there is a lack of synthesis of search-based PLA (SBPLA) research. A comprehensive review would offer valuable insights into previous contributions and identify areas for further research. Objective: The objective of this work is to identify and summarize quality-assessed peer-reviewed studies on search-based PLA design from the aspects of the research scope, problems, contributions, evaluation, and open issues. Methods: We conducted a systematic literature review based on Kitchenham's methodology. Based on a predefined search protocol we identified related studies limited to the ones published between 2000 and 2022 in journals and conference proceedings. Results: Out of 686 initial search results, 34 papers were finally selected after a set of deep search, and criteria application activities. We provided a taxonomy of optimization problems in SBPLA and found that PLA remodularization and refactoring were the two categories most emphasized by the researchers. We also provided several other categorizations regarding contributions, research design, open issues, and other subjects of interest. Conclusions: The interest in SBPLA design has been growing since 2014. PLA cloning and re-engineering problems have never been addressed in the literature. Performing subjective evaluation with the participation of experts from the industry will be profitable, as a complementary method to objective experimental evaluation, and therefore carrying out quanti-qualitative research. © 2024 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Khoshnevis, Sedigheh and Ardestani, Omid},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, Software design, Software Product Line, Computer software, Configurable systems, Metrics as fitness functions, Product line architecture, Quality control, Search-based, Search-based software architecture, Search-based software engineering, Software architecture, Software product line architecture},
	annote = {Cited by: 0},
}

@inproceedings{riccio_ramses_2024,
	title = {{RAMSES}: {An} {Artifact} {Exemplar} for {Engineering} {Self}-{Adaptive} {Microservice} {Applications}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196372553&doi=10.1145%2f3643915.3644110&partnerID=40&md5=9925b1b31be9744f048f6e2781f309f3},
	doi = {10.1145/3643915.3644110},
	abstract = {This paper introduces RAMSES, an exemplar tailored for both practitioners and researchers working on self-adaptive microservice applications. By emphasizing a clear separation of concerns between the application and its adaptation logic, RAMSES realizes a reusable autonomic manager that implements a MAPE-K feedback loop whose components are microservices themselves. Its primary focus lies in addressing user-defined QoS attributes at runtime, like availability and performance. To illustrate its usage, we provide a practical example showing its mechanics in an e-food microservice application. Initial experiments indicate the advantages of utilizing RAMSES, as shown by a comparative analysis of the quality properties of a microservice application with and without self-adaptation. © 2024 is held by the owner/author(s). Publication rights licensed to ACM.},
	booktitle = {Proceedings - 2024 {IEEE}/{ACM} 19th {Symposium} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems}, {SEAMS} 2024},
	publisher = {Association for Computing Machinery, Inc},
	author = {Riccio, Vincenzo and Sorrentino, Giancarlo and Zamponi, Ettore and Camilli, Matteo and Mirandola, Raffaela and Scandurra, Patrizia},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Computer software reusability, Runtimes, Autonomic managers, Exemplar, Feedback loops, Mape, MAPE-K, Microservice application, QoS attributes, Self- adaptations, Separation of concerns},
	pages = {161 -- 167},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{galbin-nasui_bug_2024-2,
	title = {Bug reports priority classification models. {Replication} study},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189942059&doi=10.1007%2fs10515-024-00432-1&partnerID=40&md5=084ec887cec6bbb2f20eb711594e7b6a},
	doi = {10.1007/s10515-024-00432-1},
	abstract = {Bug tracking systems receive a large number of bugs on a daily basis. The process of maintaining the integrity of the software and producing high-quality software is challenging. The bug-sorting process is usually a manual task that can lead to human errors and be time-consuming. The purpose of this research is twofold: first, to conduct a literature review on the bug report priority classification approaches, and second, to replicate existing approaches with various classifiers to extract new insights about the priority classification approaches. We used a Systematic Literature Review methodology to identify the most relevant existing approaches related to the bug report priority classification problem. Furthermore, we conducted a replication study on three classifiers: Naive Bayes (NB), Support Vector Machines (SVM), and Convolutional Neural Network (CNN). Two sets of experiments are performed: first, our own NLTK implementation based on NB and CNN, and second, based on Weka implementation for NB, SVM, and CNN. The dataset used consists of several Eclipse projects and one project related to database systems. The obtained results are better for the bug priority P3 for the CNN classifier, and overall the quality relation between the three classifiers is preserved as in the original studies. The replication study confirmed the findings of the original studies, emphasizing the need to further investigate the relationship between the characteristics of the projects used as training and those used as testing. © The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature 2024.},
	number = {1},
	journal = {Automated Software Engineering},
	author = {Galbin-Nasui, Andreea and Vescan, Andreea},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	keywords = {Program debugging, BN, Bug priority prediction, Bug reports, Bug tracking system, Classification approach, Classification models, Convolutional neural network, Convolutional neural networks, Naive bayes, Replication study, Support vector machines, Support vectors machine},
	annote = {Cited by: 0},
}

@article{biazotto_technical_2024-2,
	title = {Technical debt management automation: {State} of the art and future perspectives},
	volume = {167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180417326&doi=10.1016%2fj.infsof.2023.107375&partnerID=40&md5=db74021970a8b325b1de4b98f924dcab},
	doi = {10.1016/j.infsof.2023.107375},
	abstract = {Context: Technical debt (TD) refers to non-optimal decisions made in software projects that may lead to short-term benefits, but potentially harm the system's maintenance in the long-term. Technical debt management (TDM) refers to a set of activities that are performed to handle TD, e.g., identification or measurement of TD. These activities typically entail tasks such as code and architectural analysis, which can be time-consuming if done manually. Thus, substantial research work has focused on automating TDM tasks (e.g., automatic identification of code smells). However, there is a lack of studies that summarize current approaches in TDM automation. This can hinder practitioners in selecting optimal automation strategies to efficiently manage TD. It can also prevent researchers from understanding the research landscape and addressing the research problems that matter the most. Objectives: The main objective of this study is to provide an overview of the state of the art in TDM automation, analyzing the available tools, their use, and the challenges in automating TDM. Methods: We conducted a systematic mapping study (SMS), following the guidelines proposed by Kitchenham et al. From an initial set of 1086 primary studies, 178 were selected to answer three research questions covering different facets of TDM automation. Results: We found 121 automation artifacts that can be used to automate TDM activities. The artifacts were classified in 4 different types (i.e., tools, plugins, scripts, and bots); the inputs/outputs and interfaces were also collected and reported. Finally, a conceptual model is proposed that synthesizes the results and allows to discuss the current state of TDM automation and related challenges. Conclusion: The research community has investigated to a large extent how to perform various TDM activities automatically, considering the number of studies and automation artifacts we identified. Nonetheless, more research is needed towards fully automated TDM, specially concerning the integration of the automation artifacts. © 2023 The Author(s)},
	journal = {Information and Software Technology},
	author = {Biazotto, João Paulo and Feitosa, Daniel and Avgeriou, Paris and Nakagawa, Elisa Yumi},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {State of the art, Mapping, Systematic mapping studies, Automation, 'current, Technical debts, Codes (symbols), Future perspectives, Management activities, Management automations, Optimal decisions, Technical debt management},
	annote = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{chueca_consolidation_2024-2,
	title = {The consolidation of game software engineering: {A} systematic literature review of software engineering for industry-scale computer games},
	volume = {165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172685092&doi=10.1016%2fj.infsof.2023.107330&partnerID=40&md5=ef2b467462eb9ad92be33ef6e585775f},
	doi = {10.1016/j.infsof.2023.107330},
	abstract = {Context: Game Software Engineering (GSE) is a branch of Software Engineering (SE) that focuses on the development of video game applications. In past years, GSE has achieved enough volume, differences from traditional software engineering, and interest by the community to be considered an independent scientific domain, veering out from traditional SE. Objective: This study evaluates the current state of the art in software engineering for industry-scale computer games identifying gaps and consolidating the magnitude and growth of this field. Method: A Systematic Literature Review is performed following best practices to ensure the relevance of the studies included in the review. We analyzed 98 GSE studies to extract the current intensity, topics, methods, and quality of GSE. Results: The GSE research community has been growing over the years, producing over four times more research than before the previous GSE survey. However, this community is still very dispersed, with no main venues holding most of the GSE scientific studies. A broader range of topics is covered in this area, evolving towards those of a mature field such as architecture and design. Also, the reviewed studies employ more elaborated empirical research methods, even though the study reports need to be more rigorous in sections related to the critical examination of the work. Conclusion: The results of the SLR lead to the identification of 13 potential future research directions for this domain. GSE is an independent, mature, and growing field that presents new ways of software creation where the gap between industry and academia is narrowing. Video games present themselves as powerful tools to push the boundaries of software knowledge. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Chueca, Jorge and Verón, Javier and Font, Jaime and Pérez, Francisca and Cetina, Carlos},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, State of the art, Application programs, Human computer interaction, 'current, Computer games, Best practices, Engineering research, Game software, Game software engineering, Industry-scale, SLR, Video-games, Volume difference},
	annote = {Cited by: 0},
}

@article{soto_automated_2024-2,
	title = {Automated {Diagnosis} of {Prostate} {Cancer} {Using} {Artificial} {Intelligence}. {A} {Systematic} {Literature} {Review}},
	volume = {1874 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175991217&doi=10.1007%2f978-3-031-46813-1_6&partnerID=40&md5=3de2e59f800c9fe45cbfe0f5dbe022d5},
	doi = {10.1007/978-3-031-46813-1_6},
	abstract = {Prostate cancer is one of the most preventable causes of death. Periodic testing, seconded by precursors such as living habits, heritage, and exposure to specific materials, help healthcare providers achieve early detection, a desirable scenario that positively correlates with survival. However, the currently available diagnosing mechanisms have a great opportunity of improvement in terms of invasiveness, sensitivity and timing before patients reach advanced stages with a significant probability of metastasis. Supervised artificial intelligence enables early diagnosis and excludes patients from unpleasant biopsies. In this work, we gathered information about methodologies, techniques, metrics, and benchmarks to accomplish early prostate cancer detection, including pipelines with associated patents and knowledge transfer mechanisms, intending to find the reasons precluding the solutions from being massively adopted in the standards of care. © 2024, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Soto, Salvador and Pollo-Cattaneo, María F. and Yepes-Calderon, Fernando},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Artificial intelligence, Systematic literature review, Knowledge management, Automated diagnosis, Automatic pathology diagnose, Causes of death, Diagnosis, Diseases, Early diagnosis, Health care providers, Invasiveness, Pathology, Periodic testing, Prostate cancers, Specific materials, Urology},
	pages = {77 -- 92},
	annote = {Cited by: 0},
}

@article{jui_fairness_2024-2,
	title = {Fairness issues, current approaches, and challenges in machine learning models},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183668121&doi=10.1007%2fs13042-023-02083-2&partnerID=40&md5=c5b3de8f74c7dc7e2d5d8886887b03d0},
	doi = {10.1007/s13042-023-02083-2},
	abstract = {With the increasing influence of machine learning algorithms in decision-making processes, concerns about fairness have gained significant attention. This area now offers significant literature that is complex and hard to penetrate for newcomers to the domain. Thus, a mapping study of articles exploring fairness issues is a valuable tool to provide a general introduction to this field. Our paper presents a systematic approach for exploring existing literature by aligning their discoveries with predetermined inquiries and a comprehensive overview of diverse bias dimensions, encompassing training data bias, model bias, conflicting fairness concepts, and the absence of prediction transparency, as observed across several influential articles. To establish connections between fairness issues and various issue mitigation approaches, we propose a taxonomy of machine learning fairness issues and map the diverse range of approaches scholars developed to address issues. We briefly explain the responsible critical factors behind these issues in a graphical view with a discussion and also highlight the limitations of each approach analyzed in the reviewed articles. Our study leads to a discussion regarding the potential future direction in ML and AI fairness. © 2024, The Author(s).},
	journal = {International Journal of Machine Learning and Cybernetics},
	author = {Jui, Tonni Das and Rivas, Pablo},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Decision making, Decision-making process, Learning algorithms, Machine learning, Machine-learning, 'current, Mapping studies, Bias reduction, Fair prediction, Machine learning algorithms, Machine learning models, Model fairness, Training data},
	annote = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
}

@book{kumar_ethical_2024-2,
	title = {The ethical frontier of {AI} and data analysis},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85189601406&doi=10.4018%2f979-8-3693-2964-1&partnerID=40&md5=c591996a91637e79d3e1dd27cc10b011},
	abstract = {In the advancing fields of artificial intelligence (AI) and data science, a pressing ethical dilemma arises. As technology continues its relentless march forward, ethical considerations within these domains become increasingly complex and critical. Bias in algorithms, lack of transparency, data privacy breaches, and the broader societal repercussions of AI applications are demanding urgent attention. This ethical quandary poses a formidable challenge for researchers, academics, and industry professionals alike, threatening the very foundation of responsible technological innovation. Navigating this ethical minefield requires a comprehensive understanding of the multifaceted issues at hand. The Ethical Frontier of AI and Data Analysis is an indispensable resource crafted to address the ethical challenges that define the future of AI and data science. Researchers and academics who find themselves at the forefront of this challenge are grappling with the evolving landscape of AI and data science ethics. Underscoring the need for this book is the current lack of clarity on ethical frameworks, bias mitigation strategies, and the broader societal implications, which hinder progress and leave a void in the discourse. As the demand for responsible AI solutions intensifies, the imperative for this reliable guide that consolidates, explores, and advances the dialogue on ethical considerations grows exponentially. Tailored for researchers, academics, and professionals, this publication serves as a beacon of ethical excellence. With a comprehensive exploration of bias, fairness, transparency, and accountability, it guides readers through the intricate web of ethical considerations. From foundational philosophical frameworks to real-world case studies, the book offers a roadmap to not only understand but actively shape the ethical trajectory of AI and data science. It is more than a book; it serves as a transformative tool for those seeking to align technological innovation with ethical standards and societal values. © 2024 by IGI Global. All rights reserved.},
	publisher = {IGI Global},
	author = {Kumar, Rajeev and Joshi, Ankush and Sharan, Hari Om and Peng, Sheng-Lung and Dudhagara, Chetan R.},
	year = {2024},
	doi = {10.4018/979-8-3693-2964-1},
	note = {Publication Title: The Ethical Frontier of AI and Data Analysis
Type: Book},
	annote = {Cited by: 0},
}

@article{iftikhar_tertiary_2024-2,
	title = {A tertiary study on links between source code metrics and external quality attributes},
	volume = {165},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174715019&doi=10.1016%2fj.infsof.2023.107348&partnerID=40&md5=b6a93845d92de37d2044158738593673},
	doi = {10.1016/j.infsof.2023.107348},
	abstract = {Context: Several secondary studies have investigated the relationship between internal quality attributes, source code metrics and external quality attributes. Sometimes they have contradictory results. Objective: We synthesize evidence of the link between internal quality attributes, source code metrics and external quality attributes along with the efficacy of the prediction models used. Method: We conducted a tertiary review to identify, evaluate and synthesize secondary studies. We used several characteristics of secondary studies as indicators for the strength of evidence and considered them when synthesizing the results. Results: From 711 secondary studies, we identified 15 secondary studies that have investigated the link between source code and external quality. Our results show : (1) primarily, the focus has been on object-oriented systems, (2) maintainability and reliability are most often linked to internal quality attributes and source code metrics, with only one secondary study reporting evidence for security, (3) only a small set of complexity, coupling, and size-related source code metrics report a consistent positive link with maintainability and reliability, and (4) group method of data handling (GMDH) based prediction models have performed better than other prediction models for maintainability prediction. Conclusions: Based on our results, lines of code, coupling, complexity and the cohesion metrics from Chidamber \& Kemerer (CK) metrics are good indicators of maintainability with consistent evidence from high and moderate-quality secondary studies. Similarly, four CK metrics related to coupling, complexity and cohesion are good indicators of reliability, while inheritance and certain cohesion metrics show no consistent evidence of links to maintainability and reliability. Further empirical studies are needed to explore the link between internal quality attributes, source code metrics and other external quality attributes, including functionality, portability, and usability. The results will help researchers and practitioners understand the body of knowledge on the subject and identify future research directions. © 2023 The Author(s)},
	journal = {Information and Software Technology},
	author = {Iftikhar, Umar and Ali, Nauman Bin and Börstler, Jürgen and Usman, Muhammad},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Tertiary study, Forecasting, Data handling, Codes (symbols), Code quality, Computer programming languages, Evidence, External quality, Internal quality, Maintainability, Object oriented programming, Products quality, Quality attributes, Quality modeling, Reliability, Source code metrics, Tertiary review},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{karcher_quality_2024-2,
	title = {Quality methods in virtual and augmented reality with a focus on education: a systematic literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187115518&doi=10.1007%2fs11301-023-00403-y&partnerID=40&md5=1d4ec757b2a3f73d0c92a57fa035ed24},
	doi = {10.1007/s11301-023-00403-y},
	abstract = {With the goal of developing a unified approach for implementation of training for quality methods—with the help of innovative assistance systems—the current state of research is determined within the scope of this work. These quality methods include Quality Management Systems such as Lean Management and Six Sigma. A systematic literature search is conducted to determine the current state of research on Augmented and Virtual Reality data glasses, which are considered here as innovative assistance systems. This search extends without restriction to the date of data collection at the beginning of the year 2022, as Augmented and Virtual Reality data glasses are considered to be particularly immersive technologies. Based on the databases Scopus and Web of Science, an extended systematic literature review was used for the research. By answering the research question and classifying the implemented research work, an overview of the current state of virtual and augmented reality research will be given. This makes it clear that further research is needed, especially with regard to the training of quality methods, to develop specific models and action guidelines. © The Author(s) 2024.},
	journal = {Management Review Quarterly},
	author = {Karcher, Amelie and Arnold, Dominik and Kuhlenkötter, Bernd},
	year = {2024},
	note = {Publisher: Springer Nature
Type: Article},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{wairimu_evaluation_2024-2,
	title = {On the {Evaluation} of {Privacy} {Impact} {Assessment} and {Privacy} {Risk} {Assessment} {Methodologies}: {A} {Systematic} {Literature} {Review}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184332904&doi=10.1109%2fACCESS.2024.3360864&partnerID=40&md5=847d81293ca4d943618bb7001110f885},
	doi = {10.1109/ACCESS.2024.3360864},
	abstract = {Assessing privacy risks and incorporating privacy measures from the onset requires a comprehensive understanding of potential impacts on data subjects. Privacy Impact Assessments (PIAs) offer a systematic methodology for such purposes, which are closely related to Data Protection Impact Assessments (DPIAs), particularly outlined in Article 35 of the General Data Protection Regulation (GDPR). The core of a PIA is a Privacy Risk Assessment (PRA). PRAs can be integrated as part of full-fledged PIAs or independently developed to support PIA processes. Although these methodologies have been identified as essential enablers of privacy by design, their effectiveness has been criticized because of the lack of evidence of their rigorous and systematic evaluation. Hence, we conducted a Systematic Literature Review (SLR) to identify published PIA and PRA methodologies and assess how and to what extent they have been scientifically validated or evaluated. We found that these methodologies are rarely evaluated for their performance in practice, and most of them have only been validated in limited studies. Most validation evidence is found with PRA methodologies. Of the evaluated methodologies, PIAs were the most evaluated, where case studies were the predominant evaluation method. These evaluated methodologies can be easily transferred to an industrial setting or used by practitioners, as they provide evidence of their use in practice. In addition, the findings in this study can be used to inform researchers of the current state-of-the-art, and practitioners can understand the benefits and current limitations of the methodologies and adopt evidence-based practices. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Wairimu, Samuel and Iwaya, Leonardo Horn and Fritsch, Lothar and Lindskog, Stefan},
	year = {2024},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Risk management, Risk assessment, Data privacy, General data protection regulations, Systematic, Data protection impact assessments, Guideline, Impact assessments, Maturity, Privacy, Privacy impact assessment, Privacy risks, Risks management, Threat modeling, Validity},
	pages = {19625 -- 19650},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@inproceedings{quin_automating_2024,
	title = {Automating {Pipelines} of {A}/{B} {Tests} with {Population} {Split} {Using} {Self}-{Adaptation} and {Machine} {Learning}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85196397449&doi=10.1145%2f3643915.3644087&partnerID=40&md5=f709a36fbc06702214da340e2dfb0f6a},
	doi = {10.1145/3643915.3644087},
	abstract = {A/B testing is a common approach used in industry to facilitate innovation through the introduction of new features or the modification of existing software. Traditionally, A/B tests are administrated manually and conducted sequentially, with each experiment targeting the entire population that uses the corresponding application. This approach can be time-consuming and costly, particularly when the experiments are not relevant to the entire population. To tackle these problems, we present a new self-adaptive approach called AutoPABS, short for Automated Pipelines of A/B tests using Self-adaptation. AutoPABS contributes: (1) a self-adaptive architecture that enables automated execution of A/B testing pipelines, (2) a population split component that is managed by the managing system of the self-adaptive architecture to enable more efficient A/B testing pipeline execution, and (3) a notation to specify A/B testing pipelines that can be executed using the self-adaptive architecture. We started the evaluation with a small survey to probe the appraisal of the notation and infrastructure of AutoPABS. Then we performed a series of tests to measure the gains obtained by applying a population split in an automated A/B testing pipeline, using an extension of the SEAByTE artifact. The survey results show that the participants appraise the usefulness of automating A/B testing pipelines and population split. The test results show that automatically executing pipelines of A/B tests using the self-adaptation architecture with population split accelerates the identification of statistically significant results by exploiting parallel A/B test execution compared to traditional sequential execution of A/B tests. © 2024 is held by the owner/author(s). Publication rights licensed to ACM.},
	booktitle = {Proceedings - 2024 {IEEE}/{ACM} 19th {Symposium} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems}, {SEAMS} 2024},
	publisher = {Association for Computing Machinery, Inc},
	author = {Quin, Federico and Weyns, Danny},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {A/B testing, Software testing, Automation, Machine learning, Machine-learning, Pipelines, Self- adaptations, Adaptive architecture, Parallel A, Pipeline execution, Self-adaptation learning, Self-adaptive approaches, Sequential execution, Test execution},
	pages = {84 -- 97},
	annote = {Cited by: 0; All Open Access, Bronze Open Access},
}

@article{oliveira_mobile_2024-2,
	title = {Mobile {Health} from {Developers}’ {Perspective}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180425615&doi=10.1007%2fs42979-023-02455-z&partnerID=40&md5=8f2aa4333b07b435a23d001076ad88c5},
	doi = {10.1007/s42979-023-02455-z},
	abstract = {This paper extends an initial investigation of eHealth from the developers’ perspective. In this extension, our focus is on mobile health data. Despite the significant potential of this development area, few studies try to understand the challenges faced by these professionals. This perspective is relevant to identify the most used technologies and future perspectives for research investigation. Using a KDD-based process, this work analyzed eHealth and mHealth discussions from Stack Overflow (SO) to comprehend this developers’ community. We got and processed 6082 eHealth and 1832 mHealth questions. The most discussed topics include manipulating medical images, electronic health records with the HL7 standard, and frameworks to support mobile health (mHealth) development. Concerning the challenges faced by these developers, there is a lack of understanding of the DICOM and HL7 standards, the absence of data repositories for testing, and the monitoring of health data in the background using mobile and wearable devices. Our results also indicate that discussions have grown mainly on mHealth, primarily due to monitoring health data through wearables and about how to optimize resource consumption during health-monitoring. © 2023, The Author(s), under exclusive licence to Springer Nature Singapore Pte Ltd.},
	number = {1},
	journal = {SN Computer Science},
	author = {Oliveira, Pedro Almir M. and Junior, Evilasio Costa and Andrade, Rossana M. C. and Santos, Ismayle S. and Neto, Pedro A. Santos},
	year = {2024},
	note = {Publisher: Springer
Type: Article},
	annote = {Cited by: 0},
}

@article{papatheocharous_context_2024-2,
	title = {Context factors perceived important when looking for similar experiences in decision-making for software components: {An} interview study},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85190424140&doi=10.1002%2fsmr.2668&partnerID=40&md5=6ed3d43b32c8b6b926652c2475b95b30},
	doi = {10.1002/smr.2668},
	abstract = {During software evolution, decisions related to components' origin or source significantly impact the quality properties of the product and development metrics such as cost, time to market, ease of maintenance, and further evolution. Thus, such decisions should ideally be supported by evidence, i.e., using previous experiences and information from different sources, even own previous experiences. A hindering factor to such reuse of previous experiences is that these decisions are highly context-dependent and it is difficult to identify when previous experiences come from sufficiently similar contexts to be useful in a current setting. Conversely, when documenting a decision (as a decision experience), it is difficult to know which context factors will be most beneficial when reusing the experience in the future. An interview study is performed to identify a list of context factors that are perceived to be most important by practitioners when using experiences to support decision-making for component sourcing, using a specific scenario with alternative sources of experiences. We observed that the further away (from a company or an interviewee) the experience evidence is, as is the case for online experiences, the more context factors are perceived as important by practitioners to make use of the experience. Furthermore, we discuss and identify further research to make this type of decision-making more evidence-based. © 2024 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	journal = {Journal of Software: Evolution and Process},
	author = {Papatheocharous, Efi and Wohlin, Claes and Badampudi, Deepika and Carlson, Jan and Wnuk, Krzysztof},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Decision making, Decisions makings, Open source software, Software Evolution, Components off the shelves, Context factors, Decision experience, Experience source, In-house, Interview study, Open systems, Open-source softwares, Software-component},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{liang_co-design_2024-2,
	title = {Co-design personal sleep health technology for and with university students},
	volume = {6},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85191036091&doi=10.3389%2ffdgth.2024.1371808&partnerID=40&md5=8f5ef4e6d87e2f109fef9327baa66073},
	doi = {10.3389/fdgth.2024.1371808},
	abstract = {University students often experience sleep disturbances and disorders. Personal digital technologies present a great opportunity for sleep health promotion targeting this population. However, studies that engage university students in designing and implementing digital sleep health technologies are scarce. This study sought to understand how we could build digital sleep health technologies that meet the needs of university students through a co-design process. We conducted three co-design workshops with 51 university students to identify design opportunities and to generate features for sleep health apps through workshop activities. The generated ideas were organized using the stage-based model of self-tracking so that our findings could be well-situated within the context of personal health informatics. Our findings contribute new design opportunities for sleep health technologies targeting university students along the dimensions of sleep environment optimization, online community, gamification, generative AI, materializing sleep with learning, and personalization. 2024 Liang, Melcer, Khotchasing and Hoang.},
	journal = {Frontiers in Digital Health},
	author = {Liang, Zilu and Melcer, Edward and Khotchasing, Kingkarn and Hoang, Nhung Huyen},
	year = {2024},
	note = {Publisher: Frontiers Media SA
Type: Article},
	keywords = {Article, human, adult, cohort analysis, data analysis, depersonalization, female, gamification, heart rate, human experiment, learning, male, nervous system function, personal sleep health technology, sleep disorder, sleep education, sleep environment, sleep hygiene, sleep literacy education, sleep quality, university student},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{von_scherenberg_data_2024-2,
	title = {Data {Sovereignty} in {Information} {Systems}},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186488145&doi=10.1007%2fs12525-024-00693-4&partnerID=40&md5=81079ebc4fe9e7a98618fa6d1beea7fc},
	doi = {10.1007/s12525-024-00693-4},
	abstract = {Data has become a strategic asset for societal prosperity and economic competitiveness. There has long been an academic consensus that the value of data unfolds during its use. Consequently, many stakeholders have called for expanding the use and reuse of data, including the public and open variety, as well as that from private data providers. However, citizens and organizations want self-determination over their data use, that is, data sovereignty. This fundamentals paper applies a literature review to conceptualize the term in Information Systems (IS) research by summarizing current findings and definitions to add further structure to the field. It contributes to the current research streams by introducing a core conceptual model consisting of seven interacting core aspects, involving trust between data providers and consumers for data assets, supported by data infrastructure and contractual agreements on all data lifecycle stages. We evaluate and discuss this conceptual model through recent field examples and provide an overview of future research opportunities. © The Author(s) 2024.},
	number = {1},
	journal = {Electronic Markets},
	author = {von Scherenberg, Franziska and Hellmeier, Malte and Otto, Boris},
	year = {2024},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	annote = {Cited by: 2; All Open Access, Hybrid Gold Open Access},
}

@article{kosasih_review_2024-2,
	title = {A review of explainable artificial intelligence in supply chain management using neurosymbolic approaches},
	volume = {62},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177643935&doi=10.1080%2f00207543.2023.2281663&partnerID=40&md5=73d8d50d41b48d9e9b68eb4ff7b68865},
	doi = {10.1080/00207543.2023.2281663},
	abstract = {Artificial Intelligence (AI) has emerged as a complementary technology in supply chain research. However, the majority of AI approaches explored in this context afford little to no explainability, which is a significant barrier to a broader adoption of AI in supply chains. In recent years, the need for explainability has been a strong impetus for research in hybrid AI methodologies that combine neural architectures with logic-based reasoning, which are collectively referred to as Neurosymbolic AI. The aim of this paper is to provide a comprehensive overview of supply chain management literature that employs approaches within the neurosymbolic AI spectrum. To that end, a systematic review is conducted, followed by bibliometric, descriptive and thematic analyses on the identified studies. Our findings indicate that researchers have primarily focused on the limited subset of neurofuzzy approaches, while some supply chain applications, such as performance evaluation and sustainability, and sectors such as pharmaceutical and construction have received less attention. To help address these gaps, we propose five pillars of neurosymbolic AI research for supply chains and provide four use cases of applying unexplored neurosymbolic AI approaches to address typical problems in supply chain management, including a discussion of prerequisites for adopting such technologies. We envision that the findings and contributions of this survey will help encourage further research in neurosymbolic AI for supply chains and increase adoption of such technologies within supply chain practice. © 2023 The Author(s). Published by Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {4},
	journal = {International Journal of Production Research},
	author = {Kosasih, Edward Elson and Papadakis, Emmanuel and Baryannis, George and Brintrup, Alexandra},
	year = {2024},
	note = {Publisher: Taylor and Francis Ltd.
Type: Review},
	keywords = {Artificial intelligence, Systematic Review, Explainability, Based reasonings, Bibliometrics analysis, Descriptive analysis, Hybrid artificial intelligences, Neural architectures, Neural-networks, Neurosymbolic artificial intelligence, Spectra's, Supply chain management},
	pages = {1510 -- 1540},
	annote = {Cited by: 2; All Open Access, Hybrid Gold Open Access},
}

@article{zhao_unraveling_2024-2,
	title = {Unraveling quantum computing system architectures: {An} extensive survey of cutting-edge paradigms},
	volume = {167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179094225&doi=10.1016%2fj.infsof.2023.107380&partnerID=40&md5=fd9ef3dd6d9f7e6bae0e8fa8439e9741},
	doi = {10.1016/j.infsof.2023.107380},
	abstract = {Context: The convergence of physics and computer science in the realm of quantum computing systems has sparked a profound revolution within the computer industry. However, despite such promise, the existing focus on quantum software systems primarily centers on the generation of quantum source code, inadvertently overlooking the pivotal role of the overall software architecture. Objectives: In order to provide comprehensive guidance to researchers and practitioners engaged in quantum software development, employing an architecture-centered development model, an extensive literature review was conducted pertaining to existing research on quantum software architecture. The analysis encompasses a detailed examination of the characteristics exhibited by these studies and the identification of prospective challenges that lie ahead in the field of quantum software architecture. Methods: We have closely examined instances of quantum software engineering, quantum modeling languages, quantum design patterns, and quantum communication security to gain insights into the distinctive attributes associated with various software architecture approaches. Results: Our findings underscore the critical significance of prioritizing software architecture in the development of robust and efficient quantum software systems. Through the synthesis of these multifaceted aspects, both researchers and practitioners can devise quantum software solutions that are inherently architecture-centric. Conclusion: The software architecture of quantum computing systems plays a pivotal role in determining their ultimate success and usability. Given the ongoing advancements in quantum computing technology, the migration of traditional software architecture development methods to the domain of quantum software development holds significant importance. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Zhao, Xudong and Xu, Xiaolong and Qi, Lianyong and Xia, Xiaoyu and Bilal, Muhammad and Gong, Wenwen and Kou, Huaizhen},
	year = {2024},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software design, Computer software, Software-systems, Source codes, Software architecture, Computer industry, Cutting edges, Development model, Modeling languages, Quantum communication, Quantum computers, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Systems architecture},
	annote = {Cited by: 1},
}

@article{gwenhure_gamification_2024-2,
	title = {Gamification of {Cybersecurity} {Awareness} for {Non}-{IT} {Professionals}: {A} {Systematic} {Literature} {Review}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188663020&doi=10.17083%2fijsg.v11i1.719&partnerID=40&md5=c675878ca8dc3cf7e3db076c1d3b4664},
	doi = {10.17083/ijsg.v11i1.719},
	abstract = {This literature review delves into research on the gamification of cybersecurity awareness for non-IT professionals, aiming to provide an accurate report on known and unknown information regarding three key questions: the impact of gamification on cybersecurity awareness interest and engagement, measurable results related to game elements and their connection to specific learning goals, and the long-term effectiveness of gamified cybersecurity. Examining five relevant papers, the findings confirm short-term effectiveness and indicate that the incorporation of various game elements, such as storytelling, team leaderboards, and interactive scenarios, results in increased knowledge, improved engagement, and positive behavior changes aligned with specific cybersecurity awareness learning goals. However, the review also identifies recurring gaps in evaluating individual game elements and customizing gamification strategies for non-IT professionals. Highlighting a critical gap in understanding long-term effectiveness, we argue for further empirical studies to consider habituation effects, emphasizing the need for a nuanced understanding of gamification's impact on cybersecurity awareness over an extended period. Thus, the review contributes to the existing body of knowledge by emphasizing the necessity for empirical studies focusing on sustained, long-term effectiveness and habituation effects in gamified cybersecurity initiatives. © 2024, Serious Games Society. All rights reserved.},
	number = {1},
	journal = {International Journal of Serious Games},
	author = {Gwenhure, Anderson Kevin and Rahayu, Flourensia Sapty},
	year = {2024},
	note = {Publisher: Serious Games Society
Type: Article},
	pages = {83 -- 99},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{villarrubia_designscrumagility_2024-2,
	title = {{DesignScrum}–{An} agility educational resource powered by creativity},
	volume = {54},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181492975&doi=10.1002%2fspe.3308&partnerID=40&md5=4783fe66f11d0583de6f94a416b680d6},
	doi = {10.1002/spe.3308},
	abstract = {Agile methods have been widely adopted by the industry and its teaching has seen a surge, particularly in the software development field. However, these methods have a number of limitations which affect product outcomes, such as the fact that many software development companies now use Scrum to get developers to work without interruption between iterations, rather than to maintain a sustainable rhythm. Agile experts have stated the importance of incorporating creativity into Scrum, and although there are several agile resources that help with the learning process, it seems essential to approach such learning from a practical point of view. Furthermore, none of these resources introduce creativity. In this paper, we present an educational resource in the form of a serious game that allows you to acquire all the key concepts of agile and creative methods. The game is based on the use of LEGO pieces to simulate a real project, applying the key concepts of the Scrum and Design Thinking frameworks in a gamified way. It was assessed in a professional training centre of computer science by using surveys through which participants evaluated their previous knowledge of agile and creativity methods. We analysed the improvement of these competences, as well as the general level of satisfaction with the game. After the game, the results showed that the participants' knowledge of the Scrum and Design Thinking frameworks had improved and that they were very satisfied with the whole experience. © 2024 The Authors. Software: Practice and Experience published by John Wiley \& Sons Ltd.},
	number = {6},
	journal = {Software - Practice and Experience},
	author = {Villarrubia, Carlos and Vara, Juan Manuel and Granada, David and Gómez-Macías, Cristian and Pérez-Blanco, Francisco Javier},
	year = {2024},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Software design, Learning systems, Gamification, Agile, Agile methods, Agile resource, Creativity, Design thinking, Educational resource, Personnel training, Practice and experience, Scra, Serious games, Software practices},
	pages = {985 -- 1009},
	annote = {Cited by: 1; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{langner_challenges_2024-2,
	title = {Challenges for capturing data within data-driven design processes},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85194079420&doi=10.1017%2fpds.2024.212&partnerID=40&md5=b2cc9c1ee795f82f17c520d338fdb953},
	doi = {10.1017/pds.2024.212},
	abstract = {Cyber-Physical-Systems provide extensive data gathering opportunities along the lifecycle, enabling data-driven design to improve the design process. However, its implementation faces challenges, particularly in the initial data capturing stage. To identify those, a comprehensive approach combining a systematic literature review and an industry survey was applied. Four groups of interrelated challenges were identified as most relevant to practitioners: data selection, data availability in systems, knowledge about data science processes and tools, and guiding users in targeted data capturing. © 2024 Proceedings of the Design Society. All rights reserved.},
	booktitle = {Proceedings of the {Design} {Society}},
	publisher = {Cambridge University Press},
	author = {Langner, Christopher and Paliyenko, Yevgeni and Müller, Benedikt and Roth, Daniel and Guertler, Matthias R. and Kreimeyer, Matthias},
	year = {2024},
	note = {Type: Conference paper},
	keywords = {Embedded systems, Systematic literature review, Life cycle, Design, Capturing data, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Data capturing, Data gathering, Data-driven design, Design-process, Industry surveys, Internet of thing, Internet of things},
	pages = {2099 -- 2108},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{idlahcen_exploring_2024-2,
	title = {Exploring data mining and machine learning in gynecologic oncology},
	volume = {57},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183390317&doi=10.1007%2fs10462-023-10666-2&partnerID=40&md5=e20c182acb99fcbd45f1023721557013},
	doi = {10.1007/s10462-023-10666-2},
	abstract = {Gynecologic (GYN) malignancies are gaining new and much-needed attention, perpetually fueling literature. Intra-/inter-tumor heterogeneity and “frightened” global distribution by race, ethnicity, and human development index, are pivotal clues to such ubiquitous interest. To advance “precision medicine” and downplay the heavy burden, data mining (DM) is timely in clinical GYN oncology. No consolidated work has been conducted to examine the depth and breadth of DM applicability as an adjunct to GYN oncology, emphasizing machine learning (ML)-based schemes. This systematic literature review (SLR) synthesizes evidence to fill knowledge gaps, flaws, and limitations. We report this SLR in compliance with Kitchenham and Charters’ guidelines. Defined research questions and PICO crafted a search string across five libraries: PubMed, IEEE Xplore, ScienceDirect, SpringerLink, and Google Scholar—over the past decade. Of the 3499 potential records, 181 primary studies were eligible for in-depth analysis. A spike (60.53\%) corollary to cervical neoplasms is denoted onward 2019, predominantly featuring empirical solution proposals drawn from cohorts. Medical records led (23.77\%, 53 art.). DM-ML in use is primarily built on neural networks (127 art.), appoint classification (73.19\%, 172 art.) and diagnoses (42\%, 111 art.), all devoted to assessment. Summarized evidence is sufficient to guide and support the clinical utility of DM schemes in GYN oncology. Gaps persist, inculpating the interoperability of single-institute scrutiny. Cross-cohort generalizability is needed to establish evidence while avoiding outcome reporting bias to locally, site-specific trained models. This SLR is exempt from ethics approval as it entails published articles. © 2024, The Author(s).},
	number = {2},
	journal = {Artificial Intelligence Review},
	author = {Idlahcen, Ferdaous and Idri, Ali and Goceri, Evgin},
	year = {2024},
	note = {Publisher: Springer Nature
Type: Article},
	keywords = {Systematic literature review, Machine learning, Machine-learning, Diagnosis, Data mining, Female reproductive system, Global distribution, Gynecologic AI, Gynecologic malignancies, Human development index, Neoplasm, Oncology, Reproductive systems, Tumor heterogeneity, Tumors},
	annote = {Cited by: 2; All Open Access, Hybrid Gold Open Access},
}

@article{koch_cloud-based_2023-2,
	title = {Cloud-{Based} {Reinforcement} {Learning} in {Automotive} {Control} {Function} {Development}},
	volume = {5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172153714&doi=10.3390%2fvehicles5030050&partnerID=40&md5=916bf16f7660e18b017ab960469db241},
	doi = {10.3390/vehicles5030050},
	abstract = {Automotive control functions are becoming increasingly complex and their development is becoming more and more elaborate, leading to a strong need for automated solutions within the development process. Here, reinforcement learning offers a significant potential for function development to generate optimized control functions in an automated manner. Despite its successful deployment in a variety of control tasks, there is still a lack of standard tooling solutions for function development based on reinforcement learning in the automotive industry. To address this gap, we present a flexible framework that couples the conventional development process with an open-source reinforcement learning library. It features modular, physical models for relevant vehicle components, a co-simulation with a microscopic traffic simulation to generate realistic scenarios, and enables distributed and parallelized training. We demonstrate the effectiveness of our proposed method in a feasibility study to learn a control function for automated longitudinal control of an electric vehicle in an urban traffic scenario. The evolved control strategy produces a smooth trajectory with energy savings of up to 14\%. The results highlight the great potential of reinforcement learning for automated control function development and prove the effectiveness of the proposed framework. © 2023 by the authors.},
	number = {3},
	journal = {Vehicles},
	author = {Koch, Lucas and Roeser, Dennis and Badalian, Kevin and Lieb, Alexander and Andert, Jakob},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Article},
	pages = {914 -- 930},
	annote = {Cited by: 0; All Open Access, Gold Open Access, Green Open Access},
}

@article{hernandez_requirements_2023-2,
	title = {Requirements management in {DevOps} environments: a multivocal mapping study},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146221014&doi=10.1007%2fs00766-023-00396-w&partnerID=40&md5=4b84297f3c4a81569ac37e71e82d3cce},
	doi = {10.1007/s00766-023-00396-w},
	abstract = {Attention is currently being focused on DevOps, which aims to reduce software development time by means of close collaboration between the development and operations areas. However, little effort has been made to determine the role of requirements management in DevOps. The objective of this study is to help both researchers and practitioners by providing an overview of the best practices regarding requirements engineering in DevOps and identifying which areas still need to be investigated. A multivocal mapping study has, therefore, been carried out in order to study which methodologies, techniques and tools are used to support requirements management in DevOps environments. After applying the review protocol, 37 papers from formal literature and 14 references from grey literature were selected for analysis. The general conclusions obtained after analysing these papers were that, within DevOps, more attention should be paid to: (1) the reuse of requirements in order to identify systems and software artefacts that can serve as a basis for the specification of new projects; (2) the communication of requirements between the different areas of an organisation and the stakeholders of a project; (3) the traceability of requirements in order to identify the relationship with other requirements, artefacts, tasks and processes; (4) non-functional requirements in order to identify the requirements of the operations area in the early phases of a project; and finally (5) specific requirements tools that should be seamlessly integrated into the DevOps toolchain. All these issues must be considered without ignoring the agile and continuous practices of development, operations and business teams. More effort must also be made to validate new methodologies in industry so as to assess and determine their strengths and weaknesses. © 2023, The Author(s).},
	number = {3},
	journal = {Requirements Engineering},
	author = {Hernández, Rogelio and Moros, Begoña and Nicolás, Joaquín},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Software design, Computer software reusability, Mapping, Requirement engineering, Requirements engineering, Development time, Mapping studies, Best practices, Requirement management, Agile requirement engineering, Agile requirements, Development and operations, Environmental management, Multivocal mapping study},
	pages = {317 -- 346},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
}

@article{spinellis_open_2023-2,
	title = {Open reproducible scientometric research with {Alexandria3k}},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178516434&doi=10.1371%2fjournal.pone.0294946&partnerID=40&md5=81c76085ccde3fc925309b30cdec682b},
	doi = {10.1371/journal.pone.0294946},
	abstract = {Considerable scientific work involves locating, analyzing, systematizing, and synthesizing other publications, often with the help of online scientific publication databases and search engines. However, use of online sources suffers from a lack of repeatability and transparency, as well as from technical restrictions. Alexandria3k is a Python software package and an associated command-line tool that can populate embedded relational databases with slices from the complete set of several open publication metadata sets. These can then be employed for reproducible processing and analysis through versatile and performant queries. We demonstrate the software’s utility by visualizing the evolution of publications in diverse scientific fields and relationships among them, by outlining scientometric facts associated with COVID-19 research, and by replicating commonly-used bibliometric measures and findings regarding scientific productivity, impact, and disruption. Copyright: © 2023 Diomidis Spinellis.},
	number = {11 November},
	journal = {PLoS ONE},
	author = {Spinellis, Diomidis},
	year = {2023},
	pmid = {38032908},
	note = {Publisher: Public Library of Science
Type: Article},
	keywords = {Metadata, Article, coronavirus disease 2019, human, software, Alexandria3k software, bibliometrics, Bibliometrics, Databases, dynamics, Factual, factual database, journal impact factor, measurement repeatability, metadata, methodology, online analytical processing, productivity, proof of concept, publication, Research Design, scientometrics, search engine, Search Engine},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{soares_trends_2023-2,
	title = {Trends in continuous evaluation of software architectures},
	volume = {105},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147748180&doi=10.1007%2fs00607-023-01161-1&partnerID=40&md5=ff7020687f6de6e3c5a34f6dda65d4d7},
	doi = {10.1007/s00607-023-01161-1},
	abstract = {The software industry is increasingly facing the need for continuous deployment of systems. This leads to the adoption of continuous activities, including planning, integration, and deployment (a.k.a. Continuous Software Engineering (CSE)). At the same time, systems should exhibit high-quality architectures, which are often achieved through architecture evaluation methods. However, there is little insight of how such evaluation happens in the context of CSE. To cover this gap, we investigate in this work the state of the art of continuous evaluation of software architectures in CSE, including agile processes like SCRUM. For this, we systematically examine the literature to collect and summarize evidence. Our results show a diversity of means for evaluating architectures in continuous mode to support the continuous evolution of systems. We also found how such evaluation has been incorporated within continuous development processes and agile processes like SCRUM and Crystal. We finally derive the main trends and open issues in the area, aiming to support the community to better understand and further consolidate the field of continuous evaluation of software architectures. © 2023, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.},
	number = {9},
	journal = {Computing},
	author = {Soares, Rodrigo C. and Capilla, Rafael and dos Santos, Vinicius and Nakagawa, Elisa Yumi},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {State of the art, High quality, Continuous software engineerings, Agile process, Evaluation methods, Quality control, Software architecture, Agile manufacturing systems, Architecture evaluation, Continuous architecture evaluation, Software industry, Time systems},
	pages = {1957 -- 1980},
	annote = {Cited by: 1},
}

@inproceedings{zellmer_product-structuring_2023-2,
	title = {Product-{Structuring} {Concepts} for {Automotive} {Platforms}: {A} {Systematic} {Mapping} {Study}},
	volume = {A-1},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175948175&doi=10.1145%2f3579027.3608988&partnerID=40&md5=f19a532ed313f3874c3d38986b4968ef},
	doi = {10.1145/3579027.3608988},
	abstract = {The products of the automotive industry are facing one of the biggest changes: becoming digital smart devices on wheels. Driven by the rising amount of vehicle functions, electronic control units, and software, today's vehicles are becoming cyber-physical systems that are increasingly complex and hard to manage over their life cycle. To handle these challenges, the automotive industry is adopting and integrating methods like software product-line engineering, electrics/electronics platforms, and product generation. While these concepts are widely recognized in their respective research areas and various domains, there is limited research regarding the practical effectiveness of implementing these concepts in a software-driven automotive context. In this paper, we investigate existing product-structuring concepts and methods that consider both hardware and software artifacts, and their applicability to the automotive as well as other cyber-physical industries. For this purpose, we conducted a systematic mapping study to capture a comprehensive overview of existing product-structuring concepts and methods, based on which we discuss how the state-of-the-art can or cannot help solve the challenges of the automotive industry. Specifically, we analyze the practical applicability of the existing solutions to help practitioners apply them and to guide future research. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Zellmer, Philipp and Holsten, Lennart and Leich, Thomas and Krüger, Jacob},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Embedded systems, Life cycle, Mapping, Systematic mapping studies, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Big changes, Digital devices, Electric electronics, Electric lines, Lifecycle management, Product structuring, Product-structuring concept, Productline},
	pages = {170 -- 181},
	annote = {Cited by: 2},
}

@article{ali_intelligent_2023-2,
	title = {Intelligent {Decision} {Support} {Systems}—{An} {Analysis} of {Machine} {Learning} and {Multicriteria} {Decision}-{Making} {Methods}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85187262298&doi=10.3390%2fapp132212426&partnerID=40&md5=dbf125e709aa73f98b50e57c1475067f},
	doi = {10.3390/app132212426},
	abstract = {Context: The selection and use of appropriate multi-criteria decision making (MCDM) methods for solving complex problems is one of the challenging issues faced by decision makers in the search for appropriate decisions. To address these challenges, MCDM methods have effectively been used in the areas of ICT, farming, business, and trade, for example. This study explores the integration of machine learning and MCDM methods, which has been used effectively in diverse application areas. Objective: The objective of the research is to critically analyze state-of-the-art research methods used in intelligent decision support systems and to further identify their application areas, the significance of decision support systems, and the methods, approaches, frameworks, or algorithms exploited to solve complex problems. The study provides insights for early-stage researchers to design more intelligent and cost-effective solutions for solving problems in various application domains. Method: To achieve the objective, literature from the years 2015 to early 2020 was searched and considered in the study based on quality assessment criteria. The selected relevant literature was studied to respond to the research questions proposed in this study. To find answers to the research questions, pertinent literature was analyzed to identify the application domains where decision support systems are exploited, the impact and significance of the contributions, and the algorithms, methods, and techniques which are exploited in various domains to solve decision-making problems. Results: Results of the study show that decision support systems are widely used as useful decision-making tools in various application domains. The research has collectively studied machine learning, artificial intelligence, and multi-criteria decision-making models used to provide efficient solutions to complex decision-making problems. In addition, the study delivers detailed insights into the use of AI, ML and MCDM methods to the early-stage researchers to start their research in the right direction and provide them with a clear roadmap of research. Hence, the development of Intelligent Decision Support Systems (IDSS) using machine learning (ML) and multicriteria decision-making (MCDM) can assist researchers to design and develop better decision support systems. These findings can help researchers in designing more robust, efficient, and effective multicriteria-based decision models, frameworks, techniques, and integrated solutions. © 2023 by the authors.},
	number = {22},
	journal = {Applied Sciences (Switzerland)},
	author = {Ali, Rahman and Hussain, Anwar and Nazir, Shah and Khan, Sulaiman and Khan, Habib Ullah},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 2; All Open Access, Gold Open Access},
}

@article{yang_automatic_2023-2,
	title = {Automatic {Essay} {Evaluation} {Technologies} in {Chinese} {Writing}—{A} {Systematic} {Literature} {Review}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174165294&doi=10.3390%2fapp131910737&partnerID=40&md5=a2e9a5b11c5c1af33cd825ae61aeaf0e},
	doi = {10.3390/app131910737},
	abstract = {Automatic essay evaluation, an essential application of natural language processing (NLP) technology in education, has been increasingly employed in writing instruction and language proficiency assessment. Because automatic Chinese Essay Evaluation (ACEE) has made some breakthroughs due to the rapid development of upstream Chinese NLP technology, many evaluation tools have been applied in teaching practice and high-risk evaluation processes. However, the development of ACEE is still in its early stages, with many technical bottlenecks and challenges. This paper systematically explores the current research status of corpus construction, feature engineering, and scoring models in ACEE through literature to provide a technical perspective for stakeholders in the ACEE research field. Literature research has shown that constructing the ACEE public corpus is insufficient and lacks an effective platform to promote the development of ACEE research. Various shallow and deep features can be extracted using statistical and NLP techniques in ACEE. However, there are still substantial limitations in extracting grammatical errors and features related to syntax and traditional Chinese Literary style. For the construction of scoring models, existing studies have shown that traditional machine learning and deep learning methods each have advantages in different corpora and feature selections. The deep learning model, which exhibits strong adaptability and multi-task joint learning potential, has broader development space regarding model scalability. © 2023 by the authors.},
	number = {19},
	journal = {Applied Sciences (Switzerland)},
	author = {Yang, Hongwu and He, Yanshan and Bu, Xiaolong and Xu, Hongwen and Guo, Weitong},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 2; All Open Access, Gold Open Access},
}

@article{sadeghiani_what_2023-2,
	title = {What pivot is: {Touching} an elephant in the dark},
	volume = {3},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85155452238&doi=10.1016%2fj.digbus.2023.100056&partnerID=40&md5=4b53b6bc91a08e0cc741882f699640c0},
	doi = {10.1016/j.digbus.2023.100056},
	abstract = {The term ‘pivot’ first appeared in the practical literature, demonstrating its role in the Lean Startup, and immediately emerged in the academic literature. However, it suffered from conflicting perceptions and practice-academy divide. Moreover, recently, a growing number of scholars introduced it as a response to the pandemic crisis, at times as an umbrella term. In this study, we first test the concept ‘pivot’ against clarity criteria and find its problematic issues based on a qualitative content analysis of the literature; then, using a multi-case study, we get a critical distance from the pivot's origin; parsimoniously reposition it as ‘substitution’, differentiate it from ‘pivoting’ as its process theory; and discuss it in relation to vision, strategy, and business model. Finally, we propose a conceptual basis and some rules for operationalization. © 2023 The Authors},
	number = {1},
	journal = {Digital Business},
	author = {Sadeghiani, Ayoob and Anderson, Alistair},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{stupar_model-based_2023-2,
	title = {Model-based cloud service deployment optimisation method for minimisation of application service operational cost},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85148332611&doi=10.1186%2fs13677-023-00389-8&partnerID=40&md5=f64e50ae46a6dd7b46e8606ff5d35f41},
	doi = {10.1186/s13677-023-00389-8},
	abstract = {Many currently existing cloud cost optimisation solutions are aimed at cloud infrastructure providers, and they often deal only with specific types of application services. Unlike infrastructure providers, the providers of cloud applications are often left without a suitable cost optimisation solution, especially concerning the wide range of different application types. This paper presents an approach that aims to provide an optimisation solution for the providers of applications hosted in the cloud environments, applicable at the early phase of a cloud application lifecycle and for a wide range of application services. The focus of this research is the development of the method for identifying optimised service deployment option in available cloud environments based on the model of the service and its context, intending to minimise the operational cost of the cloud service while fulfilling the requirements defined by the service level agreement. A cloud application context metamodel is proposed that includes parameters related to both the application service and the cloud infrastructure relevant for the cost and quality of service. By using the proposed optimisation method, knowledge is gained about the effects of the cloud application context parameters on the service cost and quality of service, which is then used to determine the optimal service deployment option. The service models are validated using cloud applications deployed in laboratory conditions, and the optimisation method is validated using the simulations based on the proposed cloud application context metamodel. The experimental results based on two cloud application services demonstrate the ability of the proposed approach to provide relevant information about the impact of cloud application context parameters on service cost and quality of service and use this information for reducing service operational cost while preserving the acceptable service quality level. The results indicate the applicability and relevance of the proposed approach for cloud applications in the early service lifecycle phase since application providers can gain valuable insights regarding service deployment decision without acquiring extensive datasets for the analysis. © 2023, The Author(s).},
	number = {1},
	journal = {Journal of Cloud Computing},
	author = {Stupar, Ivana and Huljenic, Darko},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Application contexts, Cloud application context, Cloud applications, Cloud services, Cost management, Cost optimization method, Costs, Costs Optimization, Distributed database systems, Iaa, Life cycle, Operational cost, Optimization method, Quality of service, Quality-of-service, Saa, Service modeling},
	annote = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
}

@book{furtado_controlled_2023-2,
	title = {Controlled experimentation of software product lines},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160474401&doi=10.1007%2f978-3-031-18556-4_19&partnerID=40&md5=dd6f8e961dba036421d866a0cbda7a31},
	abstract = {The process of experimentation is one of several scientific methods that can provide evidence for a proof of a theory. This process is counterpoint to the real world observation method, thus providing a reliable body of knowledge. However, in the experimentation for emerging areas and in the consolidation process in scientific and industrial communities, such as the software product line (SPL), there has been a constant lack of adequate documentation of experiments that makes it difficult to repeat, replicate, and reproduce studies in SPL. Therefore, this chapter presents a set of guidelines for the quality assessment of SPL experiments with its conceptual model to support the understanding of the proposed guidelines, as well as an ontology for SPL experiments, called OntoExper-SPL, in addition to support the teaching experimentation in SPL. Thus, these points aim to improve the planning, conduction, analysis, sharing, and documentation of SPL experiments, supporting the construction of a reliable and reference body of knowledge in such a context in addition to enabling improvement in the teaching of SPL experiments. © Springer Nature Switzerland AG 2023. All rights reserved.},
	publisher = {Springer International Publishing},
	author = {Furtado, Viviane R. and Vignando, Henrique and Luz, Carlos D. and Steinmacher, Igor F. and Kalinowski, Marcos and OliveiraJr, Edson},
	year = {2023},
	doi = {10.1007/978-3-031-18556-4_19},
	note = {Publication Title: UML-Based Software Product Line Engineering with SMarty
Type: Book chapter},
	annote = {Cited by: 0},
}

@article{yin_predicting_2023-2,
	title = {Predicting {Changes} in {User}-{Driven} {Requirements} {Using} {Conditional} {Random} {Fields} in {Agile} {Software} {Development}},
	volume = {70},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85112600382&doi=10.1109%2fTEM.2021.3083513&partnerID=40&md5=ce8def99d83c5673583af52f67d3b7a6},
	doi = {10.1109/TEM.2021.3083513},
	abstract = {Agile development encourages requirements change. The accurate predictions of changes in user requirements could help software evolve in the right direction to increase user satisfaction. Previous research on requirement predictions mostly relies on historical defects or user feedback. In this article, we aim to predict requirement changes based on user-system interactions in agile software development. We focus on the user-system interaction behaviors used to infer user intentions and predict requirement changes to drive the incremental iterations of agile development. Through a prototype system with two incremental iterations, an embedded program in the system captures the user runtime interaction behavior data. We utilize the conditional random fields to explore the user potential intentions and infer the emerging requirements accordingly. The increased accuracy of results in the iterations proves the effectiveness of our approach in predicting user requirement changes. © 1988-2012 IEEE.},
	number = {11},
	journal = {IEEE Transactions on Engineering Management},
	author = {Yin, Ming and Meng, Danli and Zhu, Dan and Wang, Yibo and Jiang, Jijiao},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Software design, Software testing, Agile software development, Behavioral research, Forecasting, Digital storage, Agile manufacturing systems, Behaviour patterns, Conditional random field, Hidden Markov models, Hidden-Markov models, Interactive computer systems, Micromechanical device, Prototype, Random fields, Real - Time system, Real time systems, Requirements change, Software, User driven, User intention inference, User-driven development, User's intentions},
	pages = {3715 -- 3727},
	annote = {Cited by: 4},
}

@article{innocente_framework_2023-2,
	title = {A framework study on the use of immersive {XR} technologies in the cultural heritage domain},
	volume = {62},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162121296&doi=10.1016%2fj.culher.2023.06.001&partnerID=40&md5=eb7e017ab19913950cb9d4779e742030},
	doi = {10.1016/j.culher.2023.06.001},
	abstract = {Most cultural promotion and dissemination are nowadays performed through the digitization of heritage sites and museums, a necessary requirement to meet the new needs of the public. Augmented Reality (AR), Mixed Reality (MR), and Virtual Reality (VR) have the potential to improve the experience quality and educational effect of these sites by stimulating users’ senses in a more natural and vivid way. In this respect, head-mounted display (HMD) devices allow visitors to enhance the experience of cultural sites by digitizing information and integrating additional virtual cues about cultural artifacts, resulting in a more immersive experience that engages the visitor both physically and emotionally. This study contributes to the development and incorporation of AR, MR, and VR applications in the cultural heritage domain by providing an overview of relevant studies utilizing fully immersive systems, such as headsets and CAVE systems, emphasizing the advantages that they bring when compared to handheld devices. We propose a framework study to identify the key features of headset-based Extended Reality (XR) technologies used in the cultural heritage domain that boost immersion, sense of presence, and agency. Furthermore, we highlight core characteristics that favor the adoption of these systems over more traditional solutions (e.g., handheld devices), as well as unsolved issues that must be addressed to improve the guests’ experience and the appreciation of the cultural heritage. An extensive search of Google Scholar, Scopus, IEEE Xplore, ACM Digital Library, and Wiley Online Library databases was conducted, including papers published from January 2018 to September 2022. To improve review reporting, the Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA) guidelines were used. Sixty-five papers met the inclusion criteria and were classified depending on the study's purpose: education, entertainment, edutainment, touristic guidance systems, accessibility, visitor profiling, and management. Immersive cultural heritage systems allow visitors to feel completely immersed and present in the virtual environment, providing a stimulating and educational cultural experience that can improve the quality and learning purposes of cultural visits. Nonetheless, the analyzed studies revealed some limitations that must be faced to give a further impulse to the adoption of these technologies in the cultural heritage domain. © 2023 Consiglio Nazionale delle Ricerche (CNR)},
	journal = {Journal of Cultural Heritage},
	author = {Innocente, Chiara and Ulrich, Luca and Moos, Sandro and Vezzetti, Enrico},
	year = {2023},
	note = {Publisher: Elsevier Masson s.r.l.
Type: Review},
	pages = {268 -- 283},
	annote = {Cited by: 12; All Open Access, Hybrid Gold Open Access},
}

@book{liu_space-air-ground_2023-2,
	title = {Space-air-ground integrated network security},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169763096&doi=10.1007%2f978-981-99-1125-7&partnerID=40&md5=0650c0f66edd4c9e68a8c9fea0b9de9a},
	abstract = {This book focuses on security science and technology, data and information security, and mobile and network security for space-air-ground integrated networks (SAGINs). SAGIN are expected to play an increasingly important role in providing real-time, flexible, and integrated communication and data transmission services in an efficient manner. Today, SAGINs have been widely developed for a range of applications in navigation, environmental monitoring, traffic management, counter-terrorism, etc. However, security becomes a major concern, since the satellites, spacecrafts, and aircrafts are susceptible to a variety of traditional/specific network-based attacks, including eavesdropping, session hijacking, and illegal access. In this book, we review the theoretical foundations of SAGIN security. We also address a range of related security threats and provide cutting-edge solutions in the aspect of ground network security, airborne network security, space network security, and provide future trends in SAGIN security. The book goes from an introduction to the topic's background, to a description of the basic theory, and then to cutting-edge technologies, making it suitable for readers at all levels including professional researchers and beginners. To gain the most from the book, readers should have taken prior courses in information theory, cryptography, network security, etc. © The Author(s), under exclusive license to Springer Nature Singapore Pte Ltd. 2023. All rights reserved.},
	publisher = {Springer Nature},
	author = {Liu, Jianwei and Bai, Lin and Jiang, Chunxiao and Zhang, Wei},
	year = {2023},
	doi = {10.1007/978-981-99-1125-7},
	note = {Publication Title: Space-Air-Ground Integrated Network Security
Type: Book},
	annote = {Cited by: 0},
}

@article{stradowski_industrial_2023-2,
	title = {Industrial applications of software defect prediction using machine learning: {A} business-driven systematic literature review},
	volume = {159},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150041533&doi=10.1016%2fj.infsof.2023.107192&partnerID=40&md5=adef39458fbdf72dbe9232624eef7f22},
	doi = {10.1016/j.infsof.2023.107192},
	abstract = {Context: Machine learning software defect prediction is a promising field of software engineering, attracting a great deal of attention from the research community; however, its industry application tents to lag behind academic achievements. Objective: This study is part of a larger project focused on improving the quality and minimising the cost of software testing of the 5G system at Nokia, and aims to evaluate the business applicability of machine learning software defect prediction and gather lessons learnt. Methods: The systematic literature review was conducted on journal and conference papers published between 2015 and 2022 in popular online databases (ACM, IEEE, Springer, Scopus, Science Direct, and Google Scholar). A quasi-gold standard procedure was used to validate the search, and SEGRESS guidelines were used for transparency, reporting, and replicability. Results: We have selected and analysed 32 publications out of 397 found by our automatic search (and seven by snowballing). We have identified highly relevant evidence of methods, features, frameworks, and datasets used. However, we found a minimal emphasis on practical lessons learnt and cost consciousness — both vital from a business perspective. Conclusion: Even though the number of machine learning software defect prediction studies validated in the industry is increasing (and we were able to identify several excellent papers on studies performed in vivo), there is still not enough practical focus on the business aspects of the effort that would help bridge the gap between the needs of the industry and academic research. © 2023 The Authors},
	journal = {Information and Software Technology},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systematic literature review, Software testing, Costs, Application programs, Machine learning, Machine-learning, Defects, Forecasting, Software defect prediction, 5G mobile communication systems, Cost minimization, Effort and cost minimization, Industry applications, Lesson learnt, Machine learning software, Real-world, Research communities},
	annote = {Cited by: 6; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{watson_augmented_2023-2,
	title = {Augmented {Behavioral} {Annotation} {Tools}, with {Application} to {Multimodal} {Datasets} and {Models}: {A} {Systematic} {Review}},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171380178&doi=10.3390%2fai4010007&partnerID=40&md5=8cad8eef1ab5b05d19d8bea8f2461e3d},
	doi = {10.3390/ai4010007},
	abstract = {Annotation tools are an essential component in the creation of datasets for machine learning purposes. Annotation tools have evolved greatly since the turn of the century, and now commonly include collaborative features to divide labor efficiently, as well as automation employed to amplify human efforts. Recent developments in machine learning models, such as Transformers, allow for training upon very large and sophisticated multimodal datasets and enable generalization across domains of knowledge. These models also herald an increasing emphasis on prompt engineering to provide qualitative fine-tuning upon the model itself, adding a novel emerging layer of direct machine learning annotation. These capabilities enable machine intelligence to recognize, predict, and emulate human behavior with much greater accuracy and nuance, a noted shortfall of which have contributed to algorithmic injustice in previous techniques. However, the scale and complexity of training data required for multimodal models presents engineering challenges. Best practices for conducting annotation for large multimodal models in the most safe and ethical, yet efficient, manner have not been established. This paper presents a systematic literature review of crowd and machine learning augmented behavioral annotation methods to distill practices that may have value in multimodal implementations, cross-correlated across disciplines. Research questions were defined to provide an overview of the evolution of augmented behavioral annotation tools in the past, in relation to the present state of the art. (Contains five figures and four tables). © 2023, Multidisciplinary Digital Publishing Institute (MDPI). All rights reserved.},
	number = {1},
	journal = {AI (Switzerland)},
	author = {Watson, Eleanor and Viana, Thiago and Zhang, Shujun},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Article},
	pages = {128 -- 171},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{xu_systematic_2023-2,
	title = {A systematic mapping study on machine learning methodologies for requirements management},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145104779&doi=10.1049%2fsfw2.12082&partnerID=40&md5=d8c1d596f27bfb9f05d19106fa83be54},
	doi = {10.1049/sfw2.12082},
	abstract = {Requirements management (RM) plays an important role in requirements engineering. The development of machine learning (ML) is in full swing, and many ML software management techniques had been used to improve the performance of RM methods. However, as no research study is known that exists systematically to summarise the ML methods used in RM. To fill this gap, this paper adopts the systematic mapping study to survey the state-of-the-art ML methods for RM primary studies and were finally selected in this mapping, which was published on 36 conferences and journals. The 24 factors affecting the ML method of RM are determined, of which 9, 11 and 4 are the three parts of RM, namely requirements baseline maintenance, requirements traceability and requirements change management separately. The 18 objectives of the ML method for RM are summarised, of which 6, 7 and 5 are the three parts of RM. The eight ML methods used in RM and their time sequence are summarised. The 18 evaluation indexes for RM in the ML method are determined, and the performance of these methods on these parameters is analysed. The research direction of this paper is of great significance to the research of researchers in demand management. © 2022 The Authors. IET Software published by John Wiley \& Sons Ltd on behalf of The Institution of Engineering and Technology.},
	number = {4},
	journal = {IET Software},
	author = {Xu, Chi and Li, Yuanbang and Wang, Bangchao and Dong, Shi},
	year = {2023},
	note = {Publisher: John Wiley and Sons Inc
Type: Article},
	keywords = {Management IS, Mapping, Systematic mapping studies, Performance, Requirement engineering, Requirements engineering, Machine learning, Machine-learning, Requirement management, Machine learning software, Full-swing, Machine learning methods, On-machines},
	pages = {405 -- 423},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{stradowski_machine_2023-2,
	title = {Machine learning in software defect prediction: {A} business-driven systematic mapping study},
	volume = {155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144824314&doi=10.1016%2fj.infsof.2022.107128&partnerID=40&md5=c3aa54434eb54f1abaa22279bb12f8ef},
	doi = {10.1016/j.infsof.2022.107128},
	abstract = {Context: Machine learning is a valuable tool in software engineering allowing fair defect prediction capabilities at a relatively small expense. However, although the practical usage of machine learning in defect prediction has been studied over many years, there is not sufficient systematic effort to analyse its potential for business application. Objective: The following systematic mapping study aims to analyse the current state-of-the-art in terms of machine learning software defect prediction modelling and to identify and classify the emerging new trends. Notably, the analysis is done from a business perspective, evaluating the opportunities to adopt the latest techniques and methods in commercial settings to improve software quality and lower the cost of development life cycle. Method: We created a broad search universe to answer our research questions, performing an automated query through the Scopus database to identify relevant primary studies. Next, we evaluated all found studies using a classification scheme to map the extent of business adoption of machine learning software defect prediction based on the keywords used in the publications. Additionally, we use PRISMA 2020 guideline to validate reporting. Results: After the application of the selection criteria, the remaining 742 primary studies included in Scopus until February 23, 2022 were mapped to classify and structure the research area. The results confirm that the usage of commercial datasets is significantly smaller than the established datasets from NASA and open-source projects. However, we have also found meaningful emerging trends considering business needs in analysed studies. Conclusions: There is still a considerable amount of work to fully internalise business applicability in the field. Performed analysis has shown that purely academic considerations dominate in published research; however, there are also traces of in vivo results becoming more available. Notably, the created maps offer insight into future machine learning software defect prediction research opportunities. © 2022 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Life cycle, Mapping, Systematic mapping studies, Open source software, Machine learning, Machine-learning, Defect prediction, Defects, Forecasting, Software defect prediction, Computer software selection and evaluation, Quality control, Cost minimization, Effort and cost minimization, Machine learning software, Business applicability, Business applications, Cost benefit analysis, NASA, Prediction capability, Query processing},
	annote = {Cited by: 10},
}

@article{laureate_systematic_2023-2,
	title = {A systematic review of the use of topic models for short text social media analysis},
	volume = {56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85154596265&doi=10.1007%2fs10462-023-10471-x&partnerID=40&md5=9739bfcc768a46989a8b24b876659c8b},
	doi = {10.1007/s10462-023-10471-x},
	abstract = {Recently, research on short text topic models has addressed the challenges of social media datasets. These models are typically evaluated using automated measures. However, recent work suggests that these evaluation measures do not inform whether the topics produced can yield meaningful insights for those examining social media data. Efforts to address this issue, including gauging the alignment between automated and human evaluation tasks, are hampered by a lack of knowledge about how researchers use topic models. Further problems could arise if researchers do not construct topic models optimally or use them in a way that exceeds the models’ limitations. These scenarios threaten the validity of topic model development and the insights produced by researchers employing topic modelling as a methodology. However, there is currently a lack of information about how and why topic models are used in applied research. As such, we performed a systematic literature review of 189 articles where topic modelling was used for social media analysis to understand how and why topic models are used for social media analysis. Our results suggest that the development of topic models is not aligned with the needs of those who use them for social media analysis. We have found that researchers use topic models sub-optimally. There is a lack of methodological support for researchers to build and interpret topics. We offer a set of recommendations for topic model researchers to address these problems and bridge the gap between development and applied research on short text topic models. © 2023, The Author(s).},
	number = {12},
	journal = {Artificial Intelligence Review},
	author = {Laureate, Caitlin Doogan Poet and Buntine, Wray and Linger, Henry},
	year = {2023},
	note = {Publisher: Springer Nature
Type: Article},
	keywords = {Systematic Review, Applied research, Evaluation measures, LDA, Short texts, Social media, Social media analysis, Social media datum, Social networking (online), Topic Modeling, Twitter},
	pages = {14223 -- 14255},
	annote = {Cited by: 12; All Open Access, Hybrid Gold Open Access},
}

@article{muhammad_human_2023-2,
	title = {Human factors in developing automated vehicles: {A} requirements engineering perspective},
	volume = {205},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166738978&doi=10.1016%2fj.jss.2023.111810&partnerID=40&md5=a7e6d5d79bc57f87ebe0adaee8fd4182},
	doi = {10.1016/j.jss.2023.111810},
	abstract = {Automated Vehicle (AV) technology has evolved significantly both in complexity and impact and is expected to ultimately change urban transportation. Due to this evolution, the development of AVs challenges the current state of automotive engineering practice, as automotive companies increasingly include agile ways of working in their plan-driven systems engineering—or even transition completely to scaled-agile approaches. However, it is unclear how knowledge about human factors (HF) and technological knowledge related to the development of AVs can be brought together in a way that effectively supports today's rapid release cycles and agile development approaches. Based on semi-structured interviews with ten experts from industry and two experts from academia, this qualitative, exploratory case study investigates the relationship between HF and AV development. The study reveals relevant properties of agile system development and HF, as well as the implications of these properties for integrating agile work, HF, and requirements engineering. According to the findings, which were evaluated in a workshop with experts from academia and industry, a culture that values HF knowledge in engineering is key. These results promise to improve the integration of HF knowledge into agile development as well as to facilitate HF research impact and time to market. © 2023 The Author(s)},
	journal = {Journal of Systems and Software},
	author = {Muhammad, Amna Pir and Knauss, Eric and Bärgman, Jonas},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Requirement engineering, Requirements engineering, Automation, 'current, Agile development, Agile, Agile manufacturing systems, Automated vehicles, Automotive companies, Engineering perspective, Engineering practices, Human engineering, Property, Urban transportation, Vehicle technology, Vehicles},
	annote = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@inproceedings{de_souza_challenges_2023-2,
	title = {On {Challenges} and {Opportunities} of {Using} {Continuous} {Experimentation} in the {Engineering} of {Contemporary} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166029896&doi=10.1145%2f3592813.3592927&partnerID=40&md5=b03d1df07b3686a2afc6fb07af95d244},
	doi = {10.1145/3592813.3592927},
	abstract = {Context: Modern Information Systems require the use of contemporary software systems such as Cyber-Physical Systems, Embedded Systems, and Smart Cities-based Systems, eventually built under the paradigm of the Internet of Things. These Contemporary Software Systems (CSS) add new challenges for their construction, maintainability, and evolution, including the involvement of many actors with the software project and the necessary management of dependencies among hardware/things, software systems, and people. Problem: These technological challenges jeopardize the final quality of modern information systems due to the lack of adequate mechanisms supporting the engineering of CSS. Solution: Continuous Experimentation (CE) deserves some investigation regarding its suitability to mitigate and reduce engineering CSS risks. IS Theory: This research is under the General Systems Theory and is consistent with the Systems Information challenges regarding building smart cities-based systems. Method: To undertake a Structured Literature Review (StLR) supported with snowballing to reveal CE’s empirical studies. Results: The StLR identified seven primary studies on CE adoption to support CSS building. Many studies are in the domain of embedded systems and CPS. Besides, the findings allowed us to conjecture a set of challenges and opportunities regarding using CE in CSS engineering. Conclusion: There are emergent technologies to support CE’s execution in the context of web-based systems. However, several challenges and gaps surround CE’s use for engineering CSS. Furthermore, the lack of software technologies, blueprints, or concrete guidance to promote CE in these software systems can motivate further investigations into its use in engineering the important parts of modern information systems. © 2023 Copyright held by the owner/author(s).},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {de Souza, Bruno P. and dos Santos, Paulo Sérgio M. and Travassos, Guilherme H.},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Embedded systems, Information systems, Information use, Literature reviews, Computer software, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Industry 4.0, Project management, Cybe-physical systems, Cyber-physical systems, Software project, Blueprints, Embedded-system, Smart city, Software systems risks, Technological challenges},
	pages = {372 -- 379},
	annote = {Cited by: 0},
}

@article{mars_survey_2023-2,
	title = {A survey on automation approaches of smart contract generation},
	volume = {79},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153378608&doi=10.1007%2fs11227-023-05262-8&partnerID=40&md5=77add21c036dff71f65ebea7c8b82f5f},
	doi = {10.1007/s11227-023-05262-8},
	abstract = {In the blockchain environment, smart contracts are computer programs that run on the blockchain platform. However, the development of smart contracts is a major challenge for developers, since blockchain platforms are still evolving. Owing to the inherited nature of blockchain, developing smart contracts without introducing vulnerabilities is not an easy task, as the deployed code is immutable and can be invoked by anyone with access to the network. Smart contracts have proved to be error-prone in practice due to the complexity of programming. Additionally, non-functional requirements, such as service cost, security, performance, authorization, and authentication, should be well implemented and defined in computer systems. In this paper, we aim to present a systematic literature review to outline in detail different approaches of smart contracts generation. Furthermore, we present a comparison of the existing approaches based on a classification according to automation paradigm and a set of defined criteria. Finally, we discuss the gaps in the literature, as well as identify a set of potential challenges which can significantly strengthen the existing work. The study shows that the examined works focused only on a limited number of specific features, such as authorization, asset control, and security. Additionally, formal verification of smart contracts and data privacy are poorly addressed. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {14},
	journal = {Journal of Supercomputing},
	author = {Mars, Rawya and Cheikhrouhou, Saoussen and Kallel, Slim and Hadj Kacem, Ahmed},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Systematic literature review, Automation, Data privacy, Network security, Non-functional requirements, Block-chain, Blockchain, Smart contract, Authentication, Authorization, Error prones, Security authentication, Security performance, Service authentication, Service costs, Service security, Smart contract generation},
	pages = {16065 -- 16097},
	annote = {Cited by: 1},
}

@article{sun_design_2023-2,
	title = {A {Design} of {Automatic} {Magnetic} {Properties} {Measurement} {System} for {Under}/{Postgraduate} {Open} {Experimental} {Project}: {Effective}, {Low} {Cost}, and {Scalable}},
	volume = {66},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85135754845&doi=10.1109%2fTE.2022.3194356&partnerID=40&md5=a88d8dccf12434e4ff710534ab824b0e},
	doi = {10.1109/TE.2022.3194356},
	abstract = {Contribution: This research designs an experimental education project of an automatic material magnetism properties measurement system. It explores how the do-it-yourself (DIY), hands-on establishment, and hardware-software integration experiment system could be leveraged to enhance the understanding of the electromagnetism theory and nonelectrical quantity measurement application. Background: Designing and establishing a prototype of an automatic measurement system are essential, inspiring and beneficial to comprehending and consolidating the theoretical principles and simulations in electromagnetism and nonelectrical quantity measurement technology courses. Furthermore, in the vision of batch deployment and flexible group teaching, a cost-efficiency, hard/soft integration system for the electromagnetism property measurement definitely fits the aim of innovation laboratory education. Intended Outcomes: This project is intended to guide the students to finish a comprehensive research-oriented project loop for multiple skills training. It is also intended to encourage student motivation and confidence through theory investigation, hands-on group collaboration, and tunning for performance calibration. Application Design: The design of the proposed project includes the literature review, hardware selection and assembly, system integration, software programming, interface visualization, and system performance calibration. The participants are assessed with both objective exam questions and subjective open survey for training objectives to reveal their increased knowledge, interests, skill confidence, and creative thinking. Findings: The proposed project has been used in 4-year laboratory open experiments with students at different levels from freshmen undergraduates to 1st-year postgraduates. A use case and survey assessment demonstrate tremendous knowledge, skills, and confidence improvement among students in electromagnetism and measurement technology courses. © 1963-2012 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Education},
	author = {Sun, Xiaohua and Liu, Haochen and Liang, Peng},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Application programs, E-learning, Students, Personnel training, Calibration, Curricula, Epstein frame, Epstein frames, Experiential learning, Hard/soft integration system, Integration systems, Magnetic properties, Magnetic property measurements, Magnetism, Material magnetic property measurement, Measurement technologies, Property measurement, Surveys, Teaching, Virtual instrument},
	pages = {104 -- 112},
	annote = {Cited by: 2},
}

@article{trieflinger_discovery_2023-2,
	title = {The discovery effort worthiness index: {How} much product discovery should you do and how can this be integrated into delivery?},
	volume = {157},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150361731&doi=10.1016%2fj.infsof.2023.107167&partnerID=40&md5=d7394c3ca1383f10bc646afa2ddc0576},
	doi = {10.1016/j.infsof.2023.107167},
	abstract = {Context: In a world of high dynamics and uncertainties, it is almost impossible to have a long-term prediction of which products, services, or features will satisfy the needs of the customer. To counter this situation, the conduction of Continuous Improvement or Design Thinking for product discovery are common approaches. A major constraint in conducting product discovery activities is the high effort to discover and validate features and requirements. In addition, companies struggle to integrate product discovery activities into their agile processes and iterations. Objective: This paper aims at suggests a supportive tool, the “Discovery Effort Worthiness (DEW) Index”, for product owners and agile teams to determine a suitable amount of effort that should be spent on Design Thinking activities. To operationalize DEW, proposals for practitioners are presented that can be used to integrate product discovery into product development and delivery. Method: A case study was conducted for the development of the DEW index. In addition, we conducted an expert workshop to develop proposals for the integration of product discovery activities into the product development and delivery process. Results: First, we present the "Discovery Effort Worthiness Index" in form of a formula. Second, we identified requirements that must be fulfilled for systematic integration of product discovery activities into product development and delivery. Third, we derived from the requirements proposals for the integration of product discovery activities with a company's product development and delivery. Conclusion: The developed "Discovery Effort Worthiness Index" provides a tool for companies and their product owners to determine how much effort they should spend on Design Thinking methods to discover and validate requirements. Integrating product discovery with product development and delivery should ensure that the results of product discovery are incorporated into product development. This aims to systematically analyze product risks to increase the chance of product success. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Trieflinger, Stefan and Lang, Dominic and Spies, Selina and Münch, Jürgen},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Product management, Integration, Uncertainty, Agile development, Design thinking, Scra, Customer satisfaction, High dynamic, Long-term prediction, Product design, Product development, Product discovery, Product roadmaps, Product service},
	annote = {Cited by: 0},
}

@article{kotti_machine_2023-2,
	title = {Machine {Learning} for {Software} {Engineering}: {A} {Tertiary} {Study}},
	volume = {55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152600375&doi=10.1145%2f3572905&partnerID=40&md5=1e148a3bcb1e10b59bf60db05eeba5f0},
	doi = {10.1145/3572905},
	abstract = {Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches. © 2023 Association for Computing Machinery.},
	number = {12},
	journal = {ACM Computing Surveys},
	author = {Kotti, Zoe and Galanopoulou, Rafaila and Spinellis, Diomidis},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Systematic literature review, Software testing, Life cycle, Tertiary study, Machine learning, Machine-learning, Industrial research, Software testings, Computer software selection and evaluation, Additional key word and phrasestertiary study, Key words, Life cycle activities, Machine learning techniques, Software engineering life-cycle, Software Quality},
	annote = {Cited by: 6; All Open Access, Green Open Access},
}

@article{sadi_webapik_2023-2,
	title = {{WEBAPIK}: a body of structured knowledge on designing web {APIs}},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149938330&doi=10.1007%2fs00766-023-00401-2&partnerID=40&md5=31e57dd7c1ca274b4ed4f36deb2f8c7f},
	doi = {10.1007/s00766-023-00401-2},
	abstract = {With the rise in initiatives such as software ecosystems and Internet of Things (IoT), developing robust web Application Programming Interfaces (web APIs) has become an increasingly important practice. One main concern in developing web APIs is that they expose back-end systems and data toward clients. This exposure threatens critical non-functional requirements, such as the security of back-end systems, the performance of provided services, and the privacy of communications with clients. Although dealing with non-functional requirements during software design has been long studied, there is still little guide on addressing these requirements in web APIs. In this paper, we present WEBAPIK, a body of structured knowledge on addressing non-functional requirements in the design of web APIs. WEBAPIK is comprised of 27 distinct non-functional requirements, 37 distinct design techniques to address some of the identified requirements, and the trade-offs of 22 design techniques, presented in two forms of natural language and knowledge graphs. The design knowledge compiled in WEBAPIK is systematically extracted and aggregated from 80 heterogeneous online literature resources, including 7 books, 15 weblogs and tutorial, 5 vendor white papers, 6 design standards, and 47 research papers. These resources are systematically retrieved from two search engines of Google and Google Scholar and five research databases of Web of Science, IEEE Xplore, ACM Digital Library, SpringerLink, and ScienceDirect in two periods of March to August 2018 and August 2022. WEBAPIK gathers and structures expert and scholarly discussions to provide insight about addressing non-functional requirements in the design of web APIs. The structure brought to the design knowledge makes it amenable towards extension and creates the potential for employing it in the database of knowledge-based systems that aid software developers in design decision-making. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	number = {3},
	journal = {Requirements Engineering},
	author = {Sadi, Mahsa H. and Yu, Eric},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Systematic Review, Decision making, Software design, Application programs, Computer software reusability, Economic and social effects, Search engines, WEB application, Web applications, Knowledge based systems, Non-functional requirements, Software architecture, Quality attributes, Internet of things, Digital libraries, Application programming interfaces (API), Applications programming interfaces, Commerce, Design Patterns, Knowledge reuse, Reviews, Trade off, Web API},
	pages = {441 -- 479},
	annote = {Cited by: 1},
}

@inproceedings{de_campos_usability_2023-2,
	title = {Usability and {User} {Experience} {Evaluation} of {Touchable} {Holographic} {Solutions}: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184291519&doi=10.1145%2f3638067.3638071&partnerID=40&md5=6667873e7cd3469430b5d782f20258a7},
	doi = {10.1145/3638067.3638071},
	abstract = {Interacting with holograms using the hands is on track to reach a broad audience in the coming years. Therefore, a current challenge is understanding how to evaluate this new interaction scenario concerning Usability and User eXperience (UX). In this use context, the user interacts with virtual objects in your real environment. Some already known evaluation technologies have been applied to this type of solution, although they were not created considering aspects such as immersion and presence, typical in this interactive environment. Thus, this paper presents a Systematic Mapping Study (SMS) that sought to identify usability and UX evaluation technologies applied to touchable holographic solutions in augmented reality or mixed reality environments. Furthermore, the SMS sought to answer questions about evaluation technologies and holographic solutions. The SMS examined 3551 publications and selected 40 that presented 106 usability or UX evaluation technologies in a touchable holographic solution. The results shed light on methods and aspects little addressed and showed patterns and preferences for combinations of devices, gestures, and feedback types. This work contributes to HCI researchers by better understanding the state of the art of usability and UX evaluation technologies applied to touchable holographic solutions, classifying them, and discussing the main gaps and opportunities. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {De Campos, Thiago Prado and Damasceno, Eduardo Filgueiras and Valentim, Natasha Malveira Costa},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Mapping, Systematic mapping studies, Users' experiences, 'current, Augmented reality, Mixed reality, Holograms, Interactive Environments, Real environments, Usability, Usability and user experience evaluation, Usability engineering, Use context, Virtual objects},
	annote = {Cited by: 0},
}

@article{molleri_backsourcing_2023-2,
	title = {Backsourcing of {IT} with focus on software development—{A} systematic literature review},
	volume = {204},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164225673&doi=10.1016%2fj.jss.2023.111771&partnerID=40&md5=b4c1364eb0462d21b5de6514e26d9187},
	doi = {10.1016/j.jss.2023.111771},
	abstract = {Context: Backsourcing is the process of insourcing previously outsourced activities. Backsourcing can be a viable alternative when companies experience environmental or strategic changes, or challenges with outsourcing. While outsourcing and related processes have been extensively studied, few studies report experiences with backsourcing. Objectives: We summarize the results of the research literature on backsourcing of IT, with a focus on software development. By identifying practically relevant experience, we present findings that may help companies considering backsourcing. In addition, we identify gaps in the current research literature and point out areas for future work. Method: Our systematic literature review (SLR) started with a search for empirical studies on the backsourcing of IT. From each study, we identified the context in which backsourcing occurred, the factors leading to the decision, the backsourcing process, and the outcomes of backsourcing. We employed inductive coding to extract textual data from the papers and qualitative cross-case analysis to synthesize the evidence. Results: We identified 17 papers that reported 26 cases of backsourcing, six of which were related to software development. The cases came from a variety of contexts. The most common reasons for backsourcing were improving quality, reducing costs, and regaining control of outsourced activities. We model the backsourcing process as containing five sub-processes: change management, vendor relationship management, competence building, organizational build-up, and transfer of ownership. We identified 14 positive outcomes and nine negative outcomes of backsourcing. We also aggregated the evidence and detailed three relationships of potential use to companies considering backsourcing. Finally, we have highlighted the knowledge areas of software engineering associated with the backsourcing of software development. Conclusion: The backsourcing of IT is a complex process; its implementation depends on the prior outsourcing relationship and other contextual factors. Our systematic literature review contributes to a better understanding of this process by identifying its components and their relationships based on the peer-reviewed literature. Our results can serve as a motivation and baseline for further research on backsourcing and provide guidelines and process fragments from which practitioners can benefit when they engage in backsourcing. © 2023 Elsevier Inc.},
	journal = {Journal of Systems and Software},
	author = {Molléri, Jefferson Seide and Lassenius, Casper and Jørgensen, Magne},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Systematic literature review, Software design, Outsourcing, 'current, Quality control, Engineering research, Backsourcing, Empirical studies, Environmental change, Insourcing, Software engineering management, Strategic challenges, Strategic changes, Textual data},
	annote = {Cited by: 1; All Open Access, Green Open Access},
}

@article{omerkhel_exploring_2023-2,
	title = {{EXPLORING} {STRATEGIES} {FOR} {OVERCOMING} {ISSUES} {OF} {USER} {INVOLVEMENT} {IN} {AGILE} {SOFTWARE} {DEVELOPMENT}: {A} {SYSTEMATIC} {LITERATURE} {REVIEW}},
	volume = {101},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175462629&partnerID=40&md5=c6798a876dfb842582a5174b0b26b09f},
	abstract = {The present systematic literature review (SLR) explores the challenges and strategies associated with managing users during requirement elicitation within agile software development. Drawing insights from an analysis of 24 relevant studies, this study comprehensively examines the issues that arise and the effective approaches to overcome them. The findings reveal five prominent challenges of user involvement during requirement elicitation. The most dominant issues identified are the lack of user involvement, insufficient user knowledge, and a deficit in the expertise of the Product Owner. These challenges can hinder the effective integration of user perspectives and needs into the development process. To address these challenges, the study identifies seven strategies that Product Owners can adopt to facilitate effective user involvement. These strategies include Mind Maps, User Interface Mockups, Workshops, Hybridism (combining agile and non-agile techniques), Face-to-Face Meetings, Continuous Delivery, and Training and Learning initiatives. The application of these strategies empowers Product Owners and software practitioners to enhance user involvement, improve communication, and streamline the requirement elicitation process in agile software development. The outcomes of this SLR provide valuable insights for both researchers and software practitioners, exploring the complex dynamics of user involvement in agile contexts. By recognizing these challenges and deploying effective strategies, software development teams can ensure more successful requirement elicitation processes, leading to the creation of software products that better align with user needs and expectations. This review contributes to a deeper understanding of user involvement challenges and offers actionable guidance for optimizing the requirement elicitation within agile software development paradigm. © 2023 Little Lion Scientific.},
	number = {19},
	journal = {Journal of Theoretical and Applied Information Technology},
	author = {Omerkhel, Qudrattullah and Yusop, Othman Mohd and Ismail, Saiful Adli and Azmi, Azri},
	year = {2023},
	note = {Publisher: Little Lion Scientific
Type: Article},
	pages = {5837 -- 5854},
	annote = {Cited by: 1},
}

@article{wang_systematic_2023-2,
	title = {A {Systematic} {Literature} {Review} of {Software} {Traceability} {Links} {Automation} {Techniques}; [软件跟踪链自动化技术研究综述]},
	volume = {46},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173597434&doi=10.11897%2fSP.J.1016.2023.01919&partnerID=40&md5=f05d3c01adf3d021d5a13587264108ce},
	doi = {10.11897/SP.J.1016.2023.01919},
	abstract = {As an important software capability, software traceability aims to capture, link and trace each crucial software artifact via constructing the traceability links between them. The stud-y of software traceability covers many aspects, including traceability modelling, traceability assessment and traceability implementation. Traceability links interconnect software artifacts with each other and use the resulting associative networks to resolve issues with software products and their development processes. Traceability links provide critical support for many software engineering activities, including impact analysis, software verification, test case selection, compliance verification, system security assurance and defect detection. Traceability links refer to a specific relationship between a pair of software artifacts, one of which is the source artifact and the other is the target artifact. They records various relationships between software artifacts such as dependencies, influences, and causal relationships. The direction of traceability links can be oneway or two-way. Various traceability links can help software developers to understand, develop and manage systems both efficiently and effectively. At the same time, traceability links can help people involved in all phases of software development activities to accomplish their development tasks. Requirements traceability links, as the most widely used traceability links, enable the construction and maintenance of traceability links between requirements and other software artifacts. Moreover, traceability links also include the establishment of links between code and tests, design and code, models and code, defects and code, and so on. In recent years, the creation, maintenance and validation of traceability links with information retrieval, natural language processing, machine learning, and deep learning can reduce the manual handling cost of traceability links by developers, and therefore have received extensive attentions from academia and industry. There arc also some works reviewing software traceability links approaches and techniques. In this paper, we focus on the automation techniques of the creation, maintenance and validation of traceability links so as to sort out and summarize the research progress in the past ten years. The main content includes the statistics and analysis of approaches and techniques for automated creation, maintenance and validation of traceability links, the application research of traceability links, the state-of-the-art traceability links related evaluation research and tools support, and the key problems of the current traceability links techniques. The problems arc summarized from the technical difficulties around seventh parts: the complexity of the tracing software, the granularity problem, the unsatisfying accuracy, the type limitation, the validation efficiency, the application scale and time, and the incomprehensive evaluation of traceability links. . Besides, several possible solution ideas and future development trends of the problems arc elaborated, including the construction of horizontal traceability links between software artifacts, the scalable and configurable automation techniques of traceability links, the integration of traditional approaches with artificial intelligence techniques, the creation of multiple types of traceability links using intermediary artifacts, the interactive verification of traceability links, the real-time retrieval of traceability links and open sourcing of related source codes. This review also reveals that: (1) The creation of traceability links has received a lot of academic attentions, but the research on the maintenance, verification, and application of traceability links needs more attention; (2) Requirements-to-code links are the most concerned type of researchers, followed by rcquirements-to-design and design-to-code, while other traceability links such as code/data-to-model and screenshot-to-defect are also starting to enter the vision of researchers; (3) With the continuous development of artificial intelligence (AI), Al-bascd techniques such as Naive Baycs, SVM, Bert, Doc2Vcc, RNN have been widely used in the creation, maintenance and verification of traceability links; (4) In the creation of traceability links, it is difficult to achieve good results by relying only on information retrieval and artificial intelligence techniques. Information retrieval, machine learning or deep learning techniques should be combined with traditional heuristics, model-based methods and so on, to make up for the deficiencies in AI technologies and traditional methods to further improve the quality of traceability links; (5) Research on traceability links automation techniques in complex environments, cross-platform and cross-language should be on the agenda in the future. © 2023 Science Press. All rights reserved.},
	number = {9},
	journal = {Jisuanji Xuebao/Chinese Journal of Computers},
	author = {Wang, Ye and Hu, Kun and Jiang, Bo and Xia, Xin and Tang, Xian-Shu},
	year = {2023},
	note = {Publisher: Science Press
Type: Article},
	keywords = {Software design, Software testing, Application programs, Deep learning, Learning algorithms, Learning systems, Natural language processing systems, Requirements engineering, Automation, Natural languages, Machine-learning, Defects, Compliance control, Language processing, Natural language processing, Codes (symbols), Verification, Computer software selection and evaluation, Software artefacts, Automation techniques, Computer software maintenance, Software traceability, Software traceability link, Traceability links},
	pages = {1919 -- 1946},
	annote = {Cited by: 0},
}

@article{torre_how_2023-2,
	title = {How consistency is handled in model-driven software engineering and {UML}: an expert opinion survey},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128744172&doi=10.1007%2fs11219-022-09585-2&partnerID=40&md5=ec55130b1fa92036e10fb41920ba4ebe},
	doi = {10.1007/s11219-022-09585-2},
	abstract = {Model-driven software engineering (MDSE) is an established approach for developing complex software systems. The unified modelling language (UML) is one of the most used model languages for applying the MDSE approach. UML has 14 diagram types that describe different perspectives of a software system under development. These diagrams are strongly dependent on each other and must be consistent with one another. The main objectives of this paper are as follows: (1) to understand (i) how aware experts are of model consistency issues and (ii) how relevant these issues are to experts, in order to understand model consistency in the MDSE/UML contexts, and more importantly, (2) to validate a set of 116 UML consistency rules that was systematically collected from the literature, so as to identify the rules that should always be enforced. We conducted a personal opinion survey with 106 experts in SE and MDSE, by means of an online questionnaire. The survey results describe an overview of how the topic of MDSE/UML consistency is handled by experts in the field. In addition, this survey identified a set of 52 UML consistency rules which should always be checked in every UML diagram. The majority of these 52 rules were understood by the majority of respondents and are general-purpose rules that are involved in the Design software development phase. This subset of 52 rules could be considered to be (1) added to the UML standard, (2) used as a reference to researchers who study UML/MDSE, and (3) used as a practical example for teaching purposes. © 2022, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {1},
	journal = {Software Quality Journal},
	author = {Torre, Damiano and Genero, Marcela and Labiche, Yvan and Elaasar, Maged},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Software design, Computer software, Unified Modeling Language, Surveys, Empirical studies, Expert opinion, Model consistency, Model-driven software engineering consistency, Model-driven software engineerings, Opinion surveys, Personal opinion survey, Software engineering model, Unified modeling language consistency rule},
	pages = {1 -- 54},
	annote = {Cited by: 7},
}

@article{bianco_reducing_2023-2,
	title = {Reducing the user labeling effort in effective high recall tasks by fine-tuning active learning},
	volume = {61},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146621225&doi=10.1007%2fs10844-022-00772-y&partnerID=40&md5=27482213cefa9ee93b873591699a61b7},
	doi = {10.1007/s10844-022-00772-y},
	abstract = {High recall Information REtrieval (HIRE) aims at identifying only and (almost) all relevant documents for a given query. HIRE is paramount in applications such as systematic literature review, medicine, legal jurisprudence, among others. To address the HIRE goals, active learning methods have proven valuable in determining informative and non-redundant documents to reduce user effort for manual labeling. We propose a new active learning framework for the HIRE task. REVEAL-HIRE selects a very reduced set of documents to be labeled, significantly mitigating the user’s effort. The proposed approach selects the most representative documents by exploiting a novel, specifically designed active learning strategy for HIRE, called REVEAL (RelEVant rulE-based Active Learning). REVEAL aims at selecting the maximum number of relevant documents for a given query based on discriminative rule-based patterns and a penalization factor. The method is applied to the top-ranked documents to choose the most informative ones to be labeled, a hard task due to data skewness – most documents are irrelevant for a given query. The enhanced active learning process is repeated incrementally until a stopping point is achieved, using REVEAL to identify the point in the process when relevant documents should stop to be sampled. Experimental results in several standard benchmark datasets (e.g. 20-Newsgroups, Trec Total Recall, and CLEF eHealth) demonstrate that REVEAL-HIRE can reduce the user labeling effort up to 3 times (320\% of reduction) in comparison with state-of-the-art baselines while keeping the effectiveness at the highest levels. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {2},
	journal = {Journal of Intelligent Information Systems},
	author = {Bianco, Guilherme Dal and Duarte, Denio and Gonçalves, Marcos André},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Artificial intelligence, Learning systems, Classification (of information), Information retrieval, Labelings, Active Learning, Fine tuning, Hire, Labeling process, Relevant documents, Rule based, SSAR, Supervised classifiers, User labeling},
	pages = {453 -- 472},
	annote = {Cited by: 3},
}

@article{yang_seeing_2023-2,
	title = {Seeing the {Whole} {Elephant}: {Systematically} {Understanding} and {Uncovering} {Evaluation} {Biases} in {Automated} {Program} {Repair}},
	volume = {32},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161982187&doi=10.1145%2f3561382&partnerID=40&md5=c5af7c7df149502ee029204053597e14},
	doi = {10.1145/3561382},
	abstract = {Evaluation is the foundation of automated program repair (APR), as it provides empirical evidence on strengths and weaknesses of APR techniques. However, the reliability of such evaluation is often threatened by various introduced biases. Consequently, bias exploration, which uncovers biases in the APR evaluation, has become a pivotal activity and performed since the early years when pioneer APR techniques were proposed. Unfortunately, there is still no methodology to support a systematic comprehension and discovery of evaluation biases in APR, which impedes the mitigation of such biases and threatens the evaluation of APR techniques.In this work, we propose to systematically understand existing evaluation biases by rigorously conducting the first systematic literature review on existing known biases and systematically uncover new biases by building a taxonomy that categorizes evaluation biases. As a result, we identify 17 investigated biases and uncover a new bias in the usage of patch validation strategies. To validate this new bias, we devise and implement an executable framework APRConfig, based on which we evaluate three typical patch validation strategies with four representative heuristic-based and constraint-based APR techniques on three bug datasets. Overall, this article distills 13 findings for bias understanding, discovery, and validation. The systematic exploration we performed and the open source executable framework we proposed in this article provide new insights as well as an infrastructure for future exploration and mitigation of biases in APR evaluation. © 2023 Association for Computing Machinery.},
	number = {3},
	journal = {ACM Transactions on Software Engineering and Methodology},
	author = {Yang, Deheng and Lei, Yan and Mao, Xiaoguang and Qi, Yuhua and Yi, Xin},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Systematic literature review, Open source software, Repair, Key words, Additional key word and phrasesautomated program repair, Bias studies, Constraint-based, Empirical evaluations, Executables, Petroleum reservoir evaluation, Repair techniques, Systematic exploration, Validation strategies},
	annote = {Cited by: 2; All Open Access, Bronze Open Access},
}

@article{alonso_systematic_2023-2,
	title = {A systematic mapping study and practitioner insights on the use of software engineering practices to develop {MVPs}},
	volume = {156},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145972508&doi=10.1016%2fj.infsof.2022.107144&partnerID=40&md5=27902f1fdcb0c646416221c72975d06a},
	doi = {10.1016/j.infsof.2022.107144},
	abstract = {Background: Many startup environments and even traditional software companies have embraced the use of MVPs (Minimum Viable Products) to allow quickly experimenting solution options. The MVP concept has influenced the way in which development teams apply Software Engineering (SE) practices. However, the overall understanding of this influence of MVPs on SE practices is still poor. Objective: Our goal is to characterize the publication landscape on practices that have been used in the context of software MVPs and to gather practitioner insights on the identified practices. Method: We conducted a systematic mapping study using a hybrid search strategy that consists of a database search and parallel forward and backward snowballing. Thereafter, we discussed the mapping study results in two focus groups sessions involving twelve industry practitioners that extensively use MVPs in their projects to capture their perceptions on the findings of the mapping study. Results: We identified 33 papers published between 2013 and 2020. We observed some trends related to MVP ideation (or MVP conception) and evaluation practices. For instance, regarding ideation, we found six different approaches (e.g., Design Thinking, Lean Inception) and mainly informal end-user involvement practices (e.g., workshops, interviews). Regarding evaluation, there is an emphasis on end-user validations based on practices such as usability tests, A/B testing, and usage data analysis. However, there is still limited research related to MVP technical feasibility assessment and effort estimation. Practitioners of the focus group sessions reinforced the confidence in our results regarding ideation and evaluation practices, being aware of most of the identified practices. They also reported how they deal with the technical feasibility assessments (involving developers during the ideation and conducting informal experiments) and effort estimation in practice (based on expert opinion and using practices common to agile methodologies, such as Planning Poker). Conclusion: Our analysis suggests that there are opportunities for solution proposals and evaluation studies to address literature gaps concerning technical feasibility assessment and effort estimation. Overall, more effort needs to be invested into empirically evaluating the existing MVP-related practices. © 2022 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Alonso, Silvio and Kalinowski, Marcos and Ferreira, Bruna and Barbosa, Simone D.J. and Lopes, Hélio},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software company, Mapping, Systematic mapping studies, Search engines, Systematic mapping, Mapping studies, Effort Estimation, Feasibility assessment, Focus groups, Minimum viable product, Software engineering practices},
	annote = {Cited by: 4; All Open Access, Green Open Access},
}

@article{reis_automated_2023-2,
	title = {Automated guided vehicles position control: a systematic literature review},
	volume = {34},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122319071&doi=10.1007%2fs10845-021-01893-x&partnerID=40&md5=d765d8c0523ba75a0782a045796b8994},
	doi = {10.1007/s10845-021-01893-x},
	abstract = {Automated Guided Vehicles (AGVs) are essential elements of manufacturing intralogistics and material handling. Improving the position accuracy along the AGV trajectory allows the vehicle to work on narrower aisles with lower error tolerance. Despite the increasing number of papers in AGVs and mobile robots’ position control research area, there is a lack of curatorial work presenting and analyzing the control strategies applied in the problem domain. Therefore, the main objective is to analyze the published researches of the past seven years on the position control of AGVs to recognize research patterns, gaps, and tendencies, outlining the research field. The paper proposes a systematic literature review to investigate the research field from the controller design perspective. Its protocol and procedures are presented in detail. Four main research topics were addressed: the control strategies used in the AGV position control problem, how the literature presents the AGV operating requirement of position accuracy, how the literature validate the proposed controller and present their results regarding the system’s position accuracy, and the technological tendencies the proposed solutions reveals. Besides, within the main topics, other points were investigated, such as the AGV application area, the considered mathematical model, the sensors and guidance system used, and the maximum payload of the vehicle and operation under different load conditions. The data synthesis shows the predominant control strategies applied to the problem and the interaction among distinct control theory areas, indicating a notable interaction of Intelligent Control techniques to the other strategies. The paper’s contributions are using a systematic literature review method over the AGV position control publications, presenting an overview of the research area, analyzing the research question topics from selected articles, and proposing a research agenda. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {4},
	journal = {Journal of Intelligent Manufacturing},
	author = {Reis, Wallace Pereira Neves dos and Couto, Giselle Elias and Junior, Orides Morandin},
	year = {2023},
	note = {Publisher: Springer
Type: Review},
	keywords = {Systematic literature review, Research areas, Automated guided vehicles, Automatic guided vehicles, Control strategies, Control system synthesis, Controllers, Essential elements, Intelligent control, Material handling, Materials handling, Mobile robots, Path tracking, Position accuracy, Position control, Research fields, Vehicle position},
	pages = {1483 -- 1545},
	annote = {Cited by: 25},
}

@article{borstler_investigating_2023-2,
	title = {Investigating acceptance behavior in software engineering—{Theoretical} perspectives},
	volume = {198},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146227386&doi=10.1016%2fj.jss.2022.111592&partnerID=40&md5=4df944097102b7c6c0fb2ecbb7397c86},
	doi = {10.1016/j.jss.2022.111592},
	abstract = {Background: Software engineering research aims to establish software development practice on a scientific basis. However, the evidence of the efficacy of technology is insufficient to ensure its uptake in industry. In the absence of a theoretical frame of reference, we mainly rely on best practices and expert judgment from industry-academia collaboration and software process improvement research to improve the acceptance of the proposed technology. Objective: To identify acceptance models and theories and discuss their applicability in the research of acceptance behavior related to software development. Method: We analyzed literature reviews within an interdisciplinary team to identify models and theories relevant to software engineering research. We further discuss acceptance behavior from the human information processing perspective of automatic and affect-driven processes (“fast” system 1 thinking) and rational and rule-governed processes (“slow” system 2 thinking). Results: We identified 30 potentially relevant models and theories. Several of them have been used in researching acceptance behavior in contexts related to software development, but few have been validated in such contexts. They use constructs that capture aspects of (automatic) system 1 and (rational) system 2 oriented processes. However, their operationalizations focus on system 2 oriented processes indicating a rational view of behavior, thus overlooking important psychological processes underpinning behavior. Conclusions: Software engineering research may use acceptance behavior models and theories more extensively to understand and predict practice adoption in the industry. Such theoretical foundations will help improve the impact of software engineering research. However, more consideration should be given to their validation, overlap, construct operationalization, and employed data collection mechanisms when using these models and theories. © 2022 The Author(s)},
	journal = {Journal of Systems and Software},
	author = {Börstler, Jürgen and Ali, Nauman bin and Svensson, Martin and Petersen, Kai},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Software design, Behavioral research, Software engineering research, Acceptance behavior, TAM, TPB, Dual-process theories, Oriented process, Software development practices, Technology acceptance, Theory, UTAUT},
	annote = {Cited by: 3; All Open Access, Hybrid Gold Open Access},
}

@article{kotti_impact_2023-2,
	title = {Impact of {Software} {Engineering} {Research} in {Practice}: {A} {Patent} and {Author} {Survey} {Analysis}},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139448523&doi=10.1109%2fTSE.2022.3208210&partnerID=40&md5=d72b5940144a9dec58e6b088e8d70a4e},
	doi = {10.1109/TSE.2022.3208210},
	abstract = {Existing work on the practical impact of software engineering (SE) research examines industrial relevance rather than adoption of study results, hence the question of how results have been practically applied remains open. To answer this and investigate the outcomes of impactful research, we performed a quantitative and qualitative analysis of 4 354 SE patents citing 1 690 SE papers published in four leading SE venues between 1975-2017. Moreover, we conducted a survey on 475 authors of 593 top-cited and awarded publications, achieving 26\% response rate. Overall, researchers have equipped practitioners with various tools, processes, and methods, and improved many existing products. SE practice values knowledge-seeking research and is impacted by diverse cross-disciplinary SE areas. Practitioner-oriented publication venues appear more impactful than researcher-oriented ones, while industry-related tracks in conferences could enhance their impact. Some research works did not reach a wide footprint due to limited funding resources or unfavorable cost-benefit trade-off of the proposed solutions. The need for higher SE research funding could be corroborated through a dedicated empirical study. In general, the assessment of impact is subject to its definition. Therefore, academia and industry could jointly agree on a formal description to set a common ground for subsequent research on the topic. © 1976-2012 IEEE.},
	number = {4},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kotti, Zoe and Gousios, Georgios and Spinellis, Diomidis},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Software engineering, Economic and social effects, Industrial research, Software engineering research, Engineering research, Software, Cost benefit analysis, Surveys, Empirical studies, Collaboration, Interview, Patent, Patent citation, Patents and inventions, Practical impact, Quantitative and qualitative analysis, Survey analysis},
	pages = {2020 -- 2038},
	annote = {Cited by: 2; All Open Access, Green Open Access},
}

@article{al-ahmad_overview_2023-2,
	title = {Overview on {Case} {Study} {Penetration} {Testing} {Models} {Evaluation}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160556929&doi=10.28991%2fESJ-2023-07-03-025&partnerID=40&md5=9da6f31eb866e14860834a06ff6466c6},
	doi = {10.28991/ESJ-2023-07-03-025},
	abstract = {Model evaluation is a cornerstone of scientific research as it represents the findings' accuracy and model performance. A case study is commonly used in evaluating software engineering models. Due to criticism in terms of generalization from a single case study and testers, deciding on the number of case studies used for evaluation and the number of testers has been one of the researchers’ challenges. Multiple case studies with multiple testers can be difficult in some domains, such as penetration testing, due to the complexity and time needed to prepare test cases. This study aims to review the literature and examine the evaluation methods used pertaining to the number of case studies and testers involved. This study is beneficial for researchers, students, and penetration testers as it provides case study design steps that are useful to determine the appropriate number of test cases and testers required. The paper's findings and novelty highlight that a single case study with a single tester is enough to evaluate a model. It also strikes a balance between what is enough for the evaluation and the need to reduce criticisms of a single case study by using two case studies with a single tester. © 2023 by the authors. Licensee ESJ, Italy.},
	number = {3},
	journal = {Emerging Science Journal},
	author = {Al-Ahmad, Ahmad S. and Kahtan, Hasan and Alzoubi, Yehia I.},
	year = {2023},
	note = {Publisher: Ital Publication
Type: Review},
	pages = {1019 -- 1036},
	annote = {Cited by: 1; All Open Access, Gold Open Access, Green Open Access},
}

@article{miloudi_systematic_2023-2,
	title = {Systematic {Review} of {Machine} {Learning}-{Based} {Open}-{Source} {Software} {Maintenance} {Effort} {Estimation}},
	volume = {16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85143977206&doi=10.2174%2f2666255816666220609110712&partnerID=40&md5=4d4bca6f178d40ff4531940a93a67f66},
	doi = {10.2174/2666255816666220609110712},
	abstract = {Background: Software maintenance is known as a laborious activity in the software lifecycle and is often considered more expensive than other activities. Open-Source Software (OSS) has gained considerable acceptance in the industry recently, and the Maintenance Effort Estimation (MEE) of such software has emerged as an important research topic. In this context, researchers have conducted a number of open-source software maintenance effort estimation (O-MEE) studies based on statistical as well as machine learning techniques for better estimation. Objective: The objective of this study is to perform a systematic literature review (SLR) to analyze and summarize the empirical evidence of O-MEE ML techniques in current research through a set of five Research Questions (RQs) related to several criteria (e.g. data pre-processing tasks, data mining tasks, tuning parameter methods, accuracy criteria and statistical tests, as well as ML techniques reported in the literature that outperformed). Methods: We performed a systematic literature review of 36 primary empirical studies published from 2000 to June 2020, selected based on an automated search of six digital databases. Results: The findings show that Bayesian networks, decision tree, support vector machines and instance-based reasoning were the ML techniques most used; few studies opted for ensemble or hybrid techniques. Researchers have paid less attention to O-MEE data pre-processing in terms of feature selection, methods that handle missing values and imbalanced datasets, and tuning parameters of ML techniques. Classification data mining is the task most addressed using different accuracy criteria such as Precision, Recall, and Accuracy, as well as Wilcoxon and Mann-Whitney statistical tests. Conclusion: This SLR identifies a number of gaps in the current research and suggests areas for further investigation. For instance, since OSS includes different data source formats, researchers should pay more attention to data pre-processing and develop new models using ensemble techniques since they have proved to perform better. © 2023 Bentham Science Publishers.},
	number = {3},
	journal = {Recent Advances in Computer Science and Communications},
	author = {Miloudi, Chaymae and Cheikhi, Laila and Abran, Alain},
	year = {2023},
	note = {Publisher: Bentham Science Publishers
Type: Review},
	keywords = {Systematic literature review, Life cycle, Learning algorithms, Learning systems, Open source software, 'current, Decision trees, Effort Estimation, Support vector machines, Open systems, Open-source softwares, Data mining, Bayesian networks, Machine learning techniques, Empirical studies, Computer software maintenance, Data preprocessing, Ensemble techniques, Maintenance effort estimation, Maintenance efforts, Statistical tests},
	annote = {Cited by: 1},
}

@article{anuar_multivocal_2023-2,
	title = {A multivocal literature review on record management potential components in {CRUD} operation for web application development},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85131085987&doi=10.1142%2fS1793962323410192&partnerID=40&md5=c69d2412dddcbff1a771c169b69a7201},
	doi = {10.1142/S1793962323410192},
	abstract = {In recent years, web application frameworks have been widely practised by many developers to increase programming productivity as the frameworks are more flexible, rapidly built using CRUD operation, MVC-based, secure and most of them are published under an open-source license which will reduce the final cost of development. Although the CRUD automation in the web application framework boosts the development process, there are many important aspects of a web application absent from the CRUD output. Therefore, this multivocal literature review investigates the record management aspects that are required in modern WA and the perceived benefit of integrating the record management aspect into CRUD operation. The study extracted 284 publications from respectable scientific resources and the grey resources literature created by WA development practitioners outside academic mediums. After a detailed review process, only 14 scientific primary studies and 13 gray studies were considered for this review based on defined inclusion and exclusion criteria. The review shows that the most important aspects required in WA are search, role-based access control, retention, appraisal, search, audit trail, digital archiving, sharing, reporting, inactive files management and several other features. These important aspects have been analyzed and characterized according to its function and features. The method and procedure for integrating the specified aspect into CRUD operation are identified and discussed. Integrating and implementing the specified record management features into CRUD operation will boost the WA development productivity by producing more features as a standard output with integrated record management functions. © 2023 World Scientific Publishing Company.},
	number = {2},
	journal = {International Journal of Modeling, Simulation, and Scientific Computing},
	author = {Anuar, Asyraf Wahi and Kama, Nazri and Azmi, Azri and Rusli, Hazlifah Mohd},
	year = {2023},
	note = {Publisher: World Scientific
Type: Article},
	keywords = {Literature reviews, Open source software, Multivocal literature review, Access control, CRUD, Electronic records managements, Final costs, Open source license, Productivity, Record management, Web application development, Web application frameworks, Web frameworks},
	annote = {Cited by: 1},
}

@article{findrik_drivers_2023-2,
	title = {Drivers and barriers for consumers purchasing bioplastics – {A} systematic literature review},
	volume = {410},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153893687&doi=10.1016%2fj.jclepro.2023.137311&partnerID=40&md5=63fcad7bc912638f6c3e6e550db8c7b5},
	doi = {10.1016/j.jclepro.2023.137311},
	abstract = {Plastic pollution has adverse effects on the ecosystem and on the human body. Bioplastics might provide an alternative to plastic for environmentally friendly consumers. A systematic literature review was conducted to analyze and summarize the state of the art regarding consumers' response to bioplastics using a hierarchy of effects model. The model holistically represents the relevant steps in understanding consumers' journey from stimuli to the behavioral stage. The review was based on 67 scientific journal articles on consumer studies related to bioplastics published in English language (all peer-reviewed). Most studies researched bioplastic packaging applications e.g., food and beverage packaging using quantitative consumer surveys. Many studies focused on consumer preferences and willingness to pay for bioplastics, while awareness, knowledge, and post-purchase behavior—usage and disposal—was the least researched. Many of the studies applied text or oral and rarely real product stimuli. The results of the synthesis pointed out some purchasing barriers e.g. consumers' low knowledge about the environmental impact, characteristics such as material source or end-of-life character of bioplastics; or consumers' uncertainty about bioplastic recognition versus conventional plastics. Drivers of consumers' purchasing bioplastics are also identified, for instance consumers' positive attitude, available product information or consumers' green value. Bioplastic products meeting consumers' preferences such as low price, biogenic resource base, and local origin also act as purchasing drivers. Studies also found that bioplastic related information of a product influences consumers’ willingness to pay. The review revealed research gaps, highlighting in particular the need for cross-cultural studies, non-hypothetical research designs and the analysis of labelling systems related to bioplastic products. © 2023 The Authors},
	journal = {Journal of Cleaner Production},
	author = {Findrik, Edina and Meixner, Oliver},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Review},
	keywords = {Systematic literature review, Systematic Review, State of the art, Adverse effect, Bio-plastics, Consumer, Consumer behavior, Consumers' preferences, Environmental impact, Human bodies, Labeling, Plastic pollutions, Purchasing, Reinforced plastics, Sales, Willingness to pay},
	annote = {Cited by: 9; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{junior_towards_2023-2,
	title = {Towards {Federated} {Ontology}-{Driven} {Data} {Integration} in {Continuous} {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174524159&doi=10.1145%2f3613372.3613380&partnerID=40&md5=3e4b6d35c3cbb7db05fb9d3cadd95ac4},
	doi = {10.1145/3613372.3613380},
	abstract = {Organizations have adopted Continuous Software Engineering (CSE) practices aiming at making software development faster, iterative, integrated, continuous, and aligned with the business. In this context, they often use different applications (e.g., project management tools, source repositories, and quality assessment tools) that store valuable data to support daily activities and decision-making. However, data items often remain spread in different applications that adopt different data and behavioral models, posing a barrier to integrated data usage. As a consequence, data-driven software development is uncommon, missing valuable opportunities for product and process improvement. In this paper, we explore an ontology network addressing CSE aspects to develop a data integration solution in which networked ontologies are the basis to build reusable and autonomous software components that work together in a system federation to provide meaningful integrated data. We achieve a comprehensive and flexible solution that can be used as a whole or partially, by extracting only the components related to the subdomains of interest. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Júnior, Paulo Sérgio Santos and Almeida, João Paulo A. and Barcellos, Monalessa},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Decision making, Decisions makings, Software design, Computer software reusability, Continuous software engineerings, Ontology, Ontology's, Project management, Quality assessment, Software engineering practices, Assessment tool, Daily activity, Data integration, Data items, Integrated data, Project management tools},
	pages = {31 -- 36},
	annote = {Cited by: 0},
}

@article{russo_exploring_2023-2,
	title = {Exploring a {Multidisciplinary} {Assessment} of {Organisational} {Maturity} in {Business} {Continuity}: {A} {Perspective} and {Future} {Research} {Outlook}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192352194&doi=10.3390%2fapp132111846&partnerID=40&md5=0304e1dbc51d2f583b2eab9b338ef4fc},
	doi = {10.3390/app132111846},
	abstract = {In a competitive business landscape heavily reliant on information and communication technology, organisations must be prepared to address disruptions in their business operations. Business continuity management involves effective planning for the swift reestablishment of business processes in the short term. However, there are still obstacles to implementing business continuity plans, which can be justified by various factors. The purpose of this study is to present the perspectives and future research paths based on a systematic literature review from the peer-reviewed literature published from 1 January 2000 to 31 December 2021. This systematic literature review adheres to the guidelines established by evidence-based software engineering and leverages the Parsifal online tool. The primary research results identify and establish connections between the common components and activities of business continuity management as defined in international standards and frameworks to identify gaps in the existing knowledge. These findings will contribute to the development of a framework that provides a practical approach applicable to organisations of all sizes, taking into account each aspect of business continuity management, with a particular emphasis on information and communication technology systems. This paper’s contribution lies in offering insights from a systematic literature review regarding the strategic principles for designing and implementing a business continuity plan, along with a comprehensive overview of related research. Furthermore, it presents a path forward to guide future research efforts aimed at addressing the gaps in the literature concerning continuity planning. © 2023 by the authors.},
	number = {21},
	journal = {Applied Sciences (Switzerland)},
	author = {Russo, Nelson and Mamede, Henrique São and Reis, Leonilde and Martins, José and Branco, Frederico},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{zhao_systematic_2023-2,
	title = {A {Systematic} {Survey} of {Just}-in-{Time} {Software} {Defect} {Prediction}},
	volume = {55},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147853196&doi=10.1145%2f3567550&partnerID=40&md5=f90ec760e71ec41329ce66005b60aa30},
	doi = {10.1145/3567550},
	abstract = {Recent years have experienced sustained focus in research on software defect prediction that aims to predict the likelihood of software defects. Moreover, with the increased interest in continuous deployment, a variant of software defect prediction called Just-in-Time Software Defect Prediction (JIT-SDP) focuses on predicting whether each incremental software change is defective. JIT-SDP is unique in that it consists of two interconnected data streams, one consisting of the arrivals of software changes stemming from design and implementation, and the other the (defective or clean) labels of software changes resulting from quality assurance processes.We present a systematic survey of 67 JIT-SDP studies with the objective to help researchers advance the state of the art in JIT-SDP and to help practitioners become familiar with recent progress. We summarize best practices in each phase of the JIT-SDP workflow, carry out a meta-analysis of prior studies, and suggest future research directions. Our meta-analysis of JIT-SDP studies indicates, among other findings, that the predictive performance correlates with change defect ratio, suggesting that JIT-SDP is most performant in projects that experience relatively high defect ratios. Future research directions for JIT-SDP include situating each technique into its application domain, reliability-aware JIT-SDP, and user-centered JIT-SDP. © 2023 Association for Computing Machinery.},
	number = {10},
	journal = {ACM Computing Surveys},
	author = {Zhao, Yunhua and Damevski, Kostadin and Chen, Hui},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Machine learning, Machine-learning, Forecasting, Software defect prediction, Computer software selection and evaluation, Change defect density, Change-level software defect prediction, Defect density, Defects density, Just in time production, Just-in-time, Just-in-time software defect prediction, Quality assurance, Release software defect prediction, Searching-based algorithm, Software change, Software change metric},
	annote = {Cited by: 21},
}

@article{khan_analysis_2023-2,
	title = {Analysis of {Cursive} {Text} {Recognition} {Systems}: {A} {Systematic} {Literature} {Review}},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167725839&doi=10.1145%2f3592600&partnerID=40&md5=bd2a595a4a3f8ecc36b5dc5846efdd8a},
	doi = {10.1145/3592600},
	abstract = {Regional and cultural diversities around the world have given birth to a large number of writing systems and scripts, which consist of varying character sets. Developing an optimal character recognition for such a varying and large character set is a challenging task. Unlimited variations in handwritten text due to mood swings, varying writing styles, changes in medium of writing, and many more puzzle the research community. To overcome this problem, researchers have proposed various techniques for the automatic recognition of cursive languages like Urdu, Pashto, and Arabic. With the passage of time, the field of text recognition matured, and the number of publications exponentially increased in the targeted field. It is very difficult to find all the techniques developed, calculate the time and resource consumptions, and understand the cost-benefit tradeoffs among these techniques. These tradeoffs resist making this technology able for practical use. To address these tradeoffs, this article systematic analysis to identify gaps in the literature and suggest new enhanced solution accordingly. A total of 153 of the most relevant articles from 2008 to 2022 are analyzed in this systematic literature review (SLR) work. This systematic review process shows (1) the list of techniques suggested for cursive text recognition purposes and its capabilities, (2) set of feature extraction techniques proposed, and (3) implementation tools used to design and simulate the empirical studies in this specialized field. We have also discussed the emerging trends and described their implications for the research community in this specialized domain. This systematic assessment will ultimately help researchers to perform an overview of the existing character/text recognition approaches, recognition capabilities, and time consumption and subsequently identify the areas that requires a significant attention in the near future. © 2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.},
	number = {7},
	journal = {ACM Transactions on Asian and Low-Resource Language Information Processing},
	author = {Khan, Sulaiman and Nazir, Shah and Khan, Habib Ullah},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Review},
	keywords = {Systematic literature review, Research communities, Cost benefit analysis, Key words, Commerce, Additional key word and phrasescursive language, Character recognition, Character sets, Cultural diversity, Feature technique, Recognition algorithm, Recognition systems, Text recognition, Time consumption},
	annote = {Cited by: 1},
}

@inproceedings{bernardes_understanding_2023-2,
	title = {On the {Understanding} of the {Role} of {Continuous} {Experimentation} in {Technology}-{Based} {Startup}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174536882&doi=10.1145%2f3613372.3613414&partnerID=40&md5=1bce7cfc7bf056d361b28ebbb75f6671},
	doi = {10.1145/3613372.3613414},
	abstract = {Technology startups are constantly emerging, trying to create innovative solutions in environments of uncertainty, and because they face numerous challenges, they have high failure rates. The scarcity of resources and the product's lack of adherence to market needs are among the challenges. In an attempt to alleviate these challenges, initiatives such as Continuous Experimentation arise. This approach supports systematical tests of hypotheses, helping teams prioritize deliveries that increase perceived value by the users. Our interview-based study aimed to identify how Continuous Experimentation is being adopted and how it underlies software engineering activities throughout the product development cycle of technology-based startups. We found that data-driven decisions and reduced development effort are benefits of adopting such an approach while the low competence and education in experimentation is among the main challenges, suggesting that there is room for qualifying professionals in the matter. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Bernardes, Matheus and Marczak, Sabrina},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software engineering, Systematic literature review, Engineering education, Uncertainty, Continuous experimentation, Entrepreneurship, Failure analysis, Failure rate, Innovative solutions, Lean startup, Market needs, Technology start-up, Technology-based},
	pages = {21 -- 30},
	annote = {Cited by: 0},
}

@article{taskeen_research_2023-2,
	title = {A research landscape on software defect prediction},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150786328&doi=10.1002%2fsmr.2549&partnerID=40&md5=d3fda2148e5228abe618078eb6140777},
	doi = {10.1002/smr.2549},
	abstract = {Software defect prediction is the process of identifying defective files and modules that need rigorous testing. In the literature, several secondary studies including systematic reviews, mapping studies, and review studies have been reported. However, no research work such as a tertiary study that combines secondary studies has focused on providing a landscape of software defect prediction useful to understand the body of knowledge. Motivated by this, we intend to perform a tertiary study by following a systematic literature review protocol to provide a research landscape of the targeted domain. We synthesize the quality of the secondary studies and investigate the employed techniques and the performance evaluation measures for evaluating the software defect prediction model. Furthermore, this study aims at exploring different datasets employed in the reported experimentation. Moreover, the current study intends at highlighting the research trends, gaps, and opportunities in the targeted research domain. The results indicate that none of the reported defect prediction techniques can be regarded as the best; however, the reported techniques performed better in different testing situations. In addition, machine learning (ML)-based techniques perform better than traditional statistical techniques mainly due to the potential of discovering the defects and generating generalized results. Moreover, the obtained results highlight the need for further work in the domain of ML-based techniques. Furthermore, publicly available datasets should be considered for experimentation or replication purposes. The potential future work can focus on data quality, ethical ML, cross-project defect prediction, early defect prediction process, class imbalance problem, and model overfitting. © 2023 John Wiley \& Sons Ltd.},
	number = {12},
	journal = {Journal of Software: Evolution and Process},
	author = {Taskeen, Anam and Khan, Saif Ur Rehman and Felix, Ebubeogu Amarachukwu},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Systematic literature review, Systematic Review, Software testing, Mapping, Systematic mapping studies, Tertiary study, Machine-learning, Body of knowledge, Defect prediction, Defects, Forecasting, Mapping studies, Performances evaluation, Software defect prediction},
	annote = {Cited by: 0},
}

@article{rodriguez_ux_2023-2,
	title = {{UX} debt in an agile development process: evidence and characterization},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173738867&doi=10.1007%2fs11219-023-09652-2&partnerID=40&md5=d1b89ddbc5b6fa37cb681b4f6d464e97},
	doi = {10.1007/s11219-023-09652-2},
	abstract = {The metaphor of technical debt (TD) has generated a conceptual framework on factors that weaken the quality of software and accumulate a repair cost. However, user-related aspects like user experience (UX) receive little consideration among TD types, for reasons like the substantial focus on code TD, some dynamics inherent to agile processes, and an apparent lack of cumulative cost over time. This article has two main goals: first, to present evidence of the existence of UXDebt as a type of TD, with a cumulative cost for the development team as well as stakeholders; second, to propose a definition and characterization of UXDebt that may serve as a frame for further research on methods and tools for continuous management within agile processes. For the first goal, we have compiled evidence on the current state of UXDebt from three sources: a literature review, a survey among software engineering professionals in agile teams, and the analysis of UX issues in GitHub. All sources have evidenced some form of UXDebt; surveyed practitioners have recognized its poor management with a cost for the entire team that accumulates over time. Moreover, issue tracking systems allow to visualize and measure a technical form of UXDebt. For the second goal, we have defined a conceptual model that characterizes UXDebt in terms of both technical and non-technical aspects. On the technical side, we propose the notion of UX smells which allows us to discuss concrete management activities. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {4},
	journal = {Software Quality Journal},
	author = {Rodriguez, Andres and Gardey, Juan Cruz and Grigera, Julian and Rossi, Gustavo and Garrido, Alejandra},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Software engineering, Development process, A/B testing, Users' experiences, Agile development, Agile process, Cumulative cost, Human resource management, Odors, Refactorings, Technical debts, User testing, UX smell},
	pages = {1467 -- 1498},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{kabir_meta-synthesis_2023-2,
	title = {A {Meta}-{Synthesis} of the {Barriers} and {Facilitators} for {Personal} {Informatics} {Systems}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173236525&doi=10.1145%2f3610893&partnerID=40&md5=55ea134f22e9de4c81f1c9058f760ae0},
	doi = {10.1145/3610893},
	abstract = {Personal informatics (PI) systems are designed for diverse users in the real world. Even when these systems are usable, people encounter barriers while engaging with them in ways designers cannot anticipate, which impacts the system's effectiveness. Although PI literature extensively reports such barriers, the volume of this information can be overwhelming. Researchers and practitioners often find themselves repeatedly addressing the same challenges since sifting through this enormous volume of knowledge looking for relevant insights is often infeasible. We contribute to alleviating this issue by conducting a meta-synthesis of the PI literature and categorizing people's barriers and facilitators to engagement with PI systems into eight themes. Based on the synthesized knowledge, we discuss specific generalizable barriers and paths for further investigations. This synthesis can serve as an index to identify barriers pertinent to each application domain and possibly to identify barriers from one domain that might apply to a different domain. Finally, to ensure the sustainability of the syntheses, we propose a Design Statements (DS) block for research articles. © 2023 Owner/Author.},
	number = {3},
	journal = {Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies},
	author = {Kabir, Kazi Sinthia and Wiese, Jason},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Different domains, Real-world, Applications domains, Barrier and facilitator, Informatics systems, Meta-synthesis, Personal informatics, Self-tracking, Synthesised, System effectiveness},
	annote = {Cited by: 2; All Open Access, Hybrid Gold Open Access},
}

@article{bertolino_devopret_2023-2,
	title = {{DevOpRET}: {Continuous} reliability testing in {DevOps}},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088248050&doi=10.1002%2fsmr.2298&partnerID=40&md5=7e6aa3c834bb767e1422efdaf28f9791},
	doi = {10.1002/smr.2298},
	abstract = {To enter the production stage, in DevOps practices candidate software releases have to pass quality gates, where they are assessed to meet established target values for key indicators of interest. We believe software reliability should be an important such indicator, as it greatly contributes to the end-user satisfaction. We propose DevOpRET, an approach for reliability testing as part of the acceptance testing stage in DevOps. DevOpRET relies on operational-profile–based testing, a common reliability assessment technique. DevOpRET leverages usage and failure data monitored in operations to continuously refine its estimate. We evaluate accuracy and efficiency of DevOpRET through controlled experiments with a real-world open source platform and with a microservice architectures benchmark. The results show that DevOpRET provides accurate and efficient estimates of the true reliability over subsequent DevOps cycles. © 2020 John Wiley \& Sons, Ltd.},
	number = {3},
	journal = {Journal of Software: Evolution and Process},
	author = {Bertolino, Antonia and Angelis, Guglielmo De and Guerriero, Antonio and Miranda, Breno and Pietrantuono, Roberto and Russo, Stefano},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Software testing, Open source software, Acceptance tests, End-user satisfactions, Key indicator, Operational profile, Production stage, Quality gates, Reliability testing, Software release, Software reliability, Software reliability testing, Software-Reliability, Target values},
	annote = {Cited by: 17},
}

@article{ahmad_requirements_2023-4,
	title = {Requirements engineering framework for human-centered artificial intelligence software systems},
	volume = {143},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163544926&doi=10.1016%2fj.asoc.2023.110455&partnerID=40&md5=de91d4822852a3733b100d5ff3c485e6},
	doi = {10.1016/j.asoc.2023.110455},
	abstract = {Context: Artificial intelligence (AI) components used in building software solutions have substantially increased in recent years. However, many of these solutions focus on technical aspects and ignore critical human-centered aspects. Objective: Including human-centered aspects during requirements engineering (RE) when building AI-based software can help achieve more responsible, unbiased, and inclusive AI-based software solutions. Method: In this paper, we present a new framework developed based on human-centered AI guidelines and a user survey to aid in collecting requirements for human-centered AI-based software. We provide a catalog to elicit these requirements and a conceptual model to present them visually. Results: The framework is applied to a case study to elicit and model requirements for enhancing the quality of 360° videos intended for virtual reality (VR) users. Conclusion: We found that our proposed approach helped the project team fully understand the human-centered needs of the project to deliver. Furthermore, the framework helped to understand what requirements need to be captured at the initial stages against later stages in the engineering process of AI-based software. © 2023 The Author(s)},
	journal = {Applied Soft Computing},
	author = {Ahmad, Khlood and Abdelrazek, Mohamed and Arora, Chetan and Agrahari Baniya, Arbind and Bano, Muneera and Grundy, John},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Article},
	keywords = {Software engineering, Requirement engineering, Requirements engineering, Intelligence software, Software-systems, Machine learning, Machine-learning, Engineering education, E-learning, Conceptual model, Empirical Software Engineering, Engineering frameworks, Human-centered, In-buildings, Software solution, Virtual reality},
	annote = {Cited by: 5; All Open Access, Hybrid Gold Open Access},
}

@article{belle_bolstering_2023-2,
	title = {Bolstering the {Persistence} of {Black} {Students} in {Undergraduate} {Computer} {Science} {Programs}: {A} {Systematic} {Mapping} {Study}},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181846534&doi=10.1145%2f3617896&partnerID=40&md5=1a67a519327ea45a06612612bb99fadb},
	doi = {10.1145/3617896},
	abstract = {Background: People who are racialized, gendered, or otherwise minoritized are underrepresented in computing professions in North America. This is reflected in undergraduate computer science (CS) programs, in which students from marginalized backgrounds continue to experience inequities that do not typically affect White cis-men. This is especially true for Black students in general, and Black women in particular, whose experience of systemic, anti-Black racism compromises their ability to persist and thrive in CS education contexts. Objectives: This systematic mapping study endeavours to (1) determine the quantity of existing non-deficit-based studies concerned with the persistence of Black students in undergraduate CS; (2) summarize the findings and recommendations in those studies; and (3) identify areas in which additional studies may be required. We aim to accomplish these objectives by way of two research questions: (RQ1) What factors are associated with Black students’ persistence in undergraduate CS programs?; and (RQ2) What recommendations have been made to further bolster Black students’ persistence in undergraduate CS education programs? Methods: This systematic mapping study was conducted in accordance with PRISMA 2020 and SEGRESS guidelines. Studies were identified by conducting keyword searches in seven databases. Inclusion and exclusion criteria were designed to capture studies illuminating persistence factors for Black students in undergraduate CS programs. To ensure the completeness of our search results, we engaged in snowballing and an expert-based search to identify additional studies of interest. Finally, data were collected from each study to address the research questions outlined above. Results: Using the methods outlined above, we identified 16 empirical studies, including qualitative, quantitative, and mixed-methods studies informed by a range of theoretical frameworks. Based on data collected from the primary studies in our sample, we identified 13 persistence factors across four categories: (I) social capital, networking, \& support; (II) career \& professional development; (III) pedagogical \& programmatic interventions; and (IV) exposure \& access. This data-collection process also yielded 26 recommendations across six stakeholder groups: (i) researchers; (ii) colleges and universities; (iii) the computing industry; (iv) K-12 systems and schools; (v) governments; and (vi) parents. Conclusion: This systematic mapping study resulted in the identification of numerous persistence factors for Black students in CS. Crucially, however, these persistence factors allow Black students to persist, but not thrive, in CS. Accordingly, we contend that more needs to be done to address the systemic inequities faced by Black people in general, and Black women in particular, in computing programs and professions. As evidenced by the relatively small number of primary studies captured by this systematic mapping study, there exists an urgent need for additional, asset-based empirical studies involving Black students in CS. In addition to foregrounding the intersectional experiences of Black women in CS, future studies should attend to the currently understudied experiences of Black men. © 2023 Copyright held by the owner/author(s).},
	number = {4},
	journal = {ACM Transactions on Computing Education},
	author = {Belle, Alvine B. and Sutherland, Callum and Adesina, Opeyemi O. and Kpodjedo, Sègla and Ojong, Nathanael and Cole, Lisa},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Mapping, Systematic mapping studies, Economic and social effects, Engineering education, Research questions, Students, Empirical studies, Anti-black racism in computer science, Black student in computer science, Computer Science Education, Computer science programs, Education computing, Employment, Equity diversity inclusion, Equity in computer science education, Student persistences},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{beke_what_2023-2,
	title = {What managers can learn from knowledge intensive technology startups? {Exploring} the skillset for developing adaptive organizational learning capabilities of a successful start-up enterprise in management education},
	volume = {45},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163816643&doi=10.1556%2f204.2022.00027&partnerID=40&md5=ce39554a7e9a704186ca5f51032ae19b},
	doi = {10.1556/204.2022.00027},
	abstract = {The study shows what management students could learn from technology startups from an organizational learning (learning organization) perspective; and whether or on what level this entrepreneurial mindset is built into management education. First, the organizational learning patterns and adaptive entrepreneurial skillset of startups are identified, based on a review of the recent literature focusing on knowledge-intensive technology startups’ organizational learning patterns. Then, qualitative interviews and document analysis are applied to find out whether or on what level the improvement of these skills for developing an adaptive and successful startup are present as ‘learning organizations’ are integrated in top Central-European higher management education curricula. Based on the literature review, the theoretical framework is introduced, consisting of five pillars of ‘start-up learning’: ambidextrous entrepreneurial learning, business model development, failure and experiential learning, benchmarking and learning from others, and agile product development. The empirical research looks for these pillars in management MSc programs of a top Central-European business school. The most important findings reveal that the analyzed management education programs strongly prepare students with benchmarking skills. However, the study also showed that the culture and experience of failure and the capability of learning from failure are missing from these education programs. © 2022 The Author(s).},
	number = {1},
	journal = {Society and Economy},
	author = {Beke, Diána Dóra and Sólyom, Andrea and Klér, Andrea Juhászné},
	year = {2023},
	note = {Publisher: Akademiai Kiado ZRt.
Type: Article},
	pages = {68 -- 90},
	annote = {Cited by: 2},
}

@article{dowlut_forecasting_2023-2,
	title = {Forecasting resort hotel tourism demand using deep learning techniques – {A} systematic literature review},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165006073&doi=10.1016%2fj.heliyon.2023.e18385&partnerID=40&md5=cbaba4fc27e8cfc275704c2f67103ae5},
	doi = {10.1016/j.heliyon.2023.e18385},
	abstract = {In the hospitality industry, revenue management is vital for the sustainability of the business. Powering this strategic concept is the occupancy rate (OR) forecast. Predicting occupancy of the hotel is essential for managers’ decision-making process as it gives an estimate of the future business performance. However, the fast-changing marketing demands in the tourism sector, boosted by the advent of online booking, generating accurate forecast figures is nowadays a tough task - needing personnel with advance technical skills and expensive software. The aim of the Systematic Literature review is to provide an insight of the use of Deep Learning techniques for OR prediction. The latest trends in this field over five years (from 2017 to 2022) are highlighted. Through this SRL, three research questions are answered. The questions are related to the variables, deep learning algorithms for prediction and the evaluation metrics used for evaluating the models developed. The Snowballing methodology was used to carry out the SLR. 50 papers were selected for the final analysis. Five categories of variables were identified. LSTM was found to be the most popular deep learning algorithm used to build prediction models. Seven performance metrics were found and among them MAPE was the most popular. To conclude it was found that the hybrid model, CNN-LSTM, to increase accuracy and required more investigation. © 2023 The Authors},
	number = {7},
	journal = {Heliyon},
	author = {Dowlut, Noomesh and Gobin-Rahimbux, Baby},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Article},
	annote = {Cited by: 5; All Open Access, Gold Open Access, Green Open Access},
}

@inproceedings{gerosa_systematic_2023-2,
	title = {A {Systematic} {Literature} {Review} on {Physical} and {Action} {Based} {Activities} in {Computing} {Education} for {Early} {Years} and {Primary}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175996894&doi=10.1145%2f3605468.3605500&partnerID=40&md5=8f2258fc74e302b7466acdc655af879e},
	doi = {10.1145/3605468.3605500},
	abstract = {Educational systems worldwide are including computer science (CS) education in compulsory curriculums from a very young age. Many activities have been proposed to teach young children CS which include different approaches such as unplugged, physical computing, or completely virtual programming interfaces. Despite this, more research is needed to understand which pedagogical approaches capitalise on young children's cognitive and affective capacities throughout their development to promote learning outcomes. Grounded cognition (GC) proposes that our perception and thought are highly influenced by our bodily experiences and that dynamic actions such as movement affect our understanding of the world around us. For young children, experiences integrating cognitive and sensory-motor aspects are often used. These activities could be conceptualised as grounded activities, as they incorporate concrete representation and action. However, the extent to which these activities impact children's learning outcomes has, to our knowledge, not been explored thus far. Moreover, the theoretical background informing these activities and how these map onto the grounded cognition background is often an under-reported aspect in the literature. This study aims to bridge this gap by conducting a systematic literature review. We identified empirical research reporting CS learning activities with a grounded cognition approach and analysed its activities, CS concepts targeted, how their theoretical background informed their pedagogical design and their outcomes. This paper has important implications for computer science education. Firstly, it presents the empirical evidence using this theoretical background with an emphasis on activity design, which will be useful for academics or practitioners looking to incorporate grounded cognition theory into their instruction. Secondly, it identifies significant gaps in the current practices, specifically in the links between theory and practice and thus is a stepping stone for further research in this interdisciplinary area. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Gerosa, Anaclara and Kallia, Maria and Cutts, Quintin},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, Engineering education, Teaching strategy, Computer Science Education, Education computing, Computation theory, Computing, Computing education, Early childhood educations, Educational systems, Learning outcome, Primary, Young children},
	annote = {Cited by: 0},
}

@article{mei_deriving_2023-2,
	title = {Deriving {Thresholds} of {Object}-{Oriented} {Metrics} to {Predict} {Defect}-{Proneness} of {Classes}: {A} {Large}-{Scale} {Meta}-{Analysis}},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153954593&doi=10.1142%2fS0218194023500110&partnerID=40&md5=61e07db7b45db195ab4a1982d2cc5279},
	doi = {10.1142/S0218194023500110},
	abstract = {Many studies have explored the methods of deriving thresholds of object-oriented (i.e. OO) metrics. Unsupervised methods are mainly based on the distributions of metric values, while supervised methods principally rest on the relationships between metric values and defect-proneness of classes. The objective of this study is to empirically examine whether there are effective threshold values of OO metrics by analyzing existing threshold derivation methods with a large-scale meta-analysis. Based on five representative threshold derivation methods (i.e. VARL, ROC, BPP, MFM, and MGM) and 3268 releases from 65 Java projects, we first employ statistical meta-analysis and sensitivity analysis techniques to derive thresholds for 62 OO metrics on the training data. Then, we investigate the predictive performance of five candidate thresholds for each metric on the validation data to explore which of these candidate thresholds can be served as the threshold. Finally, we evaluate their predictive performance on the test data. The experimental results show that 26 of 62 metrics have the threshold effect and the derived thresholds by meta-analysis achieve promising results of GM values and significantly outperform almost all five representative (baseline) thresholds. \#c World Scientific Publishing Company.},
	number = {5},
	journal = {International Journal of Software Engineering and Knowledge Engineering},
	author = {Mei, Yuanqing and Rong, Yi and Liu, Shiran and Guo, Zhaoqiang and Yang, Yibiao and Lu, Hongmin and Tang, Yutian and Zhou, Yuming},
	year = {2023},
	note = {Publisher: World Scientific
Type: Article},
	keywords = {Defects, Class A, Defect proneness, Large-scales, Meta-analysis, Metric values, Object oriented, Object oriented metrics, OO metrics, Predictive performance, Sensitivity analysis, Threshold},
	pages = {651 -- 695},
	annote = {Cited by: 0},
}

@article{barcellos_flatsat_2023-2,
	title = {{FlatSat} {Platforms} for {Small} {Satellites}: {A} {Systematic} {Mapping} and {Classification}},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85177718452&doi=10.1109%2fJMASS.2023.3249044&partnerID=40&md5=21cad600af368cb7aa793fc91cd172cd},
	doi = {10.1109/JMASS.2023.3249044},
	abstract = {Recent trends indicate an increase in the number of small satellite missions, which can be developed more quickly and at a lower cost than traditional satellites. This has led to a growing interest in university-based satellite development, despite a lack of expertise in the space field, which has resulted in a high failure rate for such missions. To address this issue, the implementation of robust and reliable verification and validation (V\&V) methods has become essential, and it has been demonstrated that the use of a FlatSat during the V\&V campaign increases reliability. Despite the significance of FlatSat, there is a dearth of information on the platforms used to implement it, as well as a classification scheme for locating them. This article contributes to bridging this gap by conducting a systematic mapping of 65 works that were selected based on specific criteria and subsequently analyzed. The primary characteristics of the platforms are enumerated, and a new classification for FlatSat platforms into Raw, Bridge, Dock, and Modular is proposed. In order to provide a comprehensive understanding of the topic, the principal tests conducted on these platforms were also covered. © 2019 IEEE.},
	number = {2},
	journal = {IEEE Journal on Miniaturization for Air and Space Systems},
	author = {Barcellos, Joao Claudio Elsen and Spengler, Anderson Wedderhoff and Seman, Laio Oriel and Silva, Raphael Diego Comesanha E and Roldan, Hector Pettenghi and Bezerra, Eduardo Augusto},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Embedded systems, Mapping, Systematic mapping, Classification (of information), Embedded-system, Failure analysis, Cubesats, Flatsats, Low-costs, Recent trends, Small satellite mission, Small satellites, Small-satellite, Verification and validation, Verification-and-validation},
	pages = {186 -- 198},
	annote = {Cited by: 0},
}

@article{boaye_belle_evidence-based_2023-2,
	title = {Evidence-based decision-making: {On} the use of systematicity cases to check the compliance of reviews with reporting guidelines such as {PRISMA} 2020},
	volume = {217},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147257221&doi=10.1016%2fj.eswa.2023.119569&partnerID=40&md5=826373e77ccc2d151c6b2bc7bfbe1bb6},
	doi = {10.1016/j.eswa.2023.119569},
	abstract = {Background and context: Systematic reviews aim to provide high-quality evidence-based syntheses for efficacy under real-world conditions and allow understanding the correlations between exposures and outcomes. They are increasingly popular and have several stakeholders (e.g., healthcare providers, researchers, educators, students, journal editors, policy makers, managers) to whom they help make informed recommendations for practice or policy. Problem: Systematic reviews usually exhibit low methodological and reporting quality. To tackle this, reporting guidelines have been developed to support systematic reviews reporting and assessment. Following such guidelines is crucial to ensure that a review is transparent, complete, trustworthy, reproducible, and unbiased. However, systematic reviewers usually fail to adhere to existing reporting guidelines, which may significantly decrease the quality of the reviews they report and may result in systematic reviews that lack methodological rigor, yield low-credible findings and may mislead decision-makers. Methods: To assure that a review complies with reporting guidelines, we rely on assurance cases that are an emerging way of arguing and relaying various safety–critical systems’ requirements in an extensive manner, as well as checking the compliance of such systems with standards to support their certification. Since the nature of assurance cases makes them applicable to various domains and requirements/properties, we therefore propose a new type of assurance cases called systematicity cases. Systematicity cases focus on the systematicity property and allow arguing that a review is systematic i.e., that it sufficiently complies with the targeted reporting guideline. The most widespread reporting guidelines include PRISMA (Preferred Reporting Items for Systematic reviews and meta-Analyses). We measure the confidence in a systematicity case representing a review as a means to quantify the systematicity of that review i.e., the extent to which that review is systematic. We rely on rule-based Artificial Intelligence to create a knowledge-based system that automatically supports the inference mechanism that a given systematicity case embodies and that allows making a decision regarding the systematicity of a given review. Results: An empirical evaluation performed on 25 reviews (self-identifying as systematic) showed that these reviews exhibit a suboptimal systematicity. More specifically, the systematicity of the analyzed reviews varies between 32.96\% and 66.49\% and its average is 54.42\%. More efforts are therefore needed to report systematic reviews of higher quality. More experiments are also needed to further explore the factors hindering and/or assuring the systematicity of reviews. Audience: The main beneficiaries of our work are journal reviewers, journal editors, managers, policymakers, researchers, organizations developing reporting guidelines, peer reviewers, students, insurers, evidence users, as well as reporting guidelines developers. © 2023 The Author(s)},
	journal = {Expert Systems with Applications},
	author = {Boaye Belle, Alvine and Zhao, Yixi},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Article},
	keywords = {Regulatory compliance, Systematic Review, Decision making, Compliance control, Knowledge-based systems, Meta-analysis, Assurance case, Assurance case (systematicity case), Guideline adherence, Knowledge representation, Knowledge representation and reasoning, Preferred reporting item for systematic review and meta-analyze statement, Reporting guideline adherence, Systematicity},
	annote = {Cited by: 11; All Open Access, Hybrid Gold Open Access},
}

@article{bermejo_arvr_2023-2,
	title = {{AR}/{VR} {Teaching}-{Learning} {Experiences} in {Higher} {Education} {Institutions} ({HEI}): {A} {Systematic} {Literature} {Review}},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163713996&doi=10.3390%2finformatics10020045&partnerID=40&md5=32e5bb794c27f1d7f8eb7f209e9158b4},
	doi = {10.3390/informatics10020045},
	abstract = {During the last few years, learning techniques have changed, both in basic education and in higher education. This change has been accompanied by new technologies such as Augmented Reality (AR) and Virtual Reality (AR). The combination of these technologies in education has allowed a greater immersion, positively affecting the learning and teaching processes. In addition, since the COVID-19 pandemic, this trend has been growing due to the diversity of the different fields of application of these technologies, such as heterogeneity in their combination and their different experiences. It is necessary to review the state of the art to determine the effectiveness of the application of these technologies in the field of university higher education. In the present paper, this aim is achieved by performing a systematic literature review from 2012 to 2022. A total of 129 papers were analyzed. Studies in our review concluded that the application of AR/VR improves learning immersion, especially in hospitality, medicine, and science studies. However, there are also negative effects of using these technologies, such as visual exhaustion and mental fatigue. © 2023 by the authors.},
	number = {2},
	journal = {Informatics},
	author = {Bermejo, Belen and Juiz, Carlos and Cortes, David and Oskam, Jeroen and Moilanen, Teemu and Loijas, Jouko and Govender, Praneschen and Hussey, Jennifer and Schmidt, Alexander Lennart and Burbach, Ralf and King, Daniel and O’Connor, Colin and Dunlea, Davin},
	year = {2023},
	note = {Publisher: MDPI
Type: Review},
	annote = {Cited by: 16; All Open Access, Gold Open Access},
}

@inproceedings{chen_murs_2023-2,
	title = {{MuRS}: {Mutant} {Ranking} and {Suppression} using {Identifier} {Templates}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180550494&doi=10.1145%2f3611643.3613901&partnerID=40&md5=9ee43fb4c7f4d5a78da0b97896dcccb4},
	doi = {10.1145/3611643.3613901},
	abstract = {Diff-based mutation testing is a mutation testing approach that only mutates lines affected by a code change under review. This approach scales independently of the code-base size and introduces test goals (mutants) that are directly relevant to an engineer's goal such as fixing a bug, adding a new feature, or refactoring existing functionality. Google's mutation testing service integrates diff-based mutation testing into the code review process and continuously gathers developer feedback on mutants surfaced during code review. To enhance the developer experience, the mutation testing service uses a number of manually-written rules that suppress not-useful mutants - mutants that have consistently received negative developer feedback. However, while effective, manually implementing suppression rules requires significant engineering time. This paper proposes and evaluates MuRS, an automated approach that groups mutants by patterns in the source code under test and uses these patterns to rank and suppress future mutants based on historical developer feedback on mutants in the same group. To evaluate MuRS, we conducted an A/B testing study, comparing MuRS to the existing mutation testing service. Despite the strong baseline, which uses manually-written suppression rules, the results show a statistically significantly lower negative feedback ratio of 11.45\% for MuRS versus 12.41\% for the baseline. The results also show that MuRS is able to recover existing suppression rules implemented in the baseline. Finally, the results show that statement-deletion mutant groups received both the most positive and negative developer feedback, suggesting a need for additional context that can distinguish between useful and not-useful mutants in these groups. Overall, MuRS is able to recover existing suppression rules and automatically learn additional, finer-grained suppression rules from developer feedback. © 2023 Owner/Author.},
	booktitle = {{ESEC}/{FSE} 2023 - {Proceedings} of the 31st {ACM} {Joint} {Meeting} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Chen, Zimin and Salawa, Małgorzata and Vijayvergiya, Manushree and Petrović, Goran and Ivanković, Marko and Just, René},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software testing, Refactorings, Automated approach, Code changes, Code review, Codes (symbols), Developer feedback, Engineering time, Google+, Mutation testing, Review process, Source codes, Verification},
	pages = {1798 -- 1808},
	annote = {Cited by: 0; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{fernandes_digital_2023-2,
	title = {Digital {Alternative} {Communication} for {Individuals} with {Amyotrophic} {Lateral} {Sclerosis}: {What} {We} {Have}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169140808&doi=10.3390%2fjcm12165235&partnerID=40&md5=e23f9bba2b47e8dc0e6ea6345f35fddd},
	doi = {10.3390/jcm12165235},
	abstract = {Amyotrophic Lateral Sclerosis is a disease that compromises the motor system and the functional abilities of the person in an irreversible way, causing the progressive loss of the ability to communicate. Tools based on Augmentative and Alternative Communication are essential for promoting autonomy and improving communication, life quality, and survival. This Systematic Literature Review aimed to provide evidence on eye-image-based Human–Computer Interaction approaches for the Augmentative and Alternative Communication of people with Amyotrophic Lateral Sclerosis. The Systematic Literature Review was conducted and guided following a protocol consisting of search questions, inclusion and exclusion criteria, and quality assessment, to select primary studies published between 2010 and 2021 in six repositories: Science Direct, Web of Science, Springer, IEEE Xplore, ACM Digital Library, and PubMed. After the screening, 25 primary studies were evaluated. These studies showcased four low-cost, non-invasive Human–Computer Interaction strategies employed for Augmentative and Alternative Communication in people with Amyotrophic Lateral Sclerosis. The strategies included Eye-Gaze, which featured in 36\% of the studies; Eye-Blink and Eye-Tracking, each accounting for 28\% of the approaches; and the Hybrid strategy, employed in 8\% of the studies. For these approaches, several computational techniques were identified. For a better understanding, a workflow containing the development phases and the respective methods used by each strategy was generated. The results indicate the possibility and feasibility of developing Human–Computer Interaction resources based on eye images for Augmentative and Alternative Communication in a control group. The absence of experimental testing in people with Amyotrophic Lateral Sclerosis reiterates the challenges related to the scalability, efficiency, and usability of these technologies for people with the disease. Although challenges still exist, the findings represent important advances in the fields of health sciences and technology, promoting a promising future with possibilities for better life quality. © 2023 by the authors.},
	number = {16},
	journal = {Journal of Clinical Medicine},
	author = {Fernandes, Felipe and Barbalho, Ingridy and Bispo Júnior, Arnaldo and Alves, Luca and Nagem, Danilo and Lins, Hertz and Arrais Júnior, Ernano and Coutinho, Karilany D. and Morais, Antônio H. F. and Santos, João Paulo Q. and Machado, Guilherme Medeiros and Henriques, Jorge and Teixeira, César and Dourado Júnior, Mário E. T. and Lindquist, Ana R. R. and Valentim, Ricardo A. M.},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	keywords = {human, systematic review, amyotrophic lateral sclerosis, augmentative and alternative communication, clinical article, eye tracking, eyelid reflex, human computer interaction, Review},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{guimaraes_responsible_2023-2,
	title = {Responsible innovation assessment tools: a systematic review and research agenda},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171182021&doi=10.1108%2fTECHS-11-2022-0043&partnerID=40&md5=85afe10c7e7cdc6cf361c0cf9ba06032},
	doi = {10.1108/TECHS-11-2022-0043},
	abstract = {Purpose: Responsible innovation assessment tools (RIATs) are key instruments that can help organizations, associations and individuals measure responsible innovation. Accordingly, this study aims to review the current status of research on responsible innovation and, in particular, of studies that either present the relevance of RIATs or provide empirical evidence of their adoption. Design/methodology/approach: A systematic literature review is conducted to identify and review how RIATs are being addressed in academic research and the applications that are proposed. A systematic process is implemented using the Web of Science and Scopus bibliographic databases, aiming not only to summarize existing studies, but also to include a perspective on gaps and future research. Findings: A total of 119 publications were identified and included in the review process. The study identifies that RIATs have attracted growing interest from the scientific community, with a greater predominance of studies involving qualitative and mixed methods. A well-balanced mix of conceptual and exploratory studies is also registered, with a greater predominance of analysis of RIATs application domains in the past years, with greater incidence in the finance, water, energy, construction, manufacturing and health sectors. Originality/value: This study is pioneering in identifying 16 dimensions and 60 sub-dimensions for measuring responsible innovation. It also suggests the need to include multidimensional perspectives and individuals with interdisciplinary competencies in this process. © 2022, Emerald Publishing Limited.},
	number = {2},
	journal = {Technological Sustainability},
	author = {Guimarães, Cristina and Amorim, Vasco and Almeida, Fernando},
	year = {2023},
	note = {Publisher: Emerald Publishing
Type: Review},
	pages = {206 -- 223},
	annote = {Cited by: 3},
}

@article{daun_context_2023-2,
	title = {Context modeling for cyber-physical systems},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128518651&doi=10.1002%2fsmr.2451&partnerID=40&md5=ae0f37165027c923e06274ec9f617fd9},
	doi = {10.1002/smr.2451},
	abstract = {When developing cyber-physical systems (CPS), the context is of vital importance. CPS interact with the world not only through sensing the environment and acting upon it (like embedded systems) but also by communicating with other CPS (like systems in the Internet of Things [IoT]). This means that the context interactions CPS must deal with are much greater than regular embedded or IoT systems: On the one hand, external systems and human users constrain the specific interaction among them. On the other hand, properties of these external systems, human users, and laws, regulations, or standards constrain the way the CPS must be developed. In this paper, we propose a comprehensive, ontologically grounded context modeling framework to systematically explore the problem space in which a CPS under development will operate. This allows for the systematic elicitation of requirements for the CPS, early validation and verification of its properties, and safety assessment of its context interactions at runtime. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	number = {7},
	journal = {Journal of Software: Evolution and Process},
	author = {Daun, Marian and Tenbergen, Bastian},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Embedded systems, Requirement engineering, Requirements engineering, Verification, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Internet of things, Collaborative system network, Collaborative systems, Context, Context analyse, Context analysis, context modeling, Context models, dynamic context, Dynamic contexts, Laws and legislation, Model dynamics, Model-based engineering, Systems networks, Validation},
	annote = {Cited by: 1; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{saleemi_ubiquitous_2023-2,
	title = {Ubiquitous healthcare: a systematic mapping study},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091477767&doi=10.1007%2fs12652-020-02513-x&partnerID=40&md5=3506b055c1c76d4dcdb81cf6a121df29},
	doi = {10.1007/s12652-020-02513-x},
	abstract = {Ubiquitous healthcare is an emerging area that employs ubiquitous technologies to enable technology oriented environment for healthcare professionals for provision of efficient and effective healthcare services. In past years, research community has proposed various technological solutions in different healthcare areas such as chronic disease monitoring, gait analysis, mood and fall detection, neuropathic monitoring, physiological and vital signs monitoring, pulmonogical monitoring, etc. However, in-depth analysis of these proposed solutions is required to analyze the form of proposed ubiquitous healthcare solutions; the extent ubiquitous technologies are integrated in these solutions; the type of real problem addressed; and how far these solutions are evaluated in real world settings? The addressal of these questions is critical to understand and evaluate the progress made in the area of ubiquitous healthcare and identify the challenges that are hindering the progress in this area. Therefore, in this research, a systematic research technique in the form of mapping study (also known as scoping study) is employed for in-depth analysis of evidences available on ubiquitous healthcare. The mapping study adopts a systematic approach to construct chain of evidences related to a particular topic and is a well-defined research technique in evidence based software engineering. This study identified a total of 103 primary studies, published between 2007 and 2018, for analysis of area under investigation. The study findings reveal that research trend in ubiquitous healthcare is horizontally spread by involving broad range of healthcare areas. The proposed solutions largely fall under the category of validation studies where experiments are conducted in laboratory settings rather real world environment. Another interesting finding is the lack of involvement of relevant healthcare community in proposed solution design. The challenges such as context awareness, data ownership, privacy and security, usability and trust are limiting the adoption of proposed solutions. Therefore, more extensive studies are required to first evaluate the applicability of proposed solutions in their respective environment, second, engagement and ownership of relevant community in solution design need to be considered. Third, the broad coverage of healthcare areas does not provide significant clusters of similar research in any particular area therefore future research should focus on strengthening these areas by conducting evaluation based longitudinal studies. In this way, the effects of proposed solutions can only be measured objectively and can be added to the body of knowledge. Finally, this research provides a thorough insight into the research on ubiquitous healthcare and offers an opportunity to conduct further research in this area. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.},
	number = {5},
	journal = {Journal of Ambient Intelligence and Humanized Computing},
	author = {Saleemi, Maria and Anjum, Maria and Rehman, Mariam},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Software engineering, Systematic literature review, Mapping, Mapping studies, Ambient assisted living, Assisted living, Bodyarea networks (BAN), Digital health, E-services, Ehealth, Emerging healthcare technology, Fall detection, Healthcare technology, Medical computing, mHealth, Patient monitoring, Pattern recognition, Telecare, Telehealth, Telemedicine, Telemedicine and wellness, Ubiquitous health care},
	pages = {5021 -- 5046},
	annote = {Cited by: 10},
}

@article{alfayez_what_2023-2,
	title = {What is asked about technical debt ({TD}) on {Stack} {Exchange} question-and-answer ({Q}\&{A}) websites? {An} observational study},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146921032&doi=10.1007%2fs10664-022-10269-5&partnerID=40&md5=148ce9655aa6c1affadbd9c4f0406f3b},
	doi = {10.1007/s10664-022-10269-5},
	abstract = {Technical debt (TD) is a term coined by agile software pioneer Ward Cunningham to account for the added software system effort or cost resulting from taking early software project shortcuts. Previous research on TD has extensively outlined and discussed the various consequences derived from accumulating TD and the difficulty in managing it. A review of the software engineering literature revealed that Stack Exchange question-and-answer (Q\&A) websites can provide valuable, real world perspectives on a number of software engineering topics. Therefore, this study aims to observe how the TD term is utilized on Stack Exchange Q\&A websites. Specifically, this study utilizes a dataset derived from three Stack Exchange Q\&A websites, which are Stack Overflow (SO), Software Engineering (SE), and Project Management (PM), to retrieve and analyze 578 TD-related questions. The results unveiled that TD-related questions can be categorized into 14 different categories, a total of 636 unique tags are utilized in the acquired set of TD-related questions, and a few TD-related categories both lack accepted answers and have a longer median time to receive an accepted answer than other categories. This study’s findings highlight the TD-related challenges that are addressed by Stack Exchange Q\&A website users, which may prove beneficial in steering future TD-related efforts. © 2023, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.},
	number = {2},
	journal = {Empirical Software Engineering},
	author = {Alfayez, Reem and Ding, Yunyan and Winn, Robert and Alfayez, Ghaida and Harman, Christopher and Boehm, Barry},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Computer software, Software-systems, Technical debts, Project management, Technical debt management, Software project, Real-world, Agile softwares, Observational study, Question-and-answer website, Stackoverflow, Websites},
	annote = {Cited by: 1},
}

@inproceedings{koana_ownership_2023-2,
	title = {Ownership in the {Hands} of {Accountability} at {Brightsquid}: {A} {Case} {Study} and a {Developer} {Survey}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180546160&doi=10.1145%2f3611643.3613890&partnerID=40&md5=ee4f1bde53f308b0d3a0a4497d826c01},
	doi = {10.1145/3611643.3613890},
	abstract = {The COVID-19 pandemic has accelerated the adoption of digital health solutions. This has presented significant challenges for software development teams to swiftly adjust to the market needs and demand. To address these challenges, product management teams have had to adapt their approach to software development, reshaping their processes to meet the demands of the pandemic. Brighsquid implemented a new task assignment process aimed at enhancing developer accountability toward the customer. To assess the impact of this change on code ownership, we conducted a code change analysis. Additionally, we surveyed 67 developers to investigate the relationship between accountability and ownership more broadly. The findings of our case study indicate that the revised assignment model not only increased the perceived sense of accountability within the production team but also improved code resilience against ownership changes. Moreover, the survey results revealed that a majority of the participating developers (67.5\%) associated perceived accountability with artifact ownership. © 2023 ACM.},
	booktitle = {{ESEC}/{FSE} 2023 - {Proceedings} of the 31st {ACM} {Joint} {Meeting} {European} {Software} {Engineering} {Conference} and {Symposium} on the {Foundations} of {Software} {Engineering}},
	publisher = {Association for Computing Machinery, Inc},
	author = {Koana, Umme Ayman and Chew, Francis and Carlson, Chris and Nayebi, Maleknaz},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Product management, Software design, COVID-19, Human resource management, Case-studies, Software development teams, Computer software selection and evaluation, Software Quality, Market needs, Accountability, Management team, Market demand, Ownership, Tasks assignments},
	pages = {2008 -- 2019},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{thajeel_machine_2023-2,
	title = {Machine and {Deep} {Learning}-based {XSS} {Detection} {Approaches}: {A} {Systematic} {Literature} {Review}},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163933483&doi=10.1016%2fj.jksuci.2023.101628&partnerID=40&md5=7e7780a4abc9d6b6e38e915244463792},
	doi = {10.1016/j.jksuci.2023.101628},
	abstract = {Web applications are paramount tools for facilitating services providing in the modern world. Unfortunately, the tremendous growth in the web application usage has resulted in a rise in cyberattacks. Cross-site scripting (XSS) is one of the most frequent cyber security attack vectors that threaten the end user as well as the service provider with the same degree of severity. Recently, an obvious increase of the Machine learning and deep learning ML/DL techniques adoption in XSS attack detection. The goal of this review is to come with a special attention and highlight of Machine learning and deep learning approaches. Thus, in this paper, we present a review of recent advances applied in ML/DL for XSS attack detection and classification. The existing proposed ML/DL approaches for XSS attack detection are analyzed and taxonomized comprehensively in terms of domain areas, data preprocessing, feature extraction, feature selection, dimensionality reduction, Data imbalance, performance metrics, datasets, and data types. Our analysis reveals that the way of how the XSS data is preprocessed considerably impacts the performance and the attack detection models. Proposing a full preprocessing cycle reveals how various ML/DL approaches for XSS attacks detection take advantage of different input data preprocessing techniques. The most used ML/DL and preprocessing stages have also been identified. The limitations of existing ML/DL-based XSS attack detection mechanisms are highlighted to identify the potential gaps and future trends. © 2023 The Author(s)},
	number = {7},
	journal = {Journal of King Saud University - Computer and Information Sciences},
	author = {Thajeel, Isam Kareem and Samsudin, Khairulmizam and Hashim, Shaiful Jahari and Hashim, Fazirulhisyam},
	year = {2023},
	note = {Publisher: King Saud bin Abdulaziz University
Type: Review},
	annote = {Cited by: 4; All Open Access, Gold Open Access},
}

@article{ferreira_towards_2023-2,
	title = {Towards an understanding of reliability of software-intensive systems-of-systems},
	volume = {158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149172749&doi=10.1016%2fj.infsof.2023.107186&partnerID=40&md5=0d674782eca671dd406f7e52554660b5},
	doi = {10.1016/j.infsof.2023.107186},
	abstract = {Context: Large-scale software-intensive Systems-of-Systems (SoS) have become present in several critical domains and have sometimes depended on diverse trending technologies, such as cloud computing and machine learning. At the same time, the SoS dynamic architecture makes it difficult to assure SoS reliability leading to diverse studies with specific solutions, while the need for a shared view of what precisely SoS reliability refers to still exists. Objective: The main contribution of this article is to go towards an understanding of SoS reliability. We present a conceptual model whose concepts as well as their definitions and relationships were defined by systematically examining the literature of the field. Methods: We surveyed 36 practitioners and researchers regarding ambiguity, explanatory power, parsimony, generality, and utility of our model. Next, we adjusted our model according to their contribution. Results: We reach a conceptual model containing 29 concepts and their relationships that help to comprehend SoS reliability. In addition, we provided a glossary with a definition of each concept of our conceptual model. We also proposed a SoS reliability definition grounded on the literature. Conclusions: By organizing the knowledge of SoS reliability, this conceptual model makes it possible to expand the body of knowledge in the area and opens several opportunities for further investigations; in particular, this model serves as a basis for novel solutions aiming to assure SoS reliability. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Ferreira, Francisco Henrique Cerdeira and Nakagawa, Elisa Yumi and Santos, Rodrigo Pereira dos},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Machine-learning, Empirical studies, Software reliability, Conceptual model, Large-scales, Cloud-computing, Critical domain, Distributed computer systems, Software intensive systems, System of systems, System reliability, System-of-systems},
	annote = {Cited by: 5},
}

@article{ataei_application_2023-2,
	title = {Application of microservices patterns to big data systems},
	volume = {10},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85158161899&doi=10.1186%2fs40537-023-00733-4&partnerID=40&md5=b9d883d26f0ffb65d220eb10e5ae0f8d},
	doi = {10.1186/s40537-023-00733-4},
	abstract = {The panorama of data is ever evolving, and big data has emerged to become one of the most hyped terms in the industry. Today, users are the perpetual producers of data that if gleaned and crunched, have the potential to reveal game-changing patterns. This has introduced an important shift regarding the role of data in organizations and many strive to harness to power of this new material. Howbeit, institutionalizing data is not an easy task and requires the absorption of a great deal of complexity. According to the literature, it is estimated that only 13\% of organizations succeeded in delivering on their data strategy. Among the root challenges, big data system development and data architecture are prominent. To this end, this study aims to facilitate data architecture and big data system development by applying well-established patterns of microservices architecture to big data systems. This objective is achieved by two systematic literature reviews, and infusion of results through thematic synthesis. The result of this work is a series of theories that explicates how microservices patterns could be useful for big data systems. These theories are then validated through expert opinion gathering with 7 experts from the industry. The findings emerged from this study indicates that big data architectures can benefit from many principles and patterns of microservices architecture. © 2023, The Author(s).},
	number = {1},
	journal = {Journal of Big Data},
	author = {Ataei, Pouya and Staegemann, Daniel},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Systematic literature review, Microservice, Architecture, Big data, Big data architecture, Computer architecture, Data architectures, Data engineering, Data systems, Microservice pattern, Power, System development},
	annote = {Cited by: 4; All Open Access, Gold Open Access},
}

@article{khan_sql_2023-2,
	title = {{SQL} and {NoSQL} {Database} {Software} {Architecture} {Performance} {Analysis} and {Assessments}—{A} {Systematic} {Literature} {Review}},
	volume = {7},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163681824&doi=10.3390%2fbdcc7020097&partnerID=40&md5=ca9df47cac308d21d9cec88cac5c7444},
	doi = {10.3390/bdcc7020097},
	abstract = {The competent software architecture plays a crucial role in the difficult task of big data processing for SQL and NoSQL databases. SQL databases were created to organize data and allow for horizontal expansion. NoSQL databases, on the other hand, support horizontal scalability and can efficiently process large amounts of unstructured data. Organizational needs determine which paradigm is appropriate, yet selecting the best option is not always easy. Differences in database design are what set SQL and NoSQL databases apart. Each NoSQL database type also consistently employs a mixed-model approach. Therefore, it is challenging for cloud users to transfer their data among different cloud storage services (CSPs). There are several different paradigms being monitored by the various cloud platforms (IaaS, PaaS, SaaS, and DBaaS). The purpose of this SLR is to examine the articles that address cloud data portability and interoperability, as well as the software architectures of SQL and NoSQL databases. Numerous studies comparing the capabilities of SQL and NoSQL of databases, particularly Oracle RDBMS and NoSQL Document Database (MongoDB), in terms of scale, performance, availability, consistency, and sharding, were presented as part of the state of the art. Research indicates that NoSQL databases, with their specifically tailored structures, may be the best option for big data analytics, while SQL databases are best suited for online transaction processing (OLTP) purposes. © 2023 by the authors.},
	number = {2},
	journal = {Big Data and Cognitive Computing},
	author = {Khan, Wisal and Kumar, Teerath and Zhang, Cheng and Raj, Kislay and Roy, Arunabha M. and Luo, Bin},
	year = {2023},
	note = {Publisher: MDPI
Type: Review},
	keywords = {Systematic literature review, Data handling, Digital storage, Software architecture, BASE, Cloud analytics, Data Analytics, Database software, Database systems, DBaaS, Horizontal expansion, Map-reduce, MapReduce, Performance assessment, Performances analysis, SQL and NoSQL database, SQL database},
	annote = {Cited by: 18; All Open Access, Gold Open Access},
}

@article{bovo_digital_2023-2,
	title = {Digital twins for the rapid startup of manufacturing processes: a case study in {PVC} tube extrusion},
	volume = {127},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164837924&doi=10.1007%2fs00170-023-11906-z&partnerID=40&md5=20ab08169248f2a738ac6318cfd26345},
	doi = {10.1007/s00170-023-11906-z},
	abstract = {In this work, a soft sensor–based digital twin (DT) was developed to reduce the startup time in manufacturing plastic tubes and enable real-time product quality monitoring, i.e., the weight per unit length and the inner and outer diameters of the tube. An experimental campaign was conducted on a real tube extrusion line using three polyvinyl chloride (PVC) compounds and different process conditions, and machine learning regression algorithms were trained and tested to create the models of the extruder and the extrusion die the DT is based on. The characterization of the considered material, whose properties were given as input to the digital models, was carried out according to a procedure based only on the data collected by the production line. The DT was tested for the startup of the production of a single-layer tube and allowed to achieve the specified customer requirements (thickness and weight) in a few minutes. The proposed solution thus proved to be a valuable tool for reducing the setup time, thus increasing the efficiency of the process. © 2023, The Author(s).},
	number = {11-12},
	journal = {International Journal of Advanced Manufacturing Technology},
	author = {Bovo, Enrico and Sorgato, Marco and Lucchetta, Giovanni},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Machine learning, Case-studies, Manufacturing process, Chlorine compounds, Data-driven model, Digital manufacturing, Extrusion, Plastic tube, Polyvinyl chlorides, Rapid startups, Real- time, Soft sensors, Startup time, Tube extrusions, Tubes (components)},
	pages = {5517 -- 5529},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{machado_literature_2023-2,
	title = {Literature review of digital twin in healthcare},
	volume = {9},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170409582&doi=10.1016%2fj.heliyon.2023.e19390&partnerID=40&md5=3b0a9a7a5fd6da02279c410ec7d5ed26},
	doi = {10.1016/j.heliyon.2023.e19390},
	abstract = {This article aims to make a bibliometric literature review using systematic scientific mapping and content analysis of digital twins in healthcare to know the evolution, domain, keywords, content type, and kind and purpose of digital twin's implementation in healthcare, so a consolidation and future improvement of existing knowledge can be made and gaps for new studies can be identified. The increase in publications of digital twins in healthcare is quite recent and it is still concentrated in the domain of technology sources. The subject is majorly concentrated in patient's digital twin group and in precision medicine and aspects, issues and/or policies subgroups, although the publications keywords mirror it only at the group side. Digital twins in healthcare are probably stepping out of the infancy phase. On the other hand, digital twins in hospital group and the device and facilities management subgroups are more mature with all knowledge gathered from the manufacturing sector. There is an absence of some publication's types in general, device and care subgroup and no whole body or hospital digital twin was reported. Based on the presented arguments, guidelines for future research were presented: advance in the creation of general frameworks, in subgroups not as much explored, and in groups and subgroups already explored, but that need more advancement to achieve the main goals of a whole human or hospital digital twin with the main issues resolved. © 2023 The Authors},
	number = {9},
	journal = {Heliyon},
	author = {Machado, Tatiana Mallet and Berssaneti, Fernando Tobal},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Article},
	annote = {Cited by: 6; All Open Access, Gold Open Access, Green Open Access},
}

@article{oliveira_systematic_2023-2,
	title = {A systematic literature review on the impact of formatting elements on code legibility},
	volume = {203},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163388981&doi=10.1016%2fj.jss.2023.111728&partnerID=40&md5=1ac99e82c24196970128ea680e222e6b},
	doi = {10.1016/j.jss.2023.111728},
	abstract = {Context: Software programs can be written in different but functionally equivalent ways. Even though previous research has compared specific formatting elements to find out which alternatives affect code legibility, seeing the bigger picture of what makes code more or less legible is challenging. Goal: We aim to find which formatting elements have been investigated in empirical studies and which alternatives were found to be more legible for human subjects. Method: We conducted a systematic literature review and identified 15 papers containing human-centric studies that directly compared alternative formatting elements. We analyzed and organized these formatting elements using a card-sorting method. Results: We identified 13 formatting elements (e.g., indentation) and 33 levels of formatting elements (e.g., two-space indentation), which are about formatting styles, spacing, block delimiters, long or complex code lines, and word boundary styles. While some levels were found to be statistically better than other equivalent ones in terms of code legibility, e.g., appropriate use of indentation with blocks, others were not, e.g., formatting layout. For identifier style, we found divergent results, where one study found a significant difference in favor of camel case, while another study found a positive result in favor of snake case. Conclusion: The number of identified papers, some of which are outdated, and the many null and contradictory results emphasize the relative lack of work in this area and underline the importance of more research. There is much to be understood about how formatting elements influence code legibility before the creation of guidelines and automated aids to help developers make their code more legible. © 2023},
	journal = {Journal of Systems and Software},
	author = {Oliveira, Delano and Santos, Reydne and Madeiral, Fernanda and Masuhara, Hidehiko and Castor, Fernando},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Systematic literature review, Codes (symbols), Software project, Empirical studies, Card-sorting, Code legibility, Formatting element, Human subjects, Human-centric, Indentation, Program understandability, Understandability},
	annote = {Cited by: 2; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{broekhuizen_ai_2023-2,
	title = {{AI} for managing open innovation: {Opportunities}, challenges, and a research agenda},
	volume = {167},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166189921&doi=10.1016%2fj.jbusres.2023.114196&partnerID=40&md5=5f455a87a517d661bcaaa134c340eeaa},
	doi = {10.1016/j.jbusres.2023.114196},
	abstract = {Artificial intelligence (AI) provides ample opportunities for enabling effective knowledge sharing among organizations seeking to foster open innovation. Past research often investigates the capability of AI to perform ‘human’ tasks in structured application fields. Yet, there is a lack of research that systematically analyzes when and how AI can be used for the more complex and unstructured tasks of open innovation (OI). We present a framework for leveraging AI-enabled applications to foster productive OI collaborations. Specifically, we create a 3x3 matrix by aligning the three OI stages (initiation, development, realization) with the three management functions of AI (mapping, coordinating, controlling). This matrix assists in identifying how various AI applications may augment or automate human intelligence, thereby helping to resolve prevailing OI challenges. It provides guidance on how organizations can use AI to establish, execute and govern exchanges across the OI stages. Finally, we lay out an agenda for future research. © 2023 The Authors},
	journal = {Journal of Business Research},
	author = {Broekhuizen, Thijs and Dekker, Henri and de Faria, Pedro and Firk, Sebastian and Nguyen, Dinh Khoi and Sofka, Wolfgang},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	annote = {Cited by: 8; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{imamura_towards_2023-2,
	title = {Towards a {Catalog} of {Heuristics} for the {Design} of {Systems}-of-{Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165978127&doi=10.1145%2f3592813.3592897&partnerID=40&md5=1c68db0b4c5b8832e2973121156381a6},
	doi = {10.1145/3592813.3592897},
	abstract = {Context: Systems-of-Systems (SoS) are arrangements of independent systems that are increasingly present in everyday life and can be observed in domains such as healthcare, transport, and Industry 4.0, to mention a few. Problem: A significant concern regarding SoS refers to the constituent systems’ (CS) independence. CS are managed by different organizations that control them independently of SoS. Hence, the design of SoS is challenging as it involves careful investigation, allocation, and integration of CS to ensure proper operation. Solution: This paper provides a catalog of good practices and recommendations, herein referred to as “heuristics”, which can be applied to the SoS design. The main purpose of the catalog is to provide directions on what practitioners should consider during the design phase to ensure the proper operation of the SoS. IS theory: This research is based on the General Systems Theory that allows understanding SoS as a complex system constructed with independent systems. Method: We conducted a systematic mapping study (SMS) to identify which heuristics have been applied to SoS design. The results were discussed in a focus group with professionals to organize the heuristics. Summary of Results: After reaching a consensus on the focus group, we organized a catalog of fifteen heuristics into five categories: initiation, CS, interoperability, emergent behavior, and monitoring. Contributions and Impact in the IS area: The heuristics catalog, which is grounded in the literature, would support researchers and professionals in identifying critical issues during the SoS design phase. © 2023 Copyright held by the owner/author(s).},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Imamura, Marcio and Ferreira, Francisco and Fernandes, Juliana and Neto, Valdemar Vicente Graciano and dos Santos, Rodrigo Pereira},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic mapping studies, Focus groups, System of systems, System-of-systems, Design phase, Electric utilities, Emergent behaviours, General systems theory, Good practices, Heuristic methods, Independent systems, Systems interoperability, Systems-of-systems design},
	pages = {128 -- 135},
	annote = {Cited by: 0},
}

@article{abdulazeem_human_2023-2,
	title = {Human {Factors} {Considerations} for {Quantifiable} {Human} {States} in {Physical} {Human}–{Robot} {Interaction}: {A} {Literature} {Review}},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85170345069&doi=10.3390%2fs23177381&partnerID=40&md5=25dd2b033ee1d3ee530c5a69317e90de},
	doi = {10.3390/s23177381},
	abstract = {As the global population rapidly ages with longer life expectancy and declining birth rates, the need for healthcare services and caregivers for older adults is increasing. Current research envisions addressing this shortage by introducing domestic service robots to assist with daily activities. The successful integration of robots as domestic service providers in our lives requires them to possess efficient manipulation capabilities, provide effective physical assistance, and have adaptive control frameworks that enable them to develop social understanding during human–robot interaction. In this context, human factors, especially quantifiable ones, represent a necessary component. The objective of this paper is to conduct an unbiased review encompassing the studies on human factors studied in research involving physical interactions and strong manipulation capabilities. We identified the prevalent human factors in physical human–robot interaction (pHRI), noted the factors typically addressed together, and determined the frequently utilized assessment approaches. Additionally, we gathered and categorized proposed quantification approaches based on the measurable data for each human factor. We also formed a map of the common contexts and applications addressed in pHRI for a comprehensive understanding and easier navigation of the field. We found out that most of the studies in direct pHRI (when there is direct physical contact) focus on social behaviors with belief being the most commonly addressed human factor type. Task collaboration is moderately investigated, while physical assistance is rarely studied. In contrast, indirect pHRI studies (when the physical contact is mediated via a third item) often involve industrial settings, with physical ergonomics being the most frequently investigated human factor. More research is needed on the human factors in direct and indirect physical assistance applications, including studies that combine physical social behaviors with physical assistance tasks. We also found that while the predominant approach in most studies involves the use of questionnaires as the main method of quantification, there is a recent trend that seeks to address the quantification approaches based on measurable data. © 2023 by the authors.},
	number = {17},
	journal = {Sensors},
	author = {Abdulazeem, Nourhan and Hu, Yue},
	year = {2023},
	pmid = {37687837},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Review},
	keywords = {human, Humans, Literature reviews, aged, Aged, Birth rates, Domestic services, ergonomics, Ergonomics, Global population, Healthcare services, Human robot interaction, industry, Industry, Life expectancies, Long life, Man machine systems, Manipulators, Physical humanrobot interaction (phri), Population statistics, Robot applications, robotics, Robotics, Robots manipulators, social behavior, Social behavior, Social Behavior, Social behaviour},
	annote = {Cited by: 3; All Open Access, Gold Open Access, Green Open Access},
}

@article{ferreira_lessons_2023-2,
	title = {Lessons learned to improve the {UX} practices in agile projects involving data science and process automation},
	volume = {155},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145568139&doi=10.1016%2fj.infsof.2022.107106&partnerID=40&md5=e1f34b13a26ed1d8c5db7eb024963f24},
	doi = {10.1016/j.infsof.2022.107106},
	abstract = {Context: User-Centered Design (UCD) and Agile methodologies focus on human issues. Nevertheless, agile methodologies focus on contact with contracting customers and generating value for them. Usually, the communication between end users (they use the software and have low decision power) and the agile team is mediated by customers (they have high decision power but do not use the software). However, they do not know the actual problems that end users (may) face in their routine, and they may not be directly affected by software shortcomings. In this context, UX issues are typically identified only after the implementation, during user testing and validation. Objective: Aiming to improve the understanding and definition of the problem in agile projects, this research investigates the practices and difficulties experienced by agile teams during the development of data science and process automation projects. Also, we analyze the benefits and the teams’ perceptions regarding user participation in these projects. Method: We collected data from four agile teams, in the context of an academia and industry collaboration focusing on delivering data science and process automation solutions. Therefore, we applied a carefully designed questionnaire answered by developers, scrum masters, and UX designers. In total, 18 subjects answered the questionnaire. Results: From the results, we identify practices used by the teams to define and understand the problem and to represent the solution. The practices most often used are prototypes and meetings with stakeholders. Another practice that helped the team to understand the problem was using Lean Inception (LI) ideation workshops. Also, our results present some specific issues regarding data science projects. Conclusion: We observed that end-user participation can be critical to understanding and defining the problem. They help to define elements of the domain and barriers in the implementation. We identified a need for approaches that facilitate user-team communication in data science projects to understand the data and its value to the users’ routine. We also identified insights about the need of more detailed requirements representations to support the development of data science solutions. © 2022 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Ferreira, Bruna and Marques, Silvio and Kalinowski, Marcos and Lopes, Hélio and Barbosa, Simone D.J.},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {End-users, Automation, Users' experiences, Agile, Agile Methodologies, Agile teams, Data Science, Decision power, Lean inception, Process automation, Process control, User centered design, User involvement, User participation},
	annote = {Cited by: 4; All Open Access, Green Open Access},
}

@article{kitchenham_segress_2023-2,
	title = {{SEGRESS}: {Software} {Engineering} {Guidelines} for {REporting} {Secondary} {Studies}},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85132534569&doi=10.1109%2fTSE.2022.3174092&partnerID=40&md5=a3ded056d25379cd7fc5bad8fa40273c},
	doi = {10.1109/TSE.2022.3174092},
	abstract = {Context: Several tertiary studies have criticized the reporting of software engineering secondary studies. Objective: Our objective is to identify guidelines for reporting software engineering (SE) secondary studies which would address problems observed in the reporting of software engineering systematic reviews (SRs). Method: We review the criticisms of SE secondary studies and identify the major areas of concern. We assess the PRISMA 2020 (Preferred Reporting Items for Systematic Reviews and Meta-Analyses) statement as a possible solution to the need for SR reporting guidelines, based on its status as the reporting guideline recommended by the Cochrane Collaboration whose SR guidelines were a major input to the guidelines developed for SE. We report its advantages and limitations in the context of SE secondary studies. We also assess reporting guidelines for mapping studies and qualitative reviews, and compare their structure and content with that of PRISMA 2020. Results: Previous tertiary studies confirm that reports of secondary studies are of variable quality. However, ad hoc recommendations that amend reporting standards may result in unnecessary duplication of text. We confirm that the PRISMA 2020 statement addresses SE reporting problems, but is mainly oriented to quantitative reviews, mixed-methods reviews and meta-analyses. However, we show that the PRISMA 2020 item definitions can be extended to cover the information needed to report mapping studies and qualitative reviews. Conclusions: In this paper and its Supplementary Material, we present and illustrate an integrated set of guidelines called SEGRESS (Software Engineering Guidelines for REporting Secondary Studies), suitable for quantitative systematic reviews (building upon PRISMA 2020), mapping studies (PRISMA-ScR), and qualitative reviews (ENTREQ and RAMESES), that addresses reporting problems found in current SE SRs. © 2022 IEEE.},
	number = {3},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kitchenham, Barbara and Madeyski, Lech and Budgen, David},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Software engineering, Systematic Review, Mapping, Risk assessment, Evidence Based Software Engineering, Mapping studies, Systematic, Quality assessment, Guideline, Software, Mixed method, Mixed-method review, PRISMA 2020, Quality reviews, Reporting guideline, Risk of bias, Threat to validity},
	pages = {1273 -- 1298},
	annote = {Cited by: 27; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@article{dakkak_continuous_2023-2,
	title = {Continuous deployment in software-intensive system-of-systems},
	volume = {159},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150259304&doi=10.1016%2fj.infsof.2023.107200&partnerID=40&md5=fcf9069aa453b5d80bc44a98117f42ad},
	doi = {10.1016/j.infsof.2023.107200},
	abstract = {Context: While continuous deployment is popular among web-based software development organizations, adopting continuous deployment in software-intensive system-of-systems is more challenging. On top of the challenges arising from deploying software to a single software-intensive embedded system, software-intensive system-of-systems (SiSoS) add a layer of complexity as new software undergoes an extensive field validation applied to individual components of the SiSoS, as well as the overall SiSoS, to ensure that both legacy and new functionalities are working as desired. Objectives: This paper aims to study how SiSoS transitions to continuous deployment by exploring how continuous deployment impacts field testing and validation activities, how continuous deployment can be practiced in SiSoS, and to identify the success factors that companies need to consider when transitioning to continuous deployment. Method: We conducted a case study at Ericsson AB focusing on the embedded software of the Third Generation Radio Access Network (3G RAN). The 3G RAN consists of two large-scale software-intensive embedded systems, representing a simple SiSoS composed of two systems. 3G RAN software was the first to transition to continuous deployment and is used as a reference case for other products within Ericsson AB. Results: Software deployment, in addition to field testing and validation, have transitioned from being a discrete activity performed at the end of software development to a continuous process performed in parallel to software development. Further, our study reveals an orchestrating approach for software deployment, which allows pre/post validation of legacy behavior and new features in a shorter release and deployment cadence. Furthermore, we identified the essential success factors that organizations should consider when transitioning to continuous deployment. Conclusion: Transition to continuous deployment, in addition to field testing and validation, shall be considered and planned carefully. In this paper, we provide a set of success factors and orchestration technique that helps organization when transitioning to continuous deployment in the software-intensive embedded system-of-systems context. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Dakkak, Anas and Bosch, Jan and Olsson, Helena Holmström and Issa Mattos, David},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Embedded systems, Software design, Software testing, Success factors, Agile software development, Legacy systems, Continuous software engineerings, Software intensive systems, System of systems, System-of-systems, Continuous deployment, Ericsson, Field testing, Field validation, Software-intensive system-of-system},
	annote = {Cited by: 2},
}

@article{toledo_algorithmic_2023-2,
	title = {Algorithmic {Thinking} and {Extension} of {Its} {Definition} for {Trainee} {Software} {Developers}: {A} {Systematic} {Literature} {Mapping}},
	volume = {18},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666630&doi=10.1109%2fRITA.2023.3323784&partnerID=40&md5=193d79d0896f7dc849cea2f0ec49c32d},
	doi = {10.1109/RITA.2023.3323784},
	abstract = {This paper exhibits a systematic literature mapping of the considerations required to develop algorithmic thinking in a first course in computer programming (CS1) in university academic programs in computing. In the methodological process of this study, 5 stages were proposed: research questions, search, selection, quality assessment and synthesis extraction. In this way, 5 guiding questions were drawn, 136 articles generated by the search stage were analyzed and the synthesis of 55 documents that met the criteria of this research was concluded, thus compiling the different practices used for the development of algorithmic thinking. In addition, as a result of the systematic literature mapping, a definition of Algorithmic Thinking oriented Software Engineering and didactics is proposed. © 2023 IEEE.},
	number = {4},
	journal = {Revista Iberoamericana de Tecnologias del Aprendizaje},
	author = {Toledo, Javier Alejandro Jiménez and Collazos, César A. and Ortega, Manuel and Ramos, Deixy Ximena},
	year = {2023},
	note = {Publisher: Education Society of IEEE (Spanish Chapter)
Type: Article},
	keywords = {Software engineering, Mapping, Research questions, Quality assessment, Computation theory, Computing, Academic program, Algorithmic thinking, Computational logic, Computational thinkings, Logic programming, Software developer},
	pages = {331 -- 343},
	annote = {Cited by: 0},
}

@article{borstler_double-counting_2023-2,
	title = {Double-counting in software engineering tertiary studies — {An} overlooked threat to validity},
	volume = {158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85150795598&doi=10.1016%2fj.infsof.2023.107174&partnerID=40&md5=62bd250321bb30d74a7637c85f9bf590},
	doi = {10.1016/j.infsof.2023.107174},
	abstract = {Context: Double-counting in a literature review occurs when the same data, population, or evidence is erroneously counted multiple times during synthesis. Detecting and mitigating the threat of double-counting is particularly challenging in tertiary studies. Although this topic has received much attention in the health sciences, it seems to have been overlooked in software engineering. Objective: We describe issues with double-counting in tertiary studies, investigate the prevalence of the issue in software engineering, and propose ways to identify and address the issue. Method: We analyze 47 tertiary studies in software engineering to investigate in which ways they address double-counting and whether double-counting might be a threat to validity in them. Results: In 19 of the 47 tertiary studies, double-counting might bias their results. Of those 19 tertiary studies, only 5 consider double-counting a threat to their validity, and 7 suggest strategies to address the issue. Overall, only 9 of the 47 tertiary studies, acknowledge double-counting as a potential general threat to validity for tertiary studies. Conclusions: Double-counting is an overlooked issue in tertiary studies in software engineering, and existing design and evaluation guidelines do not address it sufficiently. Therefore, we propose recommendations that may help to identify and mitigate double-counting in tertiary studies. © 2023 The Author(s)},
	journal = {Information and Software Technology},
	author = {Börstler, Jürgen and bin Ali, Nauman and Petersen, Kai},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Software engineering, Tertiary study, Bias, Research method, Tertiary review, Guideline, Population statistics, Double counting, Empirical, Meta-review, Overview of review, Recommendation, Review of review, Umbrella review},
	annote = {Cited by: 3; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{rokani_fault_2023-2,
	title = {Fault diagnosis of induction motors using artificial intelligence techniques: {A} systematic review},
	volume = {2769},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159667695&doi=10.1063%2f5.0129239&partnerID=40&md5=f5dc44315df865856408a2a1d79eebbd},
	doi = {10.1063/5.0129239},
	abstract = {This manuscript proposes a systematic review of the latest publications, between 2010-2021, of research articles that investigate Induction Motor Fault Diagnosis (IMFD), using Artificial Intelligence techniques. Artificial Intelligence (AI) is an innovative branch of science and engineering. AI techniques constitute the most cutting-edge method in IMFD. Induction motors are regarded as more extensively used than other electric machines. Therefore, preserving their health is critical. It is vital to prevent incipient faults by monitoring the condition of the (IMs) and using diagnostic techniques. An incipient failure in an IM should be detected as early as possible to interrupt the evolution of the fault and reduce the financial losses and the repair period. This paper aspires to: a) Condense the existing surveys concerning the fault diagnosis in induction motors using AI techniques by searching the benefits and limitations of those surveys, b) Determine the gaps in existing research to recommend ideas for further investigation, c) Implement a background in this realm of AI for novel research projects. Moreover, it is followed a particular review protocol that defines the research questions and the methods applied to conduct the systematic review. © 2023 Author(s).},
	booktitle = {{AIP} {Conference} {Proceedings}},
	publisher = {American Institute of Physics Inc.},
	author = {Rokani, V. and Karaisas, P. and Kaminaris, S.D.},
	year = {2023},
	note = {Type: Conference paper},
	annote = {Cited by: 0},
}

@inproceedings{ockiya_review_2023-2,
	title = {A {Review} of {Human} {Factors} in {Remote} {Software} {Project} {Management}: {A} {Progressive} {Look} at {Human} {Based} {Issues} in {Remote} {Software} {Development} {Environments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184518421&doi=10.1145%2f3634848.3634858&partnerID=40&md5=ed5ffbcf2eb102cece0c656199f7585b},
	doi = {10.1145/3634848.3634858},
	abstract = {There is a documented high rate of project failure within the software engineering community. Many researchers have discussed what the issues are and how to solve them using novel processes, technology, and tools, but the statistics remain mostly unchanged. We explored these issues from another perspective focusing on human based factors and how it affects remote software teams. Using a systematic literature review approach with selected criteria we explored the issues under several clusters. We found there exists a relationship between human based factors in remote software teams and success/failure, which, if better understood and managed could be used to reduce the challenges experienced by remote teams. We also identified a limitation in the availability of empirically tested data and suggest further research in understanding human based factors in remote teams as a precursor to reducing the rate of failure. © 2023 Owner/Author.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Ockiya, Tamunoemi Fancey and Lock, Russell},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software design, Project management, Human engineering, Failure analysis, Agile Methodologies, Engineering community, High rate, Novel process, Process Technologies, Project failures, Remote software development, Software project management, Software teams, Software-development environments},
	pages = {15 -- 21},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{liu_citizen_2023-2,
	title = {Citizen involvement in digital transformation: a systematic review and a framework},
	volume = {47},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165768993&doi=10.1108%2fOIR-04-2022-0237&partnerID=40&md5=11a1d93c18925a8a4b0e076a792744ef},
	doi = {10.1108/OIR-04-2022-0237},
	abstract = {Purpose: The purpose of this paper is to improve the understanding of the factors influencing the success of digital transformation (DT) and problems/challenges in DT as well as the communication methods used to involve citizens, based on a systematic literature review of research articles about citizen involvement in DT published between January 2010 and May 2021. Design/methodology/approach: After establishing inclusion and exclusion criteria, a systematic review of relevant studies was conducted. Out of a total of 547 articles, 33 met the paper selection criteria. Findings: The analysis of the included 33 empirical studies reveals that the factors influencing the success of DT can be described as the opposite side from challenges and problems in DT. These factors and challenges/problems all influence DT and they can be grouped into organisational values, management capabilities, organisational infrastructure, and workforce capabilities. The communication methods for citizen involvement in DT include: (1) communication mediated by human, (2) communication mediated by computers, and (3) mixed communication methods. Originality/value: The study identified specific factors that influence DT supported by citizen involvement, at a more fine-grained level. The findings concerning communication methods extend related studies for citizen involvement by adding town hall meetings and communication methods mediated by computers. Furthermore, this study links the research findings to develop a framework for citizen involvement in DT, assisting in better selecting communication methods to involve citizens for addressing problem areas in DT. Peer review: The peer review history for this article is available at: https://publons.com/publon/10.1108/OIR-04-2022-0237 © 2022, Emerald Publishing Limited.},
	number = {4},
	journal = {Online Information Review},
	author = {Liu, Caihua and Zowghi, Didar},
	year = {2023},
	note = {Publisher: Emerald Publishing
Type: Article},
	keywords = {Digital transformation, Systematic literature review, Systematic Review, Citizen involvement, Communication method, Design/methodology/approach, Inclusion and exclusions, Paper selections, Peer review, Selection criteria},
	pages = {644 -- 660},
	annote = {Cited by: 3},
}

@article{binamungu_behaviour_2023-2,
	title = {Behaviour driven development: {A} systematic mapping study},
	volume = {203},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160605559&doi=10.1016%2fj.jss.2023.111749&partnerID=40&md5=0d6c623a12e99c3edeb2a8a0624478da},
	doi = {10.1016/j.jss.2023.111749},
	abstract = {Context: Behaviour Driven Development (BDD) uses scenarios written in semi-structured natural language to express software requirements in a way that can be understood by all stakeholders. The resulting natural language specifications can also be executed to reveal correct and problematic parts of a software. Although BDD was introduced about two decades ago, there is a lack of secondary studies in peer-reviewed scientific literature, making it difficult to understand the state of BDD research and existing gaps. Objective: To understand the current state of BDD research by conducting a systematic mapping study that covers studies published from 2006 (when BDD was introduced) to 2021. Method: By following the guidelines for conducting systematic mapping studies in software engineering, we sought to answer research questions on types of venues in which BDD papers have been published, research types, contribution types, studied topics and their evolution, as well as evaluation methods used in published BDD research. Results: The study identified 166 papers which were mapped. Key results include the following: the dominance of conference papers; scarcity of research with insights from the industry; shortage of philosophical papers on BDD; acute shortage of metrics for measuring various aspects of BDD specifications and the processes for producing BDD specifications; the dominance of studies on using BDD for facilitating various software development endeavours, improving the BDD process and associated artefacts, and applying BDD in different contexts; scarcity of studies on using BDD alongside other software techniques and technologies; increase in diversity of studied BDD topics; and notable use of case studies and experiments to study different BDD aspects. Conclusion: The paper improves our understanding of the state of the art of BDD, and highlights important areas of focus for future BDD research. © 2023 Elsevier Inc.},
	journal = {Journal of Systems and Software},
	author = {Binamungu, Leonard Peter and Maro, Salome},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Specifications, Software design, Mapping, Systematic mapping studies, Semi-structured, Software requirements, Natural languages, 'current, Scientific literature, Petroleum reservoir evaluation, Behavior driven development, Boolean functions, Natural language specifications, Philosophical aspects, Systematic mapping study in software engineering},
	annote = {Cited by: 1; All Open Access, Green Open Access},
}

@article{alanazi_software_2023-2,
	title = {Software {Engineering} {Techniques} for {Building} {Sustainable} {Cities} with {Electric} {Vehicles}},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167892803&doi=10.3390%2fapp13158741&partnerID=40&md5=2974ae54e3ef1b855a7e8fd9235640e7},
	doi = {10.3390/app13158741},
	abstract = {As the process of urbanization continues to accelerate, the demand for sustainable cities has become more critical than ever before. The incorporation of electric vehicles (EVs) is a key component in creating sustainable cities. However, the development of smart cities for EVs entails more than just the installation of charging stations. Software engineering plays a crucial role in realizing smart cities for electric vehicles. This paper examines the role of software engineering in the creation of smart cities for electric vehicles, the techniques utilized in electric vehicle charging infrastructure, the obstacles faced by software engineers, and the future of software engineering in sustainable cities. Specifically, the paper explores the significance of software engineering in integrating EVs into the transportation system, including the design of smart charging and energy management systems, and the establishment of intelligent transportation systems. Additionally, the paper offers case studies to demonstrate successful software engineering implementations for smart cities. Finally, the paper concludes with a discussion of the challenges that software engineers encounter in implementing intelligent transportation systems for EVs and provides future directions for software engineering in sustainable cities. © 2023 by the authors.},
	number = {15},
	journal = {Applied Sciences (Switzerland)},
	author = {Alanazi, Fayez and Alenezi, Mamdouh},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Article},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{vianna_grey_2023-2,
	title = {A {Grey} {Literature} {Review} on {Data} {Stream} {Processing} applications testing},
	volume = {203},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160024077&doi=10.1016%2fj.jss.2023.111744&partnerID=40&md5=279db1183abd50513795efc85821d5f3},
	doi = {10.1016/j.jss.2023.111744},
	abstract = {Context: The Data Stream Processing (DSP) approach focuses on real-time data processing by applying specific techniques for capturing and processing relevant data for on-the-fly results, i.e. without necessarily requiring prior storage. Like in any other software, testing plays a vital role in the quality assurance of DSP applications. However, testing such kind of software is not a simple task. In this context, some factors that make challenging testing are message temporality, parallelism, data volume, complex infrastructure, variability, and speed of messages. Objective: This work aims to map and synthesize industry knowledge and experience regarding DSP application testing. Specifically, we want to know about challenges, test purposes, test approaches, test data sources, and adopted tools. Method: To achieve the objective, we performed a Grey Literature Review (e.g., blog posts, white papers, discussion lists, lecture themes at technical events, professional social networks, software repositories, and other web-published) on testing DSP applications. We searched the grey literature using Google's regular search engine in addition to specific searches on technical software development content websites. The selected studies were analyzed using qualitative and quantitative techniques. Results: Results are based on evidence from 154 selected sources. The challenges for testing DSP applications are the complexity of DSP applications, test infrastructure complexity, timing, and data acquisition issues. The main test objectives identified are functional suitability, performance efficiency, reliability, and maintainability. The main test approaches reported: Performance Testing, Regression Testing, Property-Based Testing, Chaos Testing, and Contract/Schema Testing. The strategies adopted by practitioners to obtain test data: Historical Data, Production Data Mirroring, Semi-Synthetic Data, and Synthetic Data. We also report 50 tools used in various testing activities, which are used for: automating infrastructure, generating test data, test utilities, dealing with timing issues, load generation, simulation, and others. Furthermore, we identified gaps and opportunities for future scientific work. Conclusion: This work selected and summarized content produced by practitioners regarding DSP application testing. We identified that knowledge, techniques, and tools intrinsic to the practice were not present in the formal literature, so this study helps reduce the gap between industry and academia on this topic. The document has delivered benefits to industry practitioners and academic researchers. © 2023 Elsevier Inc.},
	journal = {Journal of Systems and Software},
	author = {Vianna, Alexandre and Kamei, Fernando Kenji and Gama, Kiev and Zimmerle, Carlos and Neto, João Alexandre},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Software design, Software testing, Application programs, Social networking (online), Literature reviews, Search engines, Data handling, Software testings, Digital storage, Social sciences computing, Application testing, Complex networks, Data acquisition, Data stream, Data streams processing, Grey literature, Processing applications, Synthetic data, Test data, Testing data},
	annote = {Cited by: 1},
}

@article{ahmad_requirements_2023-5,
	title = {Requirements engineering for artificial intelligence systems: {A} systematic mapping study},
	volume = {158},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149281892&doi=10.1016%2fj.infsof.2023.107176&partnerID=40&md5=cf81845e526ed8ccc8d8ddea0bf657f1},
	doi = {10.1016/j.infsof.2023.107176},
	abstract = {Context: In traditional software systems, Requirements Engineering (RE) activities are well-established and researched. However, building Artificial Intelligence (AI) based software with limited or no insight into the system's inner workings poses significant new challenges to RE. Existing literature has focused on using AI to manage RE activities, with limited research on RE for AI (RE4AI). Objective: This paper investigates current approaches for specifying requirements for AI systems, identifies available frameworks, methodologies, tools, and techniques used to model requirements, and finds existing challenges and limitations. Method: We performed a systematic mapping study to find papers on current RE4AI approaches. We identified 43 primary studies and analyzed the existing methodologies, models, tools, and techniques used to specify and model requirements in real-world scenarios. Results: We found several challenges and limitations of existing RE4AI practices. The findings highlighted that current RE applications were not adequately adaptable for building AI systems and emphasized the need to provide new techniques and tools to support RE4AI. Conclusion: Our results showed that most of the empirical studies on RE4AI focused on autonomous, self-driving vehicles and managing data requirements, and areas such as ethics, trust, and explainability need further research. © 2023 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Ahmad, Khlood and Abdelrazek, Mohamed and Arora, Chetan and Bano, Muneera and Grundy, John},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Software engineering, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Software-systems, Engineering activities, Machine learning, Machine-learning, 'current, Engineering education, Artificial intelligence systems, Model requirements, System requirements, Tools and techniques},
	annote = {Cited by: 27; All Open Access, Green Open Access},
}

@article{ferrari_transforming_2023-2,
	title = {On transforming model-based tests into code: {A} systematic literature review},
	volume = {33},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169886627&doi=10.1002%2fstvr.1860&partnerID=40&md5=5329e23c5dcfa2e46c74b6c32b471ebf},
	doi = {10.1002/stvr.1860},
	abstract = {Model-based test design is increasingly being applied in practice and studied in research. Model-based testing (MBT) exploits abstract models of the software behaviour to generate abstract tests, which are then transformed into concrete tests ready to run on the code. Given that abstract tests are designed to cover models but are run on code (after transformation), the effectiveness of MBT is dependent on whether model coverage also ensures coverage of key functional code. In this article, we investigate how MBT approaches generate tests from model specifications and how the coverage of tests designed strictly based on the model translates to code coverage. We used snowballing to conduct a systematic literature review. We started with three primary studies, which we refer to as the initial seeds. At the end of our search iterations, we analysed 30 studies that helped answer our research questions. More specifically, this article characterizes how test sets generated at the model level are mapped and applied to the source code level, discusses how tests are generated from the model specifications, analyses how the test coverage of models relates to the test coverage of the code when the same test set is executed and identifies the technologies and software development tasks that are on focus in the selected studies. Finally, we identify common characteristics and limitations that impact the research and practice of MBT: (i) some studies did not fully describe how tools transform abstract tests into concrete tests, (ii) some studies overlooked the computational cost of model-based approaches and (iii) some studies found evidence that bears out a robust correlation between decision coverage at the model level and branch coverage at the code level. We also noted that most primary studies omitted essential details about the experiments. © 2023 John Wiley \& Sons Ltd.},
	number = {8},
	journal = {Software Testing Verification and Reliability},
	author = {Ferrari, Fabiano C. and Durelli, Vinicius H. S. and Andler, Sten F. and Offutt, Jeff and Saadatmand, Mehrdad and Müllner, Nils},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Model checking, Specifications, Systematic literature review, Software design, Software testing, Codes (symbols), Abstracting, Concretes, Model based testing, Model specifications, Model-based test, Test case, Test case generation, Test case transformation, Test coverage criteria, Test sets, Test-coverage},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{khan_software_2023-2,
	title = {Software architecture for quantum computing systems — {A} systematic review},
	volume = {201},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151881159&doi=10.1016%2fj.jss.2023.111682&partnerID=40&md5=949a3516e7bfda8a68429eeb94166d2d},
	doi = {10.1016/j.jss.2023.111682},
	abstract = {Quantum computing systems rely on the principles of quantum mechanics to perform a multitude of computationally challenging tasks more efficiently than their classical counterparts. The architecture of software-intensive systems can empower architects who can leverage architecture-centric processes, practices, description languages to model, develop, and evolve quantum computing software (quantum software for short) at higher abstraction levels. We conducted a Systematic Literature Review (SLR) to investigate (i) architectural process, (ii) modelling notations, (iii) architecture design patterns, (iv) tool support, and (iv) challenging factors for quantum software architecture. Results of the SLR indicate that quantum software represents a new genre of software-intensive systems; however, existing processes and notations can be tailored to derive the architecting activities and develop modelling languages for quantum software. Quantum bits (Qubits) mapped to Quantum gates (Qugates) can be represented as architectural components and connectors that implement quantum software. Tool-chains can incorporate reusable knowledge and human roles (e.g., quantum domain engineers, quantum code developers) to automate and customise the architectural process. Results of this SLR can facilitate researchers and practitioners to develop new hypotheses to be tested, derive reference architectures, and leverage architecture-centric principles and practices to engineer emerging and next generations of quantum software. © 2023 The Authors},
	journal = {Journal of Systems and Software},
	author = {Khan, Arif Ali and Ahmad, Aakash and Waseem, Muhammad and Liang, Peng and Fahmideh, Mahdi and Mikkonen, Tommi and Abrahamsson, Pekka},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Systematic literature review, Systematic Review, Software testing, Software architecture, Modeling languages, Quantum Computing, Quantum computing systems, Quantum optics, Quantum software architecture, Quantum software engineering, Software intensive systems, Architectural process, Architecture-centric, Classical counterpart, Qubits},
	annote = {Cited by: 9; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@inproceedings{horne_ten_2023-2,
	title = {Ten regulatory principles to scaffold the design, manufacture, and use of trustworthy autonomous systems, illustrated in a maritime context},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167979891&doi=10.1145%2f3597512.3599701&partnerID=40&md5=696a9887e71dfc7d264d1bc2b56d5fc8},
	doi = {10.1145/3597512.3599701},
	abstract = {Autonomous systems are increasingly prevalent around the world, with the benefits related to safety, efficiency, and sustainability attractive in addition to the opportunity to establish entirely new capabilities. In order to operationalise autonomous systems technology it is critical to have in place the legal, regulatory and ethical infrastructure necessary to enable safe and trusted operation. While there is a growing body of literature regarding ethical Artificial Intelligence (AI), there is a need for more academic exploration of legal and regulatory best practice for autonomous systems used in commercial and defence contexts. This paper addresses that literature gap by considering the role of regulation and its relationship with trust in a multi-disciplinary context, before proposing 10 principles to base regulatory development and implementation on. These principles, Trust-centred; Collaborative; Risk-based; Evidence-led; Facilitate experimentation; Systems-focussed; Usable; Consistent; Adaptable and Reviewable, collectively provide a domain and technology agnostic basis for a regulatory framework development and implementation approach that supports the design, manufacture and operation of safe and trusted autonomous systems. The paper concludes by recommending next steps towards the regulation of safe and trusted autonomous systems, including a focus on collaboration and experimentation. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Horne, Rachel and Law-Walsh, Caroline and Assaad, Zena and Joiner, Keith},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Best practices, Laws and legislation, Autonomous system, Ethical technology, Law 3.0, Regulation, Regulation of autonomy, Regulatory frameworks, Regulatory principles, Risk-based, Scaffolds, Trust and regulation, Trustworthy autonomous system},
	annote = {Cited by: 1},
}

@article{pereira_junior_systematic_2023-2,
	title = {Systematic {Literature} {Review} on {Virtual} {Electronics} {Laboratories} in {Education}: {Identifying} the {Need} for an {Aeronautical} {Radar} {Simulator}},
	volume = {12},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85163803143&doi=10.3390%2felectronics12122573&partnerID=40&md5=a2d0097e4a068fae9af69afceff35003},
	doi = {10.3390/electronics12122573},
	abstract = {The objective of this work is to propose the development of a virtual electronics laboratory with an aeronautical radar simulator using immersive technologies to help students learn. To verify whether this proposal was viable, the systematic literature review (SLR) methodology was used, whose objective was to verify whether immersive technologies were being used effectively in education and, also, what challenges, opportunities, and benefits they bring to Education 4.0. For this, eight Research Questions (RQs) were formulated to be answered by articles based on the highest SLR scores. The results presented by SLR were as follows: there was an increase in the use of immersive technologies in education, but virtual reality (VR) is still more used in education than AR, despite VR being more expensive than AR; the use of these new technologies brings new challenges, opportunities, and benefits for education; there was an increase in the quality of teaching for complex subjects; and there was an increase in students’ interest in the content presented. © 2023 by the authors.},
	number = {12},
	journal = {Electronics (Switzerland)},
	author = {Pereira Júnior, Enderson Luiz and Moreira, Miguel Ângelo Lellis and Portella, Anderson Gonçalves and de Azevedo Junior, Célio Manso and de Araújo Costa, Igor Pinheiro and Fávero, Luiz Paulo and Gomes, Carlos Francisco Simões and dos Santos, Marcos},
	year = {2023},
	note = {Publisher: MDPI
Type: Article},
	annote = {Cited by: 9; All Open Access, Gold Open Access},
}

@article{malcher_what_2023-2,
	title = {What do we know about requirements management in software ecosystems?},
	volume = {28},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173886355&doi=10.1007%2fs00766-023-00407-w&partnerID=40&md5=30247f2698bd677da0e7068f7c8ff3fd},
	doi = {10.1007/s00766-023-00407-w},
	abstract = {Among the activities in requirements engineering (RE), requirements management ensures that requirements are tracked throughout their life cycle, changes are controlled, and inconsistencies are corrected. Requirements management has become increasingly critical in new ways of developing software and emerging contexts such as software ecosystems (SECO). The changing nature of the SECO introduces complexity in requirements management and results in varied flows of emergent requirements, making managing requirements in SECO challenging. Hence, understanding how requirements management is performed in SECO can help requirements managers improve their practices. This work aims to characterize requirements management in SECO. We have conducted a systematic mapping study (SMS) to achieve this goal. We selected 29 studies using a hybrid search strategy (database search and snowballing). We defined nine characteristics of requirements management in SECO that differentiate it from requirements management in traditional software development. We identified four types of approaches to support requirements management in SECO: tool, method, model, and practice. We found that only three selected studies present an assessment of their approaches. Finally, we characterize requirements management in SECO as an open, informal, collaborative, and decentralized process involving multi-party actors susceptible to power relations. © 2023, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.},
	number = {4},
	journal = {Requirements Engineering},
	author = {Malcher, Paulo and Silva, Eduardo and Viana, Davi and Santos, Rodrigo},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Ecosystems, Management IS, Software design, Life cycle, Mapping, Systematic mapping studies, Requirement engineering, Requirements engineering, Search engines, Database searches, Hybrid search strategies, Managing requirements, Method model, Requirement management, Software ecosystems, Support requirements},
	pages = {567 -- 593},
	annote = {Cited by: 0},
}

@inproceedings{nikiforova_identification_2023-2,
	title = {Identification of {High}-{Value} {Dataset} determinants: is there a silver bullet for efficient sustainability-oriented data-driven development?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85167867852&doi=10.1145%2f3598469.3598556&partnerID=40&md5=617d9c410c006582371208a8f10e79d1},
	doi = {10.1145/3598469.3598556},
	abstract = {Open Government Data (OGD) are seen as one of the trends that has the potential to benefit the economy, improve the quality, efficiency, and transparency of public administration, and change the lives of citizens, and the society as a whole facilitating efficient sustainability-oriented data-driven services. However, the quick achievement of these benefits is closely related to the "value"of the OGD, i.e., how useful, and reusable the data provided by public agencies are for creating value for the above stakeholder. This is where the notion of "high-value datasets"(HVD), defined by the European Commission in Open Data Directive, comes, referring to data that can create the most value for society, the economy, and the environment. This is even more so, considering the proliferation of Artificial Intelligence (AI) and machine learning (ML) applications in various domains. While there are some efforts in that direction, there is still no available framework for identifying country-specific high-value datasets (and their determinants). The objective of the workshop is to raise awareness and build a network of key stakeholders around the HVD issue, to allow each participant to think about how and whether the determination of HVD is taking place in their country, how this can be improved with the help of portal owners, data publishers, data owners, businesses and citizens, what are and can be determinants to be used for identifying HVDs, whether they are SMART. Our main motivation is that, as members of the dg.o community, we can collaboratively answer the above questions, and those raised during the previous two editions of this workshop at ICEGOV2022 and ICOD2022, forming an initial knowledge base, as well as assessing currently used indicators. In this 3rd edition of the workshop, previously obtained results, which make up a list of the most promising indicators, will be discussed, validated and possibly refined through live discussions with the workshop participants following the DELPHI method. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Nikiforova, Anastasija and Alexopoulos, Charalampos and Rizun, Nina and Ciesielska, Magdalena},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Decision making, Economic and social effects, Data driven, High-value dataset, Open Data, Open datum, Open government data, Sustainable development, Knowledge based systems, Artificial intelligence learning, Determinant, European Commission, Machine learning applications, Public administration, Public agencies, Silver},
	pages = {676 -- 678},
	annote = {Cited by: 1},
}

@article{da_silva_relationship_2023-2,
	title = {Relationship between ecosystem innovation and performance measurement models},
	volume = {72},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134164589&doi=10.1108%2fIJPPM-06-2021-0349&partnerID=40&md5=ad93977f490e4d1e1c8aba9e3c9501c5},
	doi = {10.1108/IJPPM-06-2021-0349},
	abstract = {Purpose: This study examines the relationship between the innovation ecosystem and performance measurement models. Although the innovation ecosystem and measurement models are widely recognized, the existing literature lacks a comprehensive understanding of the relationship between the proposed themes. Furthermore, it does not reveal how studies can be grouped to propose a thematic typology of the relationship. Design/methodology/approach: The authors present a systematic literature review conducted in the Web of Science and Scopus databases, from a textual corpus that aided the proposition of the typology that aims to provide answers regarding the addressed themes. Findings: The results of this review are based on a total of sixty peer-reviewed articles from the innovation ecosystem literature and performance measurement models between 1995 and 2020. The results make several contributions to the literature. First, by integrating evidence from empirical studies, the authors identified a typology formed by three classes: (1) ecosystem agents (2) analytical focus and (3) structured measurement tools. Second, the authors verified the relationship between the themes and discovered the existence of gaps to be filled, with the proposition of three drivers. Third, the authors presented a comprehensive mapping of field studies with a descriptive analysis of the textual corpus. Originality/value: The results of the research provide important implications for researchers, managers and policy makers. Furthermore, the authors suggest directions for future research, including the need to examine the performance of the entire innovation ecosystem, integrating the different agents that exist for performance measurement. © 2022, Emerald Publishing Limited.},
	number = {10},
	journal = {International Journal of Productivity and Performance Management},
	author = {da Silva, Deoclécio Junior Cardoso and Lopes, Luis Felipe Dias and Santos Costa Vieira da Silva, Luciana and da Silva, Wesley Vieira and Teixeira, Clarissa Stefani and Veiga, Claudimar},
	year = {2023},
	note = {Publisher: Emerald Publishing
Type: Article},
	pages = {2898 -- 2918},
	annote = {Cited by: 2},
}

@article{sworna_nlp_2023-2,
	title = {{NLP} methods in host-based intrusion detection systems: {A} systematic review and future directions},
	volume = {220},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174894190&doi=10.1016%2fj.jnca.2023.103761&partnerID=40&md5=006065bd017f3587ee6b0c34803dfda5},
	doi = {10.1016/j.jnca.2023.103761},
	abstract = {Host-based Intrusion Detection System (HIDS) is an effective last line of defense for defending against cyber security attacks after perimeter defenses (e.g., Network-based Intrusion Detection System and Firewall) have failed or been bypassed. HIDS is widely adopted in the industry as HIDS is ranked among the top two most used security tools by Security Operation Centers (SOC) of organizations. Although effective and efficient HIDS is highly desirable for industrial organizations, the evolution of increasingly complex attack patterns causes several challenges resulting in performance degradation of HIDS (e.g., high false alert rate creating alert fatigue for SOC staff). Since Natural Language Processing (NLP) methods are better suited for identifying complex attack patterns, an increasing number of HIDS are leveraging the advances in NLP that have shown effective and efficient performance in precisely detecting low footprint, zero-day attacks and predicting an attacker's next steps. This active research trend of using NLP in HIDS demands a synthesized and comprehensive body of knowledge of NLP-based HIDS. Despite the drastically growing adoption of NLP in HIDS development, there has been relatively little effort allocated to systematically analyze and synthesize the available peer review literature to understand how NLP is used in HIDS development. The lack of a synthesized and comprehensive body of knowledge on such an important topic motivated us to conduct a Systematic Literature Review (SLR) of the papers on the end-to-end pipeline of the use of NLP in HIDS development. For the end-to-end NLP-based HIDS development pipeline, we identify, taxonomically categorize and systematically compare the state-of-the-art of NLP methods usage in HIDS, attacks detected by these NLP methods, datasets and evaluation metrics which are used to evaluate the NLP-based HIDS. We highlight the relevant prevalent practices, considerations, advantages and limitations to support the HIDS developers. We also outline the future research directions for the NLP-based HIDS development. © 2023 The Authors},
	journal = {Journal of Network and Computer Applications},
	author = {Sworna, Zarrin Tasnim and Mousavi, Zahra and Babar, Muhammad Ali},
	year = {2023},
	note = {Publisher: Academic Press
Type: Review},
	keywords = {Natural language processing systems, Natural languages, Language processing, Natural language processing, System development, Complex networks, Anomaly detection, Computer crime, Cyber security, Cybersecurity, Host-based intrusion detection, Host-based intrusion detection system, Intrusion detection, Pipeline processing systems, Pipelines, Processing method, Security operation center, Zero-day attack},
	annote = {Cited by: 10; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@inproceedings{fortuna_surveying_2023-2,
	title = {Surveying the {Relevance} of the {Critical} {Success} {Factors} of {Agile} {Transformation} {Initiatives} from a {Project} {Management} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180157252&doi=10.1145%2f3629479.3629515&partnerID=40&md5=ed9d9b9181f0ac254ac6c07e2875a10a},
	doi = {10.1145/3629479.3629515},
	abstract = {Background: Agile methods and practices have been consistently adopted in recent years as alternatives to traditional software development processes to address the ever-changing needs of IT organizations. In a previous systematic mapping study, we identified twelve critical success factors of agile transformations from a project management perspective. Objective: In this paper, we investigate how practitioners perceive the relevance of these factors and whether other factors should be considered. Method: We conducted a survey research involving project managers from several organizations undergoing agile transformations. Results: The participants' perceptions provided valuable insights into the relevance of the critical success factors. Additionally, we identified five new critical success factors: organizational ambidexterity, use of tools and automation, breaking down organizational silos, team commitment, and alignment of organizational goals and expectations. These newly identified factors contribute to a more comprehensive understanding of organizations' challenges during an agile transformation. Based on the results and the literature, we formulated three propositions representing recommendations that can foster agile transformation. Conclusions: The evidence gathered in this study indicates that the factors investigated previously are highly relevant. Moreover, organizations should consider them to enhance the chances of success of agile transformation initiatives. © 2023 ACM.},
	booktitle = {{ACM} {International} {Conference} {Proceeding} {Series}},
	publisher = {Association for Computing Machinery},
	author = {Fortuna, Alessandra and Mattos, Claudio Saraiva and Andrade, Álan Júnior Da Cruz and Ramos, Luiz Felipe and Dutra, Eliezer and Santos, Rodrigo Pereira Dos and Santos, Gleison},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software design, Success factors, Agile software development, Organisational, Project management, Agile methods, Agile practices, Agile transformations, Changing needs, Critical success factor, IT organizations, Software development process},
	pages = {110 -- 119},
	annote = {Cited by: 0},
}

@article{erthal_characterization_2023-3,
	title = {Characterization of continuous experimentation in software engineering: {Expressions}, models, and strategies},
	volume = {229},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159759787&doi=10.1016%2fj.scico.2023.102961&partnerID=40&md5=fe2272f51ad17194de939829974123e2},
	doi = {10.1016/j.scico.2023.102961},
	abstract = {Context: Continuous Experimentation (CE) has become increasingly popular across industry and academic communities. Major software organizations use CE to increase their revenue by adding value to end-users, and researchers are investigating the CE adoption process and usage to expand its success. Given this rapid evolution, observing a shared understanding of CE definitions, processes, and experiment strategies is difficult, potentially jeopardizing new implementations and focused research efforts. Objective: To characterize CE from the perspective of its definitions, processes, and strategies for experimentation available in the technical literature and to evolve the understanding perspectives for “continuous experimentation” and “data-driven development” definitions. Method: To select and analyze sources of information in the technical literature dealing with different aspects of continuous experimentation through a Literature Study using an ad hoc search improved with snowballing (backward and forward). Organize the findings into new perspectives for CE definitions, processes, and experiment strategies. Results: It was possible to identify many different definitions, processes, and experimental strategies used to describe CE in the 72 analyzed empirical papers, making it difficult to decide on their combination to be applied in a real software development project. Therefore, it has been proposed to evolve the CE understanding perspective, to categorize its experiment strategies, and to offer a combined development process for CE combining parts of other processes. Besides, conjectural requirements have been identified, which can contribute to better differentiating requirements and hypotheses in the CE context. Conclusion: Likely, a better understanding of CE is still missing. It can contribute towards organizing a common taxonomy to facilitate the possible choices for the experiment strategies. Therefore, there is space for more investigations on its applicability and value in different categories of software systems, despite all the advancements of CE and its promotion in developing modern software systems. © 2023 Elsevier B.V.},
	journal = {Science of Computer Programming},
	author = {Erthal, Vladimir M. and de Souza, Bruno P. and dos Santos, Paulo Sérgio M. and Travassos, Guilherme H.},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software design, Software testing, Evidence Based Software Engineering, Software-systems, Continuous experimentation, Data driven, Data-driven development, A/b testing, Controlled experiment, Engineering expression, Expression modeling, Technical literature},
	annote = {Cited by: 0},
}

@article{santos_distributed_2023-2,
	title = {Distributed {Scrum}: {A} {Case} {Meta}-analysis},
	volume = {56},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180152360&doi=10.1145%2f3626519&partnerID=40&md5=8d7c6e0cd850d803596d4150c118f884},
	doi = {10.1145/3626519},
	abstract = {Distributed Scrum adapts the Scrum project management framework for geographically distributed software teams. Experimentally evaluating the effectiveness of Distributed Scrum is impractical, but many case studies and experience reports describe teams and projects that used Distributed Scrum. This article synthesizes the results of these cases using case meta-analysis, a technique for quantitatively analyzing qualitative case reports. On balance, the evidence suggests that Distributed Scrum has no impact, positive or negative, on overall project success. Consequently, claims by agile consultants who present Distributed Scrum as a recipe for project success should be treated with great caution, while researchers should investigate more varied perspectives to identify the real drivers of success in distributed and global software development. © 2023 held by the owner/author(s).},
	number = {4},
	journal = {ACM Computing Surveys},
	author = {Santos, Ronnie De Souza and Ralph, Paul and Arshad, Arham and Stol, Klaas-Jan},
	year = {2023},
	note = {Publisher: Association for Computing Machinery
Type: Article},
	keywords = {Software design, Human resource management, Project management, Global software engineering, Scra, Meta-analysis, Software teams, Case meta-analyse, Distributed scrums, Distributed software, Distributed software development, Project management frameworks, Project success},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{minhas_lessons_2023-2,
	title = {Lessons learned from replicating a study on information-retrieval-based test case prioritization},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174265778&doi=10.1007%2fs11219-023-09650-4&partnerID=40&md5=746a6f83a8dbadb4bea7db51dccd8466},
	doi = {10.1007/s11219-023-09650-4},
	abstract = {Replication studies help solidify and extend knowledge by evaluating previous studies’ findings. Software engineering literature showed that too few replications are conducted focusing on software artifacts without the involvement of humans. This study aims to replicate an artifact-based study on software testing to address the gap related to replications. In this investigation, we focus on (i) providing a step-by-step guide of the replication, reflecting on challenges when replicating artifact-based testing research and (ii) evaluating the replicated study concerning the validity and robustness of the findings. We replicate a test case prioritization technique proposed by Kwon et al. We replicated the original study using six software programs, four from the original study and two additional software programs. We automated the steps of the original study using a Jupyter notebook to support future replications. Various general factors facilitating replications are identified, such as (1) the importance of documentation; (2) the need for assistance from the original authors; (3) issues in the maintenance of open-source repositories (e.g., concerning needed software dependencies, versioning); and (4) availability of scripts. We also noted observations specific to the study and its context, such as insights from using different mutation tools and strategies for mutant generation. We conclude that the study by Kwon et al. is partially replicable for small software programs and could be automated to facilitate software practitioners, given the availability of required information. However, it is hard to implement the technique for large software programs with the current guidelines. Based on lessons learned, we suggest that the authors of original studies need to publish their data and experimental setup to support the external replications. © 2023, The Author(s).},
	number = {4},
	journal = {Software Quality Journal},
	author = {Minhas, Nasir Mehmood and Irshad, Mohsin and Petersen, Kai and Börstler, Jürgen},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	keywords = {Software testing, Open source software, Software testings, Information retrieval, Replication study, Open systems, Prioritization techniques, Regression testing, Replication, SIR, Software artefacts, Software project, Technique, Test case prioritization},
	pages = {1527 -- 1559},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{dobaj_towards_2023-2,
	title = {Towards {DevOps} for {Cyber}-{Physical} {Systems} ({CPSs}): {Resilient} {Self}-{Adaptive} {Software} for {Sustainable} {Human}-{Centric} {Smart} {CPS} {Facilitated} by {Digital} {Twins}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175031906&doi=10.3390%2fmachines11100973&partnerID=40&md5=5e0d1126be87b6fd21b7b77f667e9194},
	doi = {10.3390/machines11100973},
	abstract = {The Industrial Revolution drives the digitization of society and industry, entailing Cyber-Physical Systems (CPSs) that form ecosystems where system owners and third parties share responsibilities within and across industry domains. Such ecosystems demand smart CPSs that continuously align their architecture and governance to the concerns of various stakeholders, including developers, operators, and users. In order to satisfy short- and long-term stakeholder concerns in a continuously evolving operational context, this work proposes self-adaptive software models that promote DevOps for smart CPS. Our architectural approach extends to the embedded system layer and utilizes embedded and interconnected Digital Twins to manage change effectively. Experiments conducted on industrial embedded control units demonstrate the approach’s effectiveness in achieving sub-millisecond real-time closed-loop control of CPS assets and the simultaneous high-fidelity twinning (i.e., monitoring) of asset states. In addition, the experiments show practical support for the adaptation and evolution of CPS through the dynamic reconfiguring and updating of real-time control services and communication links without downtime. The evaluation results conclude that, in particular, the embedded Digital Twins can enhance CPS smartness by providing service-oriented access to CPS data, monitoring, adaptation, and control capabilities. Furthermore, the embedded Digital Twins can facilitate the seamless integration of these capabilities into current and future industrial service ecosystems. At the same time, these capabilities contribute to implementing emerging industrial services such as remote asset monitoring, commissioning, and maintenance. © 2023 by the authors.},
	number = {10},
	journal = {Machines},
	author = {Dobaj, Jürgen and Riel, Andreas and Macher, Georg and Egretzberger, Markus},
	year = {2023},
	note = {Publisher: Multidisciplinary Digital Publishing Institute (MDPI)
Type: Article},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@book{bhat_engineering_2023-2,
	title = {Engineering {Challenges} in the {Development} of {Artificial} {Intelligence} and {Machine} {Learning} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85179277659&doi=10.1201%2f9781032624983-7&partnerID=40&md5=8d4c838a9eb34b83f7f86e0841ed7615},
	abstract = {In this chapter, we first introduce artificial intelligence and machine learning (AI/ML) as state-of-the-art in engineering software and then outline the major differences between AI/ML and traditional software development. In particular, we categorize AI/ML engineering challenges in different phases. Eventually, different challenges are generalized and categorized. Finally, we observe that software testing, quality assurance, and management of the data are the most challenging issues that engineers/developers are currently facing. © 2024 Taylor \& Francis Group, LLC.},
	publisher = {CRC Press},
	author = {Bhat, Mohammad Idrees and Yaqoob, Syed Irfan and Imran, Mohammad},
	year = {2023},
	doi = {10.1201/9781032624983-7},
	note = {Publication Title: System Reliability and Security: Techniques and Methodologies
Type: Book chapter},
	annote = {Cited by: 2},
}

@article{giray_use_2023-2,
	title = {On the use of deep learning in software defect prediction},
	volume = {195},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140965251&doi=10.1016%2fj.jss.2022.111537&partnerID=40&md5=abc6530954079e153d281e71264be476},
	doi = {10.1016/j.jss.2022.111537},
	abstract = {Context: Automated software defect prediction (SDP) methods are increasingly applied, often with the use of machine learning (ML) techniques. Yet, the existing ML-based approaches require manually extracted features, which are cumbersome, time consuming and hardly capture the semantic information reported in bug reporting tools. Deep learning (DL) techniques provide practitioners with the opportunities to automatically extract and learn from more complex and high-dimensional data. Objective: The purpose of this study is to systematically identify, analyze, summarize, and synthesize the current state of the utilization of DL algorithms for SDP in the literature. Method: We systematically selected a pool of 102 peer-reviewed studies and then conducted a quantitative and qualitative analysis using the data extracted from these studies. Results: Main highlights include: (1) most studies applied supervised DL; (2) two third of the studies used metrics as an input to DL algorithms; (3) Convolutional Neural Network is the most frequently used DL algorithm. Conclusion: Based on our findings, we propose to (1) develop more comprehensive DL approaches that automatically capture the needed features; (2) use diverse software artifacts other than source code; (3) adopt data augmentation techniques to tackle the class imbalance problem; (4) publish replication packages. © 2022 The Authors},
	journal = {Journal of Systems and Software},
	author = {Giray, Görkem and Bennin, Kwabena Ebo and Köksal, Ömer and Babur, Önder and Tekinerdogan, Bedir},
	year = {2023},
	note = {Publisher: Elsevier Inc.
Type: Article},
	keywords = {Systematic literature review, Deep learning, Machine-learning, Defects, Forecasting, Software defect prediction, Convolutional neural networks, Data mining, Machine learning techniques, Quality assurance, Bug reporting, Clustering algorithms, Defect prediction methods, Learning-based approach, Reporting tools, Semantics, Semantics Information},
	annote = {Cited by: 32; All Open Access, Hybrid Gold Open Access},
}

@article{fischer_data-driven_2023-2,
	title = {Data-{Driven} {Organizations}: {Review}, {Conceptual} {Framework}, and {Empirical} {Illustration}},
	volume = {27},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180313770&doi=10.3127%2fAJIS.V27I0.4425&partnerID=40&md5=e02e898eb6866062fdfeaaa30c52d11d},
	doi = {10.3127/AJIS.V27I0.4425},
	abstract = {With companies and other organizations increasingly striving to become (more) data-driven, there has been growing research interest in the notion of a data-driven organization (DDO). In existing literature, however, different understandings of such an organization emerged. The study at hand sets forth to synthesize the fragmented body of research through a review of existing DDO definitions and implicit understandings of this concept in the information systems and related literatures. Based on the review results and drawing on the established concept of the “knowing organization,” our study identifies five core dimensions of a DDO—namely, data sourcing \& sensemaking, data capabilities, data-driven culture, data-driven decision-making, and data-driven value creation—which we integrate into a conceptual DDO framework. Most notably, the proposed framework suggests that—like its predecessor, the knowing organization—a DDO may draw on an outside-in view; however, it may also draw on an inside-out view, or even combine the two views, thereby setting itself apart from the knowing organization. To illustrate our conceptual DDO framework and demonstrate its usefulness, we apply this framework to three empirical examples. Theoretical and practical contributions as well as directions for future research are discussed. © (2023), (Australasian Association for Information Systems). All Rights Reserved.},
	journal = {Australasian Journal of Information Systems},
	author = {Fischer, Hannes and Wiener, Martin and Strahringer, Susanne and Kotlarsky, Julia and Bley, Katja},
	year = {2023},
	note = {Publisher: Australasian Association for Information Systems
Type: Article},
	pages = {1 -- 26},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{cordeiro_towards_2023-2,
	title = {Towards a {Framework} {Based} on {Open} {Science} {Practices} for {Promoting} {Reproducibility} of {Software} {Engineering} {Controlled} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166021485&partnerID=40&md5=145a60cab90f6f1a29901242e43e78d6},
	abstract = {Experimentation in Software Engineering has increased in the last decades as a way to provide evidence on theories and technologies. In a controlled experiment life cycle, several artifacts are used/reused and even produced. Such artifacts are mostly in the form of data, which should favor the reproducibility of such experiments. In this context, reproducibility can be defined as the ability to reproduce a study. Different benefits, such as methodology and data reuse, can be achieved from this ability. Despite the recognized benefits, several challenges have been faced by researchers regarding the experiments’ reproducibility capability. To overcome them, we understand that Open Science practices, related to provenance, preservation, and curation, might aid in improving such a capability. Therefore, in this paper, we present the proposal for an open science-based Framework to deal with controlled experiment research artifacts towards making such experiments de facto reproducible. To do so, different models associated with open science practices are planned to be integrated into the Framework. © 2023 CIbSE 2023 - XXVI Ibero-American Conference on Software Engineering. All rights reserved.},
	booktitle = {{CIbSE} 2023 - {XXVI} {Ibero}-{American} {Conference} on {Software} {Engineering}},
	publisher = {Ibero-American Conference on Software Engineering},
	author = {Cordeiro, André F.R.},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software engineering, Life cycle, Open science, Controlled experiment, Curation, Data reuse, Experiment research, Reproducibilities, Research artefacts},
	annote = {Cited by: 0},
}

@inproceedings{ris_systemic_2023-2,
	title = {A {Systemic} {Mapping} of {Methods} and {Tools} for {Performance} {Analysis} of {Data} {Streaming} with {Containerized} {Microservices} {Architecture}},
	volume = {2023-June},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169796889&doi=10.23919%2fCISTI58278.2023.10211834&partnerID=40&md5=d6a237cf4005729c3547af7a0673b592},
	doi = {10.23919/CISTI58278.2023.10211834},
	abstract = {With the Internet of Things (IoT) growth and customer expectations, the importance of data streaming and streaming processing has increased. Data Streaming refers to the concept where data is processed and transmitted continuously and in real-time without necessarily being stored in a physical location. Personal health monitors and home security systems are examples of data streaming sources. This paper presents a systematic mapping study of the performance analysis of Data Streaming systems in the context of Containerization and Microservices. The research aimed to identify the main methods, tools, and techniques used in the last five years for the execution of this type of study. The results show that there are still few performance evaluation studies for this system niche, and there are gaps that must be filled, such as the lack of analytical modeling and the disregard for communication protocols' influence. © 2023 ITMA.},
	booktitle = {Iberian {Conference} on {Information} {Systems} and {Technologies}, {CISTI}},
	publisher = {IEEE Computer Society},
	author = {Ris, Simone and Araujo, Jean and Beserra, David},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Mapping, Performance, Containers, Microservice, Internet of things, Performances analysis, Real- time, Analysis of data, Customer expectation, Data reduction, Data streaming, Data transfer, Physical locations, Realibility, Streaming processing},
	annote = {Cited by: 1},
}

@article{silva_digital_2023-2,
	title = {The {Digital} {Twin} {Paradigm} {Applied} to {Soil} {Quality} {Assessment}: {A} {Systematic} {Literature} {Review}},
	volume = {23},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85146668292&doi=10.3390%2fs23021007&partnerID=40&md5=14555c99ca9482a8958de1c8b705d8ab},
	doi = {10.3390/s23021007},
	abstract = {This article presents the results regarding a systematic literature review procedure on digital twins applied to precision agriculture. In particular, research and development activities aimed at the use of digital twins, in the context of predictive control, with the purpose of improving soil quality. This study was carried out through an exhaustive search of scientific literature on five different databases. A total of 158 articles were extracted as a result of this search. After a first screening process, only 11 articles were considered to be aligned with the current topic. Subsequently, these articles were categorised to extract all relevant information, using the preferred reporting items for systematic reviews and meta-analyses methods. Based on the obtained results, there are two main conclusions to draw: First, when compared with industrial processes, there is only a very slight rising trend regarding the use of digital twins in agriculture. Second, within the time frame in which this work was carried out, it was not possible to find any published paper on the use of digital twins for soil quality improvement within a model predictive control context. © 2023 by the authors.},
	number = {2},
	journal = {Sensors},
	author = {Silva, Letícia and Rodríguez-Sedano, Francisco and Baptista, Paula and Coelho, João Paulo},
	year = {2023},
	pmid = {36679804},
	note = {Publisher: MDPI
Type: Review},
	keywords = {Systematic literature review, Systematic Review, Scientific literature, Development activity, Model predictive control, Precision agriculture, Precision Agriculture, Predictive control, Research activities, Research and development, soil, Soil, Soil quality assessments, Soils, Soils qualities},
	annote = {Cited by: 8; All Open Access, Gold Open Access},
}

@article{schipor_gearwheels_2023-2,
	title = {{GearWheels}: {A} {Software} {Tool} to {Support} {User} {Experiments} on {Gesture} {Input} with {Wearable} {Devices}},
	volume = {39},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85134623880&doi=10.1080%2f10447318.2022.2098907&partnerID=40&md5=5be729e9568048f5de88c0aea18ab9e7},
	doi = {10.1080/10447318.2022.2098907},
	abstract = {We introduce GearWheels, a software tool for studies about gesture input with wearables, including smartwatches, rings, and glasses. GearWheels features an event-based asynchronous software architecture design implemented exclusively with web standards, communications protocols, and data formats, which makes it flexible to support many wearables via HTTP and WebSocket communications. GearWheels differentiates from prior software tools for gesture acquisition, elicitation, recognition, and analysis with its web-based, wearable-oriented, experiment-centered architecture design. We demonstrate GearWheels with a device affixed to the index finger, wrist, and the temple of a pair of glasses to illustrate touch stroke-gesture and motion-gesture input acquisition. We also perform a technical evaluation of GearWheels in the form of a simulation experiment, and report the request-response time performance of the software components of GearWheels with off-the-shelf wearables. We release GearWheels as open source software to assist researchers and practitioners in implementing studies about gesture input with wearables. © 2022 Taylor \& Francis Group, LLC.},
	number = {18},
	journal = {International Journal of Human-Computer Interaction},
	author = {Schipor, Ovidiu-Andrei and Vatavu, Radu-Daniel},
	year = {2023},
	note = {Publisher: Taylor and Francis Ltd.
Type: Article},
	keywords = {Software design, Open source software, Open systems, Communications data, Communications protocols, Computer aided software engineering, Event-based, Gesture input, Glass, HTTP, Network architecture, Software architecture design, Software-tools, User experiments, Wearable devices, Wearable technology, Web standards, Websocket},
	pages = {3527 -- 3545},
	annote = {Cited by: 0},
}

@article{zapata_systematic_2023-2,
	title = {Systematic {Mapping} of the {Literature} on the {Conceptual} {Modeling} of {Industry} 4.0},
	volume = {1778 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173573848&doi=10.1007%2f978-3-031-34147-2_15&partnerID=40&md5=1b3b1b38bf9c300501453861ddb7a8fa},
	doi = {10.1007/978-3-031-34147-2_15},
	abstract = {The Industry 4.0 concept refers to a new way of producing through the adoption of 4.0 technologies based on solutions focused on interconnectivity, automation, and real-time data. Given the importance of conceptualizing the problem domain and its solution, this paper presents the results of a systematic mapping to identify the state of the art and discover the existing contributions to the conceptual modeling of industry 4.0. A search was carried out in the Scopus, IEEE Xplore, and ACM DL digital libraries from January 2017 to May 2022. It was found that no article describes the model through a language known for this purpose, except for two articles that use Domain Specific Modeling Languages (DSML) and Unified Modeling Language (UML). Of the total number of primary studies, 63.33\% propose a model-based solution, while 13.34\% propose the use of tools, methods, and processes. Finally, 23.33\% present the state of the art. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Zapata, Ayelén and Fransoy, Marcelo and Soto, Salvador and Di Felice, Martín and Panizzi, Marisa},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Embedded systems, State of the art, Mapping, Unified Modeling Language, Systematic mapping, Industry 4.0, Digital libraries, Technology-based, Conceptual model, Domain specific modeling languages, Domain-Specific Modelling Languages, Interconnectivity, Problem domain, Real-time data, Specification languages, Systematic mapping of the literature},
	pages = {227 -- 240},
	annote = {Cited by: 0},
}

@book{ritu_software_2023-1,
	title = {Software {Effort} {Estimation} with {Machine} {Learning} – {A} {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152307515&doi=10.1002%2f9781119896838.ch15&partnerID=40&md5=8e9a36b192b41d691f4c4960256acc6b},
	abstract = {In 1959 the concept of machine learning techniques and algorithm was introduced by Artur Samuel, an IBmer from the United States who made a name for himself in the fields of computer gaming and artificial intelligence. The influence of literature reviews which is done systematically (SLRs), which are the preferred techniques and methods for aggregating effort, is examined in this study. We conducted a systematic literature review using the conventional procedure, which included a manual search of nine periodicals and a few conference proceedings. Eight of the twenty studies that were relevant focused on latest trends in research instead of technique evaluation. Seven LRs dealt with the estimation of effort. The SLR’s quality was best suited with only those in which fields are qualitatively checked not quantitatively. SLRs currently cover a large number of topics, but not all of them. Systematic literature reviews appear to be the most popular among researchers from Asia and Europe, particularly those at the Simula Laboratory. © 2023 Scrivener Publishing LLC.},
	publisher = {wiley},
	author = {{Ritu} and Bhambri, Pankaj},
	year = {2023},
	doi = {10.1002/9781119896838.ch15},
	note = {Publication Title: Agile Software Development: Trends, Challenges and Applications
Type: Book chapter},
	annote = {Cited by: 13; All Open Access, Bronze Open Access},
}

@inproceedings{de_castro_understanding_2023-2,
	title = {Understanding {Sustainable} {Knowledge}-{Sharing} in {Agile} {Projects}: {Utilizing} {Follow}-the-{Sun} {Technique} ({FTS}) in {Virtual} {Teams}},
	volume = {225},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183565184&doi=10.1016%2fj.procs.2023.10.023&partnerID=40&md5=e8a2113a7c5e0ce18a6d80b3fec2ca8e},
	doi = {10.1016/j.procs.2023.10.023},
	abstract = {In Agile IT projects, promoting effective knowledge sharing is essential not only for achieving success but also for supporting Sustainable Development Goals (SDGs). However, Companies using virtual teams may face challenges in coordinating work, particularly when teams are distributed across different time zones, ultimately hindering their ability to consistently share knowledge. This can lead to delays and inefficiencies, ultimately impacting the project outcomes and the organization's profitability. To ensure sustainable knowledge sharing, a comprehensive framework is necessary that addresses the environmental, social, economic, and political aspects of the project. This paper proposes a framework that combines the Follow-the-Sun (FTS) technique and the Sustainable Knowledge Sharing Model, enabling 24-hour knowledge sharing in virtual teams and benefiting IT agile projects. © 2023 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (https://creativecommons.org/licenses/by-nc-nd/4.0)},
	booktitle = {Procedia {Computer} {Science}},
	publisher = {Elsevier B.V.},
	author = {de Castro, Rodrigo Oliveira and Sanin, Cesar and Levula, Andrew and Szczerbicki, Edward},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Knowledge management, Sustainable development, Project management, 24-hour knowledge-sharing cycle, Agile IT project, Economic responsibility, Environmental responsibility, Follow-the-sun technique, IT project, Knowledge-sharing, Political responsibility, Social responsibilities, Sustainable knowledge sharing, Sustainable knowledge sharing model, Virtual team},
	pages = {384 -- 393},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{vasylieva_how_2023-2,
	title = {How {Agile} {Are} you? {Discussing} {Maturity} {Levels} of {Agile} {Maturity} {Models}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183320257&doi=10.1109%2fSEAA60479.2023.00049&partnerID=40&md5=328e1188c0b79baca242d53e65b4626e},
	doi = {10.1109/SEAA60479.2023.00049},
	abstract = {With the emergence of agile software development methods, new approaches for determining agile maturity have become necessary. Other than for traditional maturity and capability models like CMMI and ISO/IEC 15504, the field of agile maturity models is not yet settled. Even worse, a common understanding regarding agility in general and the levels of agility in particular is missing. The paper at hand aims to shed light on the field of agile maturity models with a particular focus on maturity levels, their definition, and their evaluation and computation. We conducted a systematic literature review to extract maturity levels and provide an initial harmonization of the levels found. Our findings from analyzing 19 agile maturity models show that there is yet no agreement with regard to the maturity levels. In total, 69 maturity levels have been analyzed for harmonization opportunities. Two major dimensions of maturity levels of agile maturity models could be identified: (1) team-related and (2) general maturity, which is comparable to standard approaches. However, the procedures to assess organizations and processes, if at all present, are to a large extent focused on persons and their personal opinion, which paves the way for future research, e.g., in terms of developing measurement systems for assessing agile maturity. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 49th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications}, {SEAA} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Vasylieva, Kseniia and Kuhrmann, Marco and Xavier, Meenu Kadavilveetil and Klunder, Jil},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software design, Agile software development, Software process, Agile maturity model, Capability model, Harmonisation, Maturity levels, Maturity model, New approaches, Software development methods},
	pages = {270 -- 277},
	annote = {Cited by: 0},
}

@inproceedings{ntinda_aligning_2023-2,
	title = {Aligning {Academic} {Efforts} with {Key} {Industries}: {A} {Case} of {Computing} at the {University} of {Namibia}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85182994686&doi=10.1109%2fFIE58773.2023.10343344&partnerID=40&md5=f95405a625e2b7d7d2cb3afce77a7778},
	doi = {10.1109/FIE58773.2023.10343344},
	abstract = {Preparing future graduates for the workplace should involve linkage with industry. Ultimately, students gain valuable insight into real-life projects, preparing them for future careers. However, universities in some developing countries lag in their initiatives to promote industry-academia collaboration. In this paper, we explore how to strengthen academic efforts at the University of Namibia (UNAM) with the assistance of industry in Namibia. In the study, we analyse: 1) current practices of industry collaboration worldwide published in ACM and IEEE digital library through a scoping review, and 2) students' capstone projects conducted in the final year of the Bachelor of Science (Honors) in the computing discipline in 2020 - 2022 at UNAM. The analysis from the scoping review found six (6) different University-Industry Collaboration initiatives employed in universities worldwide. Additionally, the review of current students' theses indicates that they are not aligned with all four key industries in Namibia: Mining, Tourism, Fisheries, and Agriculture. Hence, we contextualised the analysis by reflecting upon the economic drivers and demands of the country. The preliminary outcomes of this study allowed us to propose the incorporation of the Conceive, Design, Implement, and Operate model in the computing degree programme that UNAM can adopt in developing an effective curriculum that aligns with the demands of the key relevant industries in Namibia. The aim is to support the development of new talent that will promote the country's economic growth. Reflecting on this process can also benefit other universities in developing countries by assisting them in contextualising their curricula and addressing their local and national requirements. © 2023 IEEE.},
	booktitle = {Proceedings - {Frontiers} in {Education} {Conference}, {FIE}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ntinda, Maria Ndapewa and Sedano, Carolina Islas and Apiola, Mikko and Sutinen, Erkki},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Students, Digital libraries, Curricula, Computing, Bachelor of science, Capstone projects, CDIO model, Computing disciplines, Current practices, Developing countries, Economic analysis, Industry collaboration, Mining, Namibia, Scoping review, University industries},
	annote = {Cited by: 0},
}

@article{jagstedt_dependencies_2023-2,
	title = {Dependencies as a barrier for continuous innovation in cyber-physical systems},
	volume = {93},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174302916&doi=10.1504%2fIJTM.2023.133915&partnerID=40&md5=09819f40631a7253d848d4f119ce6628},
	doi = {10.1504/IJTM.2023.133915},
	abstract = {In the automotive domain, as an example of cyber-physical systems, continuous software deployment is actively explored to deliver increasingly capable features to existing fleets of vehicles. The distributed nature of software coupled with tight hardware integration and potentially tremendous variability between vehicles make ensuring compatibility of updated software a significant challenge – both technically and managerially. While the automotive industry commonly forms larger multi-brand organisations to utilise economies of scale, processes for continuous deployment contradictory assumes a single organisation with full control. This paper sets out to shed light on challenges of adopting continuous deployment in the context of such a multi-brand cyber-physical systems organisation. Following a case study, the paper describes a tension between the managerial perspective concerned with platform strategies, and the engineering perspective responsible for developing products from those platforms. The paper highlights software dependencies as a barrier to continuous innovation of cyber-physical systems in multi-brand organisations. Copyright © 2023 Inderscience Enterprises Ltd.},
	number = {3-4},
	journal = {International Journal of Technology Management},
	author = {Jagstedt, Siri and Mellegård, Niklas and Lind, Kenneth},
	year = {2023},
	note = {Publisher: Inderscience Publishers
Type: Article},
	keywords = {Embedded systems, Software design, Agile software development, Continuous integrations, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Automotive industry, Automotives, Continuous deployment, Continuous innovation, Dependency, Economics, Multi-brand organization, Product architecture, Product platforms},
	pages = {194 -- 219},
	annote = {Cited by: 0},
}

@article{iannone_secret_2023-2,
	title = {The {Secret} {Life} of {Software} {Vulnerabilities}: {A} {Large}-{Scale} {Empirical} {Study}},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85122895799&doi=10.1109%2fTSE.2022.3140868&partnerID=40&md5=8645cb8f157905454c978f4ae194b36e},
	doi = {10.1109/TSE.2022.3140868},
	abstract = {Software vulnerabilities are weaknesses in source code that can be potentially exploited to cause loss or harm. While researchers have been devising a number of methods to deal with vulnerabilities, there is still a noticeable lack of knowledge on their software engineering life cycle, for example how vulnerabilities are introduced and removed by developers. This information can be exploited to design more effective methods for vulnerability prevention and detection, as well as to understand the granularity at which these methods should aim. To investigate the life cycle of known software vulnerabilities, we focus on how, when, and under which circumstances the contributions to the introduction of vulnerabilities in software projects are made, as well as how long, and how they are removed. We consider 3,663 vulnerabilities with public patches from the National Vulnerability Database-pertaining to 1,096 open-source software projects on GitHub-and define an eight-step process involving both automated parts (e.g., using a procedure based on the SZZ algorithm to find the vulnerability-contributing commits) and manual analyses (e.g., how vulnerabilities were fixed). The investigated vulnerabilities can be classified in 144 categories, take on average at least 4 contributing commits before being introduced, and half of them remain unfixed for at least more than one year. Most of the contributions are done by developers with high workload, often when doing maintenance activities, and removed mostly with the addition of new source code aiming at implementing further checks on inputs. We conclude by distilling practical implications on how vulnerability detectors should work to assist developers in timely identifying these issues. © 1976-2012 IEEE.},
	number = {1},
	journal = {IEEE Transactions on Software Engineering},
	author = {Iannone, Emanuele and Guadagni, Roberta and Ferrucci, Filomena and De Lucia, Andrea and Palomba, Fabio},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Information management, Software design, Life cycle, Software vulnerabilities, Software-systems, Open source software, Codes (symbols), Source codes, Open systems, Data mining, Software, Empirical Software Engineering, Code, Mining software, Mining software repository, Software development management, Software repositories},
	pages = {44 -- 63},
	annote = {Cited by: 14},
}

@article{mubarkoot_software_2023-2,
	title = {Software {Compliance} {Requirements}, {Factors}, and {Policies}: {A} {Systematic} {Literature} {Review}},
	volume = {124},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141927596&doi=10.1016%2fj.cose.2022.102985&partnerID=40&md5=972aacda02a7c5ba18e90461a46647d6},
	doi = {10.1016/j.cose.2022.102985},
	abstract = {Background: Recent statistics reveal that 56\% of software attacks are caused by insider negligence and 26\% are caused by malicious insiders. They also show that 67\% of organizations experience at least 21 incidents per year. Most of these incidents require significant time and effort to contain them. In this regard, ensuring compliance with corporate policies, regulations, and industry best practices is paramount. Purpose: This study investigates software compliance requirements, factors, and policies together with the challenges they address. By taking a wider perspective, this study aims at bringing an understanding of existing research foci, evolving issues, and research directions. Method: The study uses a systematic literature review and keyword analysis, to identify relevant studies that address the derived research questions. Considering scholarly articles published in the last decade, 4,772 results were retrieved and checked through an initial screening. A thorough screening is then conducted to further reduce the results to 77 primary articles. Findings: The requirement on security of end users is gaining more attention. There is an emphasis on the gap between domain and compliance experts on the one side and software engineers on the other side. The review also identified 55 factors (and their underlying theories) that impact behavioral compliance with a majority of them focusing on individuals. Our results also list nineteen policies and compliance challenges they address. No distinction is found between open-source and proprietary software among the reviewed studies. The most mentioned policies are security education, training, and awareness (SETA), compliance automation, and organizational climate. The evolving topics in the field are: theory of workarounds, compliance and privacy by design, policy as code, security stress, and home-office users. Implications: The review provides 9 recommendations, comprising practical implications for decision makers, theoretical implications for future research, and potential enhancement of the underlying theories. © 2022 The Author(s)},
	journal = {Computers and Security},
	author = {Mubarkoot, Mohammed and Altmann, Jörn and Rasti-Barzoki, Morteza and Egger, Bernhard and Lee, Hyejin},
	year = {2023},
	note = {Publisher: Elsevier Ltd
Type: Review},
	keywords = {Systematic literature review, Decision making, Open source software, Impact, Factor, Best practices, Open systems, Corporate policies, Decision theory, Malicious insiders, Policy regulations, Requirement, Software attacks, Software compliance},
	annote = {Cited by: 2; All Open Access, Hybrid Gold Open Access},
}

@article{gardey_ux-painter_2023-2,
	title = {{UX}-{Painter}: {Fostering} {UX} {Improvement} in an {Agile} {Setting}},
	volume = {1642 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149840802&doi=10.1007%2f978-3-031-25648-6_4&partnerID=40&md5=50a565c23e05f522a0b2f5833cbc014b},
	doi = {10.1007/978-3-031-25648-6_4},
	abstract = {It is generally difficult in agile teams, specially those geographically distributed, to keep up with the user experience (UX) issues that emerge on each product increment. UX designers need the help of developers to set up user testing environments and to code improvements to the user interface, while developers are too busy with functionality issues. This paper describes a tool called UX-Painter and shows through a case study, how it may help in the above setting to synchronize UX practices and allow for continuous UX improvement during an agile development. UX-Painter allows designers to set up A/B testing environments, exploring interface design alternatives without the need of programming skills, through predefined transformations called client-side web refactorings. Once a design alternative is selected to be implemented in the application’s codebase, UX-Painter may also facilitate this step, exporting the applied refactorings to different frontend frameworks. Thus, we foster a method where UX backlog items can be systematically tackled and resolved in an agile setting. © 2023, Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Gardey, Juan Cruz and Grigera, Julián and Rossi, Gustavo and Garrido, Alejandra},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {User interfaces, Users' experiences, Refactorings, User testing, Agile methods, Agile teams, Code improvement, Design alternatives, Show through, Testing environment, Web engineering},
	pages = {54 -- 65},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{nunez-agurto_traffic_2023-2,
	title = {Traffic {Classification} in {Software}-{Defined} {Networking} by {Employing} {Deep} {Learning} {Techniques}: {A} {Systematic} {Literature} {Review}},
	volume = {1873 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85175975883&doi=10.1007%2f978-3-031-45682-4_6&partnerID=40&md5=9a95f5b968d737eb60ef3bdb967e4d27},
	doi = {10.1007/978-3-031-45682-4_6},
	abstract = {Software-Defined Networking provides a global vision of the network, centralized controller, dynamic routing, dynamic update of the flow table, and traffic analysis. The features of Software-Defined Networking and the integration of Deep Learning techniques allow the introduction of intelligence to optimize, manage and maintain them better. In this context, this work aims to provide a Systematic Literature Review on traffic classification in Software-Defined Networking with Deep Learning techniques. Furthermore, we analyze and synthesize the selected studies based on the categorization of traffic classes and the employed Deep Learning techniques to draw meaningful research conclusions. Finally, we identify new challenges and future research directions on this topic. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Nuñez-Agurto, Daniel and Fuertes, Walter and Marrone, Luis and Benavides-Astudillo, Eduardo and Vásquez-Bermúdez, Mitchell},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Systematic literature review, Deep learning, Learning algorithms, Learning systems, Centralized controllers, Controller dynamics, Dynamic routing, Global vision, Learning techniques, Routing dynamics, Software defined networking, Software-defined networkings, Traffic classification},
	pages = {67 -- 80},
	annote = {Cited by: 1},
}

@article{volden_wayfinding_2023-2,
	title = {Wayfinding and {Navigation} in the {Outdoors}: {Quantitative} and {Data} {Driven} {Development} of {Personas} and {Requirements} for {Wayfinding} in {Nature}},
	volume = {14016 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85173041138&doi=10.1007%2f978-3-031-35129-7_14&partnerID=40&md5=77edf2f35a0d4ce956149ab12f497864},
	doi = {10.1007/978-3-031-35129-7_14},
	abstract = {Persona development in human-centered design processes is mostly done in a qualitative process involving procedures like interviews, focus-groups and workshops. These are methods that are criticized for being prone to biases, as not being based on rigorous empirical data and for using small and possibly non-representative populations. Quantitative approaches are an alternative or supplement to qualitative methods. Through a survey (n = 693) we have investigated how people navigate and find their way in the nature. The questionnaire contains questions on demographics, activities, and wayfinding behaviors when out in the nature. The study’s aim was twofold: First we wanted to investigate the use of a quantitative approach for exploring user behaviors and attitudes when having access to a sufficiently large data material. Secondly, we wanted to provide for persona-development for way-finding systems used in the nature. The methods applied in this study is a combination of Principal Component Analyses (PCA) and Cluster Analyses (CA). Based on these methods three factors where identified, which again lead to three clusters of respondents. The study concludes that when having access to quantitative data as we managed to have in this study, the combination of PCA and CA is an efficient and precise way to describe requirements and develop Personas. Results also indicate significant effects of demographic variables like age and gender for technology preferences. as well as for confidence in abilities when navigating and finding the way in nature. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Volden, Frode and Wattne, Ole E.},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Behavioral research, Data driven, Design-process, Population statistics, Cluster analysis, Cluster analyze, Human-centred designs, Principal component analysis, Principal-component analysis, Qualitative process, Quantitative approach, Quantitative persona development, Way finding, Wayfinding and navigations},
	pages = {199 -- 210},
	annote = {Cited by: 0},
}

@inproceedings{souza_santos_lgbtqia_2023-2,
	title = {{LGBTQIA}+ ({In}) {Visibility} in {Computer} {Science} and {Software} {Engineering} {Education}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165153862&doi=10.1109%2fCHASE58964.2023.00026&partnerID=40&md5=91575c7fa526b69d70f2e73d63079186},
	doi = {10.1109/CHASE58964.2023.00026},
	abstract = {Modern society is diverse, multicultural, and multifaceted. Because of these characteristics, we are currently observing an increase in the debates about equity, diversity, and inclusion in different areas, especially because several groups of individuals are underrepresented in many environments. In computer science and software engineering, it seems counterintuitive that these areas, which are responsible for creating technological solutions and systems for billions of users around the world, do not reflect the diversity of the society to which it serves. In trying to solve this diversity crisis in the software industry, researchers started to investigate strategies that can be applied to increase diversity and improve inclusion in academia and the software industry. However, the lack of diversity in computer science and related courses, including software engineering, is still a problem, in particular when some specific groups are considered. LGBTQIA+ students, for instance, face several challenges to fit into technology courses, even though most students in universities right now belong to Generation Z, which is described as open-minded to aspects of gender and sexuality. In this study, we aimed to discuss the state-of-art of publications about the inclusion of LGBTQIA+ students in computer science education. Using a mapping study, we identified eight studies published in the past six years that focused on this public. We present strategies developed to adapt curricula and lectures to be more inclusive to LGBTQIA+ students and discuss challenges and opportunities for future research. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 {IEEE}/{ACM} 16th {International} {Conference} on {Cooperative} and {Human} {Aspects} of {Software} {Engineering}, {CHASE} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Souza Santos, Ronnie De and Stuart-Verner, Brody and De Magalhaes, Cleyton V. C.},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Computer software, Engineering education, Students, Software industry, Computer Science Education, Education computing, Computer science and software engineerings, Computer Science course, Computer-related course, Diversity, Inclusions, LGBTQIA+, Software engineering education, Technological solution, Technological system},
	pages = {167 -- 172},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{pantoja_yepez_training_2023-2,
	title = {Training software architects suiting software industry needs: {A} literature review},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85174223101&doi=10.1007%2fs10639-023-12149-x&partnerID=40&md5=857c26b2b7109dcd5a963c38e629e347},
	doi = {10.1007/s10639-023-12149-x},
	abstract = {The ability to define, evaluate, and implement software architectures is a fundamental skill for software engineers. However, teaching software architecture can be challenging as it requires students to be involved in real-context projects with high degrees of complexity. This involves making trade-off decisions among several quality attributes. Furthermore, the academic perception of software architecture differs from the industrial viewpoint. To address this issue, a study was conducted to identify and analyze the strategies, challenges, and course experiences used for teaching software architectures. The study analyzed 56 articles reporting on teaching experiences focused specifically on software architectures or focused on software engineering in general but discussing software architecture. The main contributions of this work include identifying strategies used in educating software architecture students aligned with the needs of the software industry. These strategies include short design projects, large development projects, and projects with actual clients. Additionally, the study compared curriculum contents in software development and architecture courses and identified recurring topics such as architecture patterns, quality attributes, and architectural views. This study also recognizes the set of skills that students of software architecture should develop during training, such as leadership and negotiation. The challenges in software architecture training were discussed, such as instructors’ lack of experience in actual projects, the abstract and fuzzy nature of software architectures, and the difficulty of involving clients and industry experts. Evaluation methods commonly used in training software architects, such as surveys, pre-test/post-test, and quality metrics on architectural artifacts, were identified and described. Overall, this study guides researchers and educators in improving their software architecture courses by incorporating strategies reported by the literature review. These strategies can bring architecture courses closer to the needs and conditions of the software industry. © 2023, The Author(s).},
	journal = {Education and Information Technologies},
	author = {Pantoja Yépez, Wilson Libardo and Hurtado Alegría, Julio Ariel and Bandi, Ajay and Kiwelekar, Arvind W.},
	year = {2023},
	note = {Publisher: Springer
Type: Article},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{heyn_automotive_2023-2,
	title = {Automotive {Perception} {Software} {Development}: {An} {Empirical} {Investigation} into {Data}, {Annotation}, and {Ecosystem} {Challenges}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165140236&doi=10.1109%2fCAIN58948.2023.00011&partnerID=40&md5=73c99440d36cf7a3c3c1d5b7444837f6},
	doi = {10.1109/CAIN58948.2023.00011},
	abstract = {Software that contains machine learning algorithms is an integral part of automotive perception, for example, in driving automation systems. The development of such software, specifically the training and validation of the machine learning components, requires large annotated datasets. An industry of data and annotation services has emerged to serve the development of such data-intensive automotive software components. Wide-spread difficulties to specify data and annotation needs challenge collaborations between OEMs (Original Equipment Manufacturers) and their suppliers of software components, data, and annotations.This paper investigates the reasons for these difficulties for practitioners in the Swedish automotive industry to arrive at clear specifications for data and annotations. The results from an interview study show that a lack of effective metrics for data quality aspects, ambiguities in the way of working, unclear definitions of annotation quality, and deficits in the business ecosystems are causes for the difficulty in deriving the specifications. We provide a list of recommendations that can mitigate challenges when deriving specifications and we propose future research opportunities to overcome these challenges. Our work contributes towards the on-going research on accountability of machine learning as applied to complex software systems, especially for high-stake applications such as automated driving. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 {IEEE}/{ACM} 2nd {International} {Conference} on {AI} {Engineering} - {Software} {Engineering} for {AI}, {CAIN} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Heyn, Hans-Martin and Habibullah, Khan Mohammad and Knauss, Eric and Horkoff, Jennifer and Borg, Markus and Knauss, Alessia and Li, Polly Jing},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Data, Ecosystems, Specifications, Software design, Application programs, Learning algorithms, Automation, Machine learning, Machine-learning, Machine learning algorithms, Software-component, Automotive industry, Automotives, Accountability, Annotation, Data annotation, Empirical investigation, Large dataset, Machine components, Requirements specifications},
	pages = {13 -- 24},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@article{neves_data_2023-2,
	title = {Data privacy in the {Internet} of {Things} based on anonymization: {A} review},
	volume = {31},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161049485&doi=10.3233%2fJCS-210089&partnerID=40&md5=31fb26c942cd9ccd8d3943c25a607885},
	doi = {10.3233/JCS-210089},
	abstract = {The Internet of Things (IoT) has shown rapid growth in recent years. However, it presents challenges related to the lack of standardization of communication produced by different types of devices. Another problem area is the security and privacy of data generated by IoT devices. Thus, with the focus on grouping, analyzing, and classifying existing data security and privacy methods in IoT, based on data anonymization, we have conducted a Systematic Literature Review (SLR). We have therefore reviewed the history of works developing solutions for security and privacy in the IoT, particularly data anonymization and the leading technologies used by researchers in their work. We also discussed the challenges and future directions for research. The objective of the work is to give order to the main approaches that promise to provide or facilitate data privacy using anonymization in the IoT area. The study's results can help us understand the best anonymization techniques to provide data security and privacy in IoT environments. In addition, the findings can also help us understand the limitations of existing approaches and identify areas for improvement. The results found in most of the studies analyzed indicate a lack of consensus in the following areas: (i) with regard to a solution with a standardized methodology to be applied in all scenarios that encompass IoT; (ii) the use of different techniques to anonymize the data; and (iii), the resolution of privacy issues. On the other hand, results made available by the k-anonymity technique proved efficient in combination with other techniques. In this context, data privacy presents one of the main challenges for broadening secure domains in applying privacy with anonymity. © 2023 - IOS Press. All rights reserved.},
	number = {3},
	journal = {Journal of Computer Security},
	author = {Neves, Flávio and Souza, Rafael and Sousa, Juliana and Bonfim, Michel and Garcia, Vinicius},
	year = {2023},
	note = {Publisher: IOS Press BV
Type: Article},
	keywords = {Systematic literature review, Data privacy, Privacy, Internet of things, Anonymization, Data anonymization, Data security and privacy, Dataflow, K-Anonymity, Problem areas, Rapid growth, Security and privacy},
	pages = {261 -- 291},
	annote = {Cited by: 4},
}

@article{denecke_developing_2023-2,
	title = {Developing a {Technical}-{Oriented} {Taxonomy} to {Define} {Archetypes} of {Conversational} {Agents} in {Health} {Care}: {Literature} {Review} and {Cluster} {Analysis}},
	volume = {25},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85147143364&doi=10.2196%2f41583&partnerID=40&md5=acb31e1475488e8316c9d3c5a54d84e0},
	doi = {10.2196/41583},
	abstract = {Background: The evolution of artificial intelligence and natural language processing generates new opportunities for conversational agents (CAs) that communicate and interact with individuals. In the health domain, CAs became popular as they allow for simulating the real-life experience in a health care setting, which is the conversation with a physician. However, it is still unclear which technical archetypes of health CAs can be distinguished. Such technical archetypes are required, among other things, for harmonizing evaluation metrics or describing the landscape of health CAs. Objective: The objective of this work was to develop a technical-oriented taxonomy for health CAs and characterize archetypes of health CAs based on their technical characteristics. Methods: We developed a taxonomy of technical characteristics for health CAs based on scientific literature and empirical data and by applying a taxonomy development framework. To demonstrate the applicability of the taxonomy, we analyzed the landscape of health CAs of the last years based on a literature review. To form technical design archetypes of health CAs, we applied a k-means clustering method. Results: Our taxonomy comprises 18 unique dimensions corresponding to 4 perspectives of technical characteristics (setting, data processing, interaction, and agent appearance). Each dimension consists of 2 to 5 characteristics. The taxonomy was validated based on 173 unique health CAs that were identified out of 1671 initially retrieved publications. The 173 CAs were clustered into 4 distinctive archetypes: a text-based ad hoc supporter; a multilingual, hybrid ad hoc supporter; a hybrid, single-language temporary advisor; and, finally, an embodied temporary advisor, rule based with hybrid input and output options. Conclusions: From the cluster analysis, we learned that the time dimension is important from a technical perspective to distinguish health CA archetypes. Moreover, we were able to identify additional distinctive, dominant characteristics that are relevant when evaluating health-related CAs (eg, input and output options or the complexity of the CA personality). Our archetypes reflect the current landscape of health CAs, which is characterized by rule based, simple systems in terms of CA personality and interaction. With an increase in research interest in this field, we expect that more complex systems will arise. The archetype-building process should be repeated after some time to check whether new design archetypes emerge. © 2023 Journal of Medical Internet Research. All rights reserved.},
	journal = {Journal of Medical Internet Research},
	author = {Denecke, Kerstin and May, Richard},
	year = {2023},
	pmid = {36716093},
	note = {Publisher: JMIR Publications Inc.
Type: Review},
	keywords = {human, Humans, human computer interaction, Review, artificial intelligence, Artificial Intelligence, cluster analysis, Cluster Analysis, Communication, controlled study, data privacy, data processing, Delivery of Health Care, health care, health care delivery, intelligence, internet access, interpersonal communication, k means clustering, language, Language, machine learning, personality, sentiment analysis, taxonomy},
	annote = {Cited by: 7; All Open Access, Gold Open Access, Green Open Access},
}

@article{tsui_detect_2023-2,
	title = {Detect and {Interpret}: {Towards} {Operationalization} of {Automated} {User} {Experience} {Evaluation}},
	volume = {14032 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169436114&doi=10.1007%2f978-3-031-35702-2_6&partnerID=40&md5=71662b96b384aa4acc41fbb5146fa2f3},
	doi = {10.1007/978-3-031-35702-2_6},
	abstract = {The evaluation of user experience (UX) with software products is widely recognized as a critical aspect of supporting a product lifecycle. However, existing UX evaluation methods tend to require high levels of human involvement in data collection and analysis. This makes the ongoing UX monitoring particularly challenging, especially given the increasing number of products, growing user base and associated data. Thus, there is a strong demand in developing UX evaluation systems that are able to automatically track UX and provide insights on required design improvements. The few existing frameworks for such automated systems can help identify user-centric metrics for UX evaluation, but mostly focus on providing recommendations on best practices of determining metrics and tend to reflect only parts of the UX. Moreover, these frameworks predominantly rely on high-level UX concepts, but do not necessarily allow measurements to reveal the underlying causes of UX challenges. In this paper, we demonstrate how the above-mentioned challenges can be addressed through a combination of data gathering and analysis paths employed by the traditional UX evaluation methods. Our paper contributes to the field by providing a review of existing automated UX evaluation approaches and common UX evaluation data collection methods, and offering a two-tier measurement approach for developing automated UX evaluation system, which augments the reflective power of traditional UX evaluation methods. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Tsui, Angeline Sin Mei and Kuzminykh, Anastasia},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Life cycle, Automation, Users' experiences, Evaluation methods, Data acquisition, Automatic UX evaluation, Data collection, Evaluation method and technique, Evaluation of users, Method and technique, Product life cycles, Software products, User experience evaluations},
	pages = {82 -- 100},
	annote = {Cited by: 0},
}

@article{medeiros_visualizations_2023-2,
	title = {Visualizations for the evolution of {Variant}-{Rich} {Systems}: {A} systematic mapping study},
	volume = {154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85140215376&doi=10.1016%2fj.infsof.2022.107084&partnerID=40&md5=448386c9479f057b5f1f362d53b945fa},
	doi = {10.1016/j.infsof.2022.107084},
	abstract = {Context: Variant-Rich Systems (VRSs), such as Software Product Lines or variants created through clone \& own, aim at reusing existing assets. The long lifespan of families of variants, and the scale of both the code base and the workforce make VRS maintenance and evolution a challenge. Visualization tools are a needed companion. Objective: We aim at mapping the current state of visualization interventions in the area of VRS evolution. We tackle evolution in both functionality and architecture. Three research questions are posed: What sort of analysis is being conducted to assess VRS evolution? (Analysis perspective); What sort of visualizations are displayed? (Visualization perspective); What is the research maturity of the reported interventions? (Maturity perspective). Methods: We performed a systematic mapping study including automated search in digital libraries, expert knowledge, and snowballing. Results: The study reports on 41 visualization approaches to cope with VRS evolution. Analysis wise, feature identification and location is the most popular scenario, followed by variant integration towards a Software Product Line. As for visualization, nodelink diagram visualization is predominant while researchers have come up with a wealth of ingenious visualization approaches. Finally, maturity wise, almost half of the studies are solution proposals. Most of the studies provide proof-of-concept, some of them also include publicly available tools, yet very few face proof-of-value. Conclusions: This study introduces a comparison framework where to frame future studies. It also points out distinct research gaps worth investigating as well as shortcomings in the evidence about relevance and contextual considerations (e.g., scalability). © 2022 The Author(s)},
	journal = {Information and Software Technology},
	author = {Medeiros, Raul and Martinez, Jabier and Díaz, Oscar and Falleri, Jean-Rémy},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software design, Software Product Line, Mapping, Systematic mapping studies, Computer software, Product variants, Mapping studies, Visualization, Digital libraries, Evolution, Lifespans, System evolution, System maintenance, Variant-rich system, Visualization tools},
	annote = {Cited by: 4; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@inproceedings{claderon-blas_medical_2023-2,
	title = {Medical {Recommender} {Systems}: a {Systematic} {Literature} {Review}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192802746&doi=10.1109%2fENC60556.2023.10508695&partnerID=40&md5=eb26de296c721791c678cf1e70075fea},
	doi = {10.1109/ENC60556.2023.10508695},
	abstract = {Medical recommender systems are applications in the field of health. These systems use Artificial Intelligence techniques to provide personalized recommendations to healthcare professionals and patients, based on available and relevant patient information. Software engineering is essential in developing medical recommender systems, as these systems must be accurate, reliable, and secure for use in clinical settings. This work presents a Systematic Literature Review based on the Kitchenham and Charters guide, in order to explore the Artificial Intelligence techniques used in this type of system, which can be incorporated or improved by software developers who participate in this type of project. Twelve primary studies were selected, where mainly machine learning approaches were identified (algorithms based on decision trees, neural networks, Bayesian classifiers and clustering such as k-means), matrix approaches, based on rules, among others. Precision, Recall, and Root Mean Square Error (RMSE) were the main measures used to evaluate the performance of these systems. Finally, the studies propose always increasing the sample size of the tests carried out, including relevant patient information such as social networks and clinical information, as well as exploring other algorithms and approaches that allow improving the results of the recommendation. © 2023 IEEE.},
	booktitle = {2023 {Mexican} {International} {Conference} on {Computer} {Science}, {ENC} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Claderón-Blas, Javier A. and Cerdan, María Angélica and Sánchez-García, Ángel J. and Domingue-Isidro, Saúl},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, Learning algorithms, Artificial intelligence techniques, Machine learning, Decision trees, Software, Bayesian networks, Clinical settings, Health care professionals, K-means clustering, Mean square error, Medical recommende system, Metric, Patient information, Personalized recommendation, Recommender systems, System use},
	annote = {Cited by: 0},
}

@article{heikkinen_continual_2023-2,
	title = {Continual {Service} {Improvement}: {A} {Systematic} {Literature} {Review}},
	volume = {1871 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172415273&doi=10.1007%2f978-3-031-43703-8_3&partnerID=40&md5=68142092a8a39278b60b3241c351fb51},
	doi = {10.1007/978-3-031-43703-8_3},
	abstract = {Continual Service Improvement (CSI) is an ongoing activity to identify and improve organization practices and services to align them with changing needs. CSI is one of the core elements of IT Service Management (ITSM) frameworks. However, as a research topic it is still an emerging research area of service science. This study explores implementation of CSI and its seven-step improvement process in the context of ITSM. The goal of this paper is to present results of systematic literature review increasing understanding about the CSI and seven-step improvement process, and provide topics for future research. A Systematic Literature Review (SLR) was carried out to analyse CSI-related academic articles. Our main finding is that CSI-related terminology needs clarification and consistency both in academia and in practice to guide the future CSI research for example clarify roles and internal practices of CSI; provide a staged approach for continual improvement; and identify models that support improving and automating the seven-step improvement process. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Heikkinen, Sanna and Jäntti, Marko and Tukiainen, Markku},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Systematic literature review, Continual service improvement, IT service management, IT services, ITIL, Service improvement, Service management, Seven-step improvement process},
	pages = {30 -- 44},
	annote = {Cited by: 0},
}

@article{ouhaichi_research_2023-2,
	title = {Research trends in multimodal learning analytics: {A} systematic mapping study},
	volume = {4},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151456109&doi=10.1016%2fj.caeai.2023.100136&partnerID=40&md5=4c1ed5486737fbf780207ba5b51fa068},
	doi = {10.1016/j.caeai.2023.100136},
	abstract = {Understanding and improving education are critical goals of learning analytics. However, learning is not always mediated or aided by a digital system that can capture digital traces. Learning in such environments can be studied by recording, processing, and analyzing different signals, including video and audio, so that traces of actors’ actions and interactions are captured. Multimodal Learning Analytics refers to analyzing these signals through the use and integration of these multiple modes. However, a need exists to evaluate how research is conducted in the emerging field of multimodal learning analytics to aid and evaluate how these systems work. With the growth of multimodal learning analytics, research trends and technologies are needed to support its development. We conducted a systematic mapping study based on established systematic literature practices to identify multimodal learning analytics research types, methodologies, and trending research themes. Most mapped papers presented different solutions and used evaluation-based research methods to demonstrate an increasing interest in multimodal learning analytics technologies. In addition, we identified 14 topics under four themes––learning context, learning process, systems and modality, and technologies––that can contribute to the growth of multimodal learning analytics. © 2023 The Authors},
	journal = {Computers and Education: Artificial Intelligence},
	author = {Ouhaichi, Hamza and Spikol, Daniel and Vogel, Bahtijar},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	annote = {Cited by: 16; All Open Access, Gold Open Access},
}

@inproceedings{baron_evidence_2023-2,
	title = {Evidence {Profiles} for {Validity} {Threats} in {Program} {Comprehension} {Experiments}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85171794365&doi=10.1109%2fICSE48619.2023.00162&partnerID=40&md5=eb9f911cd27ddf564d8b8e6fbcac370a},
	doi = {10.1109/ICSE48619.2023.00162},
	abstract = {Searching for clues, gathering evidence, and reviewing case files are all techniques used by criminal investigators to draw sound conclusions and avoid wrongful convictions. Medicine, too, has a long tradition of evidence-based practice, in which administering a treatment without evidence of its efficacy is considered malpractice. Similarly, in software engineering (SE) research, we can develop sound methodologies and mitigate threats to validity by basing study design decisions on evidence. Echoing a recent call for the empirical evaluation of design decisions in program comprehension experiments, we conducted a 2-phases study consisting of systematic literature searches, snowballing, and thematic synthesis. We found out (1) which validity threat categories are most often discussed in primary studies of code comprehension, and we collected evidence to build (2) the evidence profiles for the three most commonly reported threats to validity. We discovered that few mentions of validity threats in primary studies (31 of 409) included a reference to supporting evidence. For the three most commonly mentioned threats, namely the influence of programming experience, program length, and the selected comprehension measures, almost all cited studies (17 of 18) did not meet our criteria for evidence. We show that for many threats to validity that are currently assumed to be influential across all studies, their actual impact may depend on the design and context of each specific study. Researchers should discuss threats to validity within the context of their particular study and support their discussions with evidence. The present paper can be one resource for evidence, and we call for more meta-studies of this type to be conducted, which will then inform design decisions in primary studies. Further, although we have applied our methodology in the context of program comprehension, our approach can also be used in other SE research areas to enable evidence-based experiment design decisions and meaningful discussions of threats to validity. © 2023 IEEE.},
	booktitle = {Proceedings - {International} {Conference} on {Software} {Engineering}},
	publisher = {IEEE Computer Society},
	author = {Baron, Marvin Munoz and Wyrich, Marvin and Graziotin, Daniel and Wagner, Stefan},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software engineering, Design, Study design, Software engineering research, Empirical evaluations, Evidence-based practices, Empirical Software Engineering, Threat to validity, Case files, Design decisions, Evaluation of designs, Program comprehension},
	pages = {1907 -- 1919},
	annote = {Cited by: 2; All Open Access, Green Open Access},
}

@inproceedings{dallegrave_action_2023-2,
	title = {Action {Research} for {Industry} {Academia} {Collaboration} : {A} replication {Study}},
	volume = {2023-June},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85169786536&doi=10.23919%2fCISTI58278.2023.10211674&partnerID=40&md5=947725c1c9cf30739a4b22bde3653ba7},
	doi = {10.23919/CISTI58278.2023.10211674},
	abstract = {Collaboration between industry and academic communities requires considerable work but has the power to foster innovation. This relationship with joint trust promotes knowledge exchange that helps develop more qualified researchers and professionals. The Action Research (AR) method combines theory and practice, and studies involving industry-academia collaboration (IAC) have shown encouraging results. Nevertheless, further investigation is required to verify the effects of applying this method. This research investigates the perceptions of academic master's and doctoral program students and professionals involved in projects that applied the AR method as a strategy to foster IAC. This article replicates a case study with different projects that conducted an AR in software companies. This study indicated high satisfaction among students (83\%) when using action research in the course. All students considered the practical knowledge very relevant and would like to use the method again in other opportunities throughout their academic and professional life. This investigation showed that conducting IAC projects using the AR method within the industry in an educational context was challenging. That occurred due to the lack of experience in using empirical methods. Also, the professional's unavailability delayed the results and, consequently, the activities in the project that already had a very tight schedule. © 2023 ITMA.},
	booktitle = {Iberian {Conference} on {Information} {Systems} and {Technologies}, {CISTI}},
	publisher = {IEEE Computer Society},
	author = {Dallegrave, Tamara and Santos, Wylliams Barbosa},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Industry-academia collaboration, Knowledge management, Students, Power, Research method, Replication study, Replication, Academic community, Action research, Collaborative practice research, Collaborative practices, Practice researches},
	annote = {Cited by: 0},
}

@article{sadeghiani_sayings_2023-2,
	title = {Sayings and doings become ‘practice’ through ‘practice thirdness’: pivot in recipes for practice},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162251678&doi=10.1080%2f08985626.2023.2225044&partnerID=40&md5=057becbb87581d7172bf60a6275f5339},
	doi = {10.1080/08985626.2023.2225044},
	abstract = {The abductive logic behind the practice lens allows practice researchers to contextualize theorizing and emphasize non-generalizability of their findings. However, scholars are critical of this non-generalizability flaw. In this conceptual paper, we aim to go beyond such criticisms and constructively discuss how this flaw might be resolved. In doing so, we theorize ‘practice thirdness’ as the shared understanding of knowing how to do practice, at local and universal levels, and provide a framework for discussing the generalizability of practice. We take ‘pivot’, at the heart of the Lean Startup as our case, and based on different interpretations of this practice, we argue what entrepreneurs have said and what scholars have interpreted of what entrepreneurs have said do not show what they have actually done. Therefore, despite the formation of practice local thirdness, i.e. practice thirdness in a particular context, in the case of pivot, still, we need academic conversation to reach practice universal thirdness, i.e. practice thirdness across different contexts. We suggest that practice researchers take a neopragmatic lens for studying practice patterns across different contexts. Also, we argue why practice researchers should be open to other methods besides the commonly recommended (non)participant observation. Moreover, we propose a model for communicating and generalizing practice based on Peirce’s triadic model of semiosis and Nonaka and Takeuchi’s model of knowledge management. © 2023 Informa UK Limited, trading as Taylor \& Francis Group.},
	number = {9-10},
	journal = {Entrepreneurship and Regional Development},
	author = {Sadeghiani, Ayoob and Anderson, Alistair and Ahmadi, Sadra and Shokouhyar, Sajjad and Hajipour, Bahman},
	year = {2023},
	note = {Publisher: Routledge
Type: Article},
	keywords = {knowledge, modeling, research work, theoretical study},
	pages = {788 -- 811},
	annote = {Cited by: 1},
}

@article{saarikallio_quality_2023-2,
	title = {Quality culture boosts agile transformation—{Action} research in a business-to-business software business},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137236655&doi=10.1002%2fsmr.2504&partnerID=40&md5=f87d77293e772685a8659d46dc1dbc29},
	doi = {10.1002/smr.2504},
	abstract = {Agile methodologies are sometimes adopted, with the assumption that benefits can be attained by only using a set of best practices, which can sometimes work to a degree. In this paper, a case is discussed where a software-producing organization of seven teams achieved significant improvements. The goal of the research was to answer two questions: how an already agile organization could improve its performance further and what is the impact of promoting quality aspects? The questions were answered by implementing interventions based on prior literature and data emerging from semi-structured interviews. The context was an established business with a complex revenue stream structure, meaning the mix of various project/service/product based work rendered the adoption of agile methods a challenge. Action research comprising three rounds of interventions was conducted to improve the organization and its quality culture while enforcing code review practices. Interventions resulted in a significant improvement in quality, as measured by reported defects. Therefore, it is suggested that agile methods are not sufficient on their own to take software business forward unless a quality-focused culture is simultaneously achieved through a mindset change and organizational structures to enforce quality practices. The paper contributes to research on the managerial practices of software business and agile transformation by providing empirical support to introducing formal quality improvement to the agile mix as a method for practitioners to improve organizations with complex business models and multiple teams. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	number = {1},
	journal = {Journal of Software: Evolution and Process},
	author = {Saarikallio, Matti and Tyrväinen, Pasi},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Computer software, Human resource management, Empirical, Agile adoptions, B2B, Business models, Development method, Hybrid development method, Increment planning event, Mixed business model, Quality, Revenue streams, Scaled agile, Team coordination},
	annote = {Cited by: 8; All Open Access, Green Open Access},
}

@article{nikiforova_towards_2023-2,
	title = {Towards {High}-{Value} {Datasets} {Determination} for {Data}-{Driven} {Development}: {A} {Systematic} {Literature} {Review}},
	volume = {14130 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172028920&doi=10.1007%2f978-3-031-41138-0_14&partnerID=40&md5=568da301604faa0be07050ee6fd28679},
	doi = {10.1007/978-3-031-41138-0_14},
	abstract = {Open government data (OGD) is seen as a political and socio-economic phenomenon that promises to promote civic engagement and stimulate public sector innovations in various areas of public life. To bring the expected benefits, data must be reused and transformed into value-added products or services. This, in turn, sets another precondition for data that are expected to not only be available and comply with open data principles, but also be of value, i.e., of interest for reuse by the end-user. This refers to the notion of “high-value dataset” (HVD), recognized by the European Data Portal as a key trend in the OGD area in 2022. While there is a progress in this direction, e.g., the Open Data Directive, incl. identifying 6 key categories, a list of HVDs and arrangements for their publication and re-use, they can be seen as “core”/“base” datasets aimed at increasing interoperability of public sector data with a high priority, contributing to the development of a more mature OGD initiative. Depending on the specifics of a region and country - geographical location, social, environmental, economic issues, cultural characteristics, (under)developed sectors and market specificities, more datasets can be recognized as of high value for a particular country. However, there is no standardized approach to assist chief data officers in this, and there is a clear lack of conceptualizations for the determination of HVD and systematic oversight. In this paper, we present a systematic review of existing literature on the HVD determination, which is expected to form an initial knowledge base for this process, including used approaches and indicators to determine them, data, stakeholders. © 2023, IFIP International Federation for Information Processing.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Nikiforova, Anastasija and Rizun, Nina and Ciesielska, Magdalena and Alexopoulos, Charalampos and Miletić, Andrea},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Data driven, Open Data, Open data ecosystem, Open datum, Open government data, Public values, Knowledge based systems, Public administration, High-value data, Public sector, Stakeholder, Value data},
	pages = {211 -- 229},
	annote = {Cited by: 5; All Open Access, Green Open Access},
}

@article{gabriele_human-car_2023-2,
	title = {Human-{Car} {Interface}: {A} {Systematic} {Literature} {Review}},
	volume = {449},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138724816&doi=10.1007%2f978-3-031-12547-8_50&partnerID=40&md5=3637be87138725a152cd4fa146cc5323},
	doi = {10.1007/978-3-031-12547-8_50},
	abstract = {A systematic literature review, or SLR, seeks to structure the review carried out in the defined areas in a replicable and auditable method, in order to facilitate and objectify both the search for answers to research questions and their accessibility by peers. In this study, we present an SLR carried out in November 2021 by the PRISMA method, on interaction and interface design focused on the automotive User Experience, having these three research questions: (RQ1) What are the objects of study of the articles? (RQ2) Which methods are used to analyze the object of study? (RQ3) What are the samples size of the surveys carried out? At the end of the Screening, 20 articles were selected to answer the research questions, and some data deserve attention, such as the 60\% that didn't identify the use of UX assessment questionnaires or the 35\% that had incomplete demographic data. We also saw that the objects of study are concentrated in 3 major areas and that the methodology used is, for the most part, similar in structure. The lack of studies carried out in South America prompted us to develop a research project focused on the Brazilian User. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Studies in Systems, Decision and Control},
	author = {Gabriele, Felipe and Martins, Laura},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Book chapter},
	pages = {631 -- 645},
	annote = {Cited by: 0},
}

@article{asdecker_dirty_2023-2,
	title = {A {Dirty} {Little} {Secret}? {Conducting} a {Systematic} {Literature} {Review} {Regarding} {Overstocks}},
	volume = {Part F1179},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165980153&doi=10.1007%2f978-3-031-38145-4_13&partnerID=40&md5=9d35ba41edd77488927f63ed1bf14f32},
	doi = {10.1007/978-3-031-38145-4_13},
	abstract = {Due to numerous media reports, overstocks in supply chains have recently attracted attention alongside the public sustainability debate. The goal of this paper is to aggregate the current body of knowledge and develop a better understanding regarding (1) the quantification of overstocks (what?), (2) the management approaches used (how?), and (3) the motives of managing overstocks (why?). The review synthesizes 48 relevant publications that were systematically gathered from three of the leading scientific databases. Based on the results of the review, a research agenda is derived that identifies ten particularly promising avenues for future investigations. Furthermore, the review shows that the existing knowledge about overstocks and the way they are managed is not only limited, but also very fragmented. A holistic perspective is missing, which motivates this paper to call for a conceptualization in the sense of an “overstock management” function. To initiate this process, a definition of the term is proposed. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Logistics},
	author = {Asdecker, Björn and Tscherner, Manette and Kurringer, Nikolas and Felch, Vanessa},
	year = {2023},
	note = {Publisher: Springer Science and Business Media B.V.
Type: Book chapter},
	pages = {229 -- 247},
	annote = {Cited by: 0},
}

@article{marques_gamification_2023-2,
	title = {Gamification for agile: a systematic literature review},
	volume = {16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161850209&doi=10.1504%2fIJASM.2023.130838&partnerID=40&md5=5bd2dad7d6ee9d8c774d0e1ed8fe714e},
	doi = {10.1504/IJASM.2023.130838},
	abstract = {Gamification has been used in software engineering to motivate practitioners to adopt agile. This study assesses the state of the art regarding the use of gamification in agile projects. A systematic literature review was followed by searching for peer-reviewed papers and dissertations on the topic and assessing their quality. Overall, 225 studies were found, but only 12 selected. Most studies focused on the Scrum framework, and the completion of stories/tasks was the practice subject to gamification more times. While the impact of gamification initiatives was positive, these studies lacked a proper empirical validation of the proposed gamification solutions. Despite the novelty of this field, there seems to be potential in the use of gamification to improve agile projects, but future studies should address the gaps identified in this analysis and provide more detail when reporting their results, namely regarding the discussion of the impact, benefits, and challenges of gamification. Copyright © 2023 Inderscience Enterprises Ltd.},
	number = {2},
	journal = {International Journal of Agile Systems and Management},
	author = {Marques, Rita and da Silva, Miguel Mira and Gonçalves, Daniel},
	year = {2023},
	note = {Publisher: Inderscience Publishers
Type: Review},
	pages = {226 -- 261},
	annote = {Cited by: 3},
}

@article{abdullah_controlling_2023-2,
	title = {Controlling {Automatic} {Experiment}-{Driven} {Systems} {Using} {Statistics} and {Machine} {Learning}},
	volume = {13928 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186763994&doi=10.1007%2f978-3-031-36889-9_9&partnerID=40&md5=9e42d9434fbe1e6f36858bb2983e5e75},
	doi = {10.1007/978-3-031-36889-9_9},
	abstract = {Experiments are used in many modern systems to optimize their operation. Such experiment-driven systems are used in various fields, such as web-based systems, smart-* systems, and various selfadaptive systems. There is a class of these systems that derive their data from running simulations or another type of computation, such as in digital twins, online planning using probabilistic model-checking, or performance benchmarking. To obtain statistically significant results, these systems must repeat the experiments multiple times. As a result, they consume extensive computation resources. The GraalVM benchmarking project detects performance changes in the GraalVM compiler. However, the benchmarking project has an extensive usage of computational resources and time. The doctoral research project proposed in this paper focuses on controlling the experiments with the goal of reducing computation costs. The plan is to use statistical and machine learning approaches to predict the outcomes of experiments and select the experiments yielding more useful information. As an evaluation, we are applying these methods to the GraalVM benchmarking project; the initial results confirm that these methods have the potential to significantly reduce computation costs. © The Author(s), under exclusive license to Springer Nature Switzerland AG 2023.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Abdullah, Milad},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Model checking, Machine learning, Machine-learning, Online systems, Cost reduction, Benchmarking, Computation costs, Driven system, Experiment-driven system, On-line planning, Running simulations, Self-adaptive system, Smart System, Statistics learning, Web-based system},
	pages = {105 -- 119},
	annote = {Cited by: 0},
}

@inproceedings{southier_systematic_2023-2,
	title = {Systematic {Mapping} {Review} on {Log} {Preparation} for {Process} {Mining}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164696308&doi=10.1109%2fCSCWD57460.2023.10152649&partnerID=40&md5=fff1faec33e5ffca044fa44095cb3bfa},
	doi = {10.1109/CSCWD57460.2023.10152649},
	abstract = {Process Mining (PM) is a research discipline that helps organizations track and optimize processes to support their business. Further, it focuses on providing process analysis techniques and tools, and several of its applications have been described in the literature. The start point for PM is using event logs generated by information systems to analyze processes. These event logs need to be extracted from databases and prepared for use because the quality of the event logs used as input is critical to the success of any PM effort. In this article, we present a systematic mapping review to provide the reader with highlights of the state-of-the-art techniques for event log preparation. Based on the retrieved studies, we identified six main categories of log preparation techniques: extraction, cleaning, repair, non-adequate granularity, quality evaluation, and privacy. The results are explored quantitatively and qualitatively. All results are made available through spreadsheets and charts. We believe this paper is a starting point for researchers to identify the studies that would help them prepare event logs for PM. © 2023 IEEE.},
	booktitle = {Proceedings of the 2023 26th {International} {Conference} on {Computer} {Supported} {Cooperative} {Work} in {Design}, {CSCWD} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Southier, Luiz Fernando Puttow and De Freitas, Sheila Cristiana and Pizzini, Adriano and Santos, Eduardo Alves Portela and Scalabrin, Edson Emilio},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Information systems, Information use, Mapping, It focus, Systematic mapping, Quality control, Data mining, Business Process, Business process management, Enterprise resource management, Event logs, Log preparation, Log preprocessing, Process analysis tool, Process management, Process mining},
	pages = {1619 -- 1624},
	annote = {Cited by: 0},
}

@inproceedings{jena_systematic_2023-2,
	title = {Systematic {Literature} {Review} on {Object} {Oriented} {Software} {Testing} {Techniques}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159096103&doi=10.1109%2fICIDCA56705.2023.10100236&partnerID=40&md5=bec544b3c28fab8be6514087f8c26751},
	doi = {10.1109/ICIDCA56705.2023.10100236},
	abstract = {The software industry is quickly adopting the object-oriented paradigm. OO is now widely recognized as the ideal paradigm for designing complex systems. Since flaws might be added over the course of the software development process, the finished result requires to be tested. Many software testing approaches available to test object-oriented software are thoroughly surveyed in this study. Because of principles like inheritance, polymorphism, and others, software built using object-oriented technology presents testing issues. It is crucial to trace where each object is formed and where that definition is referenced to monitor how each object behaves over its lifetime. This means that the capacity of test cases to uncover flaws is crucial to testing. Although the creation of test cases is essential for the testing process therefore it is the primary focus for study in the analysis of software testing. The entire process of thoroughly analyzing and categorizing primary studies took four months. The test cases' efficacy guarantees the system's quality while lowering the risk of system failure. Although there are several methods to approach this problem, it is obvious that a full study of the issue and a step-by-step resolution are needed. For various strategies utilized in the OO system, authors have examined and reviewed several articles. A complete study of the various OO-based Testing Techniques has been examined and reviewed by this methodical review of the literature. The recommendations made in the review can be used by future scholars to close the research gaps. It was discovered from this review that using the right software testing methods can lower the likelihood of system failure. © 2023 IEEE.},
	booktitle = {International {Conference} on {Innovative} {Data} {Communication} {Technologies} and {Application}, {ICIDCA} 2023 - {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Jena, Divya and Kumari, Ankita and Titoria, Jhanvi and {Ankita} and Rathee, Nisha and Kumar, Brijesh},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, Software design, Software testing, Software testings, Object oriented programming, Test case, Software industry, Biomimetics, Class testing, Nature inspired technique, Object-Oriented Software Testing, Software testing techniques, System failures, Testing technique},
	pages = {327 -- 333},
	annote = {Cited by: 2},
}

@article{iftikhar_catalog_2023-2,
	title = {A {Catalog} of {Source} {Code} {Metrics} – {A} {Tertiary} {Study}},
	volume = {472 LNBIP},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161231906&doi=10.1007%2f978-3-031-31488-9_5&partnerID=40&md5=e8e0766c41b52cfff333dbcc80900e58},
	doi = {10.1007/978-3-031-31488-9_5},
	abstract = {Context: A large number of source code metrics are reported in the literature. It is necessary to systematically collect, describe and classify source code metrics to support research and practice. Objective: We aim to utilize existing secondary studies to develop a catalog of source code metrics together with their descriptions. The catalog will also provide information about which units of code (e.g., operators, operands, lines of code, variables, parameters, code blocks, or functions) are used to measure the internal quality attributes and the scope on which they are collected. Method: We conducted a tertiary study to identify secondary studies reporting source code metrics. We have classified the source code metrics according to the measured internal quality attributes, the units of code used in the measures, and the scope at which the source code metrics are collected. Results: From 711 secondary studies, we identified 52 relevant secondary studies. We reported 423 source code metrics together with their descriptions and the internal quality attributes they measure. Source code metrics predominantly incorporate function as a unit of code to measure internal quality attributes. In contrast, several source code metrics use more than one unit of code when measuring internal quality attributes. Nearly 51\% of the source code metrics are collected at the class scope, while almost 12\% and 15\% of source code metrics are collected at module and application levels, respectively. Conclusions: Researchers and practitioners can use the extensive catalog to assess which source code metrics meet their individual needs based on the description and classification scheme presented. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Business Information Processing},
	author = {Iftikhar, Umar and Ali, Nauman Bin and Börstler, Jürgen and Usman, Muhammad},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Tertiary study, Codes (symbols), Code quality, Computer programming languages, Internal quality, Quality attributes, Source code metrics, Code measurement, Internal quality attribute, Line of codes, Number of sources, Variable-parameters},
	pages = {87 -- 106},
	annote = {Cited by: 1},
}

@inproceedings{adinegoro_comparison_2023-2,
	title = {Comparison of {UI}/{UX} {Development} {Using} {Design} {Thinking} vs {Lean} {UX} : {A} {Comparative} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85181532115&doi=10.1109%2fICE3IS59323.2023.10335225&partnerID=40&md5=4108ff0d37fc6ba7cb414bfaa977fe1c},
	doi = {10.1109/ICE3IS59323.2023.10335225},
	abstract = {The User Experience (UX) development method plays an important role in ensuring the successful design of the user experience of an application that is effective, satisfying and meets user needs. This study compares two popular methods that are often used in developing UX, namely Design Thinking and Lean UX. The purpose of this study is to evaluate the advantages, disadvantages and focus of each method in developing UX designs. The results of this study indicate that the Design Thinking method focuses more on understanding the user or is more user-centered, while Lean UX focuses more on flexibility and fast iteration as well as continuous experimentation or testing. Based on this, it is interesting to further investigate how a team determines which method to use in their UX design project, considering that both methods have their own strengths and weaknesses, and there are various factors that influence the UX design development stage, such as resources, time, project scale, and others. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 3rd {International} {Conference} on {Electronic} and {Electrical} {Engineering} and {Intelligent} {System}: {Responsible} {Technology} for {Sustainable} {Humanity}, {ICE3IS} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Adinegoro, Rafi and Suakanto, Sinung and Fakhrurroja, Hanif and Hardiyanti, Margareta},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Design, User interfaces, Users' experiences, Iterative methods, Design thinking, Development method, Comparatives studies, Design development, Design programs, Development stages, Lean UX, User experience, User need, User-centred, Well testing},
	pages = {147 -- 152},
	annote = {Cited by: 0},
}

@article{silva_energy_2023-2,
	title = {Energy awareness and energy efficiency in internet of things middleware: a systematic literature review},
	volume = {78},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85144045635&doi=10.1007%2fs12243-022-00936-5&partnerID=40&md5=33938e423af15f297f51ad01617f424d},
	doi = {10.1007/s12243-022-00936-5},
	abstract = {The Internet of Things (IoT) is characterized by a myriad of physical deices, together with high heterogeneity in both software and hardware. Middleware platforms have been proposed in order to alleviate such heterogeneity, providing relevant services and easing application development. In IoT systems, energy consumption is a key concern due to the proliferation of devices and their limited battery capacity. IoT middleware platforms can play an important role in providing applications with strategies, and support, for energy awareness and energy efficiency. Although there is a significant existing body of work related to IoT middleware, there is, as yet, no complementary overview of the state of the art on how these platforms can contribute to energy efficiency and energy awareness in IoT systems. This paper provides such an overview in the form of a systematic literature review (SLR). The SLR was carried out by following a systematic, rigorous procedure to search, select, and analyze primary studies available in the literature. Our corpus, as presented in this paper, is made up of twenty-two such studies, each presenting strategies and solutions on middleware support for energy efficiency and energy awareness in IoT systems. These strategies mainly focus on network adaptation, task offloading, and concrete implementations. However, most of these studies do not consider energy-aware/efficiency abstractions, and focus on solutions working at the end-user application side. In conclusion, this paper also raises relevant challenges and potential directions for further research resulting from the main SLR findings. © 2022, Institut Mines-Télécom and Springer Nature Switzerland AG.},
	number = {1-2},
	journal = {Annales des Telecommunications/Annals of Telecommunications},
	author = {Silva, Pedro Victor Borges Caldas da and Taconet, Chantal and Chabridon, Sophie and Conan, Denis and Cavalcante, Everton and Batista, Thais},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Review},
	keywords = {Systematic literature review, State of the art, Internet of things, Application development, Battery capacity, Energy efficiency, Energy utilization, Energy-awareness, Green computing, High heterogeneity, Middleware, Middleware platforms, Power management, Software and hardwares, System energy consumption, Work-related},
	pages = {115 -- 131},
	annote = {Cited by: 4; All Open Access, Green Open Access},
}

@article{li_reproducible_2023-2,
	title = {Reproducible {Searches} in {Systematic} {Reviews}: {An} {Evaluation} and {Guidelines}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85165868236&doi=10.1109%2fACCESS.2023.3299211&partnerID=40&md5=4c299fd1736b7dade073a0f83544d7d7},
	doi = {10.1109/ACCESS.2023.3299211},
	abstract = {[Context:] The Systematic Review is promoted as a more reliable way of producing a high-quality review of prior research. But there are a range of threats that can undermine the reliability and quality of such reviews. One threat is the reproducibility of automated searches. [Objectives:] To evaluate the state-of-practice of reproducible searches in secondary studies, and to consider ways to improve the reproducibility of searches. [Method:] We re-run the searches of 621 secondary studies and analyse the outcomes of those (attempted) re-runs. We use the outcomes, and our experience of re-running the searches, to propose ways to improve the reproducibility of automated searches. [Results:] With the 621 studies, more than 50\% of the literal search strings (ignoring other settings) are not reusable; about 87\% of the searches (e.g., with settings) cannot be repeated; and around 94\% of the searches (including all elements of the search) are irreproducible. We propose guidelines for automated search, directing particular attention at the formulation of search strings. [Conclusion:] While some aspects of automated search are beyond the direct control of researchers (e.g., variations in features, constraints and performance of search engines), many aspects can be effectively managed through more careful formulation and execution of the search strings themselves, and of the search settings. While the results of our evaluation are disappointing there are many simple, concrete steps that researchers can make to improve the reproducibility of their searches. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Li, Zheng and Rainer, Austen},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Systematic Review, Search engines, Evidence Based Software Engineering, Automation, Systematic, Quality control, Guideline, Software reliability, Reproducibilities, Automated searches, Reproducibility of result, Search problem, Secondary study},
	pages = {84048 -- 84060},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{silva_systematic_2023-2,
	title = {Systematic {Literature} {Review} of the {Use} of {Virtual} {Reality} in the {Inclusion} of {Children} with {Autism} {Spectrum} {Disorders} ({ASD})},
	volume = {14099 LNCS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172211707&doi=10.1007%2f978-3-031-40113-8_49&partnerID=40&md5=25bb4d5c9139fb13f06871578bff30cf},
	doi = {10.1007/978-3-031-40113-8_49},
	abstract = {Virtual reality (VR) technologies have been evolving in recent decades, allowing simulating real-life situations in controlled and safe virtual environments, where they reveal increasingly realistic details. There is an increase in the number of publications on virtual reality interventions in different areas, especially in Education, particularly in interventions with children diagnosed with Autism Spectrum Disorders (ASD). The lack of social skills prevents these children diagnosed with ASD to respond appropriately and adapt to the most diverse daily social situations. On this basis, VR has revealed a set of evidences that present promising results and show great acceptance among the diversified population with ASD. In order to understand how VR may contribute to the improvement of skills, allowing their inclusion, we conducted a systematic review of the literature. We present considerations on the selected studies, identifying the main gaps and pointing out possible directions for future research. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
	author = {Silva, Rui Manuel and Carvalho, Diana and Martins, Paulo and Rocha, Tânia},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Systematic literature review, Systematic Review, E-learning, Diseases, Virtual reality, Autism spectrum disorders, Children with autisms, Main Gap, Social skills, Virtual reality technology},
	pages = {501 -- 509},
	annote = {Cited by: 0},
}

@article{liu_information_2023-2,
	title = {Information quality of conversational agents in healthcare},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159121148&doi=10.1177%2f02666669231172434&partnerID=40&md5=0f4aa9701b5b8b1e355e46fa0ce52f4a},
	doi = {10.1177/02666669231172434},
	abstract = {Artificial Intelligence has found applications in a wide range of fields, including conversational agents designed for healthcare services. The quality of healthcare services greatly depends on the quality of the information provided by the agents. Achieving quality-assured information from conversational agents to support effective decision-making remains as a significant challenge in healthcare. Although prior review studies have shown an interest in investigating the information quality (IQ) of conversational agents in healthcare, no systematic review has been performed to present IQ definitions, factors influencing IQ, and IQ impacts. We conducted a systematic review of 45 articles published up to 2021 to investigate IQ definitions, factors influencing IQ, and IQ impacts in the context of conversational agents applied in healthcare. The findings of this review are integrated into a conceptual framework for the IQ research program in the context of conversational agents in healthcare, which has not been received attention in the literature, guiding future research directions. The present study also discusses implications for both researchers and practitioners to enhance the agents’ IQ and improve the quality of health-related services. © The Author(s) 2023.},
	journal = {Information Development},
	author = {Liu, Caihua and Zowghi, Didar and Peng, Guochao and Kong, Shufeng},
	year = {2023},
	note = {Publisher: SAGE Publications Ltd
Type: Review},
	annote = {Cited by: 1},
}

@inproceedings{de_souza_using_2023-2,
	title = {Using {Experimentation} to {Evaluate} {Security} {Requirements} in {IoT} {Software} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85184815056&doi=10.1109%2fSIoT60039.2023.10390013&partnerID=40&md5=470cbe05f85e03a3fc7e6a659e834c21},
	doi = {10.1109/SIoT60039.2023.10390013},
	abstract = {Security requirements are critical success factors for Internet of Things (IoT) software systems due to how they can mitigate vulnerabilities, for instance, prevent unauthorized access to system and device data by third parties, assuring the final quality of the software system. Then, problems related to security requirements and vulnerabilities must be addressed in the early stage of IoT development projects. In this way, Continuous Experimentation (CE) is a promising software construction practice to observe alternative security and vulnerability solutions. Thus, this paper evaluates security requirements based on the vulnerabilities of IoT software systems using such CE. First, we identified an evidence-based set of IoT vulnerability issues to be assessed. Thus, this work reports an exploratory study using CE to mitigate some vulnerabilities in IoT software systems, indicating that not all security requirements can be worked out with CE. Therefore, further studies are necessary to categorize the IoT software systems vulnerabilities that can be mitigated using continuous experimentation. © 2023 IEEE.},
	booktitle = {2023 {Symposium} on {Internet} of {Things}, {SIoT} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {De Souza, Bruno Pedraca and De Paiva, Bruno Dantas and Travassos, Guilherme Horta},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Computer software, Software-systems, Internet of things, Empirical Software Engineering, Cryptography, Development programmes, Device data, IS critical success factors, Security requirements, Security vulnerabilities, Third parties, Unauthorized access, Vulnerability},
	annote = {Cited by: 0},
}

@article{buchgeher_using_2023-2,
	title = {Using {Architecture} {Decision} {Records} in {Open} {Source} {Projects} - {An} {MSR} {Study} on {GitHub}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85162914500&doi=10.1109%2fACCESS.2023.3287654&partnerID=40&md5=04585b93cdd9a9d4124142acf3977fb7},
	doi = {10.1109/ACCESS.2023.3287654},
	abstract = {Architecture decision records (ADRs) have been proposed as a resource-efficient means for capturing architectural design decisions (ADDs), and have received attention not only from researchers but also from practitioners. We conducted a mining software repositories (MSR) study, in which we analyzed the use of ADRs in open source repositories at GitHub. Our results show that the adoption of ADRs is still low, although the number of repositories using ADRs is increasing every year. About 50\% of all repositories with ADRs contain just one to five ADRs suggesting that the concept has been tried but not yet definitively adopted. In repositories that use ADRs more systematically, we observed that recording decisions is a team activity conducted by two or more users over a longer period of time. In most repositories the template proposed by Michael Nygrad is used. We, finally, provide an interpretation of the obtained results and discuss open future research challenges by elaborating on implications of the study's findings as well as on recommendations on how to further increase the adoption of ADRs. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Buchgeher, Georg and Schoberl, Stefan and Geist, Verena and Dorninger, Bernhard and Haindl, Philipp and Weinreich, Rainer},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Decision making, Software design, Open source software, Knowledge management, Software architecture, Open systems, Open-source softwares, Mining software, Mining software repository, Software development management, Software repositories, Secondary study, Architecture decision record, Architecture decisions, Github, Open source projects, Software architecture knowledge managements},
	pages = {63725 -- 63740},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{alshahwan_software_2023-2,
	title = {Software {Testing} {Research} {Challenges}: {An} {Industrial} {Perspective}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161898000&doi=10.1109%2fICST57152.2023.00008&partnerID=40&md5=1d3e066d49cf0d1adbddebd59727ba72},
	doi = {10.1109/ICST57152.2023.00008},
	abstract = {There have been rapid recent developments in automated software test design, repair and program improvement. Advances in artificial intelligence also have great potential impact to tackle software testing research problems. In this paper we highlight open research problems and challenges from an industrial perspective. This perspective draws on our experience at Meta Platforms, which has been actively involved in software testing research and development for approximately a decade. As we set out here, there are many exciting opportunities for software testing research to achieve the widest and deepest impact on software practice. With this overview of the research landscape from an industrial perspective, we aim to stimulate further interest in the deployment of software testing research. We hope to be able to collaborate with the scientific community on some of these research challenges. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 {IEEE} 16th {International} {Conference} on {Software} {Testing}, {Verification} and {Validation}, {ICST} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Alshahwan, Nadia and Harman, Mark and Marginean, Alexandru},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Artificial intelligence, Software testing, Automation, Repair, Industrial research, Software testings, Automated program repair, Automated remediation, Automated software engineering, Genetic improvements, Research challenges, Research problems, Test designs, Test projects, Test repair},
	pages = {1 -- 10},
	annote = {Cited by: 2},
}

@inproceedings{karras_divide_2023-2,
	title = {Divide and {Conquer} the {EmpiRE}: {A} {Community}-{Maintainable} {Knowledge} {Graph} of {Empirical} {Research} in {Requirements} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85178666312&doi=10.1109%2fESEM56168.2023.10304795&partnerID=40&md5=89b0f483a3169d34b96c76308bbd4fe1},
	doi = {10.1109/ESEM56168.2023.10304795},
	abstract = {[Background.] Empirical research in requirements engineering (RE) is a constantly evolving topic, with a growing number of publications. Several papers address this topic using literature reviews to provide a snapshot of its 'current' state and evolution. However, these papers have never built on or updated earlier ones, resulting in overlap and redundancy. The underlying problem is the unavailability of data from earlier works. Researchers need technical infrastructures to conduct sustainable literature reviews. [Aims.] We examine the use of the Open Research Knowledge Graph (ORKG) as such an infrastructure to build and publish an initial Knowledge Graph of Empirical research in RE (KG-EmpiRE) whose data is openly available. Our long-term goal is to continuously maintain KG-EmpiRE with the research community to synthesize a comprehensive, up-to-date, and long-term available overview of the state and evolution of empirical research in RE. [Method.] We conduct a literature review using the ORKG to build and publish KG-EmpiRE which we evaluate against competency questions derived from a published vision of empirical research in software (requirements) engineering for 2020-2025. [Results.] From 570 papers of the IEEE International Requirements Engineering Conference (2000-2022), we extract and analyze data on the reported empirical research and answer 16 out of 77 competency questions. These answers show a positive development towards the vision, but also the need for future improvements. [Conclusions.] The ORKG is a ready-to-use and advanced infrastructure to organize data from literature reviews as knowledge graphs. The resulting knowledge graphs make the data openly available and maintainable by research communities, enabling sustainable literature reviews. © 2023 IEEE.},
	booktitle = {International {Symposium} on {Empirical} {Software} {Engineering} and {Measurement}},
	publisher = {IEEE Computer Society},
	author = {Karras, Oliver and Wernlein, Felix and Klunder, Jil and Auer, Soren},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Literature reviews, Requirement engineering, Requirements engineering, 'current, Empirical research, Research communities, Divide-and-conquer, Infrastructure, Knowledge graph, Knowledge graphs, Long-term goals, Technical infrastructure},
	annote = {Cited by: 1; All Open Access, Green Open Access},
}

@inproceedings{olsson_all_2023-2,
	title = {All data is equal or is some data more equal? {On} strategic data collection and use in the embedded systems domain},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183326861&doi=10.1109%2fSEAA60479.2023.00056&partnerID=40&md5=0daed7f6ec9ae9aa3afd9e4c555cb505},
	doi = {10.1109/SEAA60479.2023.00056},
	abstract = {Effective collection and use of data is key for companies across domains and it is only increasing in importance. For companies in the embedded systems domain, data constitutes the basis not only for quality assurance and diagnostics of their systems but also for new service development and innovation. For these companies, data is an enabler for continuous delivery of customer value and hence, a key asset for entirely new and recurring revenue streams. However, effective use of data requires careful collection of different kinds of data depending on the purpose and context for which it is intended to be used. In this paper, we identify the challenges that companies experience in their contemporary data practices and we outline the kinds of data that companies need to collect as they evolve through different maturity stages. In addition, we provide concrete guidance on the specific data to collect during each maturity stage. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 49th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications}, {SEAA} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Olsson, Helena Holmstrom and Bosch, Jan},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Embedded systems, Embedded-system, Quality assurance, Data acquisition, Data collection, Data collectio, Data exploitatio, Data practices, Maturity stages, New service development, Service innovation, Strategic data, System domain},
	pages = {319 -- 327},
	annote = {Cited by: 0},
}

@article{suzanna_continuous_2023-1,
	title = {Continuous {Software} {Engineering} for {Augmented} {Reality}},
	volume = {14},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168799327&doi=10.14569%2fIJACSA.2023.0140719&partnerID=40&md5=783a913267516c101f5c106d990de433},
	doi = {10.14569/IJACSA.2023.0140719},
	abstract = {Continuous software engineering is a new trend that has attracted increasing attention from the research community in recent years. In software engineering there are “continuous” stages that are used depending on the number of artifact repositories such as databases, meta data, virtual machines, networks and servers, various logs, and reports. Augmented Reality (AR) technology is currently growing rapidly. We can find this technology in various fields of life, but unfortunately sustainable software engineering for Augmented Reality is not found. The method shown in previous research is a general method in software engineering so that a theory is needed for sustainable software engineering for AR considering that AR is not just an ordinary application but there are 3D elements and specific components that must be met so that it can be called AR. The main idea behind this research is to find a continuous pattern from the stages of the existing method so far. For example, in general the stages of system development are planning, analysis, design, implementation and maintenance. Then after the application has been built, does it finish there? As we know software always grows and develops according to human needs. Therefore, there are continuous stages that must be patterned so that the life cycle process can be maintained. In this paper we present our initial findings about the continuous stages of continuous software engineering namely continuous planning, continuous analysis, continuous design, continuous programming, continuous integration, and continuous maintenance. © 2023, Science and Information Organization. All Rights Reserved.},
	number = {7},
	journal = {International Journal of Advanced Computer Science and Applications},
	author = {{Suzanna} and {Sasmoko} and Gaol, Ford Lumban and Oktavia, Tanty},
	year = {2023},
	note = {Publisher: Science and Information Organization
Type: Article},
	keywords = {Life cycle, Application programs, Augmented reality, Continuous integrations, Continuous software engineerings, Research communities, Computer programming, Continuous analysis, Continuous design, Continuous maintenance, Continuous planning, Continuous programming, Maintenance, Method in software engineering, Sustainable softwares},
	pages = {174 -- 181},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@article{stradowski_exploring_2023-2,
	title = {Exploring the challenges in software testing of the {5G} system at {Nokia}: {A} survey},
	volume = {153},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85138479890&doi=10.1016%2fj.infsof.2022.107067&partnerID=40&md5=0095b98ee47f2ca332b69f85235a5d54},
	doi = {10.1016/j.infsof.2022.107067},
	abstract = {Context: The ever-growing size and complexity of industrial software products pose significant quality assurance challenges to engineering researchers and practitioners, despite the constant effort to increase knowledge and improve the processes. 5G technology developed by Nokia is one example of such a grand and highly complex system with improvement potential. Objective: The following paper provides an overview of the current quality assurance processes used by Nokia to develop the 5G technology and provides insight into the most prominent challenges by an evaluation of perceived importance, urgency, and difficulty to understand the future opportunities. Method: Nokia mode of operation, briefly introduced in this paper, has been subjected to extensive analysis by a selected group of experienced test-oriented professionals to define the most critical areas of concern. Secondly, the identified problems were evaluated by Nokia gNB system-level test professionals in a dedicated survey. Results: The questionnaire was completed by 312 out of 2935 (10.63\%) possible respondents. The challenges are seen as the most important and urgent: customer scenario testing, performance testing, and competence ramp-up. Challenges seen as the most difficult to solve are low occurrence failures, hidden feature dependencies, and hardware configuration-specific problems. Conclusions: Our research identified several improvement areas in the quality assurance processes used to develop the 5G technology by determining the most important and urgent problems that at the same time have a low perceived difficulty. Such initiatives are attractive from a business perspective. On the other hand, challenges seen as the most impactful yet difficult may be of interest to the academic research community. © 2022 The Author(s)},
	journal = {Information and Software Technology},
	author = {Stradowski, Szymon and Madeyski, Lech},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Software testing, Computer software selection and evaluation, Quality control, 5G mobile communication systems, Surveys, Quality assurance, \% reductions, 5g technology, Efficiency improvement, Engineering challenges, Quality assurance process, Software engineering challenge, Software quality assurance, System level testing, Test effort reduction, Test efforts},
	annote = {Cited by: 10; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@book{scutari_pragmatic_2023-2,
	title = {The {Pragmatic} {Programmer} for {Machine} {Learning}: {Engineering} {Analytics} and {Data} {Science} {Solutions}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183218511&doi=10.1201%2f9780429292835&partnerID=40&md5=8d4692d72420d4b30ed26d6a7d295edb},
	abstract = {Machine learning has redefined the way we work with data and is increasingly becoming an indispensable part of everyday life. The Pragmatic Programmer for Machine Learning: Engineering Analytics and Data Science Solutions discusses how modern software engineering practices are part of this revolution both conceptually and in practical applictions. Comprising a broad overview of how to design machine learning pipelines as well as the state-of-the-art tools we use to make them, this book provides a multi-disciplinary view of how traditional software engineering can be adapted to and integrated with the workflows of domain experts and probabilistic models. From choosing the right hardware to designing effective pipelines architectures and adopting software development best practices, this guide will appeal to machine learning and data science specialists, whilst also laying out key high-level principlesin a way that is approachable for students of computer science and aspiring programmers. © 2023 Marco Scutari and Mauro Malvestio.},
	publisher = {CRC Press},
	author = {Scutari, Marco and Malvestio, Mauro},
	year = {2023},
	doi = {10.1201/9780429292835},
	note = {Publication Title: The Pragmatic Programmer for Machine Learning: Engineering Analytics and Data Science Solutions
Type: Book},
	annote = {Cited by: 0},
}

@inproceedings{denecke_investigating_2023-2,
	title = {Investigating conversational agents in healthcare: {Application} of a technical-oriented taxonomy},
	volume = {219},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85164261105&doi=10.1016%2fj.procs.2023.01.413&partnerID=40&md5=1ec35be7bce132042a4312cfb84fbe07},
	doi = {10.1016/j.procs.2023.01.413},
	abstract = {Conversational agents (CA) are increasingly applied to realize health applications that collect patient data, provide information or even deliver health interventions. We developed a taxonomy focusing on technical characteristics of health CA with the purpose of creating a reporting guideline towards health CA and of building technical-oriented archetypes. The taxonomy comprises 18 dimensions which can be grouped into four perspectives. In this work, we wanted to find out whether the taxonomy is complete and can be applied appropriately by researcher to describe the technical characteristics of their health CA. Through a literature review, we identified 103 unique health CA for which publications have been published in 2021 and 2022. We contacted the corresponding or first authors of those papers asking for providing the information along our taxonomy for the CA described in their paper. For this purpose, our taxonomy was transformed into a questionnaire. To study applicability and understandability of the taxonomy, we also extracted the requested information from the papers using the taxonomy and compared the results to those of the participants. 95 E-Mails could be delivered. 26 persons out of 95 replied to our request resulting in a return rate of 27.3\%. Results show that the majority of CA is simple in terms of CA personality; visualized as avatar or without embodiment. Systems are mainly rule-based, domain-specific and support one language. We recognized several differences between replies given by the participants and what has been extracted from the publications on the CA by us. We conclude that in order to apply the taxonomy as reporting guideline clear definitions must be given for the single characteristics. Some additional characteristics have to be added. © 2023 Elsevier B.V.. All rights reserved.},
	booktitle = {Procedia {Computer} {Science}},
	publisher = {Elsevier B.V.},
	author = {Denecke, Kerstin and May, Richard},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Literature reviews, Taxonomies, Chatbots, Understandability, Conversational agents, Health care application, Health chatbot, Health interventions, Hospital data processing, Patient data, Patient treatment, Simple++, Technology},
	pages = {1289 -- 1296},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{lagos_electric_2023-2,
	title = {Electric {Vehicles} and the {Use} of {Demand} {Projection} {Models}: {A} {Systematic} {Mapping} of {Studies}; [{Vehículos} eléctricos y el uso de modelos de proyección de demanda: un mapeo sistemático de estudios]},
	volume = {43},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145909076&doi=10.15446%2fing.investig.99251&partnerID=40&md5=185c4ab25d57790fba1e7cc50593cd3d},
	doi = {10.15446/ing.investig.99251},
	abstract = {In today’s world, electric vehicles have become a real solution to the problem of pollution caused by petrol and diesel-powered vehicles. However, incorporating them successfully into the global vehicle park poses new challenges. Some of these challenges have to do with meeting the electricity demand, providing the physical installations for charging, and the size and capacity of the electric grid required to deliver the necessary supply. Solving these new problems requires determining or projecting the electrical and/or physical requirements involved, but there is no single model or methodology to do this, nor any single document which summarizes the existing information. To address this situation, this work presents the result of a systematic mapping study that seeks to provide organized information about the (mathematical) models for the demand arising from electric vehicles, as well as to answer a series of questions posed for this research. The results obtained show that there is a wide variety of models used to determine demand requirements –of either physical or electrical elements– in which mathematical modelling and operations research tools are normally used. Other results indicate that demand models are mainly focused on the electrical requirements rather than on physical ones, and that, in most cases, the type of vehicle for which the demand is studied is not mentioned. © Universidad Nacional de Colombia.},
	number = {1},
	journal = {Ingenieria e Investigacion},
	author = {Lagos, Dafne and Mancilla, Rodrigo and Reinecke, Carolina and Leal, Paola},
	year = {2023},
	note = {Publisher: Universidad Nacional de Colombia
Type: Article},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@article{szabo_user-centered_2023-2,
	title = {User-centered approaches in software development processes: {Qualitative} research into the practice of {Hungarian} companies},
	volume = {35},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85137081694&doi=10.1002%2fsmr.2501&partnerID=40&md5=e408c37e01d89f1dc78aa027550e4269},
	doi = {10.1002/smr.2501},
	abstract = {Integrating user-centered approaches into development processes is one of the main challenges nowadays that derives from different objectives of software engineering (SE) and human-computer interaction (HCI) fields. For SE experts, the main goal is quality code creation, whereas for HCI professionals, it is the continuous product interaction with the users. The major question is what tools and timings can be used together to achieve these goals effectively. Therefore, this article provides comparative, exploratory, and qualitative research about possible solutions on how practitioners transfer HCI values and practices to SE processes. The current practice of software companies was studied by conducting interviews on a sample of 13 Hungarian Information Technology companies to explore the SE processes in respect of several dimensions (applied development models, the integrity of user-centered methods, and the user experience [UX] maturity). According to preliminary expectations, the development processes of the various companies proceed in different steps; nevertheless, they can be well grouped together based on the UX methods applied. The results representing the various user-centered processes can be considered useful for future decision makers of software companies worldwide. © 2022 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	number = {2},
	journal = {Journal of Software: Evolution and Process},
	author = {Szabó, Bálint and Hercegfi, Károly},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Development process, Decision making, Software design, Computer software, Human computer interaction, Users' experiences, Software process, Surveys, Interview, Hungarians, Qualitative research, User experience (UX), User-centered approach, UX maturity, UX method},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
}

@article{ciesla_analysis_2023-2,
	title = {{AN} {ANALYSIS} {OF} {THE} {IMPLEMENTATION} {OF} {ACCESSIBILITY} {TOOLS} {ON} {WEBSITES}; [{ANALIZA} {IMPLEMENTACJI} {NARZĘDZI} {DOSTĘPNOŚCI} {NA} {STRONACH} {WWW}]},
	volume = {13},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180501621&doi=10.35784%2fiapgos.4459&partnerID=40&md5=a147574e2d2512c8186ffed285bd14dc},
	doi = {10.35784/iapgos.4459},
	abstract = {The websites of higher education institutions, due to the fact that they are addressed to multiple stakeholder groups, not only need to have an appropriately designed information structure but must also be useful. Additionally, in the case of public universities, their services are expected to be accessible to the widest possible audience, especially for people with disabilities. The accessibility tools used on websites should be quickly located, easily identifiable and user-friendly. So far, no standards have been developed regarding these issues, and therefore, there are various solutions on the web. The objective of this study is to analyze various implementations of accessibility tools on university websites in terms of their location, form of presentation and ways that enable access to them. A study was conducted in which web interfaces were evaluated with the participation of users. The experiment consisted of two parts: the first one used the eye tracking technique, whereas in the second one, a survey was conducted. The research material was prototypes of websites from four different universities. Each website had two versions differing in implementation of accessibility tools. In the study, 35 participants were divided into two groups of people. Each group was shown one of the two sets of website prototypes and the users were tasked with finding and activating a specific accessibility tool. After exploring the websites, each participant completed a questionnaire that pertained to their opinions regarding aspects such as appearance, placement and a way to access tools dedicated to people with disabilities. The obtained data, processed to the form of heatmaps and fixation maps, were subjected to a qualitative analysis. The survey results and eye tracking data were analyzed quantitatively. On the basis of performed analyzes it can be concluded that the following factors have an impact on the reduction in efficiency and productivity of users: placement of accessibility tools on university websites in a place other than the upper right corner, an indirect access to t hese tools or their non-standard appearance. © 2023, Politechnika Lubelska. All rights reserved.},
	number = {4},
	journal = {Informatyka, Automatyka, Pomiary w Gospodarce i Ochronie Srodowiska},
	author = {Cieśla, Marcin and Dzieńkowski, Mariusz},
	year = {2023},
	note = {Publisher: Politechnika Lubelska
Type: Article},
	pages = {51 -- 56},
	annote = {Cited by: 0; All Open Access, Gold Open Access},
}

@inproceedings{ansyah_usability_2023-2,
	title = {Usability {Testing} of {User} {Experience} and {User} {Interface} {Design} on {Mobile} {Map} {Applications}: {A} {Comparative} {Study} of {User} {Perception} and {Interaction}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180361940&doi=10.1109%2fICTS58770.2023.10330882&partnerID=40&md5=76aa72c4a0d6b18a15a12a75a0cdd186},
	doi = {10.1109/ICTS58770.2023.10330882},
	abstract = {Map applications play a crucial role in everyday life, assisting users in finding locations and optimizing travel routes. This study aims to evaluate the usability of map applications, focusing on UX testing using GOMS and SUS techniques and UI testing using the A/B Testing method. The results show the effectiveness of the three applications (GMaps, Petal, and Waze) in completing the given tasks. GMaps demonstrated the fastest task completion time and the highest SUS score, followed by Petal and Waze. GMaps' advantage may be due to most respondents being accustomed to using Android and the native applications of this operating system. Furthermore, A/B Testing of UI elements revealed a nearly balanced preference between GMaps and Petal, indicating Petal's potential despite not being an Android-native application. Based on these findings, the development of map applications with good usability can be achieved by combining the strengths of each tested application. © 2023 IEEE.},
	booktitle = {2023 14th {International} {Conference} on {Information} and {Communication} {Technology} and {System}, {ICTS} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Ansyah, Adi Surya Suwardi and Masruri, Muhammad Zahid and Rochimah, Siti},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {User interfaces, Users' experiences, Usability engineering, Comparatives studies, Android (operating system), GOMS, Mobile-map application, SUS, Travel routes, Usability testing, User interaction, User interface designs, User perceptions},
	pages = {7 -- 12},
	annote = {Cited by: 0},
}

@article{pizard_assessing_2023-2,
	title = {Assessing attitudes towards evidence-based software engineering in a government agency},
	volume = {154},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85141479463&doi=10.1016%2fj.infsof.2022.107101&partnerID=40&md5=958be07a4396062f45df6bce9c1d9292},
	doi = {10.1016/j.infsof.2022.107101},
	abstract = {Context: Evidence-based practice (EBP) has allowed several disciplines to become more mature by emphasizing the use of evidence from well-designed and well-conducted research in decision-making. Its application in SE, Evidence-based software engineering (EBSE) can help to bridge the gap between academia and industry by bringing together academic rigor and research of practical relevance. To achieve this, it seems necessary to improve its adoption. Objective: We sought both to study the attitudes towards EBSE of stakeholders working in a government agency (GA) and to assess whether knowledge of EBSE would impact their working practices. Method: We conducted a multi-stage field investigation in an Uruguayan national GA that is responsible for digital policies. First, we organized an EBSE awareness lecture and we collected and analyzed participants’ perceptions of the value and limitations of EBSE. Sixteen months later, in a second stage, we contacted the agency and asked participants whether they had made use of the information about EBSE we presented to them. Results: Initially, participants reported that EBSE seemed useful for tackling challenging problems and, in particular, considered its use appropriate given the agency's responsibilities. Perceived barriers to EBSE adoption were the need for institutional support, the lack of government practice reports, inadequate skills or motivation, the cost of conducting systematic reviews, and the lack of evidence about emerging issues. In the follow-up survey, although the participants were not undertaking systematic reviews themselves, many reported improvements in how they searched for and evaluated information to support their work. Conclusion: Our study presents some insights to better understand EBSE adoption. With the exception of GA-specific issues, perceived value and barriers to adoption were consistent with those reported in software engineering and other disciplines. Our follow-up study confirms the potential value of evidence in the context of IT regulatory and government bodies. © 2022 Elsevier B.V.},
	journal = {Information and Software Technology},
	author = {Pizard, Sebastián and Acerenza, Fernando and Vallespir, Diego and Kitchenham, Barbara},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Article},
	keywords = {Systematic Review, Decision making, Decisions makings, Application programs, Search engines, Evidence Based Software Engineering, Surveys, Focus groups, Bridges, Evidence-based practices, Field investigation, Follow-up Studies, Government agencies, ITS applications, Working practices},
	annote = {Cited by: 2},
}

@article{kantsepolsky_exploring_2023-2,
	title = {Exploring {Quantum} {Sensing} {Potential} for {Systems} {Applications}},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85151562593&doi=10.1109%2fACCESS.2023.3262506&partnerID=40&md5=65f96769350f4d726502220c429403ef},
	doi = {10.1109/ACCESS.2023.3262506},
	abstract = {The current rise of quantum technology is compelled by quantum sensing research. Thousands of research labs are developing and testing a broad range of sensor prototypes. However, there is a lack of knowledge about specific applications and real-world use cases where the benefits of these sensors will be most pronounced. This study presents a comprehensive review of quantum sensing state-of-practice. It also provides a detailed analysis of how quantum sensing overcomes the existing limitations of sensor-driven systems' precision and performance. Based on the review of over 500 quantum sensor prototype reports, we determined four groups of quantum sensors and discussed their readiness for commercial usage. We concluded that quantum magnetometry and quantum optics are the most advanced sensing technologies with empirically proven results. In turn, quantum timing and kinetics are still in the early stages of practical validation. In addition, we defined four systems domains in which quantum sensors offer a solution for existing limitations of conventional sensing technologies. These domains are 1) GPS-free positioning and navigating services, 2) time-based operations, 3) topological visibility, and 4) environment detection, prediction, and modeling. Finally, we discussed the current constraints of quantum sensing technologies and offered directions for future research. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Kantsepolsky, Boris and Aviv, Itzhak and Weitzfeld, Roye and Bordo, Eliyahu},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Quantum optics, Current rise, Extraterrestrial measurements, Magnetometers, Magnetometry, Quantum information science, Quantum sensing, Quantum sensing technology, Quantum sensors, Quantum system, Sensing technology, Sensitivity, Superconducting magnets, System applications, Temperature measurement},
	pages = {31569 -- 31582},
	annote = {Cited by: 7; All Open Access, Gold Open Access},
}

@article{danilovaite_acoustic_2023-2,
	title = {Acoustic {Analysis} for {Vocal} {Fold} {Assessment}—{Challenges}, {Trends}, and {Opportunities}},
	volume = {1084},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85149948719&doi=10.1007%2f978-3-031-24453-7_8&partnerID=40&md5=f4509bb5138c74c510919fefff4baa37},
	doi = {10.1007/978-3-031-24453-7_8},
	abstract = {The goal of this study was a review of trends in non-invasive vocal fold assessment to identify the significance of acoustic analysis within the scope of proposed methods. A review protocol for selected relevant studies was developed using systematic review guidelines. A classification scheme was applied to process the selected relevant study set, data were extracted and mapped in a systematic map. A systematic map was used to synthesize data for a quantitative summary of the main research question. A tabulated summary was created to summarize supporting topics. Results show that non-invasive vocal fold assessment is influenced by general computer science trends. Machine learning techniques dominate studies and publications, i.e., 51\% of the set used at least one method to detect and classify vocal fold pathologies. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Studies in Computational Intelligence},
	author = {Danilovaitė, Monika and Tamulevičius, Gintautas},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Book chapter},
	pages = {147 -- 166},
	annote = {Cited by: 0},
}

@inproceedings{fischer_becoming_2023-2,
	title = {Becoming a {Data}-{Driven} {Organization}: {A} {Comparative} {Case} {Study} on {Digital} {Transformation} {Strategies}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192539148&partnerID=40&md5=1db34add5835badee7f963d868850262},
	abstract = {In today’s data-centric era, organizations increasingly aim to operate more data-driven and therefore engage in digital transformations toward becoming a data-driven organization (DDO). To govern such transformations, top managers develop digital transformation strategies (DTS) characterized by different organizational ambidexterity approaches. This study analyzes how such DTS influence the process and (intermediate) outcomes of organizations’ digital transformations toward becoming a DDO by studying two organizations undertaking such DDO transformations using the concept of organizational ambidexterity as a theoretical lens. On this empirical basis, we find that DTS characterized by different organizational ambidexterity approaches lead to different transformation processes and (intermediate) outcomes. Thereby, this study contributes to existing academic literature in the field of DDOs and DTS, as such transformation journeys toward becoming a DDO have not been studied in its entirety yet. Furthermore, our paper offers practical guidance for top managers to develop and implement a DTS suitable for their organization. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	booktitle = {International {Conference} on {Information} {Systems}, {ICIS} 2023: "{Rising} like a {Phoenix}: {Emerging} from the {Pandemic} and {Reshaping} {Human} {Endeavors} with {Digital} {Technologies}"},
	publisher = {Association for Information Systems},
	author = {Fischer, Hannes},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Digital transformation, Metadata, Information systems, Information use, Case-studies, Data driven, Organisational, Data centric, Data-driven organization, Digital transformation strategy, Organizational ambidexterity, Top managers},
	annote = {Cited by: 0},
}

@inproceedings{pauzi_descriptive_2023-2,
	title = {From {Descriptive} to {Predictive}: {Forecasting} {Emerging} {Research} {Areas} in {Software} {Traceability} {Using} {NLP} from {Systematic} {Studies}},
	volume = {2023-April},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160566966&doi=10.5220%2f0011964100003464&partnerID=40&md5=f46987b89f92b2b7189014ceb75f3cad},
	doi = {10.5220/0011964100003464},
	abstract = {Systematic literature reviews (SLRs) and systematic mapping studies (SMSs) are common studies in any discipline to describe and classify past works, and to inform a research field of potential new areas of investigation. This last task is typically achieved by observing gaps in past works, and hinting at the possibility of future research in those gaps. Using an NLP-driven methodology, this paper proposes a meta-analysis to extend current systematic methodologies of literature reviews and mapping studies. Our work leverages a Word2Vec model, pre-trained in the software engineering domain, and is combined with a time series analysis. Our aim is to forecast future trajectories of research outlined in systematic studies, rather than just describing them. Using the same dataset from our own previous mapping study, we were able to go beyond descriptively analysing the data that we gathered, or to barely 'guess' future directions. In this paper, we show how recent advancements in the field of our SMS, and the use of time series, enabled us to forecast future trends in the same field. Our proposed methodology sets a precedent for exploring the potential of language models coupled with time series in the context of systematically reviewing the literature. Copyright © 2023 by SCITEPRESS - Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	booktitle = {International {Conference} on {Evaluation} of {Novel} {Approaches} to {Software} {Engineering}, {ENASE} - {Proceedings}},
	publisher = {Science and Technology Publications, Lda},
	author = {Pauzi, Zaki and Capiluppi, Andrea},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software engineering, Systematic Review, Mapping, Systematic mapping studies, Natural language processing systems, Natural languages, Forecasting, Mapping studies, Language processing, Natural language processing, Research areas, Engineering research, Software traceability, Systematic study, Time series analysis, Times series},
	pages = {538 -- 545},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{chadbourne_applications_2023-2,
	title = {Applications of {Causality} and {Causal} {Inference} in {Software} {Engineering}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85168770994&doi=10.1109%2fSERA57763.2023.10197835&partnerID=40&md5=4585f442f1486a0ef77c40af2d43d737},
	doi = {10.1109/SERA57763.2023.10197835},
	abstract = {Causal inference is a study of causal relationships between events and the statistical study of inferring these relationships through interventions and other statistical techniques. Causal reasoning is any line of work toward determining causal relationships, including causal inference. This paper explores the relationship between causal reasoning and various fields of software engineering. This paper aims to uncover which software engineering fields are currently benefiting from the study of causal inference and causal reasoning, as well as which aspects of various problems are best addressed using this methodology. With this information, this paper also aims to find future subjects and fields that would benefit from this form of reasoning and to provide that information to future researchers. This paper follows a systematic literature review, including; the formulation of a search query, inclusion and exclusion criteria of the search results, clarifying questions answered by the found literature, and synthesizing the results from the literature review. Through close examination of the 45 found papers relevant to the research questions, it was revealed that the majority of causal reasoning as related to software engineering is related to testing through root cause localization. Furthermore, most causal reasoning is done informally through an exploratory process of forming a Causality Graph as opposed to strict statistical analysis or introduction of interventions. Finally, causal reasoning is also used as a justification for many tools intended to make the software more human-readable by providing additional causal information to logging processes or modeling languages. © 2023 IEEE.},
	booktitle = {Proceedings - 2023 {IEEE}/{ACIS} 21st {International} {Conference} on {Software} {Engineering} {Research}, {Management} and {Applications}, {SERA} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Chadbourne, Patrick and Eisty, Nasir U.},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, Software testing, Application programs, Modeling languages, Inclusion and exclusions, Causal inferences, Causal reasoning, Causal relationships, Causality graph, Engineering fields, Professional aspects, Search queries, Statistical study, Statistical techniques},
	pages = {47 -- 52},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@inproceedings{mayr_unified_2023-2,
	title = {Unified {Theory} of {Acceptance} and {Use} of {Technology} ({UTAUT}) for {Intelligent} {Process} {Automation}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192543634&partnerID=40&md5=c1660d81fb09bfe882968ce8d157b22d},
	abstract = {Intelligent process automation is a technological innovation that combines symbolic automation tools with machine learning. Intelligent process automation can automate complex tasks that otherwise have to be performed by humans when symbolic automation is not powerful enough. Regardless of the high economic potential for companies, the adoption rate in practice is comparatively low. This could be due to the adoption behavior of the employees. In our work, we iteratively develop a Unified Theory of Acceptance and use of Technology (UTAUT) model for the adoption of intelligent process automation and evaluate it with an empirical study. With our research we want to empower designers to adapt the corresponding tools in the future to increase adoption. The study shows that, in addition to established factors for technology adoption, trust, transparency, and attitude towards technology are primary decision factors. © 2023 International Conference on Information Systems, ICIS 2023: "Rising like a Phoenix: Emerging from the Pandemic and Reshaping Hu. All Rights Reserved.},
	booktitle = {International {Conference} on {Information} {Systems}, {ICIS} 2023: "{Rising} like a {Phoenix}: {Emerging} from the {Pandemic} and {Reshaping} {Human} {Endeavors} with {Digital} {Technologies}"},
	publisher = {Association for Information Systems},
	author = {Mayr, Alexander and Stahmann, Philip and Nebel, Maximilian and Janiesch, Christian},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Information systems, Information use, Technological innovation, Automation, Machine-learning, Technology adoption, Empirical studies, Process control, Adoption behavior, Automation tools, Complex task, Economic potentials, Intelligent process automation, The unified theory of acceptance and use of technology(UTAUT)},
	annote = {Cited by: 0},
}

@article{pinciroli_modeling_2023-2,
	title = {Modeling more software performance antipatterns in cyber-physical systems},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85180204421&doi=10.1007%2fs10270-023-01137-x&partnerID=40&md5=4d0a00e3145240ff506af59818085186},
	doi = {10.1007/s10270-023-01137-x},
	abstract = {The design of cyber-physical systems (CPS) is challenging due to the heterogeneity of software and hardware components that operate in uncertain environments (e.g., fluctuating workloads), hence they are prone to performance issues. Software performance antipatterns could be a key means to tackle this challenge since they recognize design problems that may lead to unacceptable system performance. This manuscript focuses on modeling and analyzing a variegate set of software performance antipatterns with the goal of quantifying their performance impact on CPS. Starting from the specification of eight software performance antipatterns, we build a baseline queuing network performance model that is properly extended to account for the corresponding bad practices. The approach is applied to a CPS consisting of a network of sensors and experimental results show that performance degradation can be traced back to software performance antipatterns. Sensitivity analysis investigates the peculiar characteristics of antipatterns, such as the frequency of checking the status of resources, that provides quantitative information to software designers to help them identify potential performance problems and their root causes. Quantifying the performance impact of antipatterns on CPS paves the way for future work enabling the automated refactoring of systems to remove these bad practices. © 2023, The Author(s).},
	journal = {Software and Systems Modeling},
	author = {Pinciroli, Riccardo and Smith, Connie U. and Trubiani, Catia},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Embedded systems, Cybe-physical systems, Cyber Physical System, Cyber-physical systems, Sensitivity analysis, Performances analysis, Anti-patterns, Model-based OPC, Model-based performance analyse, Performance impact, Software modeling, Software performance, Software performance antipattern},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@article{zervogianni_user-based_2023-2,
	title = {A user-based information rating scale to evaluate the design of technology-based supports for autism},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159588062&doi=10.1007%2fs10209-023-00995-y&partnerID=40&md5=0737802f2d1107ce78721c9b82cf6935},
	doi = {10.1007/s10209-023-00995-y},
	abstract = {The present study aimed to merge expertise from evidence-based practice and user-centered design to develop a rating scale for considering user input and other sources of information about end-users in studies reporting on the design of technology-based support for autism. We conducted a systematic review of the relevant literature to test the reliability and validity of the scale. The scale demonstrated acceptable reliability and validity based on a randomized sample of 211 studies extracted from the output of the systematic review. The scale can help provide a more complete assessment of the quality of the design process of technology-based supports for autism and be beneficial to autistic people, their families, and related professionals in making informed decisions regarding such supports. © 2023, The Author(s).},
	journal = {Universal Access in the Information Society},
	author = {Zervogianni, Vanessa and Fletcher-Watson, Sue and Herrera, Gerardo and Goodwin, Matthew S. and Triquell, Elise and Pérez-Fuster, Patricia and Brosnan, Mark and Grynszpan, Ouriel},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Article},
	keywords = {Software engineering, Systematic Review, Evidence Based Software Engineering, Diseases, Evidence-based practices, Technology-based, User centered design, Autism, Digital technologies, Rating scale, Reliability and validity, Sources of informations, User input},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{garcia_advances_2023-2,
	title = {Advances in {Web} {API} testing: {A} {Systematic} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85192841057&doi=10.1109%2fENC60556.2023.10508648&partnerID=40&md5=2ce84324710ed395808ffc09f45b2a83},
	doi = {10.1109/ENC60556.2023.10508648},
	abstract = {Web APIs serving as an intermediary for communication between distributed systems has increased recently. It has become critical to test these APIs to ensure their functionality and quality thoroughly. This study aims to systematically map the literature to analyze the techniques, methods, artifacts, and strategies employed during the testing phase of web APIs. Utilizing a systematic mapping study approach (SMS), we identified 42 studies that outlined various tests applicable to these APIs. Further, our analysis uncovered numerous methods, techniques, and strategies. Types of test artifacts such as the API specification, test cases, or test matrices were also found. Finally, testing activities were identified through approaches presented by each study's authors, in which test results were applied and analyzed. The findings will establish the basis for the development of a testing guide, which in turn will support professionals who need to test APIs and who lack specific knowledge on how to carry out this activity due to the lack of standards. © 2023 IEEE.},
	booktitle = {2023 {Mexican} {International} {Conference} on {Computer} {Science}, {ENC} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Garcia, Josue Capistran and Hernández, Jorge Octavio Ocharán and Arriaga, Juan Carlos Peréz and Riaño, Hector Javier Limón},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic Review, Software testing, Mapping, Systematic mapping studies, Software testings, Testing process, Test case, Web API, Quality assurance, API specifications, Distributed systems, Specification test, Testing phase},
	annote = {Cited by: 0},
}

@article{hanna_web_2023-2,
	title = {Web applications testing techniques: a systematic mapping study},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85153523073&doi=10.1504%2fIJWET.2022.129250&partnerID=40&md5=a26478fab291606ae9d407737eef2aaa},
	doi = {10.1504/IJWET.2022.129250},
	abstract = {Due to the importance of web application testing techniques for detecting faults and assessing quality attributes, many research papers were published in this field. For this reason, it became essential to analyse, classify and summarise the research in the field. To achieve this goal, this research conducted a systematic mapping study on 98 research papers in the field of web applications testing published between 2008 and 2021. The results showed that the most commonly used web applications testing techniques in literature are model-based testing and security testing. Besides, the most commonly used models in model-based testing are finite-state machines. The most targeted vulnerability in security testing is SQL injection. Test automation is the most targeted testing goal in both model-based and security testing. For other web applications testing techniques, the main goals of testing were test automation, test coverage, and assessing security quality attributes. Copyright © 2022 Inderscience Enterprises Ltd.},
	number = {4},
	journal = {International Journal of Web Engineering and Technology},
	author = {Hanna, Samer and Ahmad, Amro Al-Said},
	year = {2023},
	note = {Publisher: Inderscience Publishers
Type: Article},
	keywords = {Model checking, Software testing, Mapping, Systematic mapping studies, Automation, Model based testing, Test-coverage, Testing technique, Security testing, SMS, Test Automation, Testing purpose, Web application testing, Web application testing technique},
	pages = {372 -- 412},
	annote = {Cited by: 0; All Open Access, Green Open Access},
}

@inproceedings{sukmandhani_recent_2023-2,
	title = {Recent {Trends} for {Text} {Summarization} in {Scientific} {Documents}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85186658985&doi=10.1109%2fICCED60214.2023.10425025&partnerID=40&md5=ee6395ea43c82ca2bb7a21bae1fc9482},
	doi = {10.1109/ICCED60214.2023.10425025},
	abstract = {Text summarization is a natural language processing (NLP) technique in artificial intelligence that has been studied in recent years. Every document containing text is tested to get a good summary result. In producing a good summary, proper accuracy is required; therefore, various techniques and methods are used, and another challenge is the use of language in the documents being tested. Various types of documents have been used as research for text summarization, including scientific documents, so researchers are interested in studying them more intensely to get the state of the art in research. This research was conducted to review previous research using literature studies from various publications that published research on text summarization in scientific documents. The researcher collected data from several publishers who had published this type of research from 1988 to 2023 quarter 1 and obtained three hundred and eleven articles, then the researcher analyzed the data using the Kitchenham method to get the right literature to answer the research question. Thirty-seven articles were selected for further in-depth analysis. In addition, the researcher found some interesting findings beyond the research question, including other techniques used to solve the text summarization problem and other types of output that are not commonly produced so that it can increase knowledge in researching text summarization. © 2023 IEEE.},
	booktitle = {2023 {IEEE} 9th {International} {Conference} on {Computing}, {Engineering} and {Design}, {ICCED} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Sukmandhani, Arief Agus and Arifin, Yulyani and Zarlis, Muhammad and Budiharto, Widodo},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {State of the art, Natural language processing systems, Natural languages, Literature studies, Research questions, Recent trends, In-depth analysis, Kitchenham, Language processing techniques, Scientific documents, Text processing, Text Summarisation},
	annote = {Cited by: 0},
}

@article{kitchenham_how_2023-2,
	title = {How {Should} {Software} {Engineering} {Secondary} {Studies} {Include} {Grey} {Material}?},
	volume = {49},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85128295733&doi=10.1109%2fTSE.2022.3165938&partnerID=40&md5=c83dc9fa989c40ab6aa2d2b6f0c71aa3},
	doi = {10.1109/TSE.2022.3165938},
	abstract = {Context: Recent papers have proposed the use of grey literature (GL) and multivocal reviews. These papers have raised issues about the practices used for systematic reviews (SRs) in software engineering (SE) and suggested that there should be changes to the current SR guidelines. Objective: To investigate whether current SR guidelines need to be changed to support GL and multivocal reviews. Method: We discuss the definitions of GL and the importance of GL and of industry-based field studies in SE SRs. We identify properties of SRs that constrain the material used in SRs: a) the nature of primary studies; b) the requirements of SRs to be auditable, traceable, and reproducible; and explain why these requirements restrict the use of blogs in SRs. Results: SR guidelines have always considered GL as a possible source of primary studies and have never supported exclusion of field studies that incorporate the practitioners' viewpoint. However, the concept of GL, which was meant to refer to documents that were not formally published, is now being extended to information from sources such as blogs/tweets/Q\&A posts. Thus, it might seem that SRs do not make full use of GL because they do not include such information. However, the unit of analysis for an SR is the primary study. Thus, it is not the source but the type of information that is important. Any report describing a rigorous empirical evaluation is a candidate primary study. Whether it is actually included in an SR depends on the SR eligibility criteria. However, any study that cannot be guaranteed to be publicly available in the long term should not be used as a primary study in an SR. This does not prevent such information from being aggregated in surveys of social media and used in the context of evidence-based software engineering (EBSE). Conclusions: Current guidelines for SRs do not require extensions, but their scope needs to be better defined. SE researchers require guidelines for analysing social media posts (e.g., blogs, tweets, vlogs), but these should be based on qualitative primary (not secondary) study guidelines. SE researchers can use mixed-methods SRs and/or the fourth step of EBSE to incorporate findings from social media surveys with those from SRs and to develop industry-relevant recommendations. © 1976-2012 IEEE.},
	number = {2},
	journal = {IEEE Transactions on Software Engineering},
	author = {Kitchenham, Barbara and Madeyski, Lech and Budgen, David},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Software engineering, Systematic Review, Systematic mapping studies, Social networking (online), Evidence Based Software Engineering, Social sciences computing, Systematic, Guideline, Surveys, Mixed method, Mixed-method review, Grey literature, Blogs, Government, Multivocal review},
	pages = {872 -- 882},
	annote = {Cited by: 6; All Open Access, Green Open Access, Hybrid Gold Open Access},
}

@inproceedings{malik_chess_2023-2,
	title = {{CHESS}: {A} {Framework} for {Evaluation} of {Self}-{Adaptive} {Systems} {Based} on {Chaos} {Engineering}},
	volume = {2023-May},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166300952&doi=10.1109%2fSEAMS59076.2023.00033&partnerID=40&md5=320addb57c1eb68c0bc46ef60c86add5},
	doi = {10.1109/SEAMS59076.2023.00033},
	abstract = {There is an increasing need to assess the correct behavior of self-adaptive and self-healing systems due to their adoption in critical and highly dynamic environments. However, there is a lack of systematic evaluation methods for self-adaptive and self-healing systems. We proposed CHESS, a novel approach to address this gap by evaluating self-adaptive and self-healing systems through fault injection based on chaos engineering (CE).The artifact presented in this paper provides an extensive overview of the use of CHESS through two microservice-based case studies: a smart office case study and an existing demo application called Yelb. It comes with a managing system service, a self-monitoring service, as well as five fault injection scenarios covering infrastructure faults and functional faults. Each of these components can be easily extended or replaced to adopt the CHESS approach to a new case study, help explore its promises and limitations, and identify directions for future research. © 2023 IEEE.},
	booktitle = {{ICSE} {Workshop} on {Software} {Engineering} for {Adaptive} and {Self}-{Managing} {Systems}},
	publisher = {IEEE Computer Society},
	author = {Malik, Sehrish and Naqvi, Moeen Ali and Moonen, Leon},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Software testing, Case-studies, Adaptive systems, Self-adaptive system, Artifact, Chaos engineerings, Dynamic environments, Evaluation, Fault injection, Resilience, Self-healing, Self-healing materials, Self-healing systems},
	pages = {195 -- 201},
	annote = {Cited by: 0; All Open Access, Bronze Open Access, Green Open Access},
}

@article{nunes_public_2023-2,
	title = {Public {Policies} for {Renewable} {Energy}: {A} {Review} of the {Perspectives} for a {Circular} {Economy}},
	volume = {16},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85145769847&doi=10.3390%2fen16010485&partnerID=40&md5=69befd59d2f418f67bae9cf3954f42a8},
	doi = {10.3390/en16010485},
	abstract = {The development and implementation of public policies towards renewable energies are crucial in order to address the contemporary challenges faced by humanity. The 3Rs (reduce, reuse, and recycle), as a circular economic practice, are often cited as one of the best solutions for sustainable development. Therefore, this study analyzed public policies for renewable energy from the perspective of the circular economy. Accordingly, a systematic review of the literature was carried out with respect to the beneficiaries and convergences of circularities, with a focus on public policies for renewable energies. The sample had public policies classified into three types (distributive, redistributive, and regulatory policies). The results showed that the first studies began in 1999, with a significant increase in publications during the 2010s, in which Germany was the country with the greatest contribution. The analyses associated with space showed the countries committed to the use of renewable energies and the 3Rs of the circular economy to reduce greenhouse gas emissions. The economic analyses revealed that the circular economy for the generation of renewable energy has a positive economic return in terms of social well-being and the mitigation of environmental degradation. There is a barrier to the circular economy’s development posed by the cost of its implementation in the private sector and the resistance to raising awareness in society, requiring strong public sector engagement in decision making and the constant evaluation of public policies. It is concluded that the circular economy facilitates more efficient, productive structures and public policies, promoting alternatives for energy security and sustainability for the world energy matrix. © 2023 by the authors.},
	number = {1},
	journal = {Energies},
	author = {Nunes, Anna Manuella Melo and Coelho Junior, Luiz Moreira and Abrahão, Raphael and Santos Júnior, Edvaldo Pereira and Simioni, Flávio José and Rotella Junior, Paulo and Rocha, Luiz Célio Souza},
	year = {2023},
	note = {Publisher: MDPI
Type: Review},
	keywords = {Systematic Review, Decision making, Reuse, Sustainable development, Economic analysis, Bibliometric, Bio economy, Circular economy, Circularity, Classifieds, Clean energy, Energy policy, Energy security, Energy transitions, Gas emissions, Greenhouse gases, Renewable energies},
	annote = {Cited by: 7; All Open Access, Gold Open Access},
}

@article{garcia-mireles_profile_2023-2,
	title = {A {Profile} of {Practices} for {Reporting} {Systematic} {Reviews}: {A} {Conference} {Case}},
	volume = {576 LNNS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85142725070&doi=10.1007%2f978-3-031-20322-0_3&partnerID=40&md5=cf245e893be2c48982af13f33f268194},
	doi = {10.1007/978-3-031-20322-0_3},
	abstract = {Several criticisms about the quality of reporting systematic reviews (SRs) have been published and new guidelines propose to use standardized instruments to report them. To identify practices for reporting SRs, I reviewed 32 SRs published in the International Conference on Software Process Improvement (CIMPS). Well reported practices are related to the execution of the automatic database search process and the identification of selection criteria. However, issues arise related to the completeness of the search process, procedures for dealing with inconsistencies during selection, extraction, and classification of data. Besides, validity threats only are addressed by a third of SRs. As a conclusion, the identification of reporting practices can help SR authors to identify both strengths and opportunities areas for conducting and reporting SRs. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Lecture Notes in Networks and Systems},
	author = {García-Mireles, Gabriel Alberto},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	pages = {34 -- 49},
	annote = {Cited by: 0},
}

@article{fathullah_methodological_2023-2,
	title = {Methodological {Investigation}: {Traditional} and {Systematic} {Reviews} as {Preliminary} {Findings} for {Delphi} {Technique}},
	volume = {22},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85166663606&doi=10.1177%2f16094069231190747&partnerID=40&md5=d2ca2c58a0c1b475d56ad690f25c431c},
	doi = {10.1177/16094069231190747},
	abstract = {The Delphi method has been used as a way to reach consensus among experts established in the 1950s. The method was originally conceived to be used as a forecasting instrument for business in a mixed-method study. The Delphi method is usually conducted with elements of anonymity, iteration, controlled feedback and statistical group response. Delphi method had traditionally used expert opinions through methods such as interviews and brainstorming for the initial point of studies conducted. However, with the expanding volumes of research papers and articles in this age, can techniques such as Traditional Literature Review (TLR) and Systematic Literature Review (SLR) also be viable technique to be an initial point of studies for a Delphi study. As such this study aims 1) to examine and discover whether TLR and SLR are appropriate techniques to be used in a Delphi study and 2) construct a Delphi study process overview. This study adopts a methodology in which seven articles that have used TLR and SLR in their Delphi study will be analyzed. The results shows that TLR and SLR are appropriate techniques to be used in a Delphi study and that there are two types of processes to incorporate them into the study. We have also constructed a Delphi process overview through our analysis. We hope that the results of this study will be able help researchers who are interested in doing a Delphi study to know on what are the benefits of TLR and SLR in these studies along with how to incorporate them. © The Author(s) 2023.},
	journal = {International Journal of Qualitative Methods},
	author = {Fathullah, Muhammad Afif and Subbarao, Anusuyah and Muthaiyah, Saravanan},
	year = {2023},
	note = {Publisher: SAGE Publications Inc.
Type: Article},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@article{ramos_gutierrez_when_2023-2,
	title = {When business processes meet complex events in logistics: {A} systematic mapping study},
	volume = {144},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85139865511&doi=10.1016%2fj.compind.2022.103788&partnerID=40&md5=2a8944612f473febd716772e7f8b2474},
	doi = {10.1016/j.compind.2022.103788},
	abstract = {Logistics processes are attracting growing attention because of the globalisation of the market. Its growing complexity and the need for reducing costs have provoked the seek of new solutions based on the processing of the complex events that the business processes produce. Event-Driven Business Process Management (EDBPM) is a discipline that studies the integration of business processes and complex events. The analysis of the maturity level of the approaches and gaps to point out future lines of research could help not only logistics organisations, but also academia. Logistics organisation could benefit from producing more environmentally friendly and optimal solutions in transport, and academia could benefit from revealing open problems. Thus, this study aims to identify current approaches, frameworks, and tools that integrate business processes and complex events in the logistics domain. To do so, we follow a systematic approach to do a mapping study that captures and synthesises the approaches, frameworks, and tools that integrate these two fields. As a result, 10,978 articles were gathered and 169 of them were selected for extraction. We have classified the selected studies according to several criteria, including the business process life cycle in which they are being applied, the business process modelling language, and the event process modelling language, among others. Our synthesis reveals the open challenges and the most relevant frameworks and tools. However, there is no mature enough framework or tool ready to be used in companies, and a promising research must provide solutions that cover all phases in the process life cycle. © 2022 The Author(s)},
	journal = {Computers in Industry},
	author = {Ramos Gutiérrez, Belén and Reina Quintero, Antonia M. and Parody, Luisa and Gómez López, María Teresa},
	year = {2023},
	note = {Publisher: Elsevier B.V.
Type: Review},
	keywords = {Systems engineering, Systematic literature review, Life cycle, Mapping, Systematic mapping studies, Modeling languages, Business Process, Enterprise resource management, Administrative data processing, Complex event processing, Complex events, Event Processing, Event-driven, Event-driven business process, Process engineering, Process life cycles},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{gomes_engagement_2023-2,
	title = {Engagement, {Participation}, and {Liveness}: {Understanding} {Audience} {Interaction} in {Technology}-{Based} {Events}},
	volume = {2},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85160820251&doi=10.5220%2f0011848600003467&partnerID=40&md5=c623e829f3cc5063272a4783ea66b092},
	doi = {10.5220/0011848600003467},
	abstract = {Technologies have been changing how the audience participates in different events. This participation is distinct in each type of event. For example, in educational settings, polls with clickers and word clouds are usually used to involve the audience. For music festivals and other musical performances, organizers opt out of providing led sticks, necklaces and wristbands. Different uses for the smartphones, such as using them as lanterns aiming at obtaining crowd effect, are other ordinary and spontaneous ways of interaction. Recently, more research has been published in journals and scientific conferences discussing the use of these technologies, with techniques for fostering interaction and collaboration. Therefore, we conducted a literature review using forward and backward snowballing, looking for articles about how researchers use new technologies to increase audience experience in different contexts of events and what concepts are raised from that perspective. As a result, we propose a taxonomy of those concepts related to audience experience through three lenses: engagement, participation, and liveness. Copyright © 2023 by SCITEPRESS – Science and Technology Publications, Lda. Under CC license (CC BY-NC-ND 4.0)},
	booktitle = {International {Conference} on {Enterprise} {Information} {Systems}, {ICEIS} - {Proceedings}},
	publisher = {Science and Technology Publications, Lda},
	author = {Gomes, Genildo and Conte, Tayana and Castro, Thaís and Gadelha, Bruno},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Engagement, Technology-based, Audience interaction, Audience participation, Educational settings, Event, Liveness, Music, Musical performance, Smart phones, Word clouds},
	pages = {264 -- 275},
	annote = {Cited by: 0; All Open Access, Hybrid Gold Open Access},
}

@inproceedings{lopez-tenorio_comparing_2023-2,
	title = {Comparing {Machine} {Learning} for {SQL} {Injection} {Detection} in {Web} {Systems}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85188434335&doi=10.1109%2fISCMI59957.2023.10458664&partnerID=40&md5=568c3adc519c4cd0bac1461be4d6ecc9},
	doi = {10.1109/ISCMI59957.2023.10458664},
	abstract = {This work analyzes the machine learning techniques most used in SQL injection (SQLi) detection in order to make a comparison in terms of precision, as well as characterize the data with which the models for SQLi detection are generated. For the analysis, a systematic literature review is developed to extract the data reported from the state-of-the-art. A total of 31 primary studies are selected, of which 22 address the analysis and exploring ML techniques for SQLi detection; 20 conduct experiments to test the models in terms of performance and accuracy; and 14 explore the characteristics of the data with which ML models are prepared. In 22 of the 31 papers, 5 ML algorithms for classification problems stand out: Decision Tree, K-Nearest Neighbors, Naive Bayes, Random Forest, and Support Vector Machine. Decision Tree is the most used algorithm for detecting SQLi, appearing in 18 of 31 papers. The t-student test is applied for samples of unequal variances. The results demonstrate a marginal difference between techniques, although Random Forest is one of the techniques with the greatest consistency in accuracy. © 2023 IEEE.},
	booktitle = {2023 10th {International} {Conference} on {Soft} {Computing} and {Machine} {Intelligence}, {ISCMI} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Lopez-Tenorio, Brandom and Dominguez-Isidro, Saul and Cortes-Verdin, Maria Karen and Perez-Arriaga, Juan Carlos},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Systematic literature review, State of the art, Performance, Learning systems, Machine-learning, Decision trees, Support vector machines, Machine learning techniques, Nearest neighbor search, Nearest-neighbour, Random forests, SQL injection, Web system, Work analysis},
	pages = {17 -- 21},
	annote = {Cited by: 0},
}

@inproceedings{martinez_fault_2023-2,
	title = {Fault {Tree} {Analysis} and {Failure} {Modes} and {Effects} {Analysis} for {Systems} with {Artificial} {Intelligence}: {A} {Mapping} {Study}},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85183473600&doi=10.1109%2fICSRS59833.2023.10381456&partnerID=40&md5=090510ddac7703ca68c93042075ad314},
	doi = {10.1109/ICSRS59833.2023.10381456},
	abstract = {Reliability engineering has well-established analysis techniques to design critical systems that will be safe to operate. Two already field-proven techniques are the FTA (Fault Tree Analysis) and the FMECA (Failure Modes, Effects, and Criticality Analysis). These techniques, recommended or required by several supervisory authorities or independent assessments, will continue to be the main assets for the analysis of potential failures and faults. This mapping study revisits FMECA and FTA from the perspective of how they are used when dealing with systems with Artificial Intelligence (AI) components. After the literature database search and selection, 24 primary sources were leveraged to map them regarding their context, scope, considerations, and maturity. The diversity of safety-critical application domains and functions, and the need of more evidences from the evaluations of the proposed approaches, suggest that this field requires a pressing attention. The extracted considerations can be relevant elements of industrial guidelines. © 2023 IEEE.},
	booktitle = {2023 7th {International} {Conference} on {System} {Reliability} and {Safety}, {ICSRS} 2023},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Martinez, Jabier and Eguia, Alexander and Urretavizcaya, Imanol and Amparan, Estibaliz and Negro López, Pablo},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Artificial intelligence, Mapping, Search engines, Mapping studies, Analysis techniques, Critical systems, Failure (mechanical), Failure mode and effects analysis, Failure modes, Failure modes effects and criticality analysis, Fault tree analyses (FTA), Fault tree analysis, FMEA, Independent assessment, Reliability (engineering), Reliability analysis, Safety factor, Supervisory authority},
	pages = {464 -- 473},
	annote = {Cited by: 0},
}

@article{trieflinger_elevating_2023-2,
	title = {Elevating {Software} {Quality} {Through} {Product} {Discovery} {Techniques}: {Key} {Findings} from a {Grey} {Literature} {Review}},
	volume = {1871 CCIS},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85172412219&doi=10.1007%2f978-3-031-43703-8_4&partnerID=40&md5=90c881cce119c4f33fbb22896447dde3},
	doi = {10.1007/978-3-031-43703-8_4},
	abstract = {In the era of digital transformation, the notion of software quality transcends its traditional boundaries, necessitating an expansion to encompass the realms of value creation for customers and the business. Merely optimizing technical aspects of software quality can result in diminishing returns. Product discovery techniques can be seen as a powerful mechanism for crafting products that align with an expanded concept of quality—one that incorporates value creation. Previous research has shown that companies struggle to determine appropriate product discovery techniques for generating, validating, and prioritizing ideas for new products or features to ensure they meet the needs and desires of the customers and the business. For this reason, we conducted a grey literature review to identify various techniques for product discovery. First, the article provides an overview of different techniques and assesses how frequently they are mentioned in the literature review. Second, we mapped these techniques to an existing product discovery process from previous research to provide concrete guidelines for establishing product discovery in their organizations. The analysis shows, among other things, the increasing importance of techniques to structure the problem exploration process and the product strategy process. The results are interpreted regarding the importance of the techniques to practical applications and recognizable trends. © 2023, The Author(s), under exclusive license to Springer Nature Switzerland AG.},
	journal = {Communications in Computer and Information Science},
	author = {Trieflinger, Stefan and Weiss, Lukas and Münch, Jürgen},
	year = {2023},
	note = {Publisher: Springer Science and Business Media Deutschland GmbH
Type: Conference paper},
	keywords = {Digital transformation, Product management, Literature reviews, Users' experiences, Computer software selection and evaluation, Product discovery, Software Quality, Grey literature, Technical aspects, Traditional boundaries, Value creation},
	pages = {45 -- 59},
	annote = {Cited by: 0},
}

@article{usman_quality_2023-2,
	title = {A {Quality} {Assessment} {Instrument} for {Systematic} {Literature} {Reviews} in {Software} {Engineering}},
	volume = {17},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85152967598&doi=10.37190%2fe-Inf230105&partnerID=40&md5=66143efbcfdadca0a7c250438942e4e3},
	doi = {10.37190/e-Inf230105},
	abstract = {Background: Systematic literature reviews (SLRs) have become a standard practice as part of software engineering (SE) research, although their quality varies. To build on the reviews, both for future research and industry practice, they need to be of high quality. Aim: To assess the quality of SLRs in SE, we put forward an appraisal instrument for SLRs. Method: A well-established appraisal instrument from research in healthcare was used as a starting point to develop the instrument. It is adapted to SE using guidelines, checklists, and experiences from SE. The first version was reviewed by four external experts on SLRs in SE and updated based on their feedback. To demonstrate its use, the updated version was also used by the authors to assess a sample of six selected systematic literature studies. Results: The outcome of the research is an appraisal instrument for quality assessment of SLRs in SE. The instrument includes 15 items with different options to capture the quality. The instrument also supports consolidating the items into groups, which are then used to assess the overall quality of an SLR. Conclusion: The presented instrument may be helpful support for an appraiser in assessing the quality of SLRs in SE. © 2023 The Authors. Published by Wrocław University of Science and Technology Publishing House.},
	number = {1},
	journal = {E-Informatica Software Engineering Journal},
	author = {Usman, Muhammad and Ali, Nauman Bin and Wohlin, Claes},
	year = {2023},
	note = {Publisher: Wroclaw University of Science and Technology
Type: Article},
	keywords = {Software engineering, Systematic literature review, Systematic Review, Tertiary study, Industrial research, Software engineering research, Quality assessment, AMSTAR 2, Assessment instruments, Critical appraisal, Industry practices, Standard practices},
	annote = {Cited by: 1; All Open Access, Gold Open Access},
}

@inproceedings{silva_are_2023-2,
	title = {Are safety-critical systems really survivable to attacks?},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85161903777&doi=10.1109%2fSysCon53073.2023.10131114&partnerID=40&md5=f78a5de334951fc6c0a773986807fa84},
	doi = {10.1109/SysCon53073.2023.10131114},
	abstract = {Safety-Critical Systems (SCS) stand for those systems designed to tackle events that could potentially cause human injury or loss of life, significant property damage, financial loss, or damage to the environment, among others. Modern SCS are in transport, infrastructure, medicine, nuclear engineering, recreation, and many other fields. Nevertheless, SCS are prone to various attacks aiming to explore their inherent complexity and broad attack surface to jeopardize essential services and assets. Survivability is key to protecting SCS through integrating preventive, reactive, and tolerant defenses. This paper stands out by analyzing the survivability aspects of SCS under attack through a systematic literature review of recently published articles. Based on the review, we devise a classification to separate the studies that focus on survivability for SCS from those that deal with related aspects, such as resistance, recognition, or tolerance to attacks. Further, we expose literature limitations indicating why there is still no guaranteed survivability of SCS in the presence of attacks. © 2023 IEEE.},
	booktitle = {{SysCon} 2023 - 17th {Annual} {IEEE} {International} {Systems} {Conference}, {Proceedings}},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {Silva, Helber and Vieira, Marco and Neto, Augusto},
	year = {2023},
	note = {Type: Conference paper},
	keywords = {Safety engineering, Systematic literature review, Safety critical systems, Security systems, Attack, Financial loss, Human injury, Loss of life, Losses, Mission critical systems, Property damage, Survivability, Transport infrastructure},
	annote = {Cited by: 1},
}

@article{silva_extended_2023-2,
	title = {Extended {Remote} {Laboratories}: {A} {Systematic} {Review} of the {Literature} {From} 2000 to 2022},
	volume = {11},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85159716536&doi=10.1109%2fACCESS.2023.3271524&partnerID=40&md5=13a741dc197bddbc5b16f6dd86a3df80},
	doi = {10.1109/ACCESS.2023.3271524},
	abstract = {Remote Laboratories (RLs) break barriers in education since they provide real experimentation anytime and anywhere. However, their greatest drawback is the lack of immersion. However, Extended Reality (XR) can overcome this shortcoming through the integration of AR, or VR techniques, into remote experimentation, thus developing Extended Reality Remote Laboratories (XRLs), which can be Augmented Reality Remote Laboratories (ARLs), or Virtual Reality Remote Laboratories (VRLs). Our study consists in a systematic review concerning the state-of-the-art of XRLs during the period 2000-2022. Findings from the systematic review generated two main results. First, a thorough analysis that reports: a timeline of publications, studies per country, most influential universities, most popular journals and conferences, most cited publications, description of ARLs, description of VRLs, and most notable publications. Secondly, a classification of the XRLs encountered during research, and a proposed architecture for creating XRLs, in order to guide developers wishing to integrate XR into their experiments. © 2013 IEEE.},
	journal = {IEEE Access},
	author = {Silva, Isabela Nardi Da and Garcia-Zubia, Javier and Hernandez-Jayo, Unai and Alves, Joao Bosco Da Mota},
	year = {2023},
	note = {Publisher: Institute of Electrical and Electronics Engineers Inc.
Type: Article},
	keywords = {Systematic Review, Augmented reality, Virtual reality, Extended reality, Laboratories, Metaverses, Pandemic, Remote experimentation, Remote laboratories, STEM, X reality},
	pages = {94780 -- 94804},
	annote = {Cited by: 3; All Open Access, Gold Open Access},
}

@inproceedings{de_souza_framework_2022,
	title = {A {Framework} {Model} to {Support} {A}/{B} {Tests} at the {Class} and {Component} {Level}},
	isbn = {978-1-66548-810-5},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85136952154&doi=10.1109%2fCOMPSAC54236.2022.00136&partnerID=40&md5=c958208ad1be930a07abd07bf4787245},
	doi = {10.1109/COMPSAC54236.2022.00136},
	abstract = {The amount of data collected from software use facilitated systematic inquiries about users' expectations and reactions to software systems. The most well-known practice for continuous experimentation is A/B tests, in which two or more versions of a feature are compared based on users' metrics when exposed to these options. The literature shows that A/B tests are spread in the industry, but companies usually rely on in-house software platforms based on traffic split. However, these approaches are better suitable for feature tests and not implementation options, besides increasing technical debt and maintenance effort. We aim to develop a model as a guide to frameworks for introducing experiments at class and component levels without coupling this concern to business rules. To reach this goal, we followed an iterative approach based on Design Science Research. In this process, we developed a reference im-plementation of this model by creating a framework for the Java language. We evaluated the experimentation framework usage in a web project that simulates a realistic scenario for applying A/B tests. The evaluation study showed that the framework provides the functional features needed to perform A/B tests at component and class level, collecting relevant information for decision-making. Moreover, it was possible to instantiate the framework without any coupling to code related to business rules. The proposed model could guide the development of experimentation frameworks that perform A/B tests at class and component levels, even for other languages and platforms. It captured the main elements of the experimentation domain, proposing appropriate extension points and a decoupled approach to be plugged into the application. © 2022 IEEE.},
	language = {English},
	booktitle = {Proceedings - 2022 {IEEE} 46th {Annual} {Computers}, {Software}, and {Applications} {Conference}, {COMPSAC} 2022},
	publisher = {Institute of Electrical and Electronics Engineers Inc.},
	author = {De Souza, Wagner S. and Pereira, Fernando O. and Albuquerque, Vanessa G. and Melegati, Jorge and Guerra, Eduardo},
	editor = {H, Va Leong and S.S, Sarvestani and Y, Teranishi and A, Cuzzocrea and H, Kashiwazaki and D, Towey and J.-J, Yang and H, Shahriar},
	year = {2022},
	note = {Type: Conference paper},
	keywords = {Decision making, Software testing, Design, Continuous experimentation, Testing, A/B test, Business rules, Class level, Component levels, Design science, Frame-work, Framework models, Software use, User expectations},
	pages = {860 -- 865},
	annote = {Cited by: 1},
}

@article{issa_mattos_hurrier_2023,
	title = {The {HURRIER} process for experimentation in business-to-business mission-critical systems},
	volume = {35},
	issn = {20477481},
	url = {https://www.scopus.com/inward/record.uri?eid=2-s2.0-85116426998&doi=10.1002%2fsmr.2390&partnerID=40&md5=311d529027d8dbe2a8b1d5fb67fe875a},
	doi = {10.1002/smr.2390},
	abstract = {Continuous experimentation (CE) refers to a set of practices used by software companies to rapidly assess the usage, value, and performance of deployed software using data collected from customers and systems in the field using an experimental methodology. However, despite its increasing popularity in developing web-facing applications, CE has not been studied in the development process of business-to-business (B2B) mission-critical systems. By observing the CE practices of different teams, with a case study methodology inside Ericsson, we were able to identify the different practices and techniques used in B2B mission-critical systems and a description and classification of the four possible types of experiments. We present and analyze each of the four types of experiments with examples in the context of the mission-critical long-term evolution (4G) product. These examples show the general experimentation process followed by the teams and the use of the different CE practices and techniques. Based on these examples and the empirical data, we derived the HURRIER process to deliver high-quality solutions that the customers value. Finally, we discuss the challenges, opportunities, and lessons learned from applying CE and the HURRIER process in B2B mission-critical systems. © 2021 The Authors. Journal of Software: Evolution and Process published by John Wiley \& Sons Ltd.},
	language = {English},
	number = {5},
	journal = {Journal of Software: Evolution and Process},
	author = {Issa Mattos, David and Dakkak, Anas and Bosch, Jan and Olsson, Helena Holmström},
	year = {2023},
	note = {Publisher: John Wiley and Sons Ltd
Type: Article},
	keywords = {Software company, Performance, Computer software, Software Evolution, Software process, Continuous experimentation, Mission critical systems, Business-to-business, Deployed software, Experimentation process, Long Term Evolution (LTE), Usage value},
	annote = {Cited by: 4; All Open Access, Hybrid Gold Open Access},
}

@article{issa_mattos_hurrier_2023-1,
	title = {The {HURRIER} process for experimentation in business‐to‐business mission‐critical systems},
	volume = {35},
	issn = {2047-7473, 2047-7481},
	url = {https://onlinelibrary.wiley.com/doi/10.1002/smr.2390},
	doi = {10.1002/smr.2390},
	abstract = {Continuous experimentation (CE) refers to a set of practices used by software companies to rapidly assess the usage, value, and performance of deployed software using data collected from customers and systems in the field using an experimental methodology. However, despite its increasing popularity in developing web-facing applications, CE has not been studied in the development process of business-tobusiness (B2B) mission-critical systems. By observing the CE practices of different teams, with a case study methodology inside Ericsson, we were able to identify the different practices and techniques used in B2B mission-critical systems and a description and classification of the four possible types of experiments. We present and analyze each of the four types of experiments with examples in the context of the mission-critical long-term evolution (4G) product. These examples show the general experimentation process followed by the teams and the use of the different CE practices and techniques. Based on these examples and the empirical data, we derived the HURRIER process to deliver high-quality solutions that the customers value. Finally, we discuss the challenges, opportunities, and lessons learned from applying CE and the HURRIER process in B2B mission-critical systems.},
	language = {en},
	number = {5},
	urldate = {2024-07-26},
	journal = {Journal of Software: Evolution and Process},
	author = {Issa Mattos, David and Dakkak, Anas and Bosch, Jan and Olsson, Helena Holmström},
	month = may,
	year = {2023},
	pages = {e2390},
	file = {Issa Mattos et al. - 2023 - The HURRIER process for experimentation in busines.pdf:/Users/arthursena/Zotero/storage/SBNUP5QX/Issa Mattos et al. - 2023 - The HURRIER process for experimentation in busines.pdf:application/pdf},
}

@inproceedings{sauvola_towards_2015,
	address = {Madeira, Portugal},
	title = {Towards {Customer}-{Centric} {Software} {Development}: {A} {Multiple}-{Case} {Study}},
	isbn = {978-1-4673-7585-6},
	shorttitle = {Towards {Customer}-{Centric} {Software} {Development}},
	url = {http://ieeexplore.ieee.org/document/7302425/},
	doi = {10.1109/SEAA.2015.63},
	abstract = {Customer involvement in software development is essential for building successful software products. Incremental improvements and enhancements of software require an in-depth and continuous understanding of customer needs. Also, mechanisms for managing customer feedback data need to be in place. However, previous research shows that the feedback loops from customers are slow and the process for obtaining timely feedback is challenging. In this study, we investigate customer feedback mechanisms and the ways in which customer data can be used to inform continuous improvement of software products. The contribution of this paper is twofold. First, we present a multiplecase study conducted in five Finnish software companies, where we identify how customer feedback data is collected and used in different product development activities. Second, we provide an explanatory ‘customer touchpoint’ (CTP) model which provides an overall understanding of customer feedback data collection and the related challenges in the case companies during software development.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {2015 41st {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications}},
	publisher = {IEEE},
	author = {Sauvola, Tanja and Lwakatare, Lucy Ellen and Karvonen, Teemu and Kuvaja, Pasi and Olsson, Helena Holmstrom and Bosch, Jan and Oivo, Markku},
	month = aug,
	year = {2015},
	pages = {9--17},
	file = {Sauvola et al. - 2015 - Towards Customer-Centric Software Development A M.pdf:/Users/arthursena/Zotero/storage/KNKU88J8/Sauvola et al. - 2015 - Towards Customer-Centric Software Development A M.pdf:application/pdf},
}

@incollection{fernandes_hitting_2015,
	address = {Cham},
	title = {Hitting the {Target}: {Practices} for {Moving} {Toward} {Innovation} {Experiment} {Systems}},
	volume = {210},
	isbn = {978-3-319-19592-6 978-3-319-19593-3},
	shorttitle = {Hitting the {Target}},
	url = {https://link.springer.com/10.1007/978-3-319-19593-3_10},
	abstract = {The 'Stairway to Heaven' (StH) model describes a five-step evolutionary path for software development companies to evolve their ways of working towards innovation experiment systems (IES). In this paper, we investigate the practices, benefits and barriers in moving towards IES. Our research method is a multiple-case study conducted in five software development companies. The practices of the company in each case are analysed in relation to the StH model. The contributions are as follows. First, we confirm the StH model as a typical evolutionary process in companies. Second, we extend the model with practices and the adoption maturity of practices for each step in the StH model. Third, we validate the extended model in a multiple case study at five Finnish software development companies.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Software {Business}},
	publisher = {Springer International Publishing},
	author = {Karvonen, Teemu and Lwakatare, Lucy Ellen and Sauvola, Tanja and Bosch, Jan and Olsson, Helena Holmström and Kuvaja, Pasi and Oivo, Markku},
	editor = {Fernandes, João M. and Machado, Ricardo J. and Wnuk, Krzysztof},
	year = {2015},
	doi = {10.1007/978-3-319-19593-3_10},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {117--131},
	file = {Karvonen et al. - 2015 - Hitting the Target Practices for Moving Toward In.pdf:/Users/arthursena/Zotero/storage/D2WBGL4T/Karvonen et al. - 2015 - Hitting the Target Practices for Moving Toward In.pdf:application/pdf},
}

@article{olsson_opinions_2014,
	title = {From {Opinions} to {Data}-{Driven} {Software} {R}\&{D}},
	abstract = {In most software development companies the road mapping and requirements prioritization process is a complex process in which product management experiences difficulties in getting timely and accurate customer feedback. The feedback loop from customers is slow and often there is a lack of mechanisms that allow for efficient customer data collection and analysis. As a result, there is the risk that requirements prioritization becomes opinion-based rather than data-driven, and that R\&D investments are made without an accurate way of continuously validating whether they correspond to customer needs. We call this phenomenon the ‘open loop’ problem, referring to the challenges for product management to get accurate and timely feedback from customers. To address this problem, we develop the HYPEX model (Hypothesis Experiment Data-Driven Development) that supports companies in running feature experiments to shorten customer feedback loops. We evaluate the model in three software development companies and observe how feature experiments increase the opportunity for data-driven software development.},
	language = {en},
	author = {Olsson, Helena Holmström and Bosch, Jan},
	file = {Olsson and Bosch - From Opinions to Data-Driven Software R&D.pdf:/Users/arthursena/Zotero/storage/M9H3WCPS/Olsson and Bosch - From Opinions to Data-Driven Software R&D.pdf:application/pdf},
        year = {2014},
}

@inproceedings{fabijan_three_2019,
	address = {Montreal, QC, Canada},
	title = {Three {Key} {Checklists} and {Remedies} for {Trustworthy} {Analysis} of {Online} {Controlled} {Experiments} at {Scale}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72811-760-7},
	url = {https://ieeexplore.ieee.org/document/8804459/},
	doi = {10.1109/ICSE-SEIP.2019.00009},
	abstract = {Online Controlled Experiments (OCEs) are transforming the decision-making process of data-driven companies into an experimental laboratory. Despite their great power in identifying what customers actually value, experimentation is very sensitive to data loss, skipped checks, wrong designs, and many other ‘hiccups’ in the analysis process. For this purpose, experiment analysis has traditionally been done by experienced data analysts and scientists that closely monitored experiments throughout their lifecycle. Depending solely on scarce experts, however, is neither scalable nor bulletproof. To democratize experimentation, analysis should be streamlined and meticulously performed by engineers, managers, or others responsible for the development of a product. In this paper, based on synthesized experience of companies that run thousands of OCEs per year, we examined how experts inspect online experiments. We reveal that most of the experiment analysis happens before OCEs are even started, and we summarize the key analysis steps in three checklists. The value of the checklists is threefold. First, they can increase the accuracy of experiment setup and decision-making process. Second, checklists can enable novice data scientists and software engineers to become more autonomous in setting-up and analyzing experiments. Finally, they can serve as a base to develop trustworthy platforms and tools for OCE set-up and analysis.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {2019 {IEEE}/{ACM} 41st {International} {Conference} on {Software} {Engineering}: {Software} {Engineering} in {Practice} ({ICSE}-{SEIP})},
	publisher = {IEEE},
	author = {Fabijan, Aleksander and Dmitriev, Pavel and Holmstrom Olsson, Helena and Bosch, Jan and Vermeer, Lukas and Lewis, Dylan},
	month = may,
	year = {2019},
	pages = {1--10},
	file = {Fabijan et al. - 2019 - Three Key Checklists and Remedies for Trustworthy .pdf:/Users/arthursena/Zotero/storage/8839I6BE/Fabijan et al. - 2019 - Three Key Checklists and Remedies for Trustworthy .pdf:application/pdf},
}

@incollection{kuhrmann_activity_2018,
	address = {Cham},
	title = {An {Activity} and {Metric} {Model} for {Online} {Controlled} {Experiments}},
	volume = {11271},
	isbn = {978-3-030-03672-0 978-3-030-03673-7},
	url = {http://link.springer.com/10.1007/978-3-030-03673-7_14},
	abstract = {Accurate prioritization of efforts in product and services development is critical to the success of every company. Online controlled experiments, also known as A/B tests, enable software companies to establish causal relationships between changes in their systems and the movements in the metrics. By experimenting, product development can be directed towards identifying and delivering value. Previous research stresses the need for data-driven development and experimentation. However, the level of granularity in which existing models explain the experimentation process is neither sufficient, in terms of details, nor scalable, in terms of how to increase number and run different types of experiments, in an online setting. Based on a case study of multiple products running online controlled experiments at Microsoft, we provide an experimentation framework composed of two detailed experimentation models focused on two main aspects; the experimentation activities and the experimentation metrics. This work intends to provide guidelines to companies and practitioners on how to set and organize experimentation activities for running trustworthy online controlled experiments.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Product-{Focused} {Software} {Process} {Improvement}},
	publisher = {Springer International Publishing},
	author = {Issa Mattos, David and Dmitriev, Pavel and Fabijan, Aleksander and Bosch, Jan and Holmström Olsson, Helena},
	editor = {Kuhrmann, Marco and Schneider, Kurt and Pfahl, Dietmar and Amasaki, Sousuke and Ciolkowski, Marcus and Hebig, Regina and Tell, Paolo and Klünder, Jil and Küpper, Steffen},
	year = {2018},
	doi = {10.1007/978-3-030-03673-7_14},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {182--198},
	file = {Issa Mattos et al. - 2018 - An Activity and Metric Model for Online Controlled.pdf:/Users/arthursena/Zotero/storage/VUXIIAAB/Issa Mattos et al. - 2018 - An Activity and Metric Model for Online Controlled.pdf:application/pdf},
}

@article{fagerholm_right_2017,
	title = {The {RIGHT} model for {Continuous} {Experimentation}},
	volume = {123},
	issn = {01641212},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0164121216300024},
	doi = {10.1016/j.jss.2016.03.034},
	language = {en},
	urldate = {2024-08-04},
	journal = {Journal of Systems and Software},
	author = {Fagerholm, Fabian and Sanchez Guinea, Alejandro and Mäenpää, Hanna and Münch, Jürgen},
	month = jan,
	year = {2017},
	pages = {292--305},
	file = {Fagerholm et al. - 2017 - The RIGHT model for Continuous Experimentation.pdf:/Users/arthursena/Zotero/storage/HCUW4EZY/Fagerholm et al. - 2017 - The RIGHT model for Continuous Experimentation.pdf:application/pdf},
}

@inproceedings{crook_seven_2009,
	address = {Paris France},
	title = {Seven pitfalls to avoid when running controlled experiments on the web},
	isbn = {978-1-60558-495-9},
	url = {https://dl.acm.org/doi/10.1145/1557019.1557139},
	doi = {10.1145/1557019.1557139},
	abstract = {Controlled experiments, also called randomized experiments and A/B tests, have had a profound influence on multiple fields, including medicine, agriculture, manufacturing, and advertising. While the theoretical aspects of offline controlled experiments have been well studied and documented, the practical aspects of running them in online settings, such as web sites and services, are still being developed. As the usage of controlled experiments grows in these online settings, it is becoming more important to understand the opportunities and pitfalls one might face when using them in practice. A survey of online controlled experiments and lessons learned were previously documented in Controlled Experiments on the Web: Survey and Practical Guide (Kohavi, et al., 2009). In this follow-on paper, we focus on pitfalls we have seen after running numerous experiments at Microsoft. The pitfalls include a wide range of topics, such as assuming that common statistical formulas used to calculate standard deviation and statistical power can be applied and ignoring robots in analysis (a problem unique to online settings). Online experiments allow for techniques like gradual ramp-up of treatments to avoid the possibility of exposing many customers to a bad (e.g., buggy) Treatment. With that ability, we discovered that it’s easy to incorrectly identify the winning Treatment because of Simpson’s paradox.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Proceedings of the 15th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining},
	publisher = {ACM},
	author = {Crook, Thomas and Frasca, Brian and Kohavi, Ron and Longbotham, Roger},
	month = jun,
	year = {2009},
	pages = {1105--1114},
	file = {Crook et al. - 2009 - Seven pitfalls to avoid when running controlled ex.pdf:/Users/arthursena/Zotero/storage/8ZYPW3IS/Crook et al. - 2009 - Seven pitfalls to avoid when running controlled ex.pdf:application/pdf},
}

@incollection{olsson_towards_2015,
	address = {Cham},
	title = {Towards {Continuous} {Customer} {Validation}: {A} {Conceptual} {Model} for {Combining} {Qualitative} {Customer} {Feedback} with {Quantitative} {Customer} {Observation}},
	volume = {210},
	isbn = {978-3-319-19592-6 978-3-319-19593-3},
	shorttitle = {Towards {Continuous} {Customer} {Validation}},
	url = {https://link.springer.com/10.1007/978-3-319-19593-3_13},
	abstract = {Software-intensive product companies are becoming increasingly data-driven as can be witnessed by the big data and Internet of Things trends. However, optimally prioritizing customer needs in a mass-market context is notoriously difficult. While most companies use product owners or managers to represent the customer, research shows that the prioritization made is far from optimal. In earlier research, we have coined the term ‘the open loop problem’ to characterize this challenge. For instance, research shows that up to half of all the features in products are never used. This paper presents a conceptual model that emphasizes the need for combining qualitative feedback in early stages of development with quantitative customer observation in later stages of development. Our model is inductively derived from an 18 months close collaboration with six large global software-intensive companies.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Software {Business}},
	publisher = {Springer International Publishing},
	author = {Olsson, Helena Holmström and Bosch, Jan},
	editor = {Fernandes, João M. and Machado, Ricardo J. and Wnuk, Krzysztof},
	year = {2015},
	doi = {10.1007/978-3-319-19593-3_13},
	note = {Series Title: Lecture Notes in Business Information Processing},
	pages = {154--166},
	file = {Olsson and Bosch - 2015 - Towards Continuous Customer Validation A Conceptu.pdf:/Users/arthursena/Zotero/storage/WMK3X4TU/Olsson and Bosch - 2015 - Towards Continuous Customer Validation A Conceptu.pdf:application/pdf},
}

@misc{yu_new_2020,
	title = {A {New} {Framework} for {Online} {Testing} of {Heterogeneous} {Treatment} {Effect}},
	url = {http://arxiv.org/abs/2002.03277},
	abstract = {We propose a new framework for online testing of heterogeneous treatment effects. The proposed test, named sequential score test (SST), is able to control type I error under continuous monitoring and detect multi-dimensional heterogeneous treatment effects. We provide an online p-value calculation for SST, making it convenient for continuous monitoring, and extend our tests to online multiple testing settings by controlling the false discovery rate. We examine the empirical performance of the proposed tests and compare them with a state-of-art online test, named mSPRT using simulations and a real data. The results show that our proposed test controls type I error at any time, has higher detection power and allows quick inference on online A/B testing.},
	language = {en},
	urldate = {2024-08-04},
	publisher = {arXiv},
	author = {Yu, Miao and Lu, Wenbin and Song, Rui},
	month = feb,
	year = {2020},
	note = {arXiv:2002.03277 [stat]},
	keywords = {Statistics - Applications, Statistics - Methodology},
	annote = {Comment: 8 pages, no figures. To be published on AAAI 2020 proceedings},
	file = {Yu et al. - 2020 - A New Framework for Online Testing of Heterogeneou.pdf:/Users/arthursena/Zotero/storage/Y4W5L5US/Yu et al. - 2020 - A New Framework for Online Testing of Heterogeneou.pdf:application/pdf},
}

@inproceedings{fabijan_benefits_2017,
	address = {Vienna, Austria},
	title = {The {Benefits} of {Controlled} {Experimentation} at {Scale}},
	isbn = {978-1-5386-2141-7},
	url = {http://ieeexplore.ieee.org/document/8051322/},
	doi = {10.1109/SEAA.2017.47},
	abstract = {Online controlled experiments (for example A/B tests) are increasingly being performed to guide product development and accelerate innovation in online software product companies. The benefits of controlled experiments have been shown in many cases with incremental product improvement as the objective. In this paper, we demonstrate that the value of controlled experimentation at scale extends beyond this recognized scenario. Based on an exhaustive and collaborative case study in a large software-intensive company with highly developed experimentation culture, we inductively derive the benefits of controlled experimentation. The contribution of our paper is twofold. First, we present a comprehensive list of benefits and illustrate our findings with five case examples of controlled experiments conducted at Microsoft. Second, we provide guidance on how to achieve each of the benefits. With our work, we aim to provide practitioners in the online domain with knowledge on how to use controlled experimentation to maximize the benefits on the portfolio, product and team level.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {2017 43rd {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Fabijan, Aleksander and Dmitriev, Pavel and Olsson, Helena Holmstrom and Bosch, Jan},
	month = aug,
	year = {2017},
	pages = {18--26},
	file = {Fabijan et al. - 2017 - The Benefits of Controlled Experimentation at Scal.pdf:/Users/arthursena/Zotero/storage/8QQUMYKC/Fabijan et al. - 2017 - The Benefits of Controlled Experimentation at Scal.pdf:application/pdf},
}

@incollection{bures_infrastructure_2021,
	address = {Cham},
	title = {An {Infrastructure} for {Platform}-{Independent} {Experimentation} of {Software} {Changes}},
	volume = {12607},
	isbn = {978-3-030-67730-5 978-3-030-67731-2},
	url = {https://link.springer.com/10.1007/978-3-030-67731-2_33},
	abstract = {Current experimentation platforms for online controlled experimentation focus on the technical execution of an experiment. This makes them speciﬁc to the application domain, the expected infrastructure, and the used technology. Moreover, the experiment deﬁnitions include numerous implicit assumptions about the platform’s implementation. As a result, experiments are diﬃcult to replicate or compare across platforms or even platform versions.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {{SOFSEM} 2021: {Theory} and {Practice} of {Computer} {Science}},
	publisher = {Springer International Publishing},
	author = {Auer, Florian and Felderer, Michael},
	editor = {Bureš, Tomáš and Dondi, Riccardo and Gamper, Johann and Guerrini, Giovanna and Jurdziński, Tomasz and Pahl, Claus and Sikora, Florian and Wong, Prudence W.H.},
	year = {2021},
	doi = {10.1007/978-3-030-67731-2_33},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {445--457},
	file = {Auer and Felderer - 2021 - An Infrastructure for Platform-Independent Experim.pdf:/Users/arthursena/Zotero/storage/GRJZWRKX/Auer and Felderer - 2021 - An Infrastructure for Platform-Independent Experim.pdf:application/pdf},
}

@inproceedings{liu_enterprise-level_2019,
	address = {Kallithea-Chalkidiki, Greece},
	title = {Enterprise-{Level} {Controlled} {Experiments} at {Scale}: {Challenges} and {Solutions}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72813-421-5},
	shorttitle = {Enterprise-{Level} {Controlled} {Experiments} at {Scale}},
	url = {https://ieeexplore.ieee.org/document/8906760/},
	doi = {10.1109/SEAA.2019.00013},
	abstract = {Online controlled experiments, known as A/B tests, have delivered tremendous value for businesses and customers. Traditionally, online controlled experiments running in search engines, websites, mobile apps, and other Business-to-Consumer (B2C) products use individual customers as randomization units. However, this may not be possible in Business-to-Business (B2B) products that are sold to, and used by, other enterprises. B2B products typically have constraints such as the need to provide identical experiences to all users within an enterprise. In other scenarios, such as features that need to be used by multiple users to be meaningful or the metrics of interest can be only obtained by showing the same experience under one enterprise, experimentation randomized by enterprise is needed. In this paper, based on experiences from Microsoft and Outreach, we introduce the concept of Enterprise Level Experiments (ELE) in software product development. We discuss the unique technical challenges in designing and analyzing ELEs and present solutions to overcome them. Furthermore, we present and discuss a realworld example of ELE run by Microsoft. Our aims in this paper are to provide practical guidance on running trustworthy experiments in B2B scenarios, and to stimulate further research in this new and important area of experimentation.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {2019 45th {Euromicro} {Conference} on {Software} {Engineering} and {Advanced} {Applications} ({SEAA})},
	publisher = {IEEE},
	author = {Liu, Sophia and Fabijan, Aleksander and Furchtgott, Michael and Gupta, Somit and Janowski, Pawel and Qin, Wen and Dmitriev, Pavel},
	month = aug,
	year = {2019},
	pages = {29--37},
	file = {Liu et al. - 2019 - Enterprise-Level Controlled Experiments at Scale .pdf:/Users/arthursena/Zotero/storage/N8NHX79Z/Liu et al. - 2019 - Enterprise-Level Controlled Experiments at Scale .pdf:application/pdf},
}

@article{fabijan_online_2020,
	title = {The {Online} {Controlled} {Experiment} {Lifecycle}},
	volume = {37},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	issn = {0740-7459, 1937-4194},
	url = {https://ieeexplore.ieee.org/document/8501922/},
	doi = {10.1109/MS.2018.2875842},
	abstract = {Online Controlled Experiments (OCEs) enable an accurate understanding of customer value and generate millions of dollars of additional revenue at Microsoft. Unlike other techniques for learning from customers, OCEs establish an accurate and causal relationship between a change and the impact observed. Although previous research describes technical and statistical dimensions, the key phases of online experimentation are not widely known, their impact and importance are obscure, and how to establish OCEs in an organization is underexplored. In this paper, using a longitudinal in-depth case study, we address this gap by (1) presenting the Experiment Lifecycle, and (2) demonstrating with four example experiments their profound impact. We show that OECs help optimize infrastructure needs and aid in project planning and measuring team efforts, in addition to their primary goal of accurately identifying what customers value. We conclude that product development should fully integrate the Experiment Lifecycle to benefit from the OCEs.},
	language = {en},
	number = {2},
	urldate = {2024-08-04},
	journal = {IEEE Software},
	author = {Fabijan, Aleksander and Dmitriev, Pavel and Holmstrom Olsson, Helena and Bosch, Jan},
	month = mar,
	year = {2020},
	pages = {60--67},
	file = {Fabijan et al. - 2020 - The Online Controlled Experiment Lifecycle.pdf:/Users/arthursena/Zotero/storage/WSI7N5P4/Fabijan et al. - 2020 - The Online Controlled Experiment Lifecycle.pdf:application/pdf},
}

@incollection{le_goues_towards_2014,
	address = {Cham},
	title = {Towards {Automated} {A}/{B} {Testing}},
	volume = {8636},
	isbn = {978-3-319-09939-2 978-3-319-09940-8},
	url = {http://link.springer.com/10.1007/978-3-319-09940-8_13},
	abstract = {User-intensive software, such as Web and mobile applications, heavily depends on the interactions with large and unknown populations of users. Knowing the preferences and behaviors of these populations is crucial for the success of this class of systems. A/B testing is an increasingly popular technique that supports the iterative development of user-intensive software based on controlled experiments performed on live users. However, as currently performed, A/B testing is a time consuming, error prone and costly manual activity. In this paper, we investigate a novel approach to automate A/B testing. More speciﬁcally, we rephrase A/B testing as a search-based software engineering problem and we propose an initial approach that supports automated A/B testing through aspect-oriented programming and genetic algorithms.},
	language = {en},
	urldate = {2024-08-04},
	booktitle = {Search-{Based} {Software} {Engineering}},
	publisher = {Springer International Publishing},
	author = {Tamburrelli, Giordano and Margara, Alessandro},
	editor = {Le Goues, Claire and Yoo, Shin},
	year = {2014},
	doi = {10.1007/978-3-319-09940-8_13},
	note = {Series Title: Lecture Notes in Computer Science},
	pages = {184--198},
	file = {Tamburrelli and Margara - 2014 - Towards Automated AB Testing.pdf:/Users/arthursena/Zotero/storage/3SGPMT3M/Tamburrelli and Margara - 2014 - Towards Automated AB Testing.pdf:application/pdf},
}

@article{chen_automatic_2018,
	title = {Automatic {Detection} and {Diagnosis} of {Biased} {Online} {Experiments}},
	abstract = {We have seen a massive growth of online experiments at LinkedIn, and in industry at large. It is now more important than ever to create an intelligent A/B platform that can truly democratize A/B testing by allowing everyone to make quality decisions, regardless of their skillset. With the tremendous knowledge base created around experimentation, we are able to mine through historical data, and discover the most common causes for biased experiments. In this paper, we share four of such common causes, and how we build into our A/B testing platform the automatic detection and diagnosis of such root causes. These root causes range from design-imposed bias, self-selection bias, novelty effect and trigger-day effect. We will discuss in detail what each bias is and the scalable algorithm we developed to detect the bias. Surfacing up the existence and root cause of bias automatically for every experiment is an important milestone towards intelligent A/B testing.},
	language = {en},
	author = {Chen, Nanyu and Liu, Min and Xu, Ya},
	file = {Chen et al. - Automatic Detection and Diagnosis of Biased Online.pdf:/Users/arthursena/Zotero/storage/FNPLQ2NS/Chen et al. - Automatic Detection and Diagnosis of Biased Online.pdf:application/pdf},
        year = {2018},
}

@inproceedings{melegati_hypotheses_2019,
	address = {Montreal, QC, Canada},
	title = {Hypotheses {Engineering}: {First} {Essential} {Steps} of {Experiment}-{Driven} {Software} {Development}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72812-247-2},
	shorttitle = {Hypotheses {Engineering}},
	url = {https://ieeexplore.ieee.org/document/8818178/},
	doi = {10.1109/RCoSE/DDrEE.2019.00011},
	abstract = {Recent studies have proposed the use of experiments to guide software development in order to build features that the user really wants. Some authors argue that this approach represents a new way to develop software that is different from the traditional requirement-driven one. In this position paper, we propose the discipline of Hypotheses Engineering in comparison to Requirements Engineering, highlighting the importance of proper handling hypotheses that guide experiments. We derive a set of practices within this discipline and present how the literature has tackled them up to now. Finally, we propose a set of research questions that could guide future work towards helping practitioners.},
	language = {en},
	urldate = {2024-08-05},
	booktitle = {2019 {IEEE}/{ACM} {Joint} 4th {International} {Workshop} on {Rapid} {Continuous} {Software} {Engineering} and 1st {International} {Workshop} on {Data}-{Driven} {Decisions}, {Experimentation} and {Evolution} ({RCoSE}/{DDrEE})},
	publisher = {IEEE},
	author = {Melegati, Jorge and Wang, Xiaofeng and Abrahamsson, Pekka},
	month = may,
	year = {2019},
	pages = {16--19},
	file = {Melegati et al. - 2019 - Hypotheses Engineering First Essential Steps of E.pdf:/Users/arthursena/Zotero/storage/PKBAN85X/Melegati et al. - 2019 - Hypotheses Engineering First Essential Steps of E.pdf:application/pdf},
}

@inproceedings{melegati_hypotheses_2019-1,
	address = {Montreal, QC, Canada},
	title = {Hypotheses {Engineering}: {First} {Essential} {Steps} of {Experiment}-{Driven} {Software} {Development}},
	copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
	isbn = {978-1-72812-247-2},
	shorttitle = {Hypotheses {Engineering}},
	url = {https://ieeexplore.ieee.org/document/8818178/},
	doi = {10.1109/RCoSE/DDrEE.2019.00011},
	abstract = {Recent studies have proposed the use of experiments to guide software development in order to build features that the user really wants. Some authors argue that this approach represents a new way to develop software that is different from the traditional requirement-driven one. In this position paper, we propose the discipline of Hypotheses Engineering in comparison to Requirements Engineering, highlighting the importance of proper handling hypotheses that guide experiments. We derive a set of practices within this discipline and present how the literature has tackled them up to now. Finally, we propose a set of research questions that could guide future work towards helping practitioners.},
	language = {en},
	urldate = {2024-08-05},
	booktitle = {2019 {IEEE}/{ACM} {Joint} 4th {International} {Workshop} on {Rapid} {Continuous} {Software} {Engineering} and 1st {International} {Workshop} on {Data}-{Driven} {Decisions}, {Experimentation} and {Evolution} ({RCoSE}/{DDrEE})},
	publisher = {IEEE},
	author = {Melegati, Jorge and Wang, Xiaofeng and Abrahamsson, Pekka},
	month = may,
	year = {2019},
	pages = {16--19},
	file = {Melegati et al. - 2019 - Hypotheses Engineering First Essential Steps of E.pdf:/Users/arthursena/Zotero/storage/8B7B4IHF/Melegati et al. - 2019 - Hypotheses Engineering First Essential Steps of E.pdf:application/pdf},
}

@article{melegati_understanding_2021,
	title = {Understanding {Hypotheses} {Engineering} in {Software} {Startups} through a {Gray} {Literature} {Review}},
	volume = {133},
	issn = {09505849},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950584920302111},
	doi = {10.1016/j.infsof.2020.106465},
	abstract = {Objective: The goal of this study is to understand what activities, as proposed in industry, are entailed to handle hypotheses, facilitating the comparison, creation, and evaluation of relevant techniques.
Methods: We performed a gray literature review (GLR) on the practices proposed by practitioners to handle hypotheses in the context of software startups. We analyzed the identified documents using thematic synthesis.
Results: The analysis revealed that techniques proposed for software startups in practice compress five different activities: elicitation, prioritization, specification, analysis, and management. It also showed that practitioners often classify hypotheses in types and which qualities they aim for these statements.
Conclusion: Our results represent the first description for hypotheses engineering grounded in practice data. This mapping of the state-of-practice indicates how research could go forward in investigating hypotheses for experimentation in the context of software startups. For practitioners, they represent a catalog of available practices to be used in this context.},
	language = {en},
	urldate = {2024-08-05},
	journal = {Information and Software Technology},
	author = {Melegati, Jorge and Guerra, Eduardo and Wang, Xiaofeng},
	month = may,
	year = {2021},
	pages = {106465},
	file = {Melegati et al. - 2021 - Understanding Hypotheses Engineering in Software S.pdf:/Users/arthursena/Zotero/storage/IXMWVGFP/Melegati et al. - 2021 - Understanding Hypotheses Engineering in Software S.pdf:application/pdf},
}

@article{tang_overlapping_nodate-1,
	title = {Overlapping {Experiment} {Infrastructure}: {More}, {Better}, {Faster} {Experimentation}},
	abstract = {At Google, experimentation is practically a mantra; we evaluate almost every change that potentially affects what our users experience. Such changes include not only obvious user-visible changes such as modiﬁcations to a user interface, but also more subtle changes such as different machine learning algorithms that might affect ranking or content selection. Our insatiable appetite for experimentation has led us to tackle the problems of how to run more experiments, how to run experiments that produce better decisions, and how to run them faster. In this paper, we describe Google’s overlapping experiment infrastructure that is a key component to solving these problems. In addition, because an experiment infrastructure alone is insufﬁcient, we also discuss the associated tools and educational processes required to use it effectively. We conclude by describing trends that show the success of this overall experimental environment. While the paper speciﬁcally describes the experiment system and experimental processes we have in place at Google, we believe they can be generalized and applied by any entity interested in using experimentation to improve search engines and other web applications.},
	language = {en},
	author = {Tang, Diane and Agarwal, Ashish and O’Brien, Deirdre and Meyer, Mike},
	file = {Tang et al. - Overlapping Experiment Infrastructure More, Bette.pdf:/Users/arthursena/Zotero/storage/ELAWHEZT/Tang et al. - Overlapping Experiment Infrastructure More, Bette.pdf:application/pdf},
}
